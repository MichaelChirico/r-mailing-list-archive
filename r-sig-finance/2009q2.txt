From just.tawfiq at gmail.com  Wed Apr  1 10:22:57 2009
From: just.tawfiq at gmail.com (tawfiq just)
Date: Wed, 1 Apr 2009 03:22:57 -0500
Subject: [R-SIG-Finance] Elliptical Copula simulation
Message-ID: <eb0669d70904010122h6d362cf6if02cbe2bcc8295ef@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090401/840c7911/attachment.pl>

From Xiaochen.Sun at brunel.ac.uk  Wed Apr  1 12:06:33 2009
From: Xiaochen.Sun at brunel.ac.uk (Xiaochen Sun)
Date: Wed, 1 Apr 2009 11:06:33 +0100
Subject: [R-SIG-Finance] Elliptical Copula simulation
References: <eb0669d70904010122h6d362cf6if02cbe2bcc8295ef@mail.gmail.com>
Message-ID: <E386E504246A9249A9176B5BEEC13B6F018CEC4E@UXEXMBU116.academic.windsor>

Hi there,
 
If I understand right you want to calibrate (fit) normal/t copula with the given data set, right? Then you can do whatever you want, such as simulate random samples from the fitted copula functions.
 
Firstly, I think the package {QRMlib} is more preferable. You can estimate the two copulas by using functions "fit.gausscopula" and "fit.tcopula". (And of course, you can also fit Archimedean Copulas with relevant functions, but limited to 2 dimensions.)
The input data here must be pseudo-copula data, which means you have transform your original data into uniform data. This could be done either by using empirical distribution function (CML method) or fit it with certain distribution family (IFM method).
 
Secondly, in terms of nonparametric estimation you can find relationship between spearman's rho/kendall's tau and copulas. For example, the relationship between elliptical copula and kendall's tau is give, tau = 2/pi arc sin rho. 
 
Lastly, there are some good references on this topic, for example
[1] Book, "Quantitative Risk Management: concepts, Techniques, and Tools" by McNeil, A.J., Frey, R. and Embrechts, P.
[2] Paper, "Kendall's tau for elliptical distributions" by Lindskog, F. McNeil, A.J. and Schmock, U.
 
Hope this helps.
 
Cheers
Michael
P.S the example you listed below is to construct a copula object with known copula parameters, distribution family.
 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
Michael(Xiaochen) Sun, BA(BTBU), MSc(Hull), PhD Student,
CARISMA, www.carisma.brunel.ac.uk <http://www.carisma.brunel.ac.uk/>  
The Centre for the Analysis of Risk and OptimISation Modelling Applications
School of Information Systems, Computing and Mathematics
Brunel University, Uxbridge, UB8 3PH, Middlesex, United Kingdom
Telephone: +44 1895 265625 [M503], Fax: +44 1895 269732
Webpage:http://people.brunel.ac.uk/~mapgxcs <http://people.brunel.ac.uk/~mapgxcs>  
http://optirisk.googlepages.com <http://optirisk.googlepages.com/>  
http://ssrn.com/author=974259 <http://ssrn.com/author=974259> 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

________________________________

From: r-sig-finance-bounces at stat.math.ethz.ch on behalf of tawfiq just
Sent: Wed 01/04/2009 09:22
To: R-SIG-Finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Elliptical Copula simulation



hi everyone
I have  a multivariate time series let say a 3*50 matrix dimension. the
correlation matrix is

1            0.22      0.25
 0.22       1          0.43
 0.25       0.43        1

i would like to simulate both normal and student copulas, from the help in R
they didn't explain how to use the correlation matrix or to evaluate the
degre of freedom to fit the copula for the student copula estimation.


Examples and copula functions in the copula packadge document
ftp://ftp.auckland.ac.nz/pub/software/CRAN/doc/packages/copula.pdf


norm.cop <- normalCopula(c(0.5, 0.6, 0.7), dim = 3, dispstr = "un")
t.cop <- tCopula(c(0.5, 0.3), dim = 3, dispstr = "toep", df = 2)
## from the wrapper
norm.cop <- ellipCopula("normal", param = c(0.5, 0.6, 0.7),
dim = 3, dispstr = "un")

*Fit a copula model to multivariate data.*
Usage
loglikCopula(param, x, copula)
loglikMvdc(param, x, mvdc)
fitCopula(data, copula, start, optim.control = list(NULL), method = "BFGS")
fitMvdc(data, mvdc, start, optim.control = list(NULL), method = "BFGS")

any help please on this topic.

think you very much

        [[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From ghsohn at gmail.com  Wed Apr  1 12:19:56 2009
From: ghsohn at gmail.com (Hyun-U Sohn)
Date: Wed, 1 Apr 2009 12:19:56 +0200
Subject: [R-SIG-Finance] Elliptical Copula simulation
In-Reply-To: <eb0669d70904010122h6d362cf6if02cbe2bcc8295ef@mail.gmail.com>
References: <eb0669d70904010122h6d362cf6if02cbe2bcc8295ef@mail.gmail.com>
Message-ID: <f7eb7ef40904010319w79d498dbubf947bd98ae12ee2@mail.gmail.com>

you might want to look at RMetrics's copula package -->
http://cran.r-project.org/web/packages/fCopulae/index.html or at
QRMlib which also has copula functions -->
http://cran.r-project.org/web/packages/QRMlib/index.html

cheers, HS

On Wed, Apr 1, 2009 at 10:22 AM, tawfiq just <just.tawfiq at gmail.com> wrote:
> hi everyone
> I have ?a multivariate time series let say a 3*50 matrix dimension. the
> correlation matrix is
>
> 1 ? ? ? ? ? ?0.22 ? ? ?0.25
> ?0.22 ? ? ? 1 ? ? ? ? ?0.43
> ?0.25 ? ? ? 0.43 ? ? ? ?1
>
> i would like to simulate both normal and student copulas. from the help in R
> they didn't explain how to use the correlation matrix or to evaluate the
> degre of freedom to fit the copula for the student copula estimation.
>
>
> Examples and copula functions in the copula packadge document
> ftp://ftp.auckland.ac.nz/pub/software/CRAN/doc/packages/copula.pdf
>
>
> norm.cop <- normalCopula(c(0.5, 0.6, 0.7), dim = 3, dispstr = "un")
> t.cop <- tCopula(c(0.5, 0.3), dim = 3, dispstr = "toep", df = 2)
> ## from the wrapper
> norm.cop <- ellipCopula("normal", param = c(0.5, 0.6, 0.7),
> dim = 3, dispstr = "un")
>
> *Fit a copula model to multivariate data.*
> Usage
> loglikCopula(param, x, copula)
> loglikMvdc(param, x, mvdc)
> fitCopula(data, copula, start, optim.control = list(NULL), method = "BFGS")
> fitMvdc(data, mvdc, start, optim.control = list(NULL), method = "BFGS")
>
> any help please on this topic.
>
> think you very much
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From michael.larsson at gmail.com  Thu Apr  2 10:57:11 2009
From: michael.larsson at gmail.com (Michael Larsson)
Date: Thu, 2 Apr 2009 09:57:11 +0100
Subject: [R-SIG-Finance] Historical data: FX spot and options
Message-ID: <6440aa460904020157u5463001ahd932aa6fa3c6e328@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090402/9ed2c612/attachment.pl>

From Shaohui.Wang at dzbank.de  Thu Apr  2 12:22:04 2009
From: Shaohui.Wang at dzbank.de (Shaohui.Wang at dzbank.de)
Date: Thu, 2 Apr 2009 12:22:04 +0200
Subject: [R-SIG-Finance] =?iso-8859-1?q?Shaohui_Wang_ist_au=DFer_Haus=2E_/?=
 =?iso-8859-1?q?_is_out_of_the_office=2E?=
Message-ID: <OFAD3A48B6.B3E4E666-ONC125758C.0038F3E5-C125758C.0038F3E5@none.none>


Ich werde au?er Haus sein von :  /  I will be out of the office from :
02.04.2009 bis / until 06.04.2009.

Ich werde Ihre Nachrichten nach meiner R?ckkehr beantworten. /  I will
respond to your message when I return.

From michael.larsson at gmail.com  Thu Apr  2 23:59:00 2009
From: michael.larsson at gmail.com (Michael Larsson)
Date: Thu, 2 Apr 2009 22:59:00 +0100
Subject: [R-SIG-Finance] FX options data - CME or others
Message-ID: <6440aa460904021459t21e995dej97d7dd6e934fba2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090402/c8ad425d/attachment.pl>

From mdowle at mdowle.plus.com  Fri Apr  3 02:17:36 2009
From: mdowle at mdowle.plus.com (Matthew Dowle)
Date: Fri, 3 Apr 2009 01:17:36 +0100
Subject: [R-SIG-Finance] data.table is on CRAN (higher speed time series
	joins and more)
Message-ID: <gr3kj4$o2f$1@ger.gmane.org>

Dear all on r-sig-finance,

Since r-help is deluged, and you may not be subscribed to r-pkgs, then you
may have missed this recent post about data.table which has been on CRAN
since summer 2008 :

    http://tolstoy.newcastle.edu.au/R/packages/09/0538.html

A financial data example to add to that ... 190 million rows in a data.table
(all non-US closing daily stock prices ever since 1986) takes 3GB ram in a 3
column data.table (symbol, date, price). Vector scan DT[id=="VOD"] takes 81
seconds, binary search DT[J("VOD")] takes 0.002 seconds instead. Including
US stock prices (so all of world in a 500 million row table) still takes
under 0.01 seconds since its O(log n) search time. Thats on 64bit with
enough ram to hold the data in memory, which is commonly available these
days. But if you're still on 32bit, 3GB almost fits so some filtering by
region can be done first for example if 32bit really is the only option.
This is just an example anyway to illustrate. Note that its not the memory
footprint that is more efficient (its the same as a properly used
data.frame) but the query methods.

The package may give programming time benefits even if you don't have
compute
time or memory issues. For example less code is required than lengthy
statements involving select, from and where keywords in SQL (which might be
in strings paste'd together and sent with sqlQuery), or R code peppered with
$'s which makes your eyes water sometimes (mine anyway) if people write lots
of code that way.

If X and Y are data.tables :
    X[Y] is a fast time series join between them
    X[Y,roll=TRUE] rolls prevailing prices forward directly.
    X[Y,rolltolast=TRUE] same as roll but doesn't roll the last price
forward
after a stock dies

No particular date class is imposed, just as long as storage.mode() is
integer.

Nothing stops you using data.table's badly (just like most things in life)
e.g. you can still do vector scans with it. But its harder than data.frames
to use badly e.g. it won't allow character row names, ever, so no chance
(famous last words) of ending up with a 10 times memory bloated data.frame -
that old chestnut.

As ever, comments and feedback appreciated.

Regards,
Matthew


From ahmed_shamiri at yahoo.com  Fri Apr  3 11:56:23 2009
From: ahmed_shamiri at yahoo.com (ahmed_shamiri at yahoo.com)
Date: Fri, 3 Apr 2009 02:56:23 -0700 (PDT)
Subject: [R-SIG-Finance] MSE from GARCH forecast
Message-ID: <766037.88021.qm@web32503.mail.mud.yahoo.com>




Hi every one
?
I would like to compute MSE on out of sample forecast from GARCH model. I used rgrach package to obtain the estimated variance (sigma2.hat). then I run an out of sample forecast as
forc = ugarchforecast(fit.garch, n.ahead=100)
this will give me 100 observation of sigma.forc( the forecasted values)
to obtain the MSE theoretically is 
MSE = 1/h+1 { sum(sigma2.hat ? sigma2.forc)^2}
What confuse me is that the length of sigma2.hat actually is equal to my data length (2000 observation), while the length of sigma2.forc is only 100.
?
Can someone kindly enlighten me about this. 
?
Sincerely,
?Shamiri





From johnpaul.taylor at ryerson.ca  Fri Apr  3 14:30:33 2009
From: johnpaul.taylor at ryerson.ca (John-Paul Taylor)
Date: Fri, 03 Apr 2009 08:30:33 -0400
Subject: [R-SIG-Finance] Geometric Brownian Motion with Jumps MLE
Message-ID: <f87bcb1844fbc.49d5c929@ryerson.ca>


Hi,


I have been using maxLik to do some MLE of Geometric Brownian Motion Process and everything has been going fine, but know I have tried to do it with jumps. I have create a vector of jumps and then added this into my log-likelihood equation, know I am getting a message:

NA in the initial gradient

My codes is hear

#
n<-length(combinedlr)
j<-c(1,2,3,4,5,6,7,8,9,10)
gbmploglik<-function(param){
	mu<-param[1]
	sigma<-param[2]
	lamda<-param[3]
	nu<-param[4]
	gama<-param[5]
	logLikVal<- - n*lamda - .5*n*log(2*pi) + sum(log(sum(for(j in 1:10)(cat((lamda^j/factorial(j))*(1/((sigma^2+j*gama^2)^.5)*exp( - (combinedlr-mu-j*nu)^2/2*(sigma^2+j*gama^2))))))))
	logLikVal
}
rescbj<- maxLik(gbmploglik, grad = NULL, hess = NULL, start=c(0,1,1,1,1), method = "Newton-Raphson")
summary(rescbj)
#

I am also was wondering if anyone know if there was a package that dealt with Geometric Brownian Motion Process augmented with jumps. Then I could just put that into my code and might resolve the issue. 

Any suggest as to how to resolved this issue, are greatly apprecaited.

Yours truly,

JP


From andyzhu35 at yahoo.com  Fri Apr  3 15:56:03 2009
From: andyzhu35 at yahoo.com (Andy Zhu)
Date: Fri, 3 Apr 2009 06:56:03 -0700 (PDT)
Subject: [R-SIG-Finance] help: yahoo special tags
Message-ID: <807555.89915.qm@web56205.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090403/1747e6de/attachment.pl>

From guygreen at netvigator.com  Fri Apr  3 16:25:41 2009
From: guygreen at netvigator.com (gug)
Date: Fri, 3 Apr 2009 07:25:41 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] help: yahoo special tags
In-Reply-To: <807555.89915.qm@web56205.mail.re3.yahoo.com>
References: <807555.89915.qm@web56205.mail.re3.yahoo.com>
Message-ID: <22869511.post@talk.nabble.com>


How many stocks/sectors/industries are you looking for data on?

I don't believe that the Yahoo tags include sector or industry information,
but there are other pages within Yahoo which have that data, including the
ability to download lists of stocks for a specific industry into Excel.

If you're trying to do it for the entire market, this is probably not
workable, but if it is for specific sectors or stocks, have a look at these
sector & industry pages:

http://biz.yahoo.com/p/
http://biz.yahoo.com/ic/ind_index.html
http://biz.yahoo.com/p/776conameu.html




Andy Zhu wrote:
> 
> Hi,
> 
> http://www.gummy-stuff.org/Yahoo-data.htm shows special tags for items
> regarding stock data in R download. Question is: are the tags complete? I
> am looking for tags on sector and industry information of a company.
> Thanks.
> 
> 

-- 
View this message in context: http://www.nabble.com/help%3A-yahoo-special-tags-tp22869175p22869511.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From jeff.a.ryan at gmail.com  Fri Apr  3 16:26:40 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 3 Apr 2009 09:26:40 -0500
Subject: [R-SIG-Finance] help: yahoo special tags
In-Reply-To: <807555.89915.qm@web56205.mail.re3.yahoo.com>
References: <807555.89915.qm@web56205.mail.re3.yahoo.com>
Message-ID: <e8e755250904030726h58aa1a17oab477988f7ae9ebb@mail.gmail.com>

Hi Andy,

I don't have any knowledge on whether there are hidden tags available,
though I would suspect not.

The getQuote function, along with yahooQF in quantmod interface yahoo
to get the data alluded to by gummy-stuff.  It is generally useful,
though some Yahoo functionality/return data isn't supported perfectly.
 That said most everything I have tried works.

library(quantmod)
?getQuote

and yahooQF()

HTH,
Jeff

On Fri, Apr 3, 2009 at 8:56 AM, Andy Zhu <andyzhu35 at yahoo.com> wrote:
> Hi,
>
> http://www.gummy-stuff.org/Yahoo-data.htm shows special tags for items regarding stock data in R download. Question is: are the tags complete? I am looking for tags on sector and industry information of a company. Thanks.
>
>
>
>
>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From andyzhu35 at yahoo.com  Fri Apr  3 16:39:19 2009
From: andyzhu35 at yahoo.com (Andy Zhu)
Date: Fri, 3 Apr 2009 07:39:19 -0700 (PDT)
Subject: [R-SIG-Finance] help: yahoo special tags
Message-ID: <6143.297.qm@web56203.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090403/03af70d2/attachment.pl>

From alexios at 4dscape.com  Fri Apr  3 20:08:13 2009
From: alexios at 4dscape.com (alexios)
Date: Fri, 03 Apr 2009 19:08:13 +0100
Subject: [R-SIG-Finance] MSE from GARCH forecast
In-Reply-To: <766037.88021.qm@web32503.mail.mud.yahoo.com>
References: <766037.88021.qm@web32503.mail.mud.yahoo.com>
Message-ID: <49D6508D.4090000@4dscape.com>

I am not sure I understand your question:

- the estimated variance is supposed to equal the length of the data, so 
  length(sigma2.hat)==length(data)
- the length of the sigma2.forc was made with ugarchforecast(fit.garch, 
n.ahead=100), i.e. you requested 100 forecast values.

HTH
-Alexios


ahmed_shamiri at yahoo.com wrote:
> 
> 
> Hi every one
>  
> I would like to compute MSE on out of sample forecast from GARCH model. I used rgrach package to obtain the estimated variance (sigma2.hat). then I run an out of sample forecast as
> forc = ugarchforecast(fit.garch, n.ahead=100)
> this will give me 100 observation of sigma.forc( the forecasted values)
> to obtain the MSE theoretically is 
> MSE = 1/h+1 { sum(sigma2.hat ? sigma2.forc)^2}
> What confuse me is that the length of sigma2.hat actually is equal to my data length (2000 observation), while the length of sigma2.forc is only 100.
>  
> Can someone kindly enlighten me about this. 
>  
> Sincerely,
>  Shamiri
> 
> 
> 
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From alexios at 4dscape.com  Sat Apr  4 01:34:18 2009
From: alexios at 4dscape.com (alexios)
Date: Sat, 04 Apr 2009 02:34:18 +0300
Subject: [R-SIG-Finance] MSE from GARCH forecast
In-Reply-To: <766037.88021.qm@web32503.mail.mud.yahoo.com>
References: <766037.88021.qm@web32503.mail.mud.yahoo.com>
Message-ID: <49D69CFA.7030209@4dscape.com>

I have now preliminarily added some forecast performance measures (incl. 
mse, rmse etc) to the package which should be available to download from 
r-forge during the next build cycle (see the example in the 
ugarchforecast help).
regards,
Alexios

ahmed_shamiri at yahoo.com wrote:
> 
> 
> Hi every one
>  
> I would like to compute MSE on out of sample forecast from GARCH model. I used rgrach package to obtain the estimated variance (sigma2.hat). then I run an out of sample forecast as
> forc = ugarchforecast(fit.garch, n.ahead=100)
> this will give me 100 observation of sigma.forc( the forecasted values)
> to obtain the MSE theoretically is 
> MSE = 1/h+1 { sum(sigma2.hat ? sigma2.forc)^2}
> What confuse me is that the length of sigma2.hat actually is equal to my data length (2000 observation), while the length of sigma2.forc is only 100.
>  
> Can someone kindly enlighten me about this. 
>  
> Sincerely,
>  Shamiri
> 
> 
> 
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From ahmed_shamiri at yahoo.com  Sat Apr  4 04:43:35 2009
From: ahmed_shamiri at yahoo.com (ahmed_shamiri at yahoo.com)
Date: Fri, 3 Apr 2009 19:43:35 -0700 (PDT)
Subject: [R-SIG-Finance] MSE from GARCH forecast
Message-ID: <676839.42037.qm@web32501.mail.mud.yahoo.com>


Hi Alexios,
First I would like to thank you for such a package (rgarch). I downloaded the package from R-Froge, but it seems it is not updated as u said. However, since you are the right person to ask, may you help to compute the MSE for OUT-OF-SAMPLE forecasts? I am familiar with your package and how to obtain MSE for in-sample forecast. What I meant about the length of sigma2.hat, is that the length of estimated variance (model output) has the same length as my data (model input). However, I my have the wrong formula to obtain MSE.
I appreciate your help and consideration you may give me.
Regards,
Shamiri


From alexios at 4dscape.com  Sat Apr  4 07:30:56 2009
From: alexios at 4dscape.com (alexios)
Date: Sat, 04 Apr 2009 08:30:56 +0300
Subject: [R-SIG-Finance] MSE from GARCH forecast
In-Reply-To: <676839.42037.qm@web32501.mail.mud.yahoo.com>
References: <676839.42037.qm@web32501.mail.mud.yahoo.com>
Message-ID: <49D6F090.9090708@4dscape.com>

Hi Shamiri,

The package will be updated during the next build cycle which means in 
about 24 hours.
The package calculates mse of variance forecast as:
mse = mean[('realized' - forecast).('realized' - forecast)]. Given a 
dataset of 1000 obs for example, and selecting to test mse on the last 
100, means that we treat those 100 as the realized observations. For 
conditional variance we usually take the actual data for those 100 obs, 
and filter (based on information from the previous 900 obs) to obtain 
residuals which you square to get 'realized' variance which you can then 
compare with the garch forecast variance.

HTH,
Alexios

ahmed_shamiri at yahoo.com wrote:
> Hi Alexios,
> First I would like to thank you for such a package (rgarch). I downloaded the package from R-Froge, but it seems it is not updated as u said. However, since you are the right person to ask, may you help to compute the MSE for OUT-OF-SAMPLE forecasts? I am familiar with your package and how to obtain MSE for in-sample forecast. What I meant about the length of sigma2.hat, is that the length of estimated variance (model output) has the same length as my data (model input). However, I my have the wrong formula to obtain MSE.
> I appreciate your help and consideration you may give me.
> Regards,
> Shamiri
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
>


From DAvery at marketingleverage.com  Sun Apr  5 17:28:52 2009
From: DAvery at marketingleverage.com (Dan Avery)
Date: Sun, 5 Apr 2009 11:28:52 -0400
Subject: [R-SIG-Finance] Quantmod
References: <mailman.3.1238839201.30064.r-sig-finance@stat.math.ethz.ch>
Message-ID: <E390E12F18C406459525A2C25BA9FC67B873@SBS-MLI.MLI.local>

Does anyone have experience with Quantmod? Does it work or is it more of a developmental concept?
I have tried some of the graphics and they seem to be somewhat unstable. 
Has anyone used the trading model function? 

Are there other packages that may be more fully developed?

Thanks
Dan Avery
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/ms-tnef
Size: 2509 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090405/3fa1c54a/attachment.bin>

From jeff.a.ryan at gmail.com  Sun Apr  5 21:25:37 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Sun, 5 Apr 2009 14:25:37 -0500
Subject: [R-SIG-Finance] Quantmod
In-Reply-To: <E390E12F18C406459525A2C25BA9FC67B873@SBS-MLI.MLI.local>
References: <mailman.3.1238839201.30064.r-sig-finance@stat.math.ethz.ch>
	<E390E12F18C406459525A2C25BA9FC67B873@SBS-MLI.MLI.local>
Message-ID: <e8e755250904051225t1caafeeah28e9dc6eaf1f057b@mail.gmail.com>

Hi Dan,

Searching the R-Sig-Finance list nets about 230 hits, so *yes* is the
short answer to the first question.  A better question might get a
better reply.

Of course it works is a quick answer to the second.  Of course, "it"
is a bit vague, so you'd need to specify exactly what you mean
again...

"somewhat unstable" I'll interpret to mean you don't understand how to
use.  Though again, your comments are being interpreted by me as not
very clear, and it should go without saying (though I will be
explicit) that they are certainly not helpful.

The trading model side of things, as been pointed out in a few of the
230 R-SIG conversations, as well as 3 presentations posted on
quantmod.com + examples, is indeed not very well sorted out.  There
are design ideas that are still open, and until a myriad of
collaborative projects come into focus, will remain unfinished.

quantmod can be made more fully developed by 2 things:

#1 me contributing more time to it, which I am, as time permit of
course.  This isn't my FT job though :)
#2 others contributing ideas, feedback and *constructive* criticism to
the effort.

Of course we can all just wait for the 'fully developed' packages that
do everything to appear on CRAN.  Were you volunteering? ;-)

Until then,
Jeff

On Sun, Apr 5, 2009 at 10:28 AM, Dan Avery <DAvery at marketingleverage.com> wrote:
> Does anyone have experience with Quantmod? Does it work or is it more of a developmental concept?
> I have tried some of the graphics and they seem to be somewhat unstable.
> Has anyone used the trading model function?
>
> Are there other packages that may be more fully developed?
>
> Thanks
> Dan Avery
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From johnpaul.taylor at ryerson.ca  Mon Apr  6 01:34:30 2009
From: johnpaul.taylor at ryerson.ca (John-Paul Taylor)
Date: Sun, 05 Apr 2009 16:34:30 -0700
Subject: [R-SIG-Finance] How to code Geometric Brownian Motion Process with
	Jumps
Message-ID: <f706ab3146d9b.49d8dd96@ryerson.ca>



I was just wondering if anyone knows if there is a canned package that included a coding for a GBMP with Jump Diffusion process or had any suggestion on how to code the log-likelihood function.

I have try to look the jumps which i am cutting of at 10 with the vector I think i need to add another look but I am not sure if this is correct. 

I have asked around my department at school but know idea so any suggestion greatly appreciated.

I have attached a file with the code and a vector of data.

Your truly

JP

Here is my code for the MLE: 
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Code GBMPJ.txt
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090405/068d72c1/attachment.txt>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: TP-Combined-Phase1-Spot.txt
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090405/068d72c1/attachment-0001.txt>

From public_browe at muxspace.com  Mon Apr  6 08:37:51 2009
From: public_browe at muxspace.com (B. Rowe)
Date: Mon, 06 Apr 2009 02:37:51 -0400
Subject: [R-SIG-Finance] Introducing tawny,
	a package for filtering correlation matrices	via random matrix
	theory and shrinkage estimation
Message-ID: <1238999871.4954.62.camel@localhost>

Hello,

I am pleased to announce the initial version of tawny, a portfolio
optimization package in R. Tawny is an R package for studying various
correlation matrix filtering methods applied to asset returns. The
filtering techniques included in the package are random matrix theory
and shrinkage estimation. In addition to the filtering methods, also
included in the package is a simple portfolio optimizer and functions to
measure the effectiveness of the methods, including an implementation of
the Kullback-Leibler divergence function.

More information, code samples, and images can be found at:
  https://nurometic.com/quantitative-finance/tawny

The code should be available on CRAN in a while, but in the mean time it
is possible to download it directly via the link above.

To run tawny, you will also need to install a utility package that tawny
depends on:
  https://nurometic.com/quantitative-finance/futile

Pending availability on CRAN, you can download it from the above link.

Both packages are licensed GPL-2. Comments and questions are welcome.

Regards,
Brian Lee Yung Rowe


From finbref.2006 at gmail.com  Mon Apr  6 09:19:12 2009
From: finbref.2006 at gmail.com (Thomas Steiner)
Date: Mon, 6 Apr 2009 09:19:12 +0200
Subject: [R-SIG-Finance] How to code Geometric Brownian Motion Process
	with Jumps
In-Reply-To: <f706ab3146d9b.49d8dd96@ryerson.ca>
References: <f706ab3146d9b.49d8dd96@ryerson.ca>
Message-ID: <d0f55a670904060019v7ef53d1r7062161fbe20ba31@mail.gmail.com>

Hi John-Paul,
on http://commons.wikimedia.org/wiki/Image:Gamma-OU.png you find a
Ornstein-Uhlenbeck process which is driven by a Levy process (Gamma
process). It should be easy to adapt the code for GBM.
This is a very simplistic approach (Euler approximation), for a
special process there should be more sophisticated (faster, more
accurate) methods.
Let me know if you need help.
Thomas


From schreiber_irene at web.de  Mon Apr  6 17:39:09 2009
From: schreiber_irene at web.de (Irene Schreiber)
Date: Mon, 6 Apr 2009 17:39:09 +0200
Subject: [R-SIG-Finance] ARMA-GARCH package in R?
Message-ID: <003501c9b6cd$d427d160$7c777420$@de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090406/52c3ac2d/attachment.pl>

From Jose at erini.ac.uk  Mon Apr  6 18:03:40 2009
From: Jose at erini.ac.uk (Jose Iparraguirre D'Elia)
Date: Mon, 6 Apr 2009 17:03:40 +0100
Subject: [R-SIG-Finance] ARMA-GARCH package in R?
Message-ID: <C9328F0EEDC3BC439FDABD12060E91098E1BD4@erini1.ERINI.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090406/09373043/attachment.pl>

From john.kerpel at gmail.com  Mon Apr  6 18:05:15 2009
From: john.kerpel at gmail.com (John Kerpel)
Date: Mon, 6 Apr 2009 11:05:15 -0500
Subject: [R-SIG-Finance] ARMA-GARCH package in R?
In-Reply-To: <C9328F0EEDC3BC439FDABD12060E91098E1BD4@erini1.ERINI.local>
References: <C9328F0EEDC3BC439FDABD12060E91098E1BD4@erini1.ERINI.local>
Message-ID: <6555fd730904060905w52b6631kaf34aa37575ced5b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090406/e073df60/attachment.pl>

From mohammad_sabr at yahoo.com  Mon Apr  6 21:34:50 2009
From: mohammad_sabr at yahoo.com (Mohammad Sabr)
Date: Mon, 6 Apr 2009 12:34:50 -0700 (PDT)
Subject: [R-SIG-Finance] Problem with Extracting Fitted Values from fGarch
	package
Message-ID: <633285.33343.qm@web52107.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090406/464f5708/attachment.pl>

From sebastian.hauer at gmail.com  Mon Apr  6 22:51:43 2009
From: sebastian.hauer at gmail.com (Sebastian Hauer)
Date: Mon, 6 Apr 2009 16:51:43 -0400
Subject: [R-SIG-Finance] as.xts of a data.frame
Message-ID: <fb92d9f00904061351t23c23c50wc54116affeb4bab3@mail.gmail.com>

Hello,
I am trying to use quantmod to chart some of my existing OHLC data
which I've loaded into my R session from a CSV file.

> str(gbpusd)
'data.frame':	34361 obs. of  5 variables:
 $ Tm: POSIXct, format: "2003-07-25 01:00:00" "2003-07-25 02:00:00" ...
 $ Op: num  1.61 1.61 1.61 1.62 1.62 ...
 $ Hi: num  1.61 1.61 1.62 1.62 1.62 ...
 $ Lo: num  1.61 1.61 1.61 1.62 1.62 ...
 $ Cl: num  1.61 1.61 1.62 1.62 1.62 ...

But I am having a bit of a hard time coercing it from a data.frame into an xts.

> as.xts(gbpusd)
Error in as.POSIXlt.character(x, tz, ...) :
  character string is not in a standard unambiguous format

Or if I try it this way:

> as.xts(gbpusd$Tm, gbpusd)
Data:
numeric(0)
Index:
 POSIXct[1:34361], format: "2003-07-25 01:00:00" "2003-07-25 02:00:00" ...


Can anyone help me and explain what I am doing wrong, I would greatly
appreciated it.

Thanks,
Sebastian


From ggrothendieck at gmail.com  Mon Apr  6 22:58:44 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 6 Apr 2009 16:58:44 -0400
Subject: [R-SIG-Finance] as.xts of a data.frame
In-Reply-To: <fb92d9f00904061351t23c23c50wc54116affeb4bab3@mail.gmail.com>
References: <fb92d9f00904061351t23c23c50wc54116affeb4bab3@mail.gmail.com>
Message-ID: <971536df0904061358n197e1f48p533922b63afceb7a@mail.gmail.com>

Try this:

> Lines <- "Date,Open,High,Low,Close
+ 2003-07-25 01:00:00,1.61,1.61,1.61,1.61
+ 2003-07-25 02:00:00,1.61,1.61,1.61,1.61"
> library(xts)
> z <- read.zoo(textConnection(Lines), header = TRUE, sep = ",", tz = "")
> as.xts(z)
                    Open High  Low Close
2003-07-25 01:00:00 1.61 1.61 1.61  1.61
2003-07-25 02:00:00 1.61 1.61 1.61  1.61


On Mon, Apr 6, 2009 at 4:51 PM, Sebastian Hauer
<sebastian.hauer at gmail.com> wrote:
> Hello,
> I am trying to use quantmod to chart some of my existing OHLC data
> which I've loaded into my R session from a CSV file.
>
>> str(gbpusd)
> 'data.frame': ? 34361 obs. of ?5 variables:
> ?$ Tm: POSIXct, format: "2003-07-25 01:00:00" "2003-07-25 02:00:00" ...
> ?$ Op: num ?1.61 1.61 1.61 1.62 1.62 ...
> ?$ Hi: num ?1.61 1.61 1.62 1.62 1.62 ...
> ?$ Lo: num ?1.61 1.61 1.61 1.62 1.62 ...
> ?$ Cl: num ?1.61 1.61 1.62 1.62 1.62 ...
>
> But I am having a bit of a hard time coercing it from a data.frame into an xts.
>
>> as.xts(gbpusd)
> Error in as.POSIXlt.character(x, tz, ...) :
> ?character string is not in a standard unambiguous format
>
> Or if I try it this way:
>
>> as.xts(gbpusd$Tm, gbpusd)
> Data:
> numeric(0)
> Index:
> ?POSIXct[1:34361], format: "2003-07-25 01:00:00" "2003-07-25 02:00:00" ...
>
>
> Can anyone help me and explain what I am doing wrong, I would greatly
> appreciated it.
>
> Thanks,
> Sebastian
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From stefano.iacus at unimi.it  Mon Apr  6 23:13:21 2009
From: stefano.iacus at unimi.it (stefano iacus)
Date: Mon, 06 Apr 2009 23:13:21 +0200
Subject: [R-SIG-Finance] How to code Geometric Brownian Motion
 Process	with Jumps
In-Reply-To: <d0f55a670904060019v7ef53d1r7062161fbe20ba31@mail.gmail.com>
References: <f706ab3146d9b.49d8dd96@ryerson.ca>
	<d0f55a670904060019v7ef53d1r7062161fbe20ba31@mail.gmail.com>
Message-ID: <47E933F2-B162-4062-BE1D-A2890D9191F1@unimi.it>

If it is diffusion + jumps, than Thomas's code looks not appropriate  
at first glance (apologizes to Thomas in advance if I'm wrong). That  
code does indeed correctly simulate an OU driven only by levy jumps,  
i.e. pure jump process + drift.

One approach is the following: simulate, between times t and, say, t 
+dt, the number k of jumps (for example via Poisson in (0,lambda*dt)).  
Then simulate the k random increments of the levy part (using you  
favorite law: gamma, whatever) and sum them into J

i.e.

k = rpois(1, lambda*dt)
J <- sum(rWHATEVER(k)) # according to levy density of your choice

and finally do something like (assuming mu, sigma are the parameters  
of the GBM process)

  X[t+dt] = X[t] + mu*X[t]*dt + sigma*X[t]*rnorm(1)*sqrt(dt) + J


this is VERY imprecise, but just to give you an idea, and of course,  
as Thomas says, if you specify better the model  you want to simulate,  
you may find some ad hoc solutions. There are many flavors of Levy's  
(compound, infinite activity, stable, etc)

Also, if dt is not very small, simulation of GBM by Euler scheme is  
really bad.
So take dt <- 1/100000 and then resample the trajectory X at, say  
1/100 or so

stefano



On 06/apr/09, at 09:19, Thomas Steiner wrote:

> Hi John-Paul,
> on http://commons.wikimedia.org/wiki/Image:Gamma-OU.png you find a
> Ornstein-Uhlenbeck process which is driven by a Levy process (Gamma
> process). It should be easy to adapt the code for GBM.
> This is a very simplistic approach (Euler approximation), for a
> special process there should be more sophisticated (faster, more
> accurate) methods.
> Let me know if you need help.
> Thomas
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From mohammad_sabr at yahoo.com  Tue Apr  7 06:44:45 2009
From: mohammad_sabr at yahoo.com (Mohammad Sabr)
Date: Mon, 6 Apr 2009 21:44:45 -0700 (PDT)
Subject: [R-SIG-Finance] Problem with Extracting Fitted Values from
	fGarchpackage
Message-ID: <486330.44389.qm@web52106.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090406/87340106/attachment.pl>

From CVorlow at eurobank.gr  Tue Apr  7 15:10:02 2009
From: CVorlow at eurobank.gr (Vorlow Constantinos)
Date: Tue, 7 Apr 2009 16:10:02 +0300
Subject: [R-SIG-Finance] Curvature related question
Message-ID: <BCEA70B53E1BE64290556580EA0708D6C12400@EH002EXC.eurobank.efg.gr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090407/da8743d1/attachment.pl>

From mohammad_sabr at yahoo.com  Tue Apr  7 19:30:49 2009
From: mohammad_sabr at yahoo.com (Mohammad Sabr)
Date: Tue, 7 Apr 2009 10:30:49 -0700 (PDT)
Subject: [R-SIG-Finance] Extracting AIC or Log-Likelihood from a fitted GARCH
Message-ID: <821688.78233.qm@web52106.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090407/2efec97a/attachment.pl>

From ezivot at u.washington.edu  Tue Apr  7 19:50:18 2009
From: ezivot at u.washington.edu (Eric Zivot)
Date: Tue, 7 Apr 2009 10:50:18 -0700
Subject: [R-SIG-Finance] Curvature related question
In-Reply-To: <BCEA70B53E1BE64290556580EA0708D6C12400@EH002EXC.eurobank.efg.gr>
References: <BCEA70B53E1BE64290556580EA0708D6C12400@EH002EXC.eurobank.efg.gr>
Message-ID: <002701c9b7a9$50c566c0$f2503440$@washington.edu>

You might want to look at the following paper which describes some simple
algorithms for determining turning points in business cycles.
Ez

http://www.eabcn.org/research/documents/Harding_Pagan.pdf

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Vorlow
Constantinos
Sent: Tuesday, April 07, 2009 6:10 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Curvature related question

Hello...

Is there a quick and easy way to determine how curvature changes (along
time) on a smoothed version (or trend) of a time series?

i.e.

in the following code, I wan to check (possibly vith a combination of
rollapply function, where the peaks and troughs are on the idx variable
-and, IF they are peaks or troughs- which is a 15 day moving average of
the SP500 index...

    library(tseries)
    SP500 <- get.hist.quote("^GSPC", start = "2006-01-01", quote =
"Close")
    idx1 <- rollapply(SP500, 15, align="right", function(x) mean(x))

Have experimented a bit with the derivatives functions but I can't make
it really work properly... The fact that the sequences are also  zoo
objects and I would like to retain their time stamp properties
complicates things for my code...

If there is a better and faster way (possibly finding a local max or
min), I would appreciate some pointers. 

Thanks in advance,

Costas

 


P Think before you print.

Disclaimer:
This e-mail is confidential. If you are not the intended recipient, you
should not copy it, re-transmit it, use it or disclose its contents, but
should return it to the sender immediately and delete the copy from your
system.
EFG Eurobank Ergasias S.A. is not responsible for, nor endorses, any
opinion, recommendation, conclusion, solicitation, offer or agreement or any
information contained in this communication.
EFG Eurobank Ergasias S.A. cannot accept any responsibility for the accuracy
or completeness of this message as it has been transmitted over a public
network. If you suspect that the message may have been intercepted or
amended, please call the sender.


	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From mohammad_sabr at yahoo.com  Tue Apr  7 21:10:12 2009
From: mohammad_sabr at yahoo.com (Mohammad Sabr)
Date: Tue, 7 Apr 2009 12:10:12 -0700 (PDT)
Subject: [R-SIG-Finance] Extracting AIC or Log-Likelihood from a fitted
	GARCH
Message-ID: <18426.95313.qm@web52112.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090407/d2746cc7/attachment.pl>

From HodgessE at uhd.edu  Tue Apr  7 23:48:38 2009
From: HodgessE at uhd.edu (Hodgess, Erin)
Date: Tue, 7 Apr 2009 16:48:38 -0500
Subject: [R-SIG-Finance] Extracting AIC or Log-Likelihood from a fitted
	GARCH
References: <18426.95313.qm@web52112.mail.re2.yahoo.com>
Message-ID: <70A5AC06FDB5E54482D19E1C04CDFCF307C36B4E@BALI.uhd.campus>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090407/0f6520a4/attachment.pl>

From mohammad_sabr at yahoo.com  Wed Apr  8 00:18:37 2009
From: mohammad_sabr at yahoo.com (Mohammad Sabr)
Date: Tue, 7 Apr 2009 15:18:37 -0700 (PDT)
Subject: [R-SIG-Finance] Extracting AIC or Log-Likelihood from a fitted
	GARCH
Message-ID: <124117.42663.qm@web52111.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090407/ca313579/attachment.pl>

From bearxu83 at gmail.com  Wed Apr  8 00:50:13 2009
From: bearxu83 at gmail.com (BearXu)
Date: Tue, 7 Apr 2009 23:50:13 +0100
Subject: [R-SIG-Finance] When will the ebook "Portfolio Optimization with
	R/Rmetrics" be published?
Message-ID: <82527b5d0904071550k51d9f916yb92d345cc2091b8c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090407/f129668b/attachment.pl>

From mohammad_sabr at yahoo.com  Wed Apr  8 01:25:38 2009
From: mohammad_sabr at yahoo.com (Mohammad Sabr)
Date: Tue, 7 Apr 2009 16:25:38 -0700 (PDT)
Subject: [R-SIG-Finance] Estimating ARMA-GRACH model through a loop
Message-ID: <793321.24232.qm@web52104.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090407/70d897ca/attachment.pl>

From martin.becker at mx.uni-saarland.de  Wed Apr  8 09:30:38 2009
From: martin.becker at mx.uni-saarland.de (Martin Becker)
Date: Wed, 08 Apr 2009 09:30:38 +0200
Subject: [R-SIG-Finance] Estimating ARMA-GRACH model through a loop
In-Reply-To: <793321.24232.qm@web52104.mail.re2.yahoo.com>
References: <793321.24232.qm@web52104.mail.re2.yahoo.com>
Message-ID: <49DC529E.6050403@mx.uni-saarland.de>

Try this:

fit <- garchFit(substitute(~arma(p,q)+garch(1,1),list(p=p,q=q)), data)

  Martin


Mohammad Sabr wrote:
> Good day everyone,
>  
> I am trying to estimate an ARMA-GARCH model through a loop. The main function in the loop is garchFit from the fGrach package. I wrote the following loop
>  
> for (i in 1:5) {
> for (j in 1:5) {
> p=i-1
> q=j-1
> p=1
> q=1
> fit = garchFit(~arma(p,q)+garch(1,1), data))
> }
> }
>  
> The objective of this loop is to enable me to extract some values from the fitted model without have to run it every time manually. The problem with this loop is that the garchFit function does not recognize the values of "p" and "q" in the ARMA specification. When I try to run the loop I get the following error message:
>  
> [1] "data" "p"    "q"   
> [1] "data"
> Error in .garchArgsParser(formula = formula, data = data, trace = FALSE) : 
> Formula and data units do not match.
>
>  
> However, when I substitute p and q with 1 for example, the model is executed without any problem.
>  
> Did any one encounter such a problem or knows the solution to this problem. 
>  
> Thanks in advance,
> 	[[alternative HTML version deleted]]
>
>   
> ------------------------------------------------------------------------
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.

-- 
Dr. Martin Becker
Statistics and Econometrics
Saarland University
Campus C3 1, Room 206
66123 Saarbruecken
Germany


From chirantan at 2pirad.com  Thu Apr  9 10:15:41 2009
From: chirantan at 2pirad.com (Chirantan Kundu)
Date: Thu, 9 Apr 2009 13:45:41 +0530
Subject: [R-SIG-Finance] Invoking bond_prices function of termstrc package
Message-ID: <fc408ad80904090115of8b88c5gf56d1fe65fedcb85@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090409/aad7eb83/attachment.pl>

From chirantan at 2pirad.com  Thu Apr  9 11:46:55 2009
From: chirantan at 2pirad.com (Chirantan Kundu)
Date: Thu, 9 Apr 2009 15:16:55 +0530
Subject: [R-SIG-Finance] Invoking bond_prices function of termstrc
	package
In-Reply-To: <3EB0FCF9-662C-4D0B-B839-AF079CD30362@wu-wien.ac.at>
References: <fc408ad80904090115of8b88c5gf56d1fe65fedcb85@mail.gmail.com>
	<3EB0FCF9-662C-4D0B-B839-AF079CD30362@wu-wien.ac.at>
Message-ID: <fc408ad80904090246o60f31603g21f4a3eede1968bc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090409/e2d32b04/attachment.pl>

From werner at erselina.nl  Thu Apr  9 14:59:14 2009
From: werner at erselina.nl (Werner Erselina)
Date: Thu, 09 Apr 2009 14:59:14 +0200
Subject: [R-SIG-Finance] Quantmod problem retrieving data
Message-ID: <49DDF122.9090106@erselina.nl>

Hi,

I'm investigating the package quantmod. However when doing the examples 
on the site i encounter some problems. Here is my output:

 > require(quantmod)
Loading required package: quantmod
Loading required package: xts
Loading required package: zoo

Attaching package: 'zoo'


       The following object(s) are masked from package:base :

        as.Date.numeric

Loading required package: Defaults
quantmod: Quantitative Financial Modelling Framework

Version 0.3-7, Revision 461
http://www.quantmod.com

 > getSymbols("DEXUSJP",src="FRED")
Error in download.file(paste(FRED.URL, "/", Symbols[[i]], "/", 
"downloaddata/",  :
 cannot open URL 
'http://research.stlouisfed.org/fred2/series/DEXUSJP/downloaddata/DEXUSJP.csv' 

In addition: Warning message:
In download.file(paste(FRED.URL, "/", Symbols[[i]], "/", 
"downloaddata/",  :
 cannot open: HTTP status was '404 Not Found'
 > getSymbols("XPT/USD",src="Oanda")
Error in do.call(paste("getSymbols.", symbol.source, sep = ""), 
list(Symbols = current.symbols,  :
 could not find function "getSymbols.Oanda"

As it seems, retrieving data via quantmod is not successful.  Anybody 
knows how i could solve this?


Regards,

Werner


From josh.m.ulrich at gmail.com  Thu Apr  9 16:20:22 2009
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Thu, 9 Apr 2009 09:20:22 -0500
Subject: [R-SIG-Finance] Quantmod problem retrieving data
In-Reply-To: <49DDF122.9090106@erselina.nl>
References: <49DDF122.9090106@erselina.nl>
Message-ID: <8cca69990904090720j23de6ae7lbad02ef5cc1182db@mail.gmail.com>

Werner,

It's as easy as using the correct FRED symbol:
getSymbols("DEXJPUS",src="FRED")

Best,
Josh
--
http://quantemplation.blogspot.com
http://www.fosstrading.com



On Thu, Apr 9, 2009 at 7:59 AM, Werner Erselina <werner at erselina.nl> wrote:
> Hi,
>
> I'm investigating the package quantmod. However when doing the examples on
> the site i encounter some problems. Here is my output:
>
>> require(quantmod)
> Loading required package: quantmod
> Loading required package: xts
> Loading required package: zoo
>
> Attaching package: 'zoo'
>
>
> ? ? ?The following object(s) are masked from package:base :
>
> ? ? ? as.Date.numeric
>
> Loading required package: Defaults
> quantmod: Quantitative Financial Modelling Framework
>
> Version 0.3-7, Revision 461
> http://www.quantmod.com
>
>> getSymbols("DEXUSJP",src="FRED")
> Error in download.file(paste(FRED.URL, "/", Symbols[[i]], "/",
> "downloaddata/", ?:
> cannot open URL
> 'http://research.stlouisfed.org/fred2/series/DEXUSJP/downloaddata/DEXUSJP.csv'
> In addition: Warning message:
> In download.file(paste(FRED.URL, "/", Symbols[[i]], "/", "downloaddata/", ?:
> cannot open: HTTP status was '404 Not Found'
>> getSymbols("XPT/USD",src="Oanda")
> Error in do.call(paste("getSymbols.", symbol.source, sep = ""), list(Symbols
> = current.symbols, ?:
> could not find function "getSymbols.Oanda"
>
> As it seems, retrieving data via quantmod is not successful. ?Anybody knows
> how i could solve this?
>
>
> Regards,
>
> Werner
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From werner at erselina.nl  Fri Apr 10 12:49:04 2009
From: werner at erselina.nl (Werner Erselina)
Date: Fri, 10 Apr 2009 12:49:04 +0200
Subject: [R-SIG-Finance] Quantmod problem retrieving data
In-Reply-To: <49DDF122.9090106@erselina.nl>
References: <49DDF122.9090106@erselina.nl>
Message-ID: <49DF2420.6020101@erselina.nl>

Thanks Michael and Josh,

Then there is only one problem remaining and that is retrieving the 
currency data from oanda. Using the same syntax as the examples from the 
website i get this error:

getSymbols("XPT/USD",src="oanda")
Error in (grep("PRE", fr)[1]):(grep("PRE", fr)[2]) : NA/NaN argument


Kind regards


From jeff.a.ryan at gmail.com  Fri Apr 10 15:49:15 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 10 Apr 2009 08:49:15 -0500
Subject: [R-SIG-Finance] Quantmod problem retrieving data
In-Reply-To: <49DF2420.6020101@erselina.nl>
References: <49DDF122.9090106@erselina.nl> <49DF2420.6020101@erselina.nl>
Message-ID: <e8e755250904100649o37cc7252i9baf8726b650ceb@mail.gmail.com>

Hello Werner,

The issue is that oanda has changed the length of time that you can
download in one request from 2000 days to 500 days.

I will fix the default in the next CRAN release of quantmod, but for
now you can just specify a time-period that is inside that range.

HTH,
Jeff

On Fri, Apr 10, 2009 at 5:49 AM, Werner Erselina <werner at erselina.nl> wrote:
> Thanks Michael and Josh,
>
> Then there is only one problem remaining and that is retrieving the currency
> data from oanda. Using the same syntax as the examples from the website i
> get this error:
>
> getSymbols("XPT/USD",src="oanda")
> Error in (grep("PRE", fr)[1]):(grep("PRE", fr)[2]) : NA/NaN argument
>
>
> Kind regards
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From werner at erselina.nl  Fri Apr 10 16:02:46 2009
From: werner at erselina.nl (Werner Erselina)
Date: Fri, 10 Apr 2009 16:02:46 +0200
Subject: [R-SIG-Finance] Quantmod problem retrieving data
In-Reply-To: <49DDF122.9090106@erselina.nl>
References: <49DDF122.9090106@erselina.nl>
Message-ID: <49DF5186.2060605@erselina.nl>

Thanks Jeff,

This was very helpful. I will  examine the package further.

Greetings,


Werner



Werner Erselina wrote:
> Hi,
>
> I'm investigating the package quantmod. However when doing the 
> examples on the site i encounter some problems. Here is my output:
>
> > require(quantmod)
> Loading required package: quantmod
> Loading required package: xts
> Loading required package: zoo
>
> Attaching package: 'zoo'
>
>
>       The following object(s) are masked from package:base :
>
>        as.Date.numeric
>
> Loading required package: Defaults
> quantmod: Quantitative Financial Modelling Framework
>
> Version 0.3-7, Revision 461
> http://www.quantmod.com
>
> > getSymbols("DEXUSJP",src="FRED")
> Error in download.file(paste(FRED.URL, "/", Symbols[[i]], "/", 
> "downloaddata/",  :
> cannot open URL 
> 'http://research.stlouisfed.org/fred2/series/DEXUSJP/downloaddata/DEXUSJP.csv' 
>
> In addition: Warning message:
> In download.file(paste(FRED.URL, "/", Symbols[[i]], "/", 
> "downloaddata/",  :
> cannot open: HTTP status was '404 Not Found'
> > getSymbols("XPT/USD",src="Oanda")
> Error in do.call(paste("getSymbols.", symbol.source, sep = ""), 
> list(Symbols = current.symbols,  :
> could not find function "getSymbols.Oanda"
>
> As it seems, retrieving data via quantmod is not successful.  Anybody 
> knows how i could solve this?
>
>
> Regards,
>
> Werner
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From andyzhu35 at yahoo.com  Fri Apr 10 18:10:36 2009
From: andyzhu35 at yahoo.com (Andy Zhu)
Date: Fri, 10 Apr 2009 09:10:36 -0700 (PDT)
Subject: [R-SIG-Finance] yahoo historical data quality
Message-ID: <673291.11345.qm@web56207.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090410/d1acd00c/attachment.pl>

From guygreen at netvigator.com  Fri Apr 10 18:45:59 2009
From: guygreen at netvigator.com (gug)
Date: Fri, 10 Apr 2009 09:45:59 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] yahoo historical data quality
In-Reply-To: <673291.11345.qm@web56207.mail.re3.yahoo.com>
References: <673291.11345.qm@web56207.mail.re3.yahoo.com>
Message-ID: <22991022.post@talk.nabble.com>


You have to go through their help pages:
http://help.yahoo.com/l/us/yahoo/finance/basics/fin-01.html
It is not always easy and not immediate, but you can usually get them to
change something.


Andy Zhu wrote:
> 
> Hi,
> 
> Encountered problem with historical price, missing data from
> yahoo/finance. Here is an example: ticker = 'ATML', dates =
> c('2000-08-28', '2000-08-29') (Mon and Tue). Yahoo doesn't show data for
> these two dates so Quantmod simply doesn't pick them up with Yahoo.
> However, when I check with Google/finance, it does show up data for these
> 2 dates. How do you guys handle these missing data in a systematical way?
> do you have any information of how to communicate with Yahoo person?
> 
> Thanks.
> 

-- 
View this message in context: http://www.nabble.com/yahoo-historical-data-quality-tp22990415p22991022.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From james at jtoll.com  Fri Apr 10 20:32:14 2009
From: james at jtoll.com (James Toll)
Date: Fri, 10 Apr 2009 12:32:14 -0600
Subject: [R-SIG-Finance] yahoo historical data quality
In-Reply-To: <673291.11345.qm@web56207.mail.re3.yahoo.com>
References: <673291.11345.qm@web56207.mail.re3.yahoo.com>
Message-ID: <4F981999-1F4A-481A-AEE6-5B7BF1E1A772@jtoll.com>

Andy et al.,

I have a question that's really an extension/amalgam of your question  
regarding the quality of Yahoo historical data with regard to the  
market capitalization data you were inquiring about last week  
(Specifically, I'm interested in outstanding and floated shares).   
Given that it's free data, one can't be overly critical, but have you  
found Yahoo's market cap data to be both accurate and timely?  I don't  
have a reference to compare the data to, so I'm unsure.

Thanks,

James



On Apr 10, 2009, at 10:10 AM, Andy Zhu wrote:

> Hi,
>
> Encountered problem with historical price, missing data from yahoo/ 
> finance. Here is an example: ticker = 'ATML', dates =  
> c('2000-08-28', '2000-08-29') (Mon and Tue). Yahoo doesn't show data  
> for these two dates so Quantmod simply doesn't pick them up with  
> Yahoo. However, when I check with Google/finance, it does show up  
> data for these 2 dates. How do you guys handle these missing data in  
> a systematical way? do you have any information of how to  
> communicate with Yahoo person?
>
> Thanks.


From HodgessE at uhd.edu  Fri Apr 10 20:37:31 2009
From: HodgessE at uhd.edu (Hodgess, Erin)
Date: Fri, 10 Apr 2009 13:37:31 -0500
Subject: [R-SIG-Finance] yahoo historical data quality
References: <673291.11345.qm@web56207.mail.re3.yahoo.com>
	<4F981999-1F4A-481A-AEE6-5B7BF1E1A772@jtoll.com>
Message-ID: <70A5AC06FDB5E54482D19E1C04CDFCF307C36B88@BALI.uhd.campus>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090410/2fe4a045/attachment.pl>

From bearxu83 at gmail.com  Sat Apr 11 01:35:06 2009
From: bearxu83 at gmail.com (BearXu)
Date: Sat, 11 Apr 2009 00:35:06 +0100
Subject: [R-SIG-Finance] an ARIMA(0,2,1) model
Message-ID: <82527b5d0904101635s29486c8dn662d4b779d0a3616@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090411/9ff37a94/attachment.pl>

From markleeds at verizon.net  Sat Apr 11 01:43:18 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Fri, 10 Apr 2009 18:43:18 -0500 (CDT)
Subject: [R-SIG-Finance] an ARIMA(0,2,1) model
Message-ID: <773593491.941574.1239406998587.JavaMail.root@vms182.mailsrvcs.net>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090410/73c15116/attachment.html>

From fin.tiptop.1900 at gmail.com  Sat Apr 11 01:55:54 2009
From: fin.tiptop.1900 at gmail.com (From Watchman)
Date: Fri, 10 Apr 2009 19:55:54 -0400
Subject: [R-SIG-Finance] MLE Jump diffision
Message-ID: <137bba0a0904101655l555adc62v25b45f4798270721@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090410/c0ca90ee/attachment.pl>

From andyzhu35 at yahoo.com  Sat Apr 11 04:04:33 2009
From: andyzhu35 at yahoo.com (Andy Zhu)
Date: Fri, 10 Apr 2009 19:04:33 -0700 (PDT)
Subject: [R-SIG-Finance] yahoo historical data quality
Message-ID: <521940.47184.qm@web56205.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090410/4d60fa88/attachment.pl>

From guygreen at netvigator.com  Sat Apr 11 05:41:25 2009
From: guygreen at netvigator.com (gug)
Date: Fri, 10 Apr 2009 20:41:25 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] yahoo historical data quality
In-Reply-To: <521940.47184.qm@web56205.mail.re3.yahoo.com>
References: <673291.11345.qm@web56207.mail.re3.yahoo.com>
	<521940.47184.qm@web56205.mail.re3.yahoo.com>
Message-ID: <22997700.post@talk.nabble.com>


As an additional comment: you will see data errors even with paid data
services.  I have seen errors with historic prices from Reuters and also
with Thomson (prior to them being the same company).  With both of these
data providers, the data was from paid (professional-level) services.

Incidentally the only service that I have seen that has historical market
cap data (i.e. able to give a time series of historic # shares as a separate
data item) is Datastream, also now a Thomson product.


Andy Zhu wrote:
> 
> James:
> 
> MarketCap is spot data from public domain; no way to have historicals
> compared to the price series. However, comparison between google and yahoo
> for ATML is the same result for market cap. Actually it is hard to tell
> the quality regarding market cap, even some commercial data sources
> oftentime don't agree with each other, and sometimes the difference is non
> trivial. So I just live with it. A bit improvement could be that if you
> have access to different sources regarding MarketCap, take average or
> median on market cap (based on either outstanding or floating, but don't
> mix).
> 
> Another look on the historical data quality is from trading volumn. One
> can see the differences between yahoo and google. 
> 
> The further extension on MarketCap is the spinoff. We have no way directly
> from the available public domains to capture spinoff (opposed to
> dividend/split). although spinoff is a rare corp event, yet it has far
> major impact on stock metrics including MarketCap.
> 
> Erin:
> 
> Thanks for answering my email. The problem I identified in my post is
> independent of which R package one uses. If underlying data source is from
> Yahoo/Finance, one will inherit the quality problem because its root lies
> in yahoo/Finance, not in R packages/functions. The missing data example I
> gave below is obvious no matter which package you run, unless Yahoo fixes
> it or one goes to a quality (paid) data service. (By the way, I do use
> get.hist.quote).
> 
> Thanks
> 
> 

-- 
View this message in context: http://www.nabble.com/yahoo-historical-data-quality-tp22990415p22997700.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From michael.larsson at gmail.com  Sat Apr 11 10:38:25 2009
From: michael.larsson at gmail.com (Michael Larsson)
Date: Sat, 11 Apr 2009 09:38:25 +0100
Subject: [R-SIG-Finance] yahoo historical data quality
In-Reply-To: <22997700.post@talk.nabble.com>
References: <673291.11345.qm@web56207.mail.re3.yahoo.com>
	<521940.47184.qm@web56205.mail.re3.yahoo.com>
	<22997700.post@talk.nabble.com>
Message-ID: <6440aa460904110138q22dcea67j8022442aadba1d2e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090411/1b14d136/attachment.pl>

From brian at braverock.com  Sat Apr 11 16:06:50 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Sat, 11 Apr 2009 09:06:50 -0500
Subject: [R-SIG-Finance] MLE Jump diffision
In-Reply-To: <137bba0a0904101655l555adc62v25b45f4798270721@mail.gmail.com>
References: <137bba0a0904101655l555adc62v25b45f4798270721@mail.gmail.com>
Message-ID: <49E0A3FA.5010900@braverock.com>

 From Watchman wrote:
> Hi
>
> I have been having issue with a ML estimator for Jump diffusion process but
> know I am get little error I didn't notice before like I am try to create a
> vector
>   
>> #GBMPJ MLE Combined Ph 1 LR
>> #
>> n<-length(combinedlrph1)
>> j<-c(1,2,3,4,5,6,7,8,9,10)
>>     
> Error in c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) :
>   unused argument(s) (3, 4, 5, 6, 7, 8, 9, 10)
>   
It appears that you have redefined the function c()

In my environment:

> j<-c(1,2,3,4,5,6,7,8,9,10)
> c
function (..., recursive = FALSE)  .Primitive("c")

Regards,

  - Brian
 
-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From daler at uic.edu  Sat Apr 11 16:18:19 2009
From: daler at uic.edu (Rosenthal, Dale W.R.)
Date: Sat, 11 Apr 2009 09:18:19 -0500 (CDT)
Subject: [R-SIG-Finance] R-SIG-Finance Digest, Vol 59, Issue 10
In-Reply-To: <mailman.1.1239444002.7228.r-sig-finance@stat.math.ethz.ch>
References: <mailman.1.1239444002.7228.r-sig-finance@stat.math.ethz.ch>
Message-ID: <55078.99.145.88.21.1239459499.squirrel@webmail.uic.edu>

This has been a problem with Yahoo data for years.  They usually will
return data from your start date; but, the data ends at a random date
before your specified stop date.

My (crude, imperfect) solution was to try multiple times (4-5) and then
toss an exception if that didn't work.  I then made a second pass of
exceptions and then cleaned up the last handful by hand.

Dale


> Message: 4
> Date: Fri, 10 Apr 2009 09:10:36 -0700 (PDT)
> From: Andy Zhu <andyzhu35 at yahoo.com>
> Subject: [R-SIG-Finance] yahoo historical data quality
> To: r-sig-finance at stat.math.ethz.ch
> Message-ID: <673291.11345.qm at web56207.mail.re3.yahoo.com>
> Content-Type: text/plain
>
> Hi,
>
> Encountered problem with historical price, missing data from
> yahoo/finance. Here is an example: ticker = 'ATML', dates =
> c('2000-08-28', '2000-08-29') (Mon and Tue). Yahoo doesn't show data for
> these two dates so Quantmod simply doesn't pick them up with Yahoo.
> However, when I check with Google/finance, it does show up data for these
> 2 dates. How do you guys handle these missing data in a systematical way?
> do you have any information of how to communicate with Yahoo person?
>
> Thanks.


From babel at centrum.sk  Sun Apr 12 00:46:26 2009
From: babel at centrum.sk (babel at centrum.sk)
Date: Sun, 12 Apr 2009 00:46:26 +0200
Subject: [R-SIG-Finance] ARIMA,GARCH and differences
Message-ID: <200904120046.16786@centrum.cz>

Dear friends

This is maybe trivial question for you, but why I get different results when I use return series and the original price series with 1.differences speciefied in ARIMA model? For example
x<-priceSeries
ret<-diff(log(x))
ret1<-diff(x)

fit_arma = armaFit(~ arma(5,0), data = ret)
fit_arma1 = armaFit(~ arma(5,0), data = ret1)
fit_arima = armaFit(~ arima(5,1,0), data = x)

The first model has the smallest AIC. AR parameters are very similar but not the same. 

The second question is more critical for me. Is there any possibility to enlarge  the package fGarch for the joint estiamtion of ARIMA+GARCH? Because in mean equation you can specifie only AR, MA, or ARMA model, not ARIMA. This extension I consider very important, because using the differences biult-in model,( I mean ARIMA) simplify the entire fitting and forecasting proces. If you use ARIMA, you dont need to make reverse transformation (from return to price series - from diff(log) to original scale). For evauation part it is not a big problem but for prediction part I find it difficult, therefore, I will prefer to use ARIMA so the garchFit function would produce the prediction and make the reverse transformation for me. (like the armaFit does)

Thank you very much for any suggestion you provide


From Zeno.Adams at ebs.edu  Sun Apr 12 15:33:15 2009
From: Zeno.Adams at ebs.edu (Adams, Zeno)
Date: Sun, 12 Apr 2009 15:33:15 +0200
Subject: [R-SIG-Finance] ARIMA,GARCH and differences
References: <200904120046.16786@centrum.cz>
Message-ID: <9064522880125945B98983BBAECBA1CC21CBFF@exchsrv001.ebs.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090412/dcfacbcf/attachment.pl>

From babel at centrum.sk  Sun Apr 12 20:01:32 2009
From: babel at centrum.sk (babel at centrum.sk)
Date: Sun, 12 Apr 2009 20:01:32 +0200
Subject: [R-SIG-Finance] ARIMA,GARCH and differences
In-Reply-To: <9064522880125945B98983BBAECBA1CC21CBFF@exchsrv001.ebs.local>
References: 200904120046.16786@centrum.cz>
	<9064522880125945B98983BBAECBA1CC21CBFF@exchsrv001.ebs.local>
Message-ID: <200904122001.2916@centrum.cz>

Thank you for your reply, you are right with that logaritmisation, I fully agree, but the problem is this.  The I in ARIMA makes differences for you. It is obvious from the output, because the AR coefficients are very similar in the case, when I use the non stationary price series and ARMA(5,1,0) and when I use return series (lets say stationary) and model ARMA(5,0) , but NOT THE SAME. Please take o look to the following output:


Call:
armaFit(formula = ~arma(5, 0), data = ret)
Model:
 ARIMA(5,0,0) with method: CSS-ML

Coefficient(s):
      ar1           ar2               ar3               ar4              ar5            intercept  
 0.067939   0.155447   0.067962   0.026540   0.070215   0.000086  
log likelihood:       5399.57
AIC Criterion:        -10785.14

Call:
 armaFit(formula = ~arima(5, 1, 0), data = x)

Model:
 ARIMA(5,1,0) with method: CSS-ML

Coefficient(s):
    ar1          ar2            ar3           ar4          ar5  
0.07284  0.15856  0.07088  0.03291  0.07734  
log likelihood:       5168.35
AIC Criterion:        -10324.71



To the second answer. If the I in ARIMA just tells you how many times you need to difference, why the function armaFit makes these differenciation for you? And if it does, why it cannot be included in garchFit function in the same way? Or maybe one strange idea came to my head, what about making the ARMA model on data to get residuals, then make another ARMA model on these residuals, make 2 forecasting - forecasting of series and forecasting of errors from ARMA model and then just add these 2 results together?

I believe there is a mistake in your formula. I try it on a few data, which I diff(log) and then try to transformed it back, but it didnt work. Results was totally different. I use this formula instead, which works perfectly for me, but maybe the problem is on my side :))) :

for (t in 1:n-1)
x[t+1]]<-exp(ret[t]+ log(x[t]))

But for the prediction, I dont have the original price time series (xt) so how can I make the reverse transformation for that particular period? I mean in the case of ex-ante predictions, with the ex-post, it is OK. 

And the last thing, we have learnt that only AR part needs to be stationary. So the coefficients of ARMA model for example (0,2) can be estimated on non-stationary time series. 

Your sincerely 
                                   Jan Babel


______________________________________________________________
> Od: Zeno.Adams at ebs.edu
> Komu: <babel at centrum.sk>, "R-SIG-Finance" <r-sig-finance at stat.math.ethz.ch>
> Datum: 12.04.2009 15:33
> P?edm?t: RE: [R-SIG-Finance] ARIMA,GARCH and differences
>
>Taking logs is not a linear transformation. Generally, values that have
been very large or very small before the transformation become more
similar after the transformation. The difference of the logs therefore
tends to be smaller than the simple difference. The simple difference
furthermore is not a percentage but an absolute difference so that scaling
matters. From what I know, most people would want to take ret and not ret1
since ret can be directly interpreted as continuous returns.
>
>For your second question: From what I know, in an ARIMA model the
coefficients are not estimated on the nonstationary series but always on
the stationary series so that the I in ARIMA just tells you how many times
you had to differences the series to estimate the ARMA model. I dont know
if I understand you correctly but the reverse transformation is easy to
do: After estimating the model on the stationary returns (ret) you
generate the forecast of the returns and then transform this back to
prices using
>
>x_t = exp[ret_t + x_t-1]
>
>where x_0 is the is the beginning of your price series. I use a loop for
the transformation although there is probably a more elegant vector
function for that.
>
>
>-----Original Message-----
>From: r-sig-finance-bounces at stat.math.ethz.ch on behalf of
babel at centrum.sk
>Sent: Sun 4/12/2009 12:46 AM
>To: R-SIG-Finance
>Subject: [R-SIG-Finance] ARIMA,GARCH and differences
> 
>Dear friends
>
>This is maybe trivial question for you, but why I get different results
when I use return series and the original price series with 1.differences
speciefied in ARIMA model? For example
>x<-priceSeries
>ret<-diff(log(x))
>ret1<-diff(x)
>
>fit_arma = armaFit(~ arma(5,0), data = ret)
>fit_arma1 = armaFit(~ arma(5,0), data = ret1)
>fit_arima = armaFit(~ arima(5,1,0), data = x)
>
>The first model has the smallest AIC. AR parameters are very similar but
not the same. 
>
>The second question is more critical for me. Is there any possibility to
enlarge  the package fGarch for the joint estiamtion of ARIMA+GARCH?
Because in mean equation you can specifie only AR, MA, or ARMA model, not
ARIMA. This extension I consider very important, because using the
differences biult-in model,( I mean ARIMA) simplify the entire fitting and
forecasting proces. If you use ARIMA, you dont need to make reverse
transformation (from return to price series - from diff(log) to original
scale). For evauation part it is not a big problem but for prediction part
I find it difficult, therefore, I will prefer to use ARIMA so the garchFit
function would produce the prediction and make the reverse transformation
for me. (like the armaFit does)
>
>Thank you very much for any suggestion you provide
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only.
>-- If you want to post, subscribe first.
>
>
>
>EBS European Business School gemeinnuetzige GmbH - Sitz der Gesellschaft:
Wiesbaden, Amtsgericht Wiesbaden HRB 19951 - Umsatzsteuer-ID DE 113891213
Geschaeftsfuehrer: Prof. Dr. Christopher Jahns,  Rektor/CEO; Dr. Reimar
Palte,  Kanzler/CFO;  Sabine Fuchs, CMO; Verwaltungsrat: Dr. Hellmut K.
Albrecht, Vorsitzender


From Zeno.Adams at ebs.edu  Sun Apr 12 21:29:19 2009
From: Zeno.Adams at ebs.edu (Adams, Zeno)
Date: Sun, 12 Apr 2009 21:29:19 +0200
Subject: [R-SIG-Finance] ARIMA,GARCH and differences
References: 200904120046.16786@centrum.cz><9064522880125945B98983BBAECBA1CC21CBFF@exchsrv001.ebs.local>
	<200904122001.2916@centrum.cz>
Message-ID: <9064522880125945B98983BBAECBA1CC21CC03@exchsrv001.ebs.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090412/dfb41fd9/attachment.pl>

From hkahra at gmail.com  Mon Apr 13 08:49:37 2009
From: hkahra at gmail.com (Hannu Kahra)
Date: Mon, 13 Apr 2009 09:49:37 +0300
Subject: [R-SIG-Finance] ARIMA,GARCH and differences
In-Reply-To: <9064522880125945B98983BBAECBA1CC21CC03@exchsrv001.ebs.local>
References: <9064522880125945B98983BBAECBA1CC21CBFF@exchsrv001.ebs.local>
	<200904122001.2916@centrum.cz>
	<9064522880125945B98983BBAECBA1CC21CC03@exchsrv001.ebs.local>
Message-ID: <3d35a2ca0904122349q5d158732s7dffbd441290962c@mail.gmail.com>

If you have a look at the arma and arima outputs, the first one has an
intercept but in arima it is ignored:

R help for arima:
include.mean Should the ARMA model include a mean/intercept term? The
default is TRUE for undifferenced series, and it is ignored for ARIMA
models with differencing.
Further, if include.mean is true (the default for an ARMA model), this
formula applies to X - m rather than X. For ARIMA models with
differencing, the differenced series follows a zero-mean ARMA model.

> >From what I know a MA(q) model is always stationary since the MA-terms are white noise error terms.

It is true that an MA(q) model is always stationary. But in MA(q)
models a similar concept, invertibility, is required.

For example, for MA(1) r(t) = c + a(t) - theta*a(t-1) it is required
that abs(theta) < 0. Such an MA(1) model is said to be invertible.

-Hannu

On Sun, Apr 12, 2009 at 10:29 PM, Adams, Zeno <Zeno.Adams at ebs.edu> wrote:
> I do not know why both models do not produce the same result. I think they should. It may have to do with the fact that the arma function is estimated from the tseries package whereas the arima model is estimated with the help of the ts package.
>
> Yes, I forgot a log in my formula, sorry for that. See the following example for the correct code:
>
>
> set.seed(123)
> x <- 1:50 + 0.5*rnorm(50) ?; x
> dlx = diff(log(x)) ; dlx
>
> transform <- numeric(50)
> transform[1] <- x[1]
> for (i in 1:49) {
> transform[i+1] <- exp(dlx[i] + log(transform[i]))
> }
> transform
>
> if you dont have the price series, you can use x_0 = 100 if e.g. the price series is an index. Otherwise you need the price series.
>
> >From what I know a MA(q) model is always stationary since the MA-terms are white noise error terms.
>
> Lets see if we get a more satisfying answer from someone else from this list.
>
>
>
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch on behalf of babel at centrum.sk
> Sent: Sun 4/12/2009 8:01 PM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] ARIMA,GARCH and differences
>
> Thank you for your reply, you are right with that logaritmisation, I fully agree, but the problem is this. ?The I in ARIMA makes differences for you. It is obvious from the output, because the AR coefficients are very similar in the case, when I use the non stationary price series and ARMA(5,1,0) and when I use return series (lets say stationary) and model ARMA(5,0) , but NOT THE SAME. Please take o look to the following output:
>
>
> Call:
> armaFit(formula = ~arma(5, 0), data = ret)
> Model:
> ?ARIMA(5,0,0) with method: CSS-ML
>
> Coefficient(s):
> ? ? ?ar1 ? ? ? ? ? ar2 ? ? ? ? ? ? ? ar3 ? ? ? ? ? ? ? ar4 ? ? ? ? ? ? ?ar5 ? ? ? ? ? ?intercept
> ?0.067939 ? 0.155447 ? 0.067962 ? 0.026540 ? 0.070215 ? 0.000086
> log likelihood: ? ? ? 5399.57
> AIC Criterion: ? ? ? ?-10785.14
>
> Call:
> ?armaFit(formula = ~arima(5, 1, 0), data = x)
>
> Model:
> ?ARIMA(5,1,0) with method: CSS-ML
>
> Coefficient(s):
> ? ?ar1 ? ? ? ? ?ar2 ? ? ? ? ? ?ar3 ? ? ? ? ? ar4 ? ? ? ? ?ar5
> 0.07284 ?0.15856 ?0.07088 ?0.03291 ?0.07734
> log likelihood: ? ? ? 5168.35
> AIC Criterion: ? ? ? ?-10324.71
>
>
>
> To the second answer. If the I in ARIMA just tells you how many times you need to difference, why the function armaFit makes these differenciation for you? And if it does, why it cannot be included in garchFit function in the same way? Or maybe one strange idea came to my head, what about making the ARMA model on data to get residuals, then make another ARMA model on these residuals, make 2 forecasting - forecasting of series and forecasting of errors from ARMA model and then just add these 2 results together?
>
> I believe there is a mistake in your formula. I try it on a few data, which I diff(log) and then try to transformed it back, but it didnt work. Results was totally different. I use this formula instead, which works perfectly for me, but maybe the problem is on my side :))) :
>
> for (t in 1:n-1)
> x[t+1]]<-exp(ret[t]+ log(x[t]))
>
> But for the prediction, I dont have the original price time series (xt) so how can I make the reverse transformation for that particular period? I mean in the case of ex-ante predictions, with the ex-post, it is OK.
>
> And the last thing, we have learnt that only AR part needs to be stationary. So the coefficients of ARMA model for example (0,2) can be estimated on non-stationary time series.
>
> Your sincerely
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Jan Babel
>
>
> ______________________________________________________________
>> Od: Zeno.Adams at ebs.edu
>> Komu: <babel at centrum.sk>, "R-SIG-Finance" <r-sig-finance at stat.math.ethz.ch>
>> Datum: 12.04.2009 15:33
>> Predmet: RE: [R-SIG-Finance] ARIMA,GARCH and differences
>>
>>Taking logs is not a linear transformation. Generally, values that have
> been very large or very small before the transformation become more
> similar after the transformation. The difference of the logs therefore
> tends to be smaller than the simple difference. The simple difference
> furthermore is not a percentage but an absolute difference so that scaling
> matters. From what I know, most people would want to take ret and not ret1
> since ret can be directly interpreted as continuous returns.
>>
>>For your second question: From what I know, in an ARIMA model the
> coefficients are not estimated on the nonstationary series but always on
> the stationary series so that the I in ARIMA just tells you how many times
> you had to differences the series to estimate the ARMA model. I dont know
> if I understand you correctly but the reverse transformation is easy to
> do: After estimating the model on the stationary returns (ret) you
> generate the forecast of the returns and then transform this back to
> prices using
>>
>>x_t = exp[ret_t + x_t-1]
>>
>>where x_0 is the is the beginning of your price series. I use a loop for
> the transformation although there is probably a more elegant vector
> function for that.
>>
>>
>>-----Original Message-----
>>From: r-sig-finance-bounces at stat.math.ethz.ch on behalf of
> babel at centrum.sk
>>Sent: Sun 4/12/2009 12:46 AM
>>To: R-SIG-Finance
>>Subject: [R-SIG-Finance] ARIMA,GARCH and differences
>>
>>Dear friends
>>
>>This is maybe trivial question for you, but why I get different results
> when I use return series and the original price series with 1.differences
> speciefied in ARIMA model? For example
>>x<-priceSeries
>>ret<-diff(log(x))
>>ret1<-diff(x)
>>
>>fit_arma = armaFit(~ arma(5,0), data = ret)
>>fit_arma1 = armaFit(~ arma(5,0), data = ret1)
>>fit_arima = armaFit(~ arima(5,1,0), data = x)
>>
>>The first model has the smallest AIC. AR parameters are very similar but
> not the same.
>>
>>The second question is more critical for me. Is there any possibility to
> enlarge ?the package fGarch for the joint estiamtion of ARIMA+GARCH?
> Because in mean equation you can specifie only AR, MA, or ARMA model, not
> ARIMA. This extension I consider very important, because using the
> differences biult-in model,( I mean ARIMA) simplify the entire fitting and
> forecasting proces. If you use ARIMA, you dont need to make reverse
> transformation (from return to price series - from diff(log) to original
> scale). For evauation part it is not a big problem but for prediction part
> I find it difficult, therefore, I will prefer to use ARIMA so the garchFit
> function would produce the prediction and make the reverse transformation
> for me. (like the armaFit does)
>>
>>Thank you very much for any suggestion you provide
>>
>>_______________________________________________
>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>-- Subscriber-posting only.
>>-- If you want to post, subscribe first.
>>
>>
>>
>>EBS European Business School gemeinnuetzige GmbH - Sitz der Gesellschaft:
> Wiesbaden, Amtsgericht Wiesbaden HRB 19951 - Umsatzsteuer-ID DE 113891213
> Geschaeftsfuehrer: Prof. Dr. Christopher Jahns, ?Rektor/CEO; Dr. Reimar
> Palte, ?Kanzler/CFO; ?Sabine Fuchs, CMO; Verwaltungsrat: Dr. Hellmut K.
> Albrecht, Vorsitzender
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>
>
> EBS European Business School gemeinnuetzige GmbH - Sitz der Gesellschaft: Wiesbaden, Amtsgericht Wiesbaden HRB 19951 - Umsatzsteuer-ID DE 113891213 Geschaeftsfuehrer: Prof. Dr. Christopher Jahns, ?Rektor/CEO; Dr. Reimar Palte, ?Kanzler/CFO; ?Sabine Fuchs, CMO; Verwaltungsrat: Dr. Hellmut K. Albrecht, Vorsitzender
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From cedrickj at cavengerllc.com  Mon Apr 13 09:00:38 2009
From: cedrickj at cavengerllc.com (Cedrick Johnson)
Date: Mon, 13 Apr 2009 02:00:38 -0500
Subject: [R-SIG-Finance] Opentick Gone
In-Reply-To: <9064522880125945B98983BBAECBA1CC21CC03@exchsrv001.ebs.local>
References: 200904120046.16786@centrum.cz><9064522880125945B98983BBAECBA1CC21CBFF@exchsrv001.ebs.local>	<200904122001.2916@centrum.cz>
	<9064522880125945B98983BBAECBA1CC21CC03@exchsrv001.ebs.local>
Message-ID: <49E2E316.6090202@cavengerllc.com>

In case anyone missed it...looks like opentick has finally shut their 
doors while I was on vacation the month of March. I wonder how similar 
the new offering will be to the previous API they had....

 From their website:

3/16/2009


  To opentick subscribers, friends, supporters, contributors and the
  rest of the community...

It has been quite a journey for opentick, and for those of you who have 
been with us for the ride we cannot thank you enough for the support, 
contributions and guidance you have given us over the course of the last 
5 years. We could not have come as far as we have without you.

However, we are sad to say that the time has come for us to close the 
doors for opentick. However, this isn't goodbye. In fact, it's a new 
beginning. We will be introducing a fresh service with all the bells and 
whistles we've been slaving on over the course of the last year under a 
new name, a new website and a new level of service. Check back here in 
the near future for more information about the forthcoming new company 
and service. Of course amongst all these changes, there are some things 
that will remain the same - we still aim to provide a reliable free 
market data service, with an open architecture for a wide range of 
software platform support.

*If you are a current paying subscriber, this March billing cycle will 
be your final billing cycle; at your next billing date service will be 
terminated. If you are a delayed or historical data user not currently 
paying for service, your account will be deactivated as of Friday, March 
20th.*

Sincerely,
opentick Staff

/One day, we shall come back. Yes, we shall come back. Until then, there 
must be no regrets, no tears, no anxieties. Just go forward in all your 
beliefs, and prove to us we are not mistaken in ours./


From rbali at ufmg.br  Mon Apr 13 05:27:52 2009
From: rbali at ufmg.br (Robert Iquiapaza)
Date: Mon, 13 Apr 2009 00:27:52 -0300
Subject: [R-SIG-Finance] No intercep for First-Difference Estimator in PLM
	(panel data)
In-Reply-To: <49E0A3FA.5010900@braverock.com>
References: <137bba0a0904101655l555adc62v25b45f4798270721@mail.gmail.com>
	<49E0A3FA.5010900@braverock.com>
Message-ID: <D84350EE10E94C2481956E9BBB309B5B@HPR>

Hi

I want to estimate a FD model using plm, but with no intercept. It seems 
that plm ignores "intercept = FALSE".
And adding "+0" to the function produce an error.

#First-Difference Estimator
wag.fd1 <- plm(wage ~ marr, intercept = FALSE, data = wage2, model = "fd")

wag.fd2 <- plm(wage ~ marr+0, data = wage2, model = "fd")

> wag.fd2 <- plm(wage ~ marr+0, data = wage2, model = "fd")
Error in result[is.na(cond), ] <- NA :
  (subscript) logic subscript too long

My data looks like:
> wage2
   id time wage marr time1
1   1    1 1000    0     1
2   1    2 1050    0     2
3   1    3  950    0     3
4   1    4 1000    0     4
5   1    5 1100    0     5
6   1    6  900    0     6
7   2    1 2000    0     1
8   2    2 1950    0     2
9   2    3 2000    0     3
10  2    4 2000    0     4
11  2    5 1950    0     5
12  2    6 2100    0     6
13  3    1 2900    0     1
14  3    2 3000    0     2
15  3    3 3100    0     3
16  3    4 3500    1     4
17  3    5 3450    1     5
18  3    6 3550    1     6
19  4    1 3950    0     1
20  4    2 4050    0     2
21  4    3 4000    0     3
22  4    4 4500    1     4
23  4    5 4600    1     5
24  4    6 4400    1     6


I would appreciate if anyone knows how to specify no intercept for first 
difference model in panel data using plm?

Thank you

Robert


From chirantan at 2pirad.com  Tue Apr 14 14:15:49 2009
From: chirantan at 2pirad.com (Chirantan Kundu)
Date: Tue, 14 Apr 2009 17:45:49 +0530
Subject: [R-SIG-Finance] How do cubic spline coefficients contribute to
	yield formula?
Message-ID: <fc408ad80904140515q201d5076m83aa20c44d6492c6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090414/693fbfbf/attachment.pl>

From ezivot at u.washington.edu  Tue Apr 14 23:58:43 2009
From: ezivot at u.washington.edu (Eric Zivot)
Date: Tue, 14 Apr 2009 14:58:43 -0700 (PDT)
Subject: [R-SIG-Finance] EWMA covariance matrix
Message-ID: <Pine.LNX.4.43.0904141458430.26829@hymn13.u.washington.edu>

Does anyone have a suggestion for the best function to use to compute a EWMA covariance matrix? I see that the function cov.wt() (in stats) and cov.shrink (in corpcor) will compute a weighted covariance estimate. I assume I can just use exponentially declining weights as follows

> set.seed(123)
> testData = matrix(rnorm(100), 50, 2)
> lam = 0.9
> i = 0:49
> ewma.wt = lam^i
> ewma.wt = ewma.wt/sum(ewma.wt)
> cov.ewma = cov.wt(testData, wt=rev(ewma.wt))
> cov.ewma
$cov
            [,1]       [,2]
[1,]  0.8374426 -0.1366568
[2,] -0.1366568  1.0205296

$center
[1] 0.0456912 0.3136311

$n.obs
[1] 50

$wt
  [1] 0.0005756082 0.0006395647 0.0007106275 0.0007895861 0.0008773179
  [6] 0.0009747976 0.0010831085 0.0012034538 0.0013371709 0.0014857455
[11] 0.0016508283 0.0018342537 0.0020380597 0.0022645107 0.0025161230
[16] 0.0027956923 0.0031063247 0.0034514719 0.0038349688 0.0042610765
[21] 0.0047345294 0.0052605882 0.0058450980 0.0064945534 0.0072161704
[26] 0.0080179671 0.0089088523 0.0098987248 0.0109985831 0.0122206479
[31] 0.0135784977 0.0150872197 0.0167635774 0.0186261971 0.0206957746
[36] 0.0229953051 0.0255503390 0.0283892655 0.0315436284 0.0350484760
[41] 0.0389427511 0.0432697234 0.0480774705 0.0534194116 0.0593549018
[46] 0.0659498909 0.0732776566 0.0814196184 0.0904662427 0.1005180474


****************************************************************
*  Eric Zivot                  			               *
*  Professor and Gary Waterman Distinguished Scholar           *
*  Department of Economics                                     *
*  Adjunct Professor of Finance                                *
*  Adjunct Professor of Statistics
*  Box 353330                  email:  ezivot at u.washington.edu *
*  University of Washington    phone:  206-543-6715            *
*  Seattle, WA 98195-3330                                      *                                                           *
*  www:  http://faculty.washington.edu/ezivot                  *


From rbali at ufmg.br  Wed Apr 15 17:10:03 2009
From: rbali at ufmg.br (Robert Iquiapaza)
Date: Wed, 15 Apr 2009 12:10:03 -0300
Subject: [R-SIG-Finance] R: No intercep for First-Difference Estimator
	in PLM(panel data) - follow-up
In-Reply-To: <28643F754DDB094D8A875617EC4398B202AE7A0B@BEMAILEXTV03.corp.generali.net>
References: <28643F754DDB094D8A875617EC4398B202AE7A0B@BEMAILEXTV03.corp.generali.net>
Message-ID: <083A04FB888B4D1E8F99587F1F46E89D@HPR>

Thanks Giovanni,

Until the bug is fixed, I used the following code to get the result

dwage<-diff(wage2$wage,1)[wage2$time!=6]
dmarr<-diff(wage2$marr,1)[wage2$time!=6]

fd<-lm(dwage~dmarr+0)
summary(fd)
fd1<-lm(dwage~dmarr)
summary(fd1)

Robert
--------------------------------------------------
From: "Millo Giovanni" <Giovanni_Millo at Generali.com>
Sent: Wednesday, April 15, 2009 11:48 AM
To: "Robert Iquiapaza" <rbali at ufmg.br>; <r-sig-finance at stat.math.ethz.ch>
Cc: "Yves Croissant" <yves.croissant at let.ish-lyon.cnrs.fr>; "Christian 
Kleiber" <christian.kleiber at unibas.ch>
Subject: R: [R-SIG-Finance] No intercep for First-Difference Estimator in 
PLM(panel data) - follow-up

> Dear Robert, dear list,
>
> an update following a short discussion with the maintainer:
> the way to estimate an 'fd' model without intercept in plm() is the same 
> as in lm() etc., i.e. using one of the (equivalent) following formula 
> notations
> y~x+0
> y~x-1
> so the syntax in your second example is correct and should work.
> In fact it seems to be working fine unless there is only one regressor, as 
> in your example. This is a bug, probably stemming from some (nxk) matrix 
> degenerating into a vector as k=1 (maybe the most common bug ever in R?).
>
> A bug fix should appear soon; in the meantime, my observation on 
> consistency of the estimates of the model with intercept still applies. I
> take this chance to report an observation Yves just made, on which I 
> totally agree: estimating the intercept can be a useful diagnostic, as if
> it turns out significant this indicates some specification problem (the 
> difference of the intercept is nonzero=> the "true" intercept is not 
> constant, but somehow time-varying). You can get an example by estimating
> (a simplified version of-) the Arellano and Bond model:
>
>> data("EmplUK", package="plm")
>> myfdmod <- 
>> plm(log(emp)~log(wage)+log(capital)+log(output),data=EmplUK,model="fd")
>
> and looking at the summary().
>
> Best wishes,
> Giovanni
>
> Giovanni Millo
> Research Dept.,
> Assicurazioni Generali SpA
> Via Machiavelli 4,
> 34132 Trieste (Italy)
> tel. +39 040 671184
> fax  +39 040 671160
>
> -----Messaggio originale-----
> Da: Millo Giovanni
> Inviato: marted? 14 aprile 2009 16:26
> A: 'Robert Iquiapaza'; r-sig-finance at stat.math.ethz.ch
> Cc: Yves Croissant
> Oggetto: R: [R-SIG-Finance] No intercep for First-Difference Estimator in
> PLM(panel data)
>
>
> Dear Robert,
>
> in fact this should be automatic, at least at first sight, as the 
> intercept term should be differenced out. Yet there are cases where you 
> would want to retain it (see, e.g., Example 10.6 in Wooldridge, 
> Econometric Analysis of cross-section and panel data).
>
> We'll look into the formula specification. For now, let me just observe 
> that it doesn't do any harm to include the intercept anyway, as by 
> definition it is uncorrelated with the errors and the other regressors, so 
> your betas keep consistent if they were in the first place.
>
> Best wishes,
> Giovanni
>
> PS please put at least one of the authors in c/c next time
>
> Giovanni Millo
> Research Dept.,
> Assicurazioni Generali SpA
> Via Machiavelli 4,
> 34132 Trieste (Italy)
> tel. +39 040 671184
> fax  +39 040 671160
>
> -----Messaggio originale-----
> Da: Robert Iquiapaza [mailto:rbali at ufmg.br]
> Inviato: luned? 13 aprile 2009 05:28
> A: r-sig-finance at stat.math.ethz.ch
> Oggetto: [R-SIG-Finance] No intercep for First-Difference Estimator in 
> PLM(panel data)
>
>
> Hi
>
> I want to estimate a FD model using plm, but with no intercept. It seems
> that plm ignores "intercept = FALSE".
> And adding "+0" to the function produce an error.
>
> #First-Difference Estimator
> wag.fd1 <- plm(wage ~ marr, intercept = FALSE, data = wage2, model = "fd")
>
> wag.fd2 <- plm(wage ~ marr+0, data = wage2, model = "fd")
>
>> wag.fd2 <- plm(wage ~ marr+0, data = wage2, model = "fd")
> Error in result[is.na(cond), ] <- NA :
>  (subscript) logic subscript too long
>
> My data looks like:
>> wage2
>   id time wage marr time1
> 1   1    1 1000    0     1
> 2   1    2 1050    0     2
> 3   1    3  950    0     3
> 4   1    4 1000    0     4
> 5   1    5 1100    0     5
> 6   1    6  900    0     6
> 7   2    1 2000    0     1
> 8   2    2 1950    0     2
> 9   2    3 2000    0     3
> 10  2    4 2000    0     4
> 11  2    5 1950    0     5
> 12  2    6 2100    0     6
> 13  3    1 2900    0     1
> 14  3    2 3000    0     2
> 15  3    3 3100    0     3
> 16  3    4 3500    1     4
> 17  3    5 3450    1     5
> 18  3    6 3550    1     6
> 19  4    1 3950    0     1
> 20  4    2 4050    0     2
> 21  4    3 4000    0     3
> 22  4    4 4500    1     4
> 23  4    5 4600    1     5
> 24  4    6 4400    1     6
>
>
> I would appreciate if anyone knows how to specify no intercept for first
> difference model in panel data using plm?
>
> Thank you
>
> Robert 


From jeff.a.ryan at gmail.com  Fri Apr 17 18:52:20 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 17 Apr 2009 11:52:20 -0500
Subject: [R-SIG-Finance] R/Finance 2009 Schedule and Updates
Message-ID: <e8e755250904170952r2fd2e7cfw4862a4ce8ab22fac@mail.gmail.com>

R Finance Community:


The final schedule for the first R/Finance 2009: Applied Finance with
R conference in Chicago on
April 24 and 25, 2009 is now available!

The downloadable pdf is available from here: http://www.RinFinance.com/agenda

The formal conference will begin at 3:00PM on Friday on the East
Campus of UIC, in Lecture Center A1.  A formal reception will follow
Friday's talks.

Saturday will begin with breakfast at 8:00AM, with talks starting
promptly at 9:00AM.  The conference will wrap up Saturday at 6:30PM.

Pre-conference tutorials start at Noon on Friday and will be held at
Student Center East rooms 603 and 613, with more details available on
site. *All the pre-conference tutorials are currently sold out.*

************************************************************************
The final registration deadline has been pushed up to Monday!
************************************************************************


****NO REGISTRATIONS will be possible AFTER MONDAY APRIL 20th****

To register, go to:

http://www.RinFinance.com



On behalf of the organizing committee.


R/Finance 2009 Sponsors:
- International Center for Futures and Derivatives at UIC
- REvolution Computing
- Windows HPC Server
- ia: insight algorithmics


From bearxu83 at gmail.com  Fri Apr 17 23:58:46 2009
From: bearxu83 at gmail.com (BearXu)
Date: Fri, 17 Apr 2009 22:58:46 +0100
Subject: [R-SIG-Finance] error in tangencyPortfolio when Short
Message-ID: <82527b5d0904171458o9d168ai7df5124fc77f4c18@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090417/4a81bcf7/attachment.pl>

From bearxu83 at gmail.com  Fri Apr 17 23:59:38 2009
From: bearxu83 at gmail.com (BearXu)
Date: Fri, 17 Apr 2009 22:59:38 +0100
Subject: [R-SIG-Finance] error in tangencyPortfolio when Short
In-Reply-To: <82527b5d0904171458o9d168ai7df5124fc77f4c18@mail.gmail.com>
References: <82527b5d0904171458o9d168ai7df5124fc77f4c18@mail.gmail.com>
Message-ID: <82527b5d0904171459s62762331ufcee5f68385d45df@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090417/d4d07c32/attachment.pl>

From bearxu83 at gmail.com  Sat Apr 18 00:34:10 2009
From: bearxu83 at gmail.com (BearXu)
Date: Fri, 17 Apr 2009 23:34:10 +0100
Subject: [R-SIG-Finance] error in tangencyPortfolio when Short
In-Reply-To: <82527b5d0904171459s62762331ufcee5f68385d45df@mail.gmail.com>
References: <82527b5d0904171458o9d168ai7df5124fc77f4c18@mail.gmail.com> 
	<82527b5d0904171459s62762331ufcee5f68385d45df@mail.gmail.com>
Message-ID: <82527b5d0904171534s605c2913p8d95c1740a7ad471@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090417/55d7e22a/attachment.pl>

From brian at braverock.com  Sat Apr 18 01:34:06 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 17 Apr 2009 18:34:06 -0500
Subject: [R-SIG-Finance] error in tangencyPortfolio when Short
In-Reply-To: <82527b5d0904171534s605c2913p8d95c1740a7ad471@mail.gmail.com>
References: <82527b5d0904171458o9d168ai7df5124fc77f4c18@mail.gmail.com>
	<82527b5d0904171459s62762331ufcee5f68385d45df@mail.gmail.com>
	<82527b5d0904171534s605c2913p8d95c1740a7ad471@mail.gmail.com>
Message-ID: <49E911EE.2090004@braverock.com>

BearXu wrote:
> I also tried setSolver= ?solveRshortExact?
>
> but fail too.
>
> 2009/4/17 BearXu <bearxu83 at gmail.com>
>
>   
>> Oh, a typo.
>>
>> There is NO problem when I use "LongOnly"
>>
>> 2009/4/17 BearXu <bearxu83 at gmail.com>
>>
>>     
>>> When I was forming a portfolio in fPortfolio.
>>>
>>> I set:
>>>
>>> Constraints=?Short?
>>>
>>> setSolver= ?solveRquadprog?
>>>
>>> tg=tangencyPortfolio(tt, Spec, Constraints)
>>>
>>> then get an error:
>>>
>>> Error in .rquadprog(Dmat = args$Dmat, dvec = args$dvec, Amat = args$Amat,
>>> :
>>>   NA/NaN/Inf in foreign function call (arg 8)
>>>
>>> If I use "LongOnly", there is NO problem?
>>>
>>> who can help me to fix it?thanks
>>>
>>>
>>>       
You haven't provided a reproducible example per the posting guidelines. 
While you provide your code, you have not provided the data set that 
produces the error. Perhaps in the name of self-sufficiency you should 
also learn to use the R "debug" command, so that you can report where in 
the function it breaks.

Regards,

- Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From sjansevanrensburg at gmail.com  Sat Apr 18 14:47:16 2009
From: sjansevanrensburg at gmail.com (Stefan Janse van Rensburg)
Date: Sat, 18 Apr 2009 14:47:16 +0200
Subject: [R-SIG-Finance] mark areas on time series plot
Message-ID: <68cc1efe0904180547m7bf1fd5audb7daf795cbad22e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090418/6533ba48/attachment.pl>

From ggrothendieck at gmail.com  Sat Apr 18 15:14:14 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 18 Apr 2009 09:14:14 -0400
Subject: [R-SIG-Finance] mark areas on time series plot
In-Reply-To: <68cc1efe0904180547m7bf1fd5audb7daf795cbad22e@mail.gmail.com>
References: <68cc1efe0904180547m7bf1fd5audb7daf795cbad22e@mail.gmail.com>
Message-ID: <971536df0904180614s65fa1926pa37d45a71bdf2af4@mail.gmail.com>

Lines <- '"Time","IEMP (rand/US$) Index","Distress"
01/08/81,-0.02,0
01/09/81,0.08,0
01/10/81,-0.09,0
01/11/81,0.05,0
01/12/81,0.11,0
01/01/82,0.05,0
01/02/82,-0.04,0
01/03/82,0.07,0
01/04/82,0.07,0
01/05/82,0,0
01/06/82,0.06,0
01/07/82,-0.02,0
01/08/82,0.07,0
01/09/82,-0.11,0
01/10/82,0.04,0
01/11/82,-0.36,0
01/12/82,-0.01,0
01/01/83,-0.21,0
01/02/83,-0.16,1
01/03/83,0.19,1
01/04/83,-0.06,1
01/05/83,0.06,1
01/06/83,0.13,1
01/07/83,-0.01,1
01/08/83,0,1
01/09/83,-0.01,1
01/10/83,0.06,1
01/11/83,0.09,1
01/12/83,0.04,1
01/01/84,0.02,1
01/02/84,-0.01,1
01/03/84,0.03,1
01/04/84,0.03,1
01/05/84,0,1
01/06/84,0.03,1
01/07/84,0.13,1
01/08/84,0.18,1
01/09/84,0.07,1
01/10/84,0.12,1
01/11/84,-0.16,1
01/12/84,0.13,1
01/01/85,-0.06,1
01/02/85,-0.07,1
01/03/85,0.04,1
01/04/85,-0.12,1
01/05/85,-0.01,1
01/06/85,-0.17,1
01/07/85,0.09,1
01/08/85,0.08,1
01/09/85,0.08,1
01/10/85,-0.07,1
01/11/85,-0.01,1
01/12/85,-0.01,1
01/01/86,-0.26,1
01/02/86,-0.11,1
01/03/86,0.02,1
01/04/86,0.01,1
01/05/86,0.04,1
01/06/86,0.12,1
01/07/86,-0.05,0
01/08/86,-0.08,0
01/09/86,-0.17,0
01/10/86,-0.06,0
01/11/86,0,0
01/12/86,0.02,0
01/01/87,-0.16,0
01/02/87,-0.05,0
01/03/87,-0.04,0
01/04/87,0.01,0
01/05/87,0.03,0
01/06/87,0.01,0
01/07/87,0,0
01/08/87,0.02,0
01/09/87,0,0
01/10/87,-0.02,0
01/11/87,0.02,0
01/12/87,0.01,0
01/01/88,0.02,0
01/02/88,0.09,0
01/03/88,0.09,0
01/04/88,0.04,0
01/05/88,0.09,0
01/06/88,-0.05,0
01/07/88,0.11,0
01/08/88,0.08,0
01/09/88,-0.04,0
01/10/88,0.08,0
01/11/88,0.01,0
01/12/88,-0.02,0
01/01/89,0,0
01/02/89,0.07,0
01/03/89,0.06,0
01/04/89,0.03,0
01/05/89,0.09,0
01/06/89,0.02,0
01/07/89,-0.04,0
01/08/89,-0.01,0
01/09/89,0.01,0
01/10/89,-0.01,0
01/11/89,-0.03,0
01/12/89,-0.02,0
01/01/90,-0.02,0
01/02/90,0,0
01/03/90,0.01,0
01/04/90,0.04,0
01/05/90,-0.01,0
01/06/90,0.01,0
01/07/90,-0.02,0
01/08/90,-0.05,0
01/09/90,0,0
01/10/90,-0.01,0
01/11/90,-0.03,0
01/12/90,0.02,0
01/01/91,-0.01,0
01/02/91,-0.03,0
01/03/91,0.04,0
01/04/91,0.05,0
01/05/91,0.01,0
01/06/91,0.04,0
01/07/91,0.01,0
01/08/91,-0.02,0
01/09/91,-0.02,0
01/10/91,-0.02,0
01/11/91,-0.04,0
01/12/91,0.02,0
01/01/92,-0.05,0
01/02/92,-0.01,0
01/03/92,0.01,0
01/04/92,-0.04,0
01/05/92,-0.08,0
01/06/92,0,0
01/07/92,-0.08,0
01/08/92,-0.08,0
01/09/92,0.04,0
01/10/92,0.02,0
01/11/92,0.04,0
01/12/92,0.06,0
01/01/93,0,0
01/02/93,0,0
01/03/93,0.04,0
01/04/93,-0.05,0
01/05/93,0.08,0
01/06/93,0.04,0
01/07/93,0.05,0
01/08/93,0,0
01/09/93,0,0
01/10/93,-0.09,0
01/11/93,-0.03,0
01/12/93,-0.09,0
01/01/94,0.01,0
01/02/94,0.02,0
01/03/94,0.05,0
01/04/94,0.07,0
01/05/94,0.07,0
01/06/94,-0.01,0
01/07/94,-0.03,0
01/08/94,-0.08,0
01/09/94,0.06,0
01/10/94,-0.03,0
01/11/94,0.01,0
01/12/94,0,0
01/01/95,-0.01,0
01/02/95,0.02,0
01/03/95,0,0
01/04/95,0.07,0
01/05/95,-0.03,0
01/06/95,0.02,0
01/07/95,-0.01,0
01/08/95,0,0
01/09/95,0.01,0
01/10/95,-0.02,0
01/11/95,-0.03,0
01/12/95,0,0
01/01/96,-0.01,0
01/02/96,0.05,1
01/03/96,0.08,1
01/04/96,0.16,1
01/05/96,0.1,1
01/06/96,-0.07,1
01/07/96,0.03,1
01/08/96,0.06,1
01/09/96,-0.05,1
01/10/96,0.01,1
01/11/96,0.03,0
01/12/96,0.04,0
01/01/97,-0.07,0
01/02/97,-0.06,0
01/03/97,-0.02,0
01/04/97,-0.02,0
01/05/97,-0.1,0
01/06/97,-0.01,0
01/07/97,0,0
01/08/97,-0.01,0
01/09/97,-0.01,0
01/10/97,-0.01,0
01/11/97,0.04,0
01/12/97,0.01,0
01/01/98,-0.01,0
01/02/98,-0.05,0
01/03/98,-0.04,0
01/04/98,0.01,0
01/05/98,0.1,1
01/06/98,0.2,1
01/07/98,0.25,1
01/08/98,0.1,1
01/09/98,-0.08,0
01/10/98,-0.11,0
01/11/98,-0.07,0
01/12/98,0.01,0
01/01/99,-0.02,0
01/02/99,-0.02,0
01/03/99,-0.02,0
01/04/99,-0.07,0
01/05/99,0.03,0
01/06/99,-0.06,0
01/07/99,-0.07,0
01/08/99,0,0
01/09/99,-0.06,0
01/10/99,-0.02,0
01/11/99,0,0
01/12/99,0,0
01/01/00,-0.07,0
01/02/00,0.03,0
01/03/00,0.02,0
01/04/00,0.04,0
01/05/00,0.09,0
01/06/00,-0.03,0
01/07/00,-0.01,0
01/08/00,0.01,0
01/09/00,0.03,0
01/10/00,0.04,0
01/11/00,0.02,0
01/12/00,0,0
01/01/01,0,0
01/02/01,0.01,0
01/03/01,0.02,0
01/04/01,0.03,0
01/05/01,-0.02,0
01/06/01,-0.04,1
01/07/01,-0.01,1
01/08/01,0.01,1
01/09/01,-0.01,1
01/10/01,0.06,1
01/11/01,0.04,1
01/12/01,0.21,1
01/01/02,0.02,0
01/02/02,-0.01,0
01/03/02,0.07,0
01/04/02,0.01,0
01/05/02,-0.04,0
01/06/02,-0.01,0
01/07/02,-0.01,0
01/08/02,0.07,0
01/09/02,0.04,0
01/10/02,-0.02,0
01/11/02,-0.07,0
01/12/02,-0.05,0
01/01/03,-0.03,0
01/02/03,-0.02,0
01/03/03,-0.03,0
01/04/03,-0.03,0
01/05/03,-0.06,0
01/06/03,-0.05,0
01/07/03,-0.04,0
01/08/03,-0.07,0
01/09/03,-0.09,0
01/10/03,-0.13,0
01/11/03,-0.09,0
01/12/03,0,0
01/01/04,0.06,0
01/02/04,-0.02,0
01/03/04,-0.02,0
01/04/04,-0.04,0
01/05/04,0.04,0
01/06/04,-0.05,0
01/07/04,-0.05,0
01/08/04,-0.02,0
01/09/04,0.01,0
01/10/04,-0.01,0
01/11/04,-0.07,0
01/12/04,-0.04,0
01/01/05,0.02,0
01/02/05,0,0
01/03/05,-0.02,0
01/04/05,-0.02,0
01/05/05,0.01,0
01/06/05,0.06,0
01/07/05,-0.01,0
01/08/05,-0.04,0
01/09/05,-0.01,0
01/10/05,0.03,0
01/11/05,0.02,0
01/12/05,-0.06,0
01/01/06,-0.06,0
01/02/06,-0.01,0
01/03/06,0.02,0
01/04/06,-0.02,0
01/05/06,0.04,0
01/06/06,0.13,0
01/07/06,0.07,0
01/08/06,-0.02,0
01/09/06,0.08,0
01/10/06,0.06,0
01/11/06,-0.05,0
01/12/06,-0.01,0
01/01/07,0.05,0
01/02/07,-0.04,0
01/03/07,0.01,0
01/04/07,-0.02,0
01/05/07,0.01,0
01/06/07,0.06,0
01/07/07,-0.05,0
01/08/07,0.07,0
01/09/07,0,0
01/10/07,-0.01,0
01/11/07,0.02,0
01/12/07,0.02,0
01/01/08,0,0
01/02/08,0.08,0'

library(zoo)
z <- read.zoo(textConnection(Lines), format = "%d/%m/%y", sep = ",",
header = TRUE, col.names = c("", "IEMP", "Distress"))

plot(cbind(z$IEMP, ifelse(z$Distress, z, NA)), col = 1:2, screen = 1,
ylab = "IEMP")
legend("bottomright", c("Normal", "Distress"), lty = 1, col = 1:2)


From bearxu83 at gmail.com  Sun Apr 19 00:52:52 2009
From: bearxu83 at gmail.com (BearXu)
Date: Sat, 18 Apr 2009 23:52:52 +0100
Subject: [R-SIG-Finance] Best Sharpe Ratio
Message-ID: <82527b5d0904181552u387a0f1fx42415a36236b87ac@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090418/c3f9d8df/attachment.pl>

From bearxu83 at gmail.com  Sun Apr 19 01:17:43 2009
From: bearxu83 at gmail.com (BearXu)
Date: Sun, 19 Apr 2009 00:17:43 +0100
Subject: [R-SIG-Finance] Best Sharpe Ratio
In-Reply-To: <82527b5d0904181552u387a0f1fx42415a36236b87ac@mail.gmail.com>
References: <82527b5d0904181552u387a0f1fx42415a36236b87ac@mail.gmail.com>
Message-ID: <82527b5d0904181617h7962874eg12f12537c008487d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090419/a3d8e507/attachment.pl>

From bearxu83 at gmail.com  Sun Apr 19 01:57:31 2009
From: bearxu83 at gmail.com (BearXu)
Date: Sun, 19 Apr 2009 00:57:31 +0100
Subject: [R-SIG-Finance] Best Sharpe Ratio
In-Reply-To: <82527b5d0904181617h7962874eg12f12537c008487d@mail.gmail.com>
References: <82527b5d0904181552u387a0f1fx42415a36236b87ac@mail.gmail.com> 
	<82527b5d0904181617h7962874eg12f12537c008487d@mail.gmail.com>
Message-ID: <82527b5d0904181657o2fe77f51h40edcf38ec296d09@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090419/7ec28f13/attachment.pl>

From sjansevanrensburg at gmail.com  Sun Apr 19 12:10:57 2009
From: sjansevanrensburg at gmail.com (Stefan Janse van Rensburg)
Date: Sun, 19 Apr 2009 12:10:57 +0200
Subject: [R-SIG-Finance] mark areas on time series plot
In-Reply-To: <971536df0904180614s65fa1926pa37d45a71bdf2af4@mail.gmail.com>
References: <68cc1efe0904180547m7bf1fd5audb7daf795cbad22e@mail.gmail.com>
	<971536df0904180614s65fa1926pa37d45a71bdf2af4@mail.gmail.com>
Message-ID: <68cc1efe0904190310r3db603f6pa63669f08875bc72@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090419/93708504/attachment.pl>

From bearxu83 at gmail.com  Sun Apr 19 15:24:47 2009
From: bearxu83 at gmail.com (BearXu)
Date: Sun, 19 Apr 2009 14:24:47 +0100
Subject: [R-SIG-Finance] Best Sharpe Ratio
In-Reply-To: <82527b5d0904181657o2fe77f51h40edcf38ec296d09@mail.gmail.com>
References: <82527b5d0904181552u387a0f1fx42415a36236b87ac@mail.gmail.com> 
	<82527b5d0904181617h7962874eg12f12537c008487d@mail.gmail.com> 
	<82527b5d0904181657o2fe77f51h40edcf38ec296d09@mail.gmail.com>
Message-ID: <82527b5d0904190624l4168303cn1bd23ffa1a34cd97@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090419/5cff70f4/attachment.pl>

From bearxu83 at gmail.com  Mon Apr 20 02:34:06 2009
From: bearxu83 at gmail.com (BearXu)
Date: Mon, 20 Apr 2009 01:34:06 +0100
Subject: [R-SIG-Finance] [PerformanceAnalytics]why riskfree rate in
	table.capm can't be a vector?
Message-ID: <82527b5d0904191734x57907ffet6549e2a6120e98b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090420/d44cbe03/attachment.pl>

From gnolffilc at gmail.com  Mon Apr 20 04:01:23 2009
From: gnolffilc at gmail.com (gnolffilc at gmail.com)
Date: Mon, 20 Apr 2009 02:01:23 +0000
Subject: [R-SIG-Finance] mark areas on time series plot
In-Reply-To: <68cc1efe0904190310r3db603f6pa63669f08875bc72@mail.gmail.com>
Message-ID: <00032555150a416a530467f2e888@google.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090420/160d49f9/attachment.pl>

From ggrothendieck at gmail.com  Mon Apr 20 04:14:42 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 19 Apr 2009 22:14:42 -0400
Subject: [R-SIG-Finance] mark areas on time series plot
In-Reply-To: <00032555150a416a530467f2e888@google.com>
References: <68cc1efe0904190310r3db603f6pa63669f08875bc72@mail.gmail.com> 
	<00032555150a416a530467f2e888@google.com>
Message-ID: <971536df0904191914k7322b20akba237cb965207cc6@mail.gmail.com>

Upgrade to the latest version of zoo.

On Sun, Apr 19, 2009 at 10:01 PM,  <gnolffilc at gmail.com> wrote:
> Gabor (or anyone who may know),
>
> I was trying to learn something by following the example. I "cut and pasted"
> the code and data from the earlier e-mails. When I run the code with the
> provided data, the portion up to and including creating the zoo object (z <-
> read.zoo ...) appears to work just fine. However, when I try to run the code
> for creating the following plot, I get an error message (shown below).
>
>
>> plot(cbind(z$IEMP, ifelse(z$Distress, z, NA)), col = 1:2, screen = 1,
> + ylab = "IEMP")
> Error in z$IEMP : $ operator is invalid for atomic vectors
>
> I'm running R version 2.8.1 on a Windows XP machine. I'm using Package zoo
> version 1.4-1.
>
> Any idea what I've done wrong? (I figure it's probably obvious to everyone
> but me.)
>
> I'm new to 'R', and am very interested in learning to use it for various
> tasks. I'm using it in a course on Bayesian analysis and Monte Carlo
> simulation (I've graduated in 2006 with an M.Sc., and am auditing this
> course for no credit/grade). I'm also trying to learn R for more common
> regression and ANOVA methods, and am very interested in econometrics and
> time series. While I've read (and have followed along with) various books
> and several R .pdf documents, I still find it a challenge (which I'm sure
> reflects more on me than on anything else).
>
> Thank you.
>
> Cliff Long
>
>
> On Apr 19, 2009 5:10am, Stefan Janse van Rensburg
> <sjansevanrensburg at gmail.com> wrote:
>> Thank you Gabor, that worked well.
>>
>>
>>
>>
>>
>> Kind regards,
>>
>>
>>
>>
>>
>> Stefan Janse van Rensburg
>>
>>
>>
>>
>>
>> 2009/4/18 Gabor Grothendieck ggrothendieck at gmail.com>
>>
>>
>>
>>
>>
>> > Lines
>>
>> > 01/08/81,-0.02,0
>>
>>
>> > 01/09/81,0.08,0
>>
>>
>> > 01/10/81,-0.09,0
>>
>>
>> > 01/11/81,0.05,0
>>
>>
>> > 01/12/81,0.11,0
>>
>>
>> > 01/01/82,0.05,0
>>
>>
>> > 01/02/82,-0.04,0
>>
>>
>> > 01/03/82,0.07,0
>>
>>
>> > 01/04/82,0.07,0
>>
>>
>> > 01/05/82,0,0
>>
>>
>> > 01/06/82,0.06,0
>>
>>
>> > 01/07/82,-0.02,0
>>
>>
>> > 01/08/82,0.07,0
>>
>>
>> > 01/09/82,-0.11,0
>>
>>
>> > 01/10/82,0.04,0
>>
>>
>> > 01/11/82,-0.36,0
>>
>>
>> > 01/12/82,-0.01,0
>>
>>
>> > 01/01/83,-0.21,0
>>
>>
>> > 01/02/83,-0.16,1
>>
>>
>> > 01/03/83,0.19,1
>>
>>
>> > 01/04/83,-0.06,1
>>
>>
>> > 01/05/83,0.06,1
>>
>>
>> > 01/06/83,0.13,1
>>
>>
>> > 01/07/83,-0.01,1
>>
>>
>> > 01/08/83,0,1
>>
>>
>> > 01/09/83,-0.01,1
>>
>>
>> > 01/10/83,0.06,1
>>
>>
>> > 01/11/83,0.09,1
>>
>>
>> > 01/12/83,0.04,1
>>
>>
>> > 01/01/84,0.02,1
>>
>>
>> > 01/02/84,-0.01,1
>>
>>
>> > 01/03/84,0.03,1
>>
>>
>> > 01/04/84,0.03,1
>>
>>
>> > 01/05/84,0,1
>>
>>
>> > 01/06/84,0.03,1
>>
>>
>> > 01/07/84,0.13,1
>>
>>
>> > 01/08/84,0.18,1
>>
>>
>> > 01/09/84,0.07,1
>>
>>
>> > 01/10/84,0.12,1
>>
>>
>> > 01/11/84,-0.16,1
>>
>>
>> > 01/12/84,0.13,1
>>
>>
>> > 01/01/85,-0.06,1
>>
>>
>> > 01/02/85,-0.07,1
>>
>>
>> > 01/03/85,0.04,1
>>
>>
>> > 01/04/85,-0.12,1
>>
>>
>> > 01/05/85,-0.01,1
>>
>>
>> > 01/06/85,-0.17,1
>>
>>
>> > 01/07/85,0.09,1
>>
>>
>> > 01/08/85,0.08,1
>>
>>
>> > 01/09/85,0.08,1
>>
>>
>> > 01/10/85,-0.07,1
>>
>>
>> > 01/11/85,-0.01,1
>>
>>
>> > 01/12/85,-0.01,1
>>
>>
>> > 01/01/86,-0.26,1
>>
>>
>> > 01/02/86,-0.11,1
>>
>>
>> > 01/03/86,0.02,1
>>
>>
>> > 01/04/86,0.01,1
>>
>>
>> > 01/05/86,0.04,1
>>
>>
>> > 01/06/86,0.12,1
>>
>>
>> > 01/07/86,-0.05,0
>>
>>
>> > 01/08/86,-0.08,0
>>
>>
>> > 01/09/86,-0.17,0
>>
>>
>> > 01/10/86,-0.06,0
>>
>>
>> > 01/11/86,0,0
>>
>>
>> > 01/12/86,0.02,0
>>
>>
>> > 01/01/87,-0.16,0
>>
>>
>> > 01/02/87,-0.05,0
>>
>>
>> > 01/03/87,-0.04,0
>>
>>
>> > 01/04/87,0.01,0
>>
>>
>> > 01/05/87,0.03,0
>>
>>
>> > 01/06/87,0.01,0
>>
>>
>> > 01/07/87,0,0
>>
>>
>> > 01/08/87,0.02,0
>>
>>
>> > 01/09/87,0,0
>>
>>
>> > 01/10/87,-0.02,0
>>
>>
>> > 01/11/87,0.02,0
>>
>>
>> > 01/12/87,0.01,0
>>
>>
>> > 01/01/88,0.02,0
>>
>>
>> > 01/02/88,0.09,0
>>
>>
>> > 01/03/88,0.09,0
>>
>>
>> > 01/04/88,0.04,0
>>
>>
>> > 01/05/88,0.09,0
>>
>>
>> > 01/06/88,-0.05,0
>>
>>
>> > 01/07/88,0.11,0
>>
>>
>> > 01/08/88,0.08,0
>>
>>
>> > 01/09/88,-0.04,0
>>
>>
>> > 01/10/88,0.08,0
>>
>>
>> > 01/11/88,0.01,0
>>
>>
>> > 01/12/88,-0.02,0
>>
>>
>> > 01/01/89,0,0
>>
>>
>> > 01/02/89,0.07,0
>>
>>
>> > 01/03/89,0.06,0
>>
>>
>> > 01/04/89,0.03,0
>>
>>
>> > 01/05/89,0.09,0
>>
>>
>> > 01/06/89,0.02,0
>>
>>
>> > 01/07/89,-0.04,0
>>
>>
>> > 01/08/89,-0.01,0
>>
>>
>> > 01/09/89,0.01,0
>>
>>
>> > 01/10/89,-0.01,0
>>
>>
>> > 01/11/89,-0.03,0
>>
>>
>> > 01/12/89,-0.02,0
>>
>>
>> > 01/01/90,-0.02,0
>>
>>
>> > 01/02/90,0,0
>>
>>
>> > 01/03/90,0.01,0
>>
>>
>> > 01/04/90,0.04,0
>>
>>
>> > 01/05/90,-0.01,0
>>
>>
>> > 01/06/90,0.01,0
>>
>>
>> > 01/07/90,-0.02,0
>>
>>
>> > 01/08/90,-0.05,0
>>
>>
>> > 01/09/90,0,0
>>
>>
>> > 01/10/90,-0.01,0
>>
>>
>> > 01/11/90,-0.03,0
>>
>>
>> > 01/12/90,0.02,0
>>
>>
>> > 01/01/91,-0.01,0
>>
>>
>> > 01/02/91,-0.03,0
>>
>>
>> > 01/03/91,0.04,0
>>
>>
>> > 01/04/91,0.05,0
>>
>>
>> > 01/05/91,0.01,0
>>
>>
>> > 01/06/91,0.04,0
>>
>>
>> > 01/07/91,0.01,0
>>
>>
>> > 01/08/91,-0.02,0
>>
>>
>> > 01/09/91,-0.02,0
>>
>>
>> > 01/10/91,-0.02,0
>>
>>
>> > 01/11/91,-0.04,0
>>
>>
>> > 01/12/91,0.02,0
>>
>>
>> > 01/01/92,-0.05,0
>>
>>
>> > 01/02/92,-0.01,0
>>
>>
>> > 01/03/92,0.01,0
>>
>>
>> > 01/04/92,-0.04,0
>>
>>
>> > 01/05/92,-0.08,0
>>
>>
>> > 01/06/92,0,0
>>
>>
>> > 01/07/92,-0.08,0
>>
>>
>> > 01/08/92,-0.08,0
>>
>>
>> > 01/09/92,0.04,0
>>
>>
>> > 01/10/92,0.02,0
>>
>>
>> > 01/11/92,0.04,0
>>
>>
>> > 01/12/92,0.06,0
>>
>>
>> > 01/01/93,0,0
>>
>>
>> > 01/02/93,0,0
>>
>>
>> > 01/03/93,0.04,0
>>
>>
>> > 01/04/93,-0.05,0
>>
>>
>> > 01/05/93,0.08,0
>>
>>
>> > 01/06/93,0.04,0
>>
>>
>> > 01/07/93,0.05,0
>>
>>
>> > 01/08/93,0,0
>>
>>
>> > 01/09/93,0,0
>>
>>
>> > 01/10/93,-0.09,0
>>
>>
>> > 01/11/93,-0.03,0
>>
>>
>> > 01/12/93,-0.09,0
>>
>>
>> > 01/01/94,0.01,0
>>
>>
>> > 01/02/94,0.02,0
>>
>>
>> > 01/03/94,0.05,0
>>
>>
>> > 01/04/94,0.07,0
>>
>>
>> > 01/05/94,0.07,0
>>
>>
>> > 01/06/94,-0.01,0
>>
>>
>> > 01/07/94,-0.03,0
>>
>>
>> > 01/08/94,-0.08,0
>>
>>
>> > 01/09/94,0.06,0
>>
>>
>> > 01/10/94,-0.03,0
>>
>>
>> > 01/11/94,0.01,0
>>
>>
>> > 01/12/94,0,0
>>
>>
>> > 01/01/95,-0.01,0
>>
>>
>> > 01/02/95,0.02,0
>>
>>
>> > 01/03/95,0,0
>>
>>
>> > 01/04/95,0.07,0
>>
>>
>> > 01/05/95,-0.03,0
>>
>>
>> > 01/06/95,0.02,0
>>
>>
>> > 01/07/95,-0.01,0
>>
>>
>> > 01/08/95,0,0
>>
>>
>> > 01/09/95,0.01,0
>>
>>
>> > 01/10/95,-0.02,0
>>
>>
>> > 01/11/95,-0.03,0
>>
>>
>> > 01/12/95,0,0
>>
>>
>> > 01/01/96,-0.01,0
>>
>>
>> > 01/02/96,0.05,1
>>
>>
>> > 01/03/96,0.08,1
>>
>>
>> > 01/04/96,0.16,1
>>
>>
>> > 01/05/96,0.1,1
>>
>>
>> > 01/06/96,-0.07,1
>>
>>
>> > 01/07/96,0.03,1
>>
>>
>> > 01/08/96,0.06,1
>>
>>
>> > 01/09/96,-0.05,1
>>
>>
>> > 01/10/96,0.01,1
>>
>>
>> > 01/11/96,0.03,0
>>
>>
>> > 01/12/96,0.04,0
>>
>>
>> > 01/01/97,-0.07,0
>>
>>
>> > 01/02/97,-0.06,0
>>
>>
>> > 01/03/97,-0.02,0
>>
>>
>> > 01/04/97,-0.02,0
>>
>>
>> > 01/05/97,-0.1,0
>>
>>
>> > 01/06/97,-0.01,0
>>
>>
>> > 01/07/97,0,0
>>
>>
>> > 01/08/97,-0.01,0
>>
>>
>> > 01/09/97,-0.01,0
>>
>>
>> > 01/10/97,-0.01,0
>>
>>
>> > 01/11/97,0.04,0
>>
>>
>> > 01/12/97,0.01,0
>>
>>
>> > 01/01/98,-0.01,0
>>
>>
>> > 01/02/98,-0.05,0
>>
>>
>> > 01/03/98,-0.04,0
>>
>>
>> > 01/04/98,0.01,0
>>
>>
>> > 01/05/98,0.1,1
>>
>>
>> > 01/06/98,0.2,1
>>
>>
>> > 01/07/98,0.25,1
>>
>>
>> > 01/08/98,0.1,1
>>
>>
>> > 01/09/98,-0.08,0
>>
>>
>> > 01/10/98,-0.11,0
>>
>>
>> > 01/11/98,-0.07,0
>>
>>
>> > 01/12/98,0.01,0
>>
>>
>> > 01/01/99,-0.02,0
>>
>>
>> > 01/02/99,-0.02,0
>>
>>
>> > 01/03/99,-0.02,0
>>
>>
>> > 01/04/99,-0.07,0
>>
>>
>> > 01/05/99,0.03,0
>>
>>
>> > 01/06/99,-0.06,0
>>
>>
>> > 01/07/99,-0.07,0
>>
>>
>> > 01/08/99,0,0
>>
>>
>> > 01/09/99,-0.06,0
>>
>>
>> > 01/10/99,-0.02,0
>>
>>
>> > 01/11/99,0,0
>>
>>
>> > 01/12/99,0,0
>>
>>
>> > 01/01/00,-0.07,0
>>
>>
>> > 01/02/00,0.03,0
>>
>>
>> > 01/03/00,0.02,0
>>
>>
>> > 01/04/00,0.04,0
>>
>>
>> > 01/05/00,0.09,0
>>
>>
>> > 01/06/00,-0.03,0
>>
>>
>> > 01/07/00,-0.01,0
>>
>>
>> > 01/08/00,0.01,0
>>
>>
>> > 01/09/00,0.03,0
>>
>>
>> > 01/10/00,0.04,0
>>
>>
>> > 01/11/00,0.02,0
>>
>>
>> > 01/12/00,0,0
>>
>>
>> > 01/01/01,0,0
>>
>>
>> > 01/02/01,0.01,0
>>
>>
>> > 01/03/01,0.02,0
>>
>>
>> > 01/04/01,0.03,0
>>
>>
>> > 01/05/01,-0.02,0
>>
>>
>> > 01/06/01,-0.04,1
>>
>>
>> > 01/07/01,-0.01,1
>>
>>
>> > 01/08/01,0.01,1
>>
>>
>> > 01/09/01,-0.01,1
>>
>>
>> > 01/10/01,0.06,1
>>
>>
>> > 01/11/01,0.04,1
>>
>>
>> > 01/12/01,0.21,1
>>
>>
>> > 01/01/02,0.02,0
>>
>>
>> > 01/02/02,-0.01,0
>>
>>
>> > 01/03/02,0.07,0
>>
>>
>> > 01/04/02,0.01,0
>>
>>
>> > 01/05/02,-0.04,0
>>
>>
>> > 01/06/02,-0.01,0
>>
>>
>> > 01/07/02,-0.01,0
>>
>>
>> > 01/08/02,0.07,0
>>
>>
>> > 01/09/02,0.04,0
>>
>>
>> > 01/10/02,-0.02,0
>>
>>
>> > 01/11/02,-0.07,0
>>
>>
>> > 01/12/02,-0.05,0
>>
>>
>> > 01/01/03,-0.03,0
>>
>>
>> > 01/02/03,-0.02,0
>>
>>
>> > 01/03/03,-0.03,0
>>
>>
>> > 01/04/03,-0.03,0
>>
>>
>> > 01/05/03,-0.06,0
>>
>>
>> > 01/06/03,-0.05,0
>>
>>
>> > 01/07/03,-0.04,0
>>
>>
>> > 01/08/03,-0.07,0
>>
>>
>> > 01/09/03,-0.09,0
>>
>>
>> > 01/10/03,-0.13,0
>>
>>
>> > 01/11/03,-0.09,0
>>
>>
>> > 01/12/03,0,0
>>
>>
>> > 01/01/04,0.06,0
>>
>>
>> > 01/02/04,-0.02,0
>>
>>
>> > 01/03/04,-0.02,0
>>
>>
>> > 01/04/04,-0.04,0
>>
>>
>> > 01/05/04,0.04,0
>>
>>
>> > 01/06/04,-0.05,0
>>
>>
>> > 01/07/04,-0.05,0
>>
>>
>> > 01/08/04,-0.02,0
>>
>>
>> > 01/09/04,0.01,0
>>
>>
>> > 01/10/04,-0.01,0
>>
>>
>> > 01/11/04,-0.07,0
>>
>>
>> > 01/12/04,-0.04,0
>>
>>
>> > 01/01/05,0.02,0
>>
>>
>> > 01/02/05,0,0
>>
>>
>> > 01/03/05,-0.02,0
>>
>>
>> > 01/04/05,-0.02,0
>>
>>
>> > 01/05/05,0.01,0
>>
>>
>> > 01/06/05,0.06,0
>>
>>
>> > 01/07/05,-0.01,0
>>
>>
>> > 01/08/05,-0.04,0
>>
>>
>> > 01/09/05,-0.01,0
>>
>>
>> > 01/10/05,0.03,0
>>
>>
>> > 01/11/05,0.02,0
>>
>>
>> > 01/12/05,-0.06,0
>>
>>
>> > 01/01/06,-0.06,0
>>
>>
>> > 01/02/06,-0.01,0
>>
>>
>> > 01/03/06,0.02,0
>>
>>
>> > 01/04/06,-0.02,0
>>
>>
>> > 01/05/06,0.04,0
>>
>>
>> > 01/06/06,0.13,0
>>
>>
>> > 01/07/06,0.07,0
>>
>>
>> > 01/08/06,-0.02,0
>>
>>
>> > 01/09/06,0.08,0
>>
>>
>> > 01/10/06,0.06,0
>>
>>
>> > 01/11/06,-0.05,0
>>
>>
>> > 01/12/06,-0.01,0
>>
>>
>> > 01/01/07,0.05,0
>>
>>
>> > 01/02/07,-0.04,0
>>
>>
>> > 01/03/07,0.01,0
>>
>>
>> > 01/04/07,-0.02,0
>>
>>
>> > 01/05/07,0.01,0
>>
>>
>> > 01/06/07,0.06,0
>>
>>
>> > 01/07/07,-0.05,0
>>
>>
>> > 01/08/07,0.07,0
>>
>>
>> > 01/09/07,0,0
>>
>>
>> > 01/10/07,-0.01,0
>>
>>
>> > 01/11/07,0.02,0
>>
>>
>> > 01/12/07,0.02,0
>>
>>
>> > 01/01/08,0,0
>>
>>
>> > 01/02/08,0.08,0'
>>
>>
>> >
>>
>>
>> > library(zoo)
>>
>>
>> > z
>>
>> > header = TRUE, col.names = c("", "IEMP", "Distress"))
>>
>>
>> >
>>
>>
>> > plot(cbind(z$IEMP, ifelse(z$Distress, z, NA)), col = 1:2, screen = 1,
>>
>>
>> > ylab = "IEMP")
>>
>>
>> > legend("bottomright", c("Normal", "Distress"), lty = 1, col = 1:2)
>>
>>
>> >
>>
>>
>>
>>
>>
>> ? ? ? ?[[alternative HTML version deleted]]
>>
>>
>>
>>
>>
>> _______________________________________________
>>
>>
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>
>>
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>
>>
>> -- Subscriber-posting only.
>>
>>
>> -- If you want to post, subscribe first.
>>
>>
>>


From gnolffilc at gmail.com  Mon Apr 20 04:46:07 2009
From: gnolffilc at gmail.com (gnolffilc at gmail.com)
Date: Mon, 20 Apr 2009 02:46:07 +0000
Subject: [R-SIG-Finance] mark areas on time series plot
In-Reply-To: <971536df0904191914k7322b20akba237cb965207cc6@mail.gmail.com>
Message-ID: <0003255746de3f277e0467f38877@google.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090420/fc2114a7/attachment.pl>

From kafka at centras.lt  Mon Apr 20 11:53:49 2009
From: kafka at centras.lt (kafkaz)
Date: Mon, 20 Apr 2009 02:53:49 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] xts and to.weekly function
Message-ID: <23133625.post@talk.nabble.com>


Hello,
I have a xts object, which contains daily data. The object contains Saturday
quotes and the function "to.weekly" returns closing prices of Saturday. How
can I get Friday's closing price through xts functionality? I was looking at
the "endpoints" function, but it seems, that it is impossible through that
function.
Thank you. 
-- 
View this message in context: http://www.nabble.com/xts-and-to.weekly-function-tp23133625p23133625.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From josh.m.ulrich at gmail.com  Mon Apr 20 15:56:34 2009
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Mon, 20 Apr 2009 08:56:34 -0500
Subject: [R-SIG-Finance] [R-sig-finance] xts and to.weekly function
In-Reply-To: <23133625.post@talk.nabble.com>
References: <23133625.post@talk.nabble.com>
Message-ID: <8cca69990904200656q40af54dkb008aed9343edaa4@mail.gmail.com>

Does this provide what you want?

require(quantmod)
getSymbols("USD/EUR",src="oanda",from="2009-01-01")

# endpoints(..., on='weeks') returns Sundays
fri <- endpoints(USDEUR, on='weeks')-2

# replace first observation with zero, and remove last observation
# see ?endpoints
fri[1] <- 0;  fri <- head(fri,-1)

# data series of only Fridays
USDEUR[fri]

HTH,
Josh
--
http://quantemplation.blogspot.com
http://www.fosstrading.com



On Mon, Apr 20, 2009 at 4:53 AM, kafkaz <kafka at centras.lt> wrote:
>
> Hello,
> I have a xts object, which contains daily data. The object contains Saturday
> quotes and the function "to.weekly" returns closing prices of Saturday. How
> can I get Friday's closing price through xts functionality? I was looking at
> the "endpoints" function, but it seems, that it is impossible through that
> function.
> Thank you.
> --
> View this message in context: http://www.nabble.com/xts-and-to.weekly-function-tp23133625p23133625.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From schreiber_irene at web.de  Mon Apr 20 16:28:32 2009
From: schreiber_irene at web.de (Irene Schreiber)
Date: Mon, 20 Apr 2009 16:28:32 +0200
Subject: [R-SIG-Finance] How estimate VAR(p)-model robustly?
Message-ID: <000a01c9c1c4$486243c0$d926cb40$@de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090420/1d291fb0/attachment.pl>

From brian at braverock.com  Mon Apr 20 19:21:38 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 20 Apr 2009 12:21:38 -0500
Subject: [R-SIG-Finance] How estimate VAR(p)-model robustly?
In-Reply-To: <000a01c9c1c4$486243c0$d926cb40$@de>
References: <000a01c9c1c4$486243c0$d926cb40$@de>
Message-ID: <49ECAF22.6080504@braverock.com>

Irene Schreiber wrote:
> Hello,
>
>  
>
> Does anyone know about robust estimation of vector autoregressive models
> (VAR(p)) in R? Or in Matlab?
>
> Currently I am using the function ar().
>
> The problem is, that the variances of my data change a lot with time, and we
> also have some outliers in the data. That is why, I presume, that we would
> get quite different results when estimating robustly.
>
>  
>
> I would be very grateful if someone could help!
>
> Thanks a lot!
>
>  
>
> Irene.
>   
I'll try to remember to respond in greater detail after 
http://www.RinFinance.com/ this Friday/Saturday, but I'll suggest two 
avenues now.

A Bayesian smoothing method should improve your forecasts.  There are 
several Bayesian time series implementations in R.

Also, you may want to take a look at our (Boudt,Peterson,Croux) Journal 
of Risk paper from last year and the Return.clean method implementation 
in PerformanceAnalytics, which implements a robust filtering of time 
series outliers aimed squarely at making better risk predictions out of 
sample.

Regards,

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From chalabi at phys.ethz.ch  Tue Apr 21 23:08:26 2009
From: chalabi at phys.ethz.ch (Yohan Chalabi)
Date: Tue, 21 Apr 2009 23:08:26 +0200
Subject: [R-SIG-Finance] updated Rmetrics packages
Message-ID: <20090421230826.07485dea@mimi>

Dear list,

We are pleased to announce the update of the Rmetrics packages.

To update a current installation you can use the function
'update.packages()'

Or re-install all Rmetrics packages with the small R script available
at http://www.rmetrics.org:

   source("http://www.rmetrics.org/Rmetrics.R")
   install.Rmetrics()

Most of the work has been focused on 'timeSeries', formerly known as
'fSeries', and 'fPortfolio'.  The other packages have been updated to
be compliant with the new R version 2.9.0. 

* timeSeries

The overall efficiency of 'timeSeries' objects and key functions has
been further improved.

To give you an idea you can try the following example 

   # timeSeries (2100.83)
   library(timeSeries)
   ts <- timeSeries(1:1e6L, 1:1e6L) # one million entries
   ts
   ts[c("1970-01-01 01:00:00", "1970-01-01 12:33:00"), ]
   cbind(ts, ts)
   rbind(ts, ts)
   diff(ts)
   lag(ts)
   ts + ts

Please note that ts[1] returns the first element of the vector ts and
ts[1,] does subset the timeSeries object.

This speed improvement is possible thanks to a careful implementation
of the S4 classes and methods. Note that we are not using additional C
routines. Because of the inner changes, we decided to rename the former
package 'fSeries' to 'timeSeries'. The old package 'fSeries' is still
available on CRAN if you do not want to use the new package.

If you have data sets stored in the former 'timeSeries' definition, you
can use the function '.old2newTimeSeries()' to convert them to the new
format.

* fPortfolio

A lot of work has been done in 'fPortfolio' to improve its overall ease
of use. More specifically, we have added new solvers and new type of
constraints. In-depth information regarding the new features and the
general use of 'fPortfolio' can be found in the forthcoming eBook
"Portfolio Optimization with R/Rmetrics" (expected May 11).

As usual feedback, bug reports and suggestions are always welcome.

Best Regards,

Yohan
For the Rmetrics team


PS: If you are looking for more information, you can meet us in Chicago
at the R/Finance conference or this summer at the Rmetrics Workshop
(more information at http://www.rmetrics.org).


-- 
PhD student
Swiss Federal Institute of Technology
Zurich

www.ethz.ch


From kafka at centras.lt  Wed Apr 22 17:01:40 2009
From: kafka at centras.lt (kafkaz)
Date: Wed, 22 Apr 2009 08:01:40 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] xts and to.weekly function
In-Reply-To: <8cca69990904200656q40af54dkb008aed9343edaa4@mail.gmail.com>
References: <23133625.post@talk.nabble.com>
	<8cca69990904200656q40af54dkb008aed9343edaa4@mail.gmail.com>
Message-ID: <23175510.post@talk.nabble.com>


I did in a similar way, but using "which" function 


Josh Ulrich-2 wrote:
> 
> Does this provide what you want?
> 
> require(quantmod)
> getSymbols("USD/EUR",src="oanda",from="2009-01-01")
> 
> # endpoints(..., on='weeks') returns Sundays
> fri <- endpoints(USDEUR, on='weeks')-2
> 
> # replace first observation with zero, and remove last observation
> # see ?endpoints
> fri[1] <- 0;  fri <- head(fri,-1)
> 
> # data series of only Fridays
> USDEUR[fri]
> 
> HTH,
> Josh
> 
> 
> 

-- 
View this message in context: http://www.nabble.com/xts-and-to.weekly-function-tp23133625p23175510.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From philjoubert at yahoo.com  Fri Apr 24 15:49:04 2009
From: philjoubert at yahoo.com (Phil Joubert)
Date: Fri, 24 Apr 2009 06:49:04 -0700 (PDT)
Subject: [R-SIG-Finance] zoo: bug / feature replacing coredata - subsetting
	by dates
Message-ID: <428888.16392.qm@web111514.mail.gq1.yahoo.com>


Hi all

I'm using RBloomberg to download some price and dividend data. I want to construct a time series of total returns, so I need to add the dividends to the price data before taking log returns. My problem is that I obtain very strange behaviour when I try to change a subset of one zoo object (prices) using the indices of the other (dividends).

Any ideas, or smarter ways to achieve this?

#   Connect Bloomberg
library("RBloomberg")
oBbgConn <- blpConnect(show.days="trading"


dtStart <- chron("31/12/1988", format="d/m/y")
dtEnd <- chron("31/12/2008", format="d/m/y"))
vdPrices <- blpGetData(oBbgConn, sTicker, "PX_LAST", dtStart, dtEnd)

# BBG gives dividend data as an awkward table, convert it to a zoo with XD dates and dividend amounts
vdDividends <- unlist(blpGetData(oBbgConn, sTicker, "DVD_HIST", retval="raw"))
cnstszDivColumns <- 7
szDividends <- length(vdDividends) / cnstszDivColumns
dim(vdDividends) <- c(szDividends, cnstszDivColumns)
vdDividends <- merge(zoo(as.numeric(vdDividends[,5]), as.chron(vdDividends[,2])), all=FALSE)


vdPricesPlusDivs <- vdPrices
length(vdPricesPlusDivs)

#This line returns the expected values
coredata(vdPricesPlusDivs[index(vdPrices + vdDividends)])

#This should replace the coredata with the sum of the price + div for XD dates only
coredata(vdPricesPlusDivs[index(vdPrices + vdDividends)]) <- coredata(vdPrices + vdDividends)

length(vdPricesPlusDivs)

#but it does not - it adds in approximately length(vdPrices* vdDividends) NA's to the coredata vector without changing the index vector, and puts the new values at the end. The zoo object vdPricesPlusDivs is now unusable.

I suspect this is an issue with date/index handling at some point - if I create dummy timeseries using integers as index variables I cannot replicate the problem. I'm working on a Windows box.

Any help or ideas appreciated!
Phil


From philjoubert at yahoo.com  Fri Apr 24 16:52:52 2009
From: philjoubert at yahoo.com (Phil Joubert)
Date: Fri, 24 Apr 2009 07:52:52 -0700 (PDT)
Subject: [R-SIG-Finance] zoo: bug / feature replacing coredata -
	subsetting by dates
Message-ID: <900080.65319.qm@web111511.mail.gq1.yahoo.com>



Bloomberg free example below:

vdPrices <- zoo(c(0.01,0.01,0.01,0.01,0.01), order.by=seq(chron("31/12/1998", format="d/m/y"), chron("04/01/1999", format="d/m/y"), by="day"))
length(vdPrices)

vdDivs <- zoo(c(100,100), as.chron(c("01/01/1999","01/02/2009")))

coredata(vdPrices[index(vdDivs+vdPrices)]) <- coredata(vdDivs+vdPrices)
length(vdPrices)

Its a little clunky, but thats because I've tried to replicate the returns from the BBG calls as closely as possible.

I also noticed a mistake in the earlier code snippet - the merge() call should not be there.

thanks
Phil


--- On Fri, 4/24/09, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:

> From: Gabor Grothendieck <ggrothendieck at gmail.com>
> Subject: Re: [R-SIG-Finance] zoo: bug / feature replacing coredata -  subsetting by dates
> To: "Phil Joubert" <philjoubert at yahoo.com>
> Date: Friday, April 24, 2009, 5:02 PM
> Can you provide a minimal self
> contained example, i.e. one that
> does not require bloomberg.
> 
> On Fri, Apr 24, 2009 at 9:49 AM, Phil Joubert <philjoubert at yahoo.com>
> wrote:
> >
> > Hi all
> >
> > I'm using RBloomberg to download some price and
> dividend data. I want to construct a time series of total
> returns, so I need to add the dividends to the price data
> before taking log returns. My problem is that I obtain very
> strange behaviour when I try to change a subset of one zoo
> object (prices) using the indices of the other (dividends).
> >
> > Any ideas, or smarter ways to achieve this?
> >
> > # ? Connect Bloomberg
> > library("RBloomberg")
> > oBbgConn <- blpConnect(show.days="trading"
> >
> >
> > dtStart <- chron("31/12/1988", format="d/m/y")
> > dtEnd <- chron("31/12/2008", format="d/m/y"))
> > vdPrices <- blpGetData(oBbgConn, sTicker,
> "PX_LAST", dtStart, dtEnd)
> >
> > # BBG gives dividend data as an awkward table, convert
> it to a zoo with XD dates and dividend amounts
> > vdDividends <- unlist(blpGetData(oBbgConn, sTicker,
> "DVD_HIST", retval="raw"))
> > cnstszDivColumns <- 7
> > szDividends <- length(vdDividends) /
> cnstszDivColumns
> > dim(vdDividends) <- c(szDividends,
> cnstszDivColumns)
> > vdDividends <-
> merge(zoo(as.numeric(vdDividends[,5]),
> as.chron(vdDividends[,2])), all=FALSE)
> >
> >
> > vdPricesPlusDivs <- vdPrices
> > length(vdPricesPlusDivs)
> >
> > #This line returns the expected values
> > coredata(vdPricesPlusDivs[index(vdPrices +
> vdDividends)])
> >
> > #This should replace the coredata with the sum of the
> price + div for XD dates only
> > coredata(vdPricesPlusDivs[index(vdPrices +
> vdDividends)]) <- coredata(vdPrices + vdDividends)
> >
> > length(vdPricesPlusDivs)
> >
> > #but it does not - it adds in approximately
> length(vdPrices* vdDividends) NA's to the coredata vector
> without changing the index vector, and puts the new values
> at the end. The zoo object vdPricesPlusDivs is now
> unusable.
> >
> > I suspect this is an issue with date/index handling at
> some point - if I create dummy timeseries using integers as
> index variables I cannot replicate the problem. I'm working
> on a Windows box.
> >
> > Any help or ideas appreciated!
> > Phil
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch
> mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
> 





From ggrothendieck at gmail.com  Fri Apr 24 17:32:07 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 24 Apr 2009 11:32:07 -0400
Subject: [R-SIG-Finance] zoo: bug / feature replacing coredata -
	subsetting by dates
In-Reply-To: <900080.65319.qm@web111511.mail.gq1.yahoo.com>
References: <900080.65319.qm@web111511.mail.gq1.yahoo.com>
Message-ID: <971536df0904240832g5959c6beo262a15afb0b96922@mail.gmail.com>

This has nothing to do with zoo but is just how R works.  For
example consider this which does not use zoo at all:

      library(chron)
      p <- 1:5
      p[chron("01/01/09")] <- 100
      length(p)

The length of the result is 14245 because the index is unclassed
giving 14245 so 100 is assigned to the 14245th element of p and
since p only has 5 elements its extended to have 14245 elements.

Normally in zoo (as is the case with ts in the core of R) one uses a
window<- method like this:

     v <- vdDivs + vdPrices
     window(vdPrices, index(v)) <- v


On Fri, Apr 24, 2009 at 10:52 AM, Phil Joubert <philjoubert at yahoo.com> wrote:
>
>
> Bloomberg free example below:
>
> vdPrices <- zoo(c(0.01,0.01,0.01,0.01,0.01), order.by=seq(chron("31/12/1998", format="d/m/y"), chron("04/01/1999", format="d/m/y"), by="day"))
> length(vdPrices)
>
> vdDivs <- zoo(c(100,100), as.chron(c("01/01/1999","01/02/2009")))
>
> coredata(vdPrices[index(vdDivs+vdPrices)]) <- coredata(vdDivs+vdPrices)
> length(vdPrices)
>
> Its a little clunky, but thats because I've tried to replicate the returns from the BBG calls as closely as possible.
>
> I also noticed a mistake in the earlier code snippet - the merge() call should not be there.
>
> thanks
> Phil
>
>
> --- On Fri, 4/24/09, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>
>> From: Gabor Grothendieck <ggrothendieck at gmail.com>
>> Subject: Re: [R-SIG-Finance] zoo: bug / feature replacing coredata - ?subsetting by dates
>> To: "Phil Joubert" <philjoubert at yahoo.com>
>> Date: Friday, April 24, 2009, 5:02 PM
>> Can you provide a minimal self
>> contained example, i.e. one that
>> does not require bloomberg.
>>
>> On Fri, Apr 24, 2009 at 9:49 AM, Phil Joubert <philjoubert at yahoo.com>
>> wrote:
>> >
>> > Hi all
>> >
>> > I'm using RBloomberg to download some price and
>> dividend data. I want to construct a time series of total
>> returns, so I need to add the dividends to the price data
>> before taking log returns. My problem is that I obtain very
>> strange behaviour when I try to change a subset of one zoo
>> object (prices) using the indices of the other (dividends).
>> >
>> > Any ideas, or smarter ways to achieve this?
>> >
>> > # ? Connect Bloomberg
>> > library("RBloomberg")
>> > oBbgConn <- blpConnect(show.days="trading"
>> >
>> >
>> > dtStart <- chron("31/12/1988", format="d/m/y")
>> > dtEnd <- chron("31/12/2008", format="d/m/y"))
>> > vdPrices <- blpGetData(oBbgConn, sTicker,
>> "PX_LAST", dtStart, dtEnd)
>> >
>> > # BBG gives dividend data as an awkward table, convert
>> it to a zoo with XD dates and dividend amounts
>> > vdDividends <- unlist(blpGetData(oBbgConn, sTicker,
>> "DVD_HIST", retval="raw"))
>> > cnstszDivColumns <- 7
>> > szDividends <- length(vdDividends) /
>> cnstszDivColumns
>> > dim(vdDividends) <- c(szDividends,
>> cnstszDivColumns)
>> > vdDividends <-
>> merge(zoo(as.numeric(vdDividends[,5]),
>> as.chron(vdDividends[,2])), all=FALSE)
>> >
>> >
>> > vdPricesPlusDivs <- vdPrices
>> > length(vdPricesPlusDivs)
>> >
>> > #This line returns the expected values
>> > coredata(vdPricesPlusDivs[index(vdPrices +
>> vdDividends)])
>> >
>> > #This should replace the coredata with the sum of the
>> price + div for XD dates only
>> > coredata(vdPricesPlusDivs[index(vdPrices +
>> vdDividends)]) <- coredata(vdPrices + vdDividends)
>> >
>> > length(vdPricesPlusDivs)
>> >
>> > #but it does not - it adds in approximately
>> length(vdPrices* vdDividends) NA's to the coredata vector
>> without changing the index vector, and puts the new values
>> at the end. The zoo object vdPricesPlusDivs is now
>> unusable.
>> >
>> > I suspect this is an issue with date/index handling at
>> some point - if I create dummy timeseries using integers as
>> index variables I cannot replicate the problem. I'm working
>> on a Windows box.
>> >
>> > Any help or ideas appreciated!
>> > Phil
>> >
>> > _______________________________________________
>> > R-SIG-Finance at stat.math.ethz.ch
>> mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> > -- Subscriber-posting only.
>> > -- If you want to post, subscribe first.
>> >
>>
>
>
>
>


From knguyen at cs.umb.edu  Fri Apr 24 18:58:04 2009
From: knguyen at cs.umb.edu (Khanh Nguyen)
Date: Fri, 24 Apr 2009 12:58:04 -0400
Subject: [R-SIG-Finance] [Google Summer of Code 2009] Hi from a student
Message-ID: <2871c9e10904240958q5b66054cnc917c7a31aeef3b9@mail.gmail.com>

Dear All,

I am writing to introduce myself. I am Khanh Nguyen, one of the accepted
student for this summer Google Summer of Code. My mentor is Dirk
Eddelbuettel. Our project this summer is to extent the current RQuantLib
package. We aim to cross quantlib's fixed income portion to R by adding
corresponding wrapper functions. An abstract of my proposal is here
http://www.r-project.org/soc09/index.html. I also attached my full version
if you are interested about the project.

We're finalizing the scope of the project, i.e decide explicitly the set of
functionalities to implement. We'd love to hear any suggestions from users.
If you use both R and quantlib, what are things in quantlib you'd like to
see available in R? A few examples of RQuantlib is here
http://dirk.eddelbuettel.com/code/rquantlib.html

Thank you very much for your time. I'm very excited about this coming
summer. I look forward to work and learn from you.

Sincerely,

Khanh Nguyen
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090424/6c667b6c/attachment.html>
-------------- next part --------------
Title:  	 RQuantLib - Bridging R and QuantLib
Student: 	Khanh Nguyen
Abstract: 	Since statistical computing plays a major role in all financial modeling and risk-management tasks, it is highly desirable to combine the features and power of R and QuantLib. This project aims to provide a major extension to the existing RQuantLib package, includes expanding instruments coverage and integrating R's graphical engine for better visualization of modeling.
Content: 	

R Foundation for Statistical Computing

Application for the Google Summer of Code 2009 project

Applicant: Khanh Nguyen
Email: nguyen.h.khanh at gmail.com
Phone: 717-357-0219
Project Name: RQuantLib - Bridging R and QuantLib

Synopsis
Since QuantLib is a complex and large library that spans over many aspects of finance, I will focus mostly on areas that I have knowledge of. In particular, my priority is:

   1. to provide a substantial coverage for fixed income instruments and possibly credit or swap if time permitted. 
   2. investigate an effective way to take advantage of R's graphical capability in visualizing modeling results.

Benefits for community and myself

   1. increase the functionalities and scope of RQuantLib
   2. provides an easy pathway for users of R who are interested in financial modeling but lack the skills to make use of QuantLib. 
   3. by adding visualization, users will have a better, more comprehensive grasp of their model. 
   4. I'll have an opportunity to solidify what I know, acquire new knowledge and skills in both quantitative finance and coding financial algorithms. I consider this a fantastic opportunity to prepare myself for a career in finance. 

Project Description

The project will be built on the current RQuantLib. The structure is well defined by R package building standard. I've successfully tried to add a new function into RQuantLib to calculate a value of an Asian option using geometric averaging. It is currently in revision 58 in the repository. 

From brian at braverock.com  Sat Apr 25 07:33:59 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Sat, 25 Apr 2009 00:33:59 -0500
Subject: [R-SIG-Finance] zoo: bug / feature replacing coredata -
 subsetting by dates
In-Reply-To: <428888.16392.qm@web111514.mail.gq1.yahoo.com>
References: <428888.16392.qm@web111514.mail.gq1.yahoo.com>
Message-ID: <49F2A0C7.1080405@braverock.com>

Phil Joubert wrote:
> Hi all
>
> I'm using RBloomberg to download some price and dividend data. I want to construct a time series of total returns, so I need to add the dividends to the price data before taking log returns. My problem is that I obtain very strange behaviour when I try to change a subset of one zoo object (prices) using the indices of the other (dividends).
>
> Any ideas, or smarter ways to achieve this?
>
> #   Connect Bloomberg
> library("RBloomberg")
> oBbgConn <- blpConnect(show.days="trading"
>
>
> dtStart <- chron("31/12/1988", format="d/m/y")
> dtEnd <- chron("31/12/2008", format="d/m/y"))
> vdPrices <- blpGetData(oBbgConn, sTicker, "PX_LAST", dtStart, dtEnd)
>
> # BBG gives dividend data as an awkward table, convert it to a zoo with XD dates and dividend amounts
> vdDividends <- unlist(blpGetData(oBbgConn, sTicker, "DVD_HIST", retval="raw"))
> cnstszDivColumns <- 7
> szDividends <- length(vdDividends) / cnstszDivColumns
> dim(vdDividends) <- c(szDividends, cnstszDivColumns)
> vdDividends <- merge(zoo(as.numeric(vdDividends[,5]), as.chron(vdDividends[,2])), all=FALSE)
>
>
> vdPricesPlusDivs <- vdPrices
> length(vdPricesPlusDivs)
>
> #This line returns the expected values
> coredata(vdPricesPlusDivs[index(vdPrices + vdDividends)])
>
> #This should replace the coredata with the sum of the price + div for XD dates only
> coredata(vdPricesPlusDivs[index(vdPrices + vdDividends)]) <- coredata(vdPrices + vdDividends)
>
> length(vdPricesPlusDivs)
>
> #but it does not - it adds in approximately length(vdPrices* vdDividends) NA's to the coredata vector without changing the index vector, and puts the new values at the end. The zoo object vdPricesPlusDivs is now unusable.
>
> I suspect this is an issue with date/index handling at some point - if I create dummy timeseries using integers as index variables I cannot replicate the problem. I'm working on a Windows box.
>
> Any help or ideas appreciated!
> Phil
>   
Use xts instead.  It's written by one of the zoo authors (Jeff Ryan) and 
is smart about financial time series, rather than an arbitrary index.  
Also has overall intelligent sub-setting.

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From jeff.a.ryan at gmail.com  Sat Apr 25 08:16:53 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Sat, 25 Apr 2009 01:16:53 -0500
Subject: [R-SIG-Finance] zoo: bug / feature replacing coredata -
	subsetting by dates
In-Reply-To: <49F2A0C7.1080405@braverock.com>
References: <428888.16392.qm@web111514.mail.gq1.yahoo.com>
	<49F2A0C7.1080405@braverock.com>
Message-ID: <e8e755250904242316ma9e7685t4279e1706c36a3ca@mail.gmail.com>

This won't work at all in xts.

While the indexing in xts is time-aware (as opposed to simply ordered
like zoo), the internals make use of POSIXct time, which chron does
not map to well.

Gabor is correct in that absent another indexing class, this is simply
a fundamental R issue.  I would advise not using chron, as Date would
be a better fit to daily data in this case.

Jeff

PS.  To clarify, I am merely a developer with the zoo project, Achim
and Gabor are the "authors".

On Sat, Apr 25, 2009 at 12:33 AM, Brian G. Peterson <brian at braverock.com> wrote:
> Phil Joubert wrote:
>>
>> Hi all
>>
>> I'm using RBloomberg to download some price and dividend data. I want to
>> construct a time series of total returns, so I need to add the dividends to
>> the price data before taking log returns. My problem is that I obtain very
>> strange behaviour when I try to change a subset of one zoo object (prices)
>> using the indices of the other (dividends).
>>
>> Any ideas, or smarter ways to achieve this?
>>
>> # ? Connect Bloomberg
>> library("RBloomberg")
>> oBbgConn <- blpConnect(show.days="trading"
>>
>>
>> dtStart <- chron("31/12/1988", format="d/m/y")
>> dtEnd <- chron("31/12/2008", format="d/m/y"))
>> vdPrices <- blpGetData(oBbgConn, sTicker, "PX_LAST", dtStart, dtEnd)
>>
>> # BBG gives dividend data as an awkward table, convert it to a zoo with XD
>> dates and dividend amounts
>> vdDividends <- unlist(blpGetData(oBbgConn, sTicker, "DVD_HIST",
>> retval="raw"))
>> cnstszDivColumns <- 7
>> szDividends <- length(vdDividends) / cnstszDivColumns
>> dim(vdDividends) <- c(szDividends, cnstszDivColumns)
>> vdDividends <- merge(zoo(as.numeric(vdDividends[,5]),
>> as.chron(vdDividends[,2])), all=FALSE)
>>
>>
>> vdPricesPlusDivs <- vdPrices
>> length(vdPricesPlusDivs)
>>
>> #This line returns the expected values
>> coredata(vdPricesPlusDivs[index(vdPrices + vdDividends)])
>>
>> #This should replace the coredata with the sum of the price + div for XD
>> dates only
>> coredata(vdPricesPlusDivs[index(vdPrices + vdDividends)]) <-
>> coredata(vdPrices + vdDividends)
>>
>> length(vdPricesPlusDivs)
>>
>> #but it does not - it adds in approximately length(vdPrices* vdDividends)
>> NA's to the coredata vector without changing the index vector, and puts the
>> new values at the end. The zoo object vdPricesPlusDivs is now unusable.
>>
>> I suspect this is an issue with date/index handling at some point - if I
>> create dummy timeseries using integers as index variables I cannot replicate
>> the problem. I'm working on a Windows box.
>>
>> Any help or ideas appreciated!
>> Phil
>>
>
> Use xts instead. ?It's written by one of the zoo authors (Jeff Ryan) and is
> smart about financial time series, rather than an arbitrary index. ?Also has
> overall intelligent sub-setting.
>
> --
> Brian G. Peterson
> http://braverock.com/brian/
> Ph: 773-459-4973
> IM: bgpbraverock
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From nabilwm at hotmail.com  Mon Apr 27 07:46:23 2009
From: nabilwm at hotmail.com (Nabil Meslmani)
Date: Mon, 27 Apr 2009 05:46:23 +0000
Subject: [R-SIG-Finance] dinvgamma(sigma, shape, scale)
Message-ID: <BLU131-W21BABD86F9B5010AC48D72BD710@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090427/b77a664b/attachment.pl>

From krishna.dagli at gmail.com  Mon Apr 27 09:28:01 2009
From: krishna.dagli at gmail.com (Krishna Dagli)
Date: Mon, 27 Apr 2009 12:58:01 +0530
Subject: [R-SIG-Finance] How to do a real time graph
Message-ID: <ee9e4b740904270028q65f07304r5a5892ef0c568855@mail.gmail.com>

Hello;

I have a simple application that reads a pipe delimited line (price and time)
from a network; over a socket and I want to plot the movement of same
in real time using a graph.

What packages / approach should I take?

Thanks and Regards
Krishna


From edd at debian.org  Mon Apr 27 13:30:13 2009
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 27 Apr 2009 06:30:13 -0500
Subject: [R-SIG-Finance] How to do a real time graph
In-Reply-To: <ee9e4b740904270028q65f07304r5a5892ef0c568855@mail.gmail.com>
References: <ee9e4b740904270028q65f07304r5a5892ef0c568855@mail.gmail.com>
Message-ID: <18933.38725.503942.459648@ron.nulle.part>


On 27 April 2009 at 12:58, Krishna Dagli wrote:
| I have a simple application that reads a pipe delimited line (price and time)
| from a network; over a socket and I want to plot the movement of same
| in real time using a graph.
| 
| What packages / approach should I take?

R is not really set up for that. It is neither meant for real-time work, nor
does it incremental plotting all that well. It can be done, but not well. 

In the narrow sense, you need to read the documentation about R socket
connections, and about plot() and maybe some of the time-series classes (zoo,
xts, ...) and then just do it. 

Two things I have used and am using are

 - kst (kst.kde.org) which can handle enormous amounts of data (coming from
   astrophysics) really efficiently where you only have to write out a txt
   file from which kst reads---that sounds odd, but you keep a cache of the
   data outside your app that way which is nice; your existing listener
   writes the text file, you only need to parameterise kst and don't need to
   write code for it

 - do-it-yourself where I use C++ and the qwt plotting widget library (from
   sourceforge) on top of Qt (www.qtsoftware.com) along with a few lines of
   C++ code---this works great for transformations (spreads, ratios, ...)
   too.

But there surely are a number of other choices.

Dirk

-- 
Three out of two people have difficulties with fractions.


From manojsw at gmail.com  Mon Apr 27 15:34:22 2009
From: manojsw at gmail.com (Manoj)
Date: Mon, 27 Apr 2009 23:34:22 +1000
Subject: [R-SIG-Finance] Dates manipulation
Message-ID: <829e6c8a0904270634j62697e2qf4878b8da943ea0d@mail.gmail.com>

Hi All,
   I am using version 2,8,1 and working with zoo library.

   The sample/dummy data-set i am working off takes the following structure:

Date	Ticker	Price
2009-04-21?????	X?	2.32
2009-04-22?????	X?	2.35
2009-04-23?????	X?	2.34
2009-04-21?????	Y	10.2
2009-04-22?????	Y	10.32
2009-04-23?????	Y	10.15

   The data-frame consist of Date as a Date object, Ticker and Price.
Price can be uniquely identified by Date & Ticker.

   One of the issues with creating a zoo object with above data-set is
the fact that the unique key is basically a composite key so it's not
purely time-series data but does take the format of panel data.

 I want to be able to perform normal data function (lag,time etc) on
the above data-set for example, lag would return all the rows except
for ticker X&Y for 2009-04-23.

  Is there anyway, i can sub-class zoo object to achieve this functionality?

  Any suggestions would be greatly appreciated - thanks in advance.


Manoj


From ggrothendieck at gmail.com  Mon Apr 27 16:13:56 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 27 Apr 2009 10:13:56 -0400
Subject: [R-SIG-Finance] Dates manipulation
In-Reply-To: <829e6c8a0904270634j62697e2qf4878b8da943ea0d@mail.gmail.com>
References: <829e6c8a0904270634j62697e2qf4878b8da943ea0d@mail.gmail.com>
Message-ID: <971536df0904270713h9999018y71b745591bc5df75@mail.gmail.com>

zoo objects represent time series and that is not a time series
in its current form but you could read it in using read.table and
then its only one line of code to transform it to one.


library(zoo)
Lines <-
"Date    Ticker  Price
2009-04-21      X       2.32
2009-04-22      X       2.35
2009-04-23      X       2.34
2009-04-21      Y       10.2
2009-04-22      Y       10.32
2009-04-23      Y       10.15"

DF <- read.table(textConnection(Lines), header = TRUE)
DF$Date <- as.Date(DF$Date)
do.call(merge, by(DF[-2], DF[2], function(d) zoo(d$Price, d$Date)))


# Using read.zoo from the development version of zoo
# the last line could even be reduced to this:

source("http://r-forge.r-project.org/plugins/scmsvn/viewcvs.php/*checkout*/pkg/R/read.zoo.R?rev=519&root=zoo")
do.call(merge, by(DF[-2], DF[2], read.zoo))



Make sure you have a sufficiently recent version of zoo since
read.zoo


On Mon, Apr 27, 2009 at 9:34 AM, Manoj <manojsw at gmail.com> wrote:
> Hi All,
> ? I am using version 2,8,1 and working with zoo library.
>
> ? The sample/dummy data-set i am working off takes the following structure:
>
> Date ? ?Ticker ?Price
> 2009-04-21????? X? ? ? ?2.32
> 2009-04-22????? X? ? ? ?2.35
> 2009-04-23????? X? ? ? ?2.34
> 2009-04-21????? Y ? ? ? 10.2
> 2009-04-22????? Y ? ? ? 10.32
> 2009-04-23????? Y ? ? ? 10.15
>
> ? The data-frame consist of Date as a Date object, Ticker and Price.
> Price can be uniquely identified by Date & Ticker.
>
> ? One of the issues with creating a zoo object with above data-set is
> the fact that the unique key is basically a composite key so it's not
> purely time-series data but does take the format of panel data.
>
> ?I want to be able to perform normal data function (lag,time etc) on
> the above data-set for example, lag would return all the rows except
> for ticker X&Y for 2009-04-23.
>
> ?Is there anyway, i can sub-class zoo object to achieve this functionality?
>
> ?Any suggestions would be greatly appreciated - thanks in advance.
>
>
> Manoj
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From HodgessE at uhd.edu  Mon Apr 27 20:16:27 2009
From: HodgessE at uhd.edu (Hodgess, Erin)
Date: Mon, 27 Apr 2009 13:16:27 -0500
Subject: [R-SIG-Finance] great conference and a question
Message-ID: <70A5AC06FDB5E54482D19E1C04CDFCF307C36C92@BALI.uhd.campus>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090427/3720af55/attachment.pl>

From brian at braverock.com  Mon Apr 27 20:25:11 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 27 Apr 2009 13:25:11 -0500
Subject: [R-SIG-Finance] great conference and a question
In-Reply-To: <70A5AC06FDB5E54482D19E1C04CDFCF307C36C92@BALI.uhd.campus>
References: <70A5AC06FDB5E54482D19E1C04CDFCF307C36C92@BALI.uhd.campus>
Message-ID: <49F5F887.8080203@braverock.com>

Hodgess, Erin wrote:
> Hi all!
>
> Wasn't the R in finance conference great this weekend?  Thanks to the program committee!
>
> Anyhow, I have a question, too, please:  the S&P returns that we typically see in examples - where can I find them online please?
>
> Thanks,
> Erin
>   
You can use quantmod to download a price series for the index, and then 
use Return.calculate or other methods to generate a return series.  
Perhaps this goes without saying, but to do returns analysis, it is 
generally best to have the price series first, as what you most likely 
want is to calculate simple per-period returns before doing your 
analysis; log returns are only useful for things like a cumulative 
return chart or wealth index.

Regards,

     - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From herrold at owlriver.com  Mon Apr 27 20:38:51 2009
From: herrold at owlriver.com (R P Herrold)
Date: Mon, 27 Apr 2009 14:38:51 -0400 (EDT)
Subject: [R-SIG-Finance] great conference and a question
In-Reply-To: <70A5AC06FDB5E54482D19E1C04CDFCF307C36C92@BALI.uhd.campus>
References: <70A5AC06FDB5E54482D19E1C04CDFCF307C36C92@BALI.uhd.campus>
Message-ID: <alpine.LRH.2.00.0904271438200.8080@arj.bjyevire.pbz>

On Mon, 27 Apr 2009, Hodgess, Erin wrote:

> Wasn't the R in finance conference great this weekend? 
> Thanks to the program committee!

Those not there missed a great one:
 	http://orcorc.blogspot.com/2009/04/r-you-experienced.html

-- Russ herrold


From B_Rowe at ml.com  Mon Apr 27 20:44:36 2009
From: B_Rowe at ml.com (Rowe, Brian Lee Yung (Portfolio Analytics))
Date: Mon, 27 Apr 2009 14:44:36 -0400
Subject: [R-SIG-Finance] great conference and a question
In-Reply-To: <49F5F887.8080203@braverock.com>
Message-ID: <3BAD818D9407B043817CC6D89ABA14EC032B84C6@MLNYC20MB051.amrs.win.ml.com>

If you want the returns for the underlying index constituents, you can
also use some functions in tawny that do this for you (leverages
quantmod under the hood):

h <- getPortfolioReturns(getIndexComposition('^DJI'), obs=100,
reload=TRUE)

The getIndexComposition method retrieves index data from Yahoo! (if it
exists). It works for most major indices, although their service is at
times unreliable.

Brian


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Brian G.
Peterson
Sent: Monday, April 27, 2009 2:25 PM
To: Hodgess, Erin
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] great conference and a question


Hodgess, Erin wrote:
> Hi all!
>
> Wasn't the R in finance conference great this weekend?  Thanks to the
program committee!
>
> Anyhow, I have a question, too, please:  the S&P returns that we
typically see in examples - where can I find them online please?
>
> Thanks,
> Erin
>   
You can use quantmod to download a price series for the index, and then 
use Return.calculate or other methods to generate a return series.  
Perhaps this goes without saying, but to do returns analysis, it is 
generally best to have the price series first, as what you most likely 
want is to calculate simple per-period returns before doing your 
analysis; log returns are only useful for things like a cumulative 
return chart or wealth index.

Regards,

     - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

--------------------------------------------------------------------------
This message w/attachments (message) may be privileged, confidential or proprietary, and if you are not an intended recipient, please notify the sender, do not use or share it and delete it. Unless specifically indicated, this message is not an offer to sell or a solicitation of any investment products or other financial product or service, an official confirmation of any transaction, or an official statement of Merrill Lynch. Subject to applicable law, Merrill Lynch may monitor, review and retain e-communications (EC) traveling through its networks/systems. The laws of the country of each sender/recipient may impact the handling of EC, and EC may be archived, supervised and produced in countries other than the country in which you are located. This message cannot be guaranteed to be secure or error-free. References to "Merrill Lynch" are references to any company in the Merrill Lynch & Co., Inc. group of companies, which are wholly-owned by Bank of America Corporation. Securities and Insurance Products: * Are Not FDIC Insured * Are Not Bank Guaranteed * May Lose Value * Are Not a Bank Deposit * Are Not a Condition to Any Banking Service or Activity * Are Not Insured by Any Federal Government Agency. Attachments that are part of this E-communication may have additional important disclosures and disclaimers, which you should read. This message is subject to terms available at the following link: http://www.ml.com/e-communications_terms/. By messaging with Merrill Lynch you consent to the foregoing.
--------------------------------------------------------------------------


From david at revolution-computing.com  Tue Apr 28 01:30:12 2009
From: david at revolution-computing.com (David M Smith)
Date: Mon, 27 Apr 2009 16:30:12 -0700
Subject: [R-SIG-Finance] Review of some R/Finance 2009 talks
Message-ID: <475a3c8f0904271630x2cc665fbu68c28238211cd15b@mail.gmail.com>

If you didn't make it to R/Finance 2009 in Chicago this weekend, I've
written up my notes from several of the talks, with links to resources
where I could find them. (The presentations themselves aren't online
yet, but should be soon.) The notes are at the Revolutions blog:

http://blog.revolution-computing.com/2009/04/rfinance-2009-roundup.html

Cheers,
# David Smith

--
David M Smith <david at revolution-computing.com>
Director of Community, REvolution Computing www.revolution-computing.com
Tel: +1 (206) 577-4778 x3203 (San Francisco, USA)

Check out our upcoming events schedule at www.revolution-computing.com/events


From megh700004 at yahoo.com  Tue Apr 28 05:06:11 2009
From: megh700004 at yahoo.com (megh)
Date: Mon, 27 Apr 2009 20:06:11 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Downloading data from Yahoo
Message-ID: <23269331.post@talk.nabble.com>


Hi, I am having problem downloading quote from Yahoo (historical stock quotes
from GE) using Quantmod package. I have following quote :

from.dat <- as.Date("01/01/08", format="%m/%d/%y")
to.dat <- as.Date("02/19/09", format="%m/%d/%y")
getSymbols("^GE", src="yahoo", from = from.dat, to = to.dat)

Error is :
Error in download.file(paste(yahoo.URL, "s=", Symbols.name, "&a=", from.m, 
: 
  cannot open URL
'http://chart.yahoo.com/table.csv?s=^GE&a=0&b=01&c=2008&d=1&e=19&f=2009&g=d&q=q&y=0&z=^GE&x=.csv'
In addition: Warning message:
In download.file(paste(yahoo.URL, "s=", Symbols.name, "&a=", from.m,  :
  cannot open: HTTP status was '404 Not Found'

Can anyone please point out that what is the wrong here? How can I download
data from Yahoo efficiently?

Regards,
-- 
View this message in context: http://www.nabble.com/Downloading-data-from-Yahoo-tp23269331p23269331.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From josh.m.ulrich at gmail.com  Tue Apr 28 05:14:06 2009
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Mon, 27 Apr 2009 22:14:06 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Downloading data from Yahoo
In-Reply-To: <23269331.post@talk.nabble.com>
References: <23269331.post@talk.nabble.com>
Message-ID: <8cca69990904272014s2f08441l83d3dcadc414d38a@mail.gmail.com>

"^GE" is not a valid ticker symbol; use "GE" instead.

> getSymbols("GE", src="yahoo", from = from.dat, to = to.dat)
[1] "GE"

Best,
Josh
--
http://quantemplation.blogspot.com
http://www.fosstrading.com



On Mon, Apr 27, 2009 at 10:06 PM, megh <megh700004 at yahoo.com> wrote:
>
> Hi, I am having problem downloading quote from Yahoo (historical stock quotes
> from GE) using Quantmod package. I have following quote :
>
> from.dat <- as.Date("01/01/08", format="%m/%d/%y")
> to.dat <- as.Date("02/19/09", format="%m/%d/%y")
> getSymbols("^GE", src="yahoo", from = from.dat, to = to.dat)
>
> Error is :
> Error in download.file(paste(yahoo.URL, "s=", Symbols.name, "&a=", from.m,
> :
> ?cannot open URL
> 'http://chart.yahoo.com/table.csv?s=^GE&a=0&b=01&c=2008&d=1&e=19&f=2009&g=d&q=q&y=0&z=^GE&x=.csv'
> In addition: Warning message:
> In download.file(paste(yahoo.URL, "s=", Symbols.name, "&a=", from.m, ?:
> ?cannot open: HTTP status was '404 Not Found'
>
> Can anyone please point out that what is the wrong here? How can I download
> data from Yahoo efficiently?
>
> Regards,
> --
> View this message in context: http://www.nabble.com/Downloading-data-from-Yahoo-tp23269331p23269331.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jeff.a.ryan at gmail.com  Tue Apr 28 05:19:04 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Mon, 27 Apr 2009 22:19:04 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Downloading data from Yahoo
In-Reply-To: <23269331.post@talk.nabble.com>
References: <23269331.post@talk.nabble.com>
Message-ID: <e8e755250904272019p31b75b24n20be749db30089e@mail.gmail.com>

Try and remove the ^

> getSymbols("GE", src="yahoo", from = from.dat, to = to.dat)
[1] "GE"

> str(GE)
An 'xts' object from 2008-01-02 to 2009-02-19 containing:
  Data: num [1:286, 1:6] 37.1 36.8 36.5 36.2 36.4 ...
 - attr(*, "dimnames")=List of 2
  ..$ : NULL
  ..$ : chr [1:6] "GE.Open" "GE.High" "GE.Low" "GE.Close" ...
  Indexed by objects of class: [POSIXt,POSIXct] TZ: America/Chicago
  xts Attributes:
List of 2
 $ src    : chr "yahoo"
 $ updated: POSIXct[1:1], format: "2009-04-27 22:18:22"

HTH
Jeff

On Mon, Apr 27, 2009 at 10:06 PM, megh <megh700004 at yahoo.com> wrote:
>
> Hi, I am having problem downloading quote from Yahoo (historical stock quotes
> from GE) using Quantmod package. I have following quote :
>
> from.dat <- as.Date("01/01/08", format="%m/%d/%y")
> to.dat <- as.Date("02/19/09", format="%m/%d/%y")
> getSymbols("^GE", src="yahoo", from = from.dat, to = to.dat)
>
> Error is :
> Error in download.file(paste(yahoo.URL, "s=", Symbols.name, "&a=", from.m,
> :
> ?cannot open URL
> 'http://chart.yahoo.com/table.csv?s=^GE&a=0&b=01&c=2008&d=1&e=19&f=2009&g=d&q=q&y=0&z=^GE&x=.csv'
> In addition: Warning message:
> In download.file(paste(yahoo.URL, "s=", Symbols.name, "&a=", from.m, ?:
> ?cannot open: HTTP status was '404 Not Found'
>
> Can anyone please point out that what is the wrong here? How can I download
> data from Yahoo efficiently?
>
> Regards,
> --
> View this message in context: http://www.nabble.com/Downloading-data-from-Yahoo-tp23269331p23269331.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From knguyen at cs.umb.edu  Tue Apr 28 05:29:47 2009
From: knguyen at cs.umb.edu (Khanh Nguyen)
Date: Mon, 27 Apr 2009 23:29:47 -0400
Subject: [R-SIG-Finance] [R-sig-finance] Downloading data from Yahoo
In-Reply-To: <e8e755250904272019p31b75b24n20be749db30089e@mail.gmail.com>
References: <23269331.post@talk.nabble.com>
	<e8e755250904272019p31b75b24n20be749db30089e@mail.gmail.com>
Message-ID: <2871c9e10904272029g36c632a6ma1191fb30acb6867@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090427/11c939a2/attachment.pl>

From HodgessE at uhd.edu  Tue Apr 28 05:46:30 2009
From: HodgessE at uhd.edu (Hodgess, Erin)
Date: Mon, 27 Apr 2009 22:46:30 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Downloading data from Yahoo
References: <23269331.post@talk.nabble.com><e8e755250904272019p31b75b24n20be749db30089e@mail.gmail.com>
	<2871c9e10904272029g36c632a6ma1191fb30acb6867@mail.gmail.com>
Message-ID: <70A5AC06FDB5E54482D19E1C04CDFCF307C36C97@BALI.uhd.campus>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090427/afb66aaa/attachment.pl>

From singularitaet at gmx.net  Tue Apr 28 14:31:48 2009
From: singularitaet at gmx.net (Stefan Grosse)
Date: Tue, 28 Apr 2009 14:31:48 +0200
Subject: [R-SIG-Finance] weightsSlider in fPortfolio broken?
Message-ID: <20090428143148.7157c367@gmx.net>

Hi,

I am experiencing an error with the weightsSlider function of
fPortfolio (2100.77, R 2.9.0, WinXP pro and Fedora 10).

To reproduce: 

example(weightsSlider)
weightsSlider(frontier)

The error tells that  getTargetRisk(object)[, 1] has the wrong number
of dimensions. (Same on windows and Linux) Probably the ,1 is wrong.

(weightsPlot(frontier) does work for example.)

Am I the only one with that error? How do I submit a bug?

Stefan


From megh700004 at yahoo.com  Tue Apr 28 14:46:30 2009
From: megh700004 at yahoo.com (megh)
Date: Tue, 28 Apr 2009 05:46:30 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Downloading data from Yahoo
In-Reply-To: <70A5AC06FDB5E54482D19E1C04CDFCF307C36C97@BALI.uhd.campus>
References: <23269331.post@talk.nabble.com>
	<e8e755250904272019p31b75b24n20be749db30089e@mail.gmail.com>
	<2871c9e10904272029g36c632a6ma1191fb30acb6867@mail.gmail.com>
	<70A5AC06FDB5E54482D19E1C04CDFCF307C36C97@BALI.uhd.campus>
Message-ID: <23276393.post@talk.nabble.com>


Thanks everyone for answering my query. I need one more extension. Suppose I
want to download GE, Intel and CTS data from Yahoo-finance and "USD/CAD"
exchange rate data from Oanda. Obviously I can do that by writing two
separate lines of codes, one for Yahoo and another for Oanda. My question
is, are there any privilege wherein I can download all four data in a single
go?

Thanks,
 

statone wrote:
> 
> The o in oanda should be lower case.
> 
> 
> Erin M. Hodgess, PhD
> Associate Professor
> Department of Computer and Mathematical Sciences
> University of Houston - Downtown
> mailto: hodgesse at uhd.edu
> 
> 
> 
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch on behalf of Khanh Nguyen
> Sent: Mon 4/27/2009 10:29 PM
> To: Jeff Ryan
> Cc: r-sig-finance at stat.math.ethz.ch; megh
> Subject: Re: [R-SIG-Finance] [R-sig-finance] Downloading data from Yahoo
>  
> I have a related question. Does quantmod still support oanda?
> 
> I got the following message when I tried
> 
>> getSymbols("XPT/USD",src="Oanda")
> Error in do.call(paste("getSymbols.", symbol.source, sep = ""),
> list(Symbols
> = current.symbols,  : could not find function "getSymbols.Oanda"
> 
> Thanks
> 
> -k
> 
> On Mon, Apr 27, 2009 at 11:19 PM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> 
>> Try and remove the ^
>>
>> > getSymbols("GE", src="yahoo", from = from.dat, to = to.dat)
>> [1] "GE"
>>
>> > str(GE)
>> An 'xts' object from 2008-01-02 to 2009-02-19 containing:
>>  Data: num [1:286, 1:6] 37.1 36.8 36.5 36.2 36.4 ...
>>  - attr(*, "dimnames")=List of 2
>>  ..$ : NULL
>>  ..$ : chr [1:6] "GE.Open" "GE.High" "GE.Low" "GE.Close" ...
>>  Indexed by objects of class: [POSIXt,POSIXct] TZ: America/Chicago
>>  xts Attributes:
>> List of 2
>>  $ src    : chr "yahoo"
>>  $ updated: POSIXct[1:1], format: "2009-04-27 22:18:22"
>>
>> HTH
>> Jeff
>>
>> On Mon, Apr 27, 2009 at 10:06 PM, megh <megh700004 at yahoo.com> wrote:
>> >
>> > Hi, I am having problem downloading quote from Yahoo (historical stock
>> quotes
>> > from GE) using Quantmod package. I have following quote :
>> >
>> > from.dat <- as.Date("01/01/08", format="%m/%d/%y")
>> > to.dat <- as.Date("02/19/09", format="%m/%d/%y")
>> > getSymbols("^GE", src="yahoo", from = from.dat, to = to.dat)
>> >
>> > Error is :
>> > Error in download.file(paste(yahoo.URL, "s=", Symbols.name, "&a=",
>> from.m,
>> > :
>> >  cannot open URL
>> > 'http://chart.yahoo.com/table.csv?s=
>> ^GE&a=0&b=01&c=2008&d=1&e=19&f=2009&g=d&q=q&y=0&z=^GE&x=.csv'
>> > In addition: Warning message:
>> > In download.file(paste(yahoo.URL, "s=", Symbols.name, "&a=", from.m,  :
>> >  cannot open: HTTP status was '404 Not Found'
>> >
>> > Can anyone please point out that what is the wrong here? How can I
>> download
>> > data from Yahoo efficiently?
>> >
>> > Regards,
>> > --
>> > View this message in context:
>> http://www.nabble.com/Downloading-data-from-Yahoo-tp23269331p23269331.html
>> > Sent from the Rmetrics mailing list archive at Nabble.com.
>> >
>> > _______________________________________________
>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> > -- Subscriber-posting only.
>> > -- If you want to post, subscribe first.
>> >
>>
>>
>>
>> --
>> Jeffrey Ryan
>> jeffrey.ryan at insightalgo.com
>>
>> ia: insight algorithmics
>> www.insightalgo.com
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/Downloading-data-from-Yahoo-tp23269331p23276393.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From sebastian.hauer at gmail.com  Tue Apr 28 14:49:56 2009
From: sebastian.hauer at gmail.com (Sebastian Hauer)
Date: Tue, 28 Apr 2009 08:49:56 -0400
Subject: [R-SIG-Finance] quantmod: overlaying time series bar charts
Message-ID: <fb92d9f00904280549t4594c8a7t77bdef33c7c1cf23@mail.gmail.com>

Hello,
I was wondering if there is an easy way of overlaying two bar charts
using quantmod?
Here is what I am trying to do:

chartSeries(ts1, type = "bars", show.grid = TRUE, up.col = "green",
dn.col = "green")
chartSeries(ts2, type = "bars", show.grid = TRUE, up.col = "red",
dn.col = "red")

But have ts2 show in the same plot frame or if that does not work have
it show in a frame below the first one.

Regards,
Sebastian


From bmck at capitalytics.com  Tue Apr 28 16:10:28 2009
From: bmck at capitalytics.com (Bill McKinnon)
Date: Tue, 28 Apr 2009 09:10:28 -0500
Subject: [R-SIG-Finance] New banking analytics company (with R computation
	engine)
Message-ID: <ec364d1f0904280710q37542b9ewb503f83063dd4e9c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090428/26795045/attachment.pl>

From haky.im at gmail.com  Wed Apr 29 00:29:44 2009
From: haky.im at gmail.com (Hae Kyung Im)
Date: Tue, 28 Apr 2009 17:29:44 -0500
Subject: [R-SIG-Finance] TTR's RSI
Message-ID: <197a5bbc0904281529t3c2df244m7e4a858830391e2a@mail.gmail.com>

Hi,

for some reason when I calculate RSI on a xts object of length n, I
get a result of length n+1 as in the following example:

> dim(data.60min)
[1] 23  5

> class(data.xmin)
[1] "xts" "zoo"

> length(RSI(data.60min[,"Close"]))
[1] 24

If I plug in a vector of length 23, RSI gives me a vector of length 23.

> length(RSI(runif(23)))
[1] 23

am I missing something here?

Thanks
Haky


--------------------------------------------
Here is the data for your reference:
> data.60min
                    Open High Low Close Volume
2008-03-16 17:00:00  119  120 119   119   1305
2008-03-16 18:00:00  119  120 119   120   3237
2008-03-16 19:00:00  120  120 120   120   2813
2008-03-16 20:00:00  120  120 120   120   2857
2008-03-16 21:00:00  120  120 120   120   4561
2008-03-16 22:00:00  120  120 120   120   1988
2008-03-16 23:00:00  120  120 120   120    805
2008-03-17 00:00:00  120  120 120   120   1177
2008-03-17 01:00:00  120  120 120   120   1245
2008-03-17 02:00:00  120  120 120   120   1263
2008-03-17 03:00:00  120  120 120   120   5565
2008-03-17 04:00:00  120  120 120   120   4081
2008-03-17 05:00:00  120  120 120   120   4462
2008-03-17 06:00:00  120  120 120   120  12409
2008-03-17 07:00:00  120  120 119   119  34083
2008-03-17 08:00:00  119  120 119   120  34742
2008-03-17 09:00:00  120  120 120   120  29412
2008-03-17 10:00:00  120  120 120   120  40087
2008-03-17 11:00:00  120  120 120   120  21834
2008-03-17 12:00:00  120  120 120   120  26442
2008-03-17 13:00:00  120  120 120   120  36671
2008-03-17 14:00:00  120  120 120   120  20468
2008-03-17 15:00:00  120  120 120   120   8802


From josh.m.ulrich at gmail.com  Wed Apr 29 00:49:29 2009
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Tue, 28 Apr 2009 17:49:29 -0500
Subject: [R-SIG-Finance] TTR's RSI
In-Reply-To: <197a5bbc0904281529t3c2df244m7e4a858830391e2a@mail.gmail.com>
References: <197a5bbc0904281529t3c2df244m7e4a858830391e2a@mail.gmail.com>
Message-ID: <8cca69990904281549t637ce0d5tf6f4bf2f2182300d@mail.gmail.com>

Hi Haky,

Could you provide more information?  I cannot replicate your results.

> Lines <- " Open High Low Close Volume
\"2008-03-16 17:00:00\"  119  120 119   119   1305
\"2008-03-16 18:00:00\"  119  120 119   120   3237
\"2008-03-16 19:00:00\"  120  120 120   120   2813
\"2008-03-16 20:00:00\"  120  120 120   120   2857
\"2008-03-16 21:00:00\"  120  120 120   120   4561
\"2008-03-16 22:00:00\"  120  120 120   120   1988
\"2008-03-16 23:00:00\"  120  120 120   120    805
\"2008-03-17 00:00:00\"  120  120 120   120   1177
\"2008-03-17 01:00:00\"  120  120 120   120   1245
\"2008-03-17 02:00:00\"  120  120 120   120   1263
\"2008-03-17 03:00:00\"  120  120 120   120   5565
\"2008-03-17 04:00:00\"  120  120 120   120   4081
\"2008-03-17 05:00:00\"  120  120 120   120   4462
\"2008-03-17 06:00:00\"  120  120 120   120  12409
\"2008-03-17 07:00:00\"  120  120 119   119  34083
\"2008-03-17 08:00:00\"  119  120 119   120  34742
\"2008-03-17 09:00:00\"  120  120 120   120  29412
\"2008-03-17 10:00:00\"  120  120 120   120  40087
\"2008-03-17 11:00:00\"  120  120 120   120  21834
\"2008-03-17 12:00:00\"  120  120 120   120  26442
\"2008-03-17 13:00:00\"  120  120 120   120  36671
\"2008-03-17 14:00:00\"  120  120 120   120  20468
\"2008-03-17 15:00:00\"  120  120 120   120   8802"
> data.60min <- read.table(textConnection(Lines))
> dim(data.60min)
[1] 23  5
> #class(data.xmin)  # I don't know what the 'data.xmin' object is...
> class(data.60min)
[1] "data.frame"
> length(RSI(data.60min[,"Close"]))
[1] 23
> length(RSI(as.xts(data.60min)[,"Close"]))
[1] 23
> sessionInfo()
R version 2.9.0 (2009-04-17)
i386-unknown-freebsd7.1

locale:
C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] TTR_0.2-1 xts_0.6-4 zoo_1.5-5

loaded via a namespace (and not attached):
[1] grid_2.9.0      lattice_0.17-22

Best,
Josh
--
http://quantemplation.blogspot.com
http://www.fosstrading.com



On Tue, Apr 28, 2009 at 5:29 PM, Hae Kyung Im <haky.im at gmail.com> wrote:
> Hi,
>
> for some reason when I calculate RSI on a xts object of length n, I
> get a result of length n+1 as in the following example:
>
>> dim(data.60min)
> [1] 23 ?5
>
>> class(data.xmin)
> [1] "xts" "zoo"
>
>> length(RSI(data.60min[,"Close"]))
> [1] 24
>
> If I plug in a vector of length 23, RSI gives me a vector of length 23.
>
>> length(RSI(runif(23)))
> [1] 23
>
> am I missing something here?
>
> Thanks
> Haky
>
>
> --------------------------------------------
> Here is the data for your reference:
>> data.60min
> ? ? ? ? ? ? ? ? ? ?Open High Low Close Volume
> 2008-03-16 17:00:00 ?119 ?120 119 ? 119 ? 1305
> 2008-03-16 18:00:00 ?119 ?120 119 ? 120 ? 3237
> 2008-03-16 19:00:00 ?120 ?120 120 ? 120 ? 2813
> 2008-03-16 20:00:00 ?120 ?120 120 ? 120 ? 2857
> 2008-03-16 21:00:00 ?120 ?120 120 ? 120 ? 4561
> 2008-03-16 22:00:00 ?120 ?120 120 ? 120 ? 1988
> 2008-03-16 23:00:00 ?120 ?120 120 ? 120 ? ?805
> 2008-03-17 00:00:00 ?120 ?120 120 ? 120 ? 1177
> 2008-03-17 01:00:00 ?120 ?120 120 ? 120 ? 1245
> 2008-03-17 02:00:00 ?120 ?120 120 ? 120 ? 1263
> 2008-03-17 03:00:00 ?120 ?120 120 ? 120 ? 5565
> 2008-03-17 04:00:00 ?120 ?120 120 ? 120 ? 4081
> 2008-03-17 05:00:00 ?120 ?120 120 ? 120 ? 4462
> 2008-03-17 06:00:00 ?120 ?120 120 ? 120 ?12409
> 2008-03-17 07:00:00 ?120 ?120 119 ? 119 ?34083
> 2008-03-17 08:00:00 ?119 ?120 119 ? 120 ?34742
> 2008-03-17 09:00:00 ?120 ?120 120 ? 120 ?29412
> 2008-03-17 10:00:00 ?120 ?120 120 ? 120 ?40087
> 2008-03-17 11:00:00 ?120 ?120 120 ? 120 ?21834
> 2008-03-17 12:00:00 ?120 ?120 120 ? 120 ?26442
> 2008-03-17 13:00:00 ?120 ?120 120 ? 120 ?36671
> 2008-03-17 14:00:00 ?120 ?120 120 ? 120 ?20468
> 2008-03-17 15:00:00 ?120 ?120 120 ? 120 ? 8802
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From haky.im at gmail.com  Wed Apr 29 01:43:36 2009
From: haky.im at gmail.com (Hae Kyung Im)
Date: Tue, 28 Apr 2009 18:43:36 -0500
Subject: [R-SIG-Finance] TTR's RSI
In-Reply-To: <8cca69990904281549t637ce0d5tf6f4bf2f2182300d@mail.gmail.com>
References: <197a5bbc0904281529t3c2df244m7e4a858830391e2a@mail.gmail.com>
	<8cca69990904281549t637ce0d5tf6f4bf2f2182300d@mail.gmail.com>
Message-ID: <197a5bbc0904281643r375fcc88rb6248ab45be9dae@mail.gmail.com>

Hi Josh,

I am attaching the following files

test.Rdata (with data.xmin, the xts object)
replic.r
replic.output.txt (output when I source replic.r)

Thanks
Haky


On Tue, Apr 28, 2009 at 5:49 PM, Josh Ulrich <josh.m.ulrich at gmail.com> wrote:
> Hi Haky,
>
> Could you provide more information? ?I cannot replicate your results.
>
>> Lines <- " Open High Low Close Volume
> \"2008-03-16 17:00:00\" ?119 ?120 119 ? 119 ? 1305
> \"2008-03-16 18:00:00\" ?119 ?120 119 ? 120 ? 3237
> \"2008-03-16 19:00:00\" ?120 ?120 120 ? 120 ? 2813
> \"2008-03-16 20:00:00\" ?120 ?120 120 ? 120 ? 2857
> \"2008-03-16 21:00:00\" ?120 ?120 120 ? 120 ? 4561
> \"2008-03-16 22:00:00\" ?120 ?120 120 ? 120 ? 1988
> \"2008-03-16 23:00:00\" ?120 ?120 120 ? 120 ? ?805
> \"2008-03-17 00:00:00\" ?120 ?120 120 ? 120 ? 1177
> \"2008-03-17 01:00:00\" ?120 ?120 120 ? 120 ? 1245
> \"2008-03-17 02:00:00\" ?120 ?120 120 ? 120 ? 1263
> \"2008-03-17 03:00:00\" ?120 ?120 120 ? 120 ? 5565
> \"2008-03-17 04:00:00\" ?120 ?120 120 ? 120 ? 4081
> \"2008-03-17 05:00:00\" ?120 ?120 120 ? 120 ? 4462
> \"2008-03-17 06:00:00\" ?120 ?120 120 ? 120 ?12409
> \"2008-03-17 07:00:00\" ?120 ?120 119 ? 119 ?34083
> \"2008-03-17 08:00:00\" ?119 ?120 119 ? 120 ?34742
> \"2008-03-17 09:00:00\" ?120 ?120 120 ? 120 ?29412
> \"2008-03-17 10:00:00\" ?120 ?120 120 ? 120 ?40087
> \"2008-03-17 11:00:00\" ?120 ?120 120 ? 120 ?21834
> \"2008-03-17 12:00:00\" ?120 ?120 120 ? 120 ?26442
> \"2008-03-17 13:00:00\" ?120 ?120 120 ? 120 ?36671
> \"2008-03-17 14:00:00\" ?120 ?120 120 ? 120 ?20468
> \"2008-03-17 15:00:00\" ?120 ?120 120 ? 120 ? 8802"
>> data.60min <- read.table(textConnection(Lines))
>> dim(data.60min)
> [1] 23 ?5
>> #class(data.xmin) ?# I don't know what the 'data.xmin' object is...
>> class(data.60min)
> [1] "data.frame"
>> length(RSI(data.60min[,"Close"]))
> [1] 23
>> length(RSI(as.xts(data.60min)[,"Close"]))
> [1] 23
>> sessionInfo()
> R version 2.9.0 (2009-04-17)
> i386-unknown-freebsd7.1
>
> locale:
> C
>
> attached base packages:
> [1] stats ? ? graphics ?grDevices utils ? ? datasets ?methods ? base
>
> other attached packages:
> [1] TTR_0.2-1 xts_0.6-4 zoo_1.5-5
>
> loaded via a namespace (and not attached):
> [1] grid_2.9.0 ? ? ?lattice_0.17-22
>
> Best,
> Josh
> --
> http://quantemplation.blogspot.com
> http://www.fosstrading.com
>
>
>
> On Tue, Apr 28, 2009 at 5:29 PM, Hae Kyung Im <haky.im at gmail.com> wrote:
>> Hi,
>>
>> for some reason when I calculate RSI on a xts object of length n, I
>> get a result of length n+1 as in the following example:
>>
>>> dim(data.60min)
>> [1] 23 ?5
>>
>>> class(data.xmin)
>> [1] "xts" "zoo"
>>
>>> length(RSI(data.60min[,"Close"]))
>> [1] 24
>>
>> If I plug in a vector of length 23, RSI gives me a vector of length 23.
>>
>>> length(RSI(runif(23)))
>> [1] 23
>>
>> am I missing something here?
>>
>> Thanks
>> Haky
>>
>>
>> --------------------------------------------
>> Here is the data for your reference:
>>> data.60min
>> ? ? ? ? ? ? ? ? ? ?Open High Low Close Volume
>> 2008-03-16 17:00:00 ?119 ?120 119 ? 119 ? 1305
>> 2008-03-16 18:00:00 ?119 ?120 119 ? 120 ? 3237
>> 2008-03-16 19:00:00 ?120 ?120 120 ? 120 ? 2813
>> 2008-03-16 20:00:00 ?120 ?120 120 ? 120 ? 2857
>> 2008-03-16 21:00:00 ?120 ?120 120 ? 120 ? 4561
>> 2008-03-16 22:00:00 ?120 ?120 120 ? 120 ? 1988
>> 2008-03-16 23:00:00 ?120 ?120 120 ? 120 ? ?805
>> 2008-03-17 00:00:00 ?120 ?120 120 ? 120 ? 1177
>> 2008-03-17 01:00:00 ?120 ?120 120 ? 120 ? 1245
>> 2008-03-17 02:00:00 ?120 ?120 120 ? 120 ? 1263
>> 2008-03-17 03:00:00 ?120 ?120 120 ? 120 ? 5565
>> 2008-03-17 04:00:00 ?120 ?120 120 ? 120 ? 4081
>> 2008-03-17 05:00:00 ?120 ?120 120 ? 120 ? 4462
>> 2008-03-17 06:00:00 ?120 ?120 120 ? 120 ?12409
>> 2008-03-17 07:00:00 ?120 ?120 119 ? 119 ?34083
>> 2008-03-17 08:00:00 ?119 ?120 119 ? 120 ?34742
>> 2008-03-17 09:00:00 ?120 ?120 120 ? 120 ?29412
>> 2008-03-17 10:00:00 ?120 ?120 120 ? 120 ?40087
>> 2008-03-17 11:00:00 ?120 ?120 120 ? 120 ?21834
>> 2008-03-17 12:00:00 ?120 ?120 120 ? 120 ?26442
>> 2008-03-17 13:00:00 ?120 ?120 120 ? 120 ?36671
>> 2008-03-17 14:00:00 ?120 ?120 120 ? 120 ?20468
>> 2008-03-17 15:00:00 ?120 ?120 120 ? 120 ? 8802
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>


From haky.im at gmail.com  Wed Apr 29 01:46:16 2009
From: haky.im at gmail.com (Hae Kyung Im)
Date: Tue, 28 Apr 2009 18:46:16 -0500
Subject: [R-SIG-Finance] TTR's RSI
In-Reply-To: <197a5bbc0904281643r375fcc88rb6248ab45be9dae@mail.gmail.com>
References: <197a5bbc0904281529t3c2df244m7e4a858830391e2a@mail.gmail.com>
	<8cca69990904281549t637ce0d5tf6f4bf2f2182300d@mail.gmail.com>
	<197a5bbc0904281643r375fcc88rb6248ab45be9dae@mail.gmail.com>
Message-ID: <197a5bbc0904281646y16769c7cg9a8158e36cc96bc@mail.gmail.com>

well... this time with the attachment



On Tue, Apr 28, 2009 at 6:43 PM, Hae Kyung Im <haky.im at gmail.com> wrote:
> Hi Josh,
>
> I am attaching the following files
>
> test.Rdata (with data.xmin, the xts object)
> replic.r
> replic.output.txt (output when I source replic.r)
>
> Thanks
> Haky
>
>
> On Tue, Apr 28, 2009 at 5:49 PM, Josh Ulrich <josh.m.ulrich at gmail.com> wrote:
>> Hi Haky,
>>
>> Could you provide more information? ?I cannot replicate your results.
>>
>>> Lines <- " Open High Low Close Volume
>> \"2008-03-16 17:00:00\" ?119 ?120 119 ? 119 ? 1305
>> \"2008-03-16 18:00:00\" ?119 ?120 119 ? 120 ? 3237
>> \"2008-03-16 19:00:00\" ?120 ?120 120 ? 120 ? 2813
>> \"2008-03-16 20:00:00\" ?120 ?120 120 ? 120 ? 2857
>> \"2008-03-16 21:00:00\" ?120 ?120 120 ? 120 ? 4561
>> \"2008-03-16 22:00:00\" ?120 ?120 120 ? 120 ? 1988
>> \"2008-03-16 23:00:00\" ?120 ?120 120 ? 120 ? ?805
>> \"2008-03-17 00:00:00\" ?120 ?120 120 ? 120 ? 1177
>> \"2008-03-17 01:00:00\" ?120 ?120 120 ? 120 ? 1245
>> \"2008-03-17 02:00:00\" ?120 ?120 120 ? 120 ? 1263
>> \"2008-03-17 03:00:00\" ?120 ?120 120 ? 120 ? 5565
>> \"2008-03-17 04:00:00\" ?120 ?120 120 ? 120 ? 4081
>> \"2008-03-17 05:00:00\" ?120 ?120 120 ? 120 ? 4462
>> \"2008-03-17 06:00:00\" ?120 ?120 120 ? 120 ?12409
>> \"2008-03-17 07:00:00\" ?120 ?120 119 ? 119 ?34083
>> \"2008-03-17 08:00:00\" ?119 ?120 119 ? 120 ?34742
>> \"2008-03-17 09:00:00\" ?120 ?120 120 ? 120 ?29412
>> \"2008-03-17 10:00:00\" ?120 ?120 120 ? 120 ?40087
>> \"2008-03-17 11:00:00\" ?120 ?120 120 ? 120 ?21834
>> \"2008-03-17 12:00:00\" ?120 ?120 120 ? 120 ?26442
>> \"2008-03-17 13:00:00\" ?120 ?120 120 ? 120 ?36671
>> \"2008-03-17 14:00:00\" ?120 ?120 120 ? 120 ?20468
>> \"2008-03-17 15:00:00\" ?120 ?120 120 ? 120 ? 8802"
>>> data.60min <- read.table(textConnection(Lines))
>>> dim(data.60min)
>> [1] 23 ?5
>>> #class(data.xmin) ?# I don't know what the 'data.xmin' object is...
>>> class(data.60min)
>> [1] "data.frame"
>>> length(RSI(data.60min[,"Close"]))
>> [1] 23
>>> length(RSI(as.xts(data.60min)[,"Close"]))
>> [1] 23
>>> sessionInfo()
>> R version 2.9.0 (2009-04-17)
>> i386-unknown-freebsd7.1
>>
>> locale:
>> C
>>
>> attached base packages:
>> [1] stats ? ? graphics ?grDevices utils ? ? datasets ?methods ? base
>>
>> other attached packages:
>> [1] TTR_0.2-1 xts_0.6-4 zoo_1.5-5
>>
>> loaded via a namespace (and not attached):
>> [1] grid_2.9.0 ? ? ?lattice_0.17-22
>>
>> Best,
>> Josh
>> --
>> http://quantemplation.blogspot.com
>> http://www.fosstrading.com
>>
>>
>>
>> On Tue, Apr 28, 2009 at 5:29 PM, Hae Kyung Im <haky.im at gmail.com> wrote:
>>> Hi,
>>>
>>> for some reason when I calculate RSI on a xts object of length n, I
>>> get a result of length n+1 as in the following example:
>>>
>>>> dim(data.60min)
>>> [1] 23 ?5
>>>
>>>> class(data.xmin)
>>> [1] "xts" "zoo"
>>>
>>>> length(RSI(data.60min[,"Close"]))
>>> [1] 24
>>>
>>> If I plug in a vector of length 23, RSI gives me a vector of length 23.
>>>
>>>> length(RSI(runif(23)))
>>> [1] 23
>>>
>>> am I missing something here?
>>>
>>> Thanks
>>> Haky
>>>
>>>
>>> --------------------------------------------
>>> Here is the data for your reference:
>>>> data.60min
>>> ? ? ? ? ? ? ? ? ? ?Open High Low Close Volume
>>> 2008-03-16 17:00:00 ?119 ?120 119 ? 119 ? 1305
>>> 2008-03-16 18:00:00 ?119 ?120 119 ? 120 ? 3237
>>> 2008-03-16 19:00:00 ?120 ?120 120 ? 120 ? 2813
>>> 2008-03-16 20:00:00 ?120 ?120 120 ? 120 ? 2857
>>> 2008-03-16 21:00:00 ?120 ?120 120 ? 120 ? 4561
>>> 2008-03-16 22:00:00 ?120 ?120 120 ? 120 ? 1988
>>> 2008-03-16 23:00:00 ?120 ?120 120 ? 120 ? ?805
>>> 2008-03-17 00:00:00 ?120 ?120 120 ? 120 ? 1177
>>> 2008-03-17 01:00:00 ?120 ?120 120 ? 120 ? 1245
>>> 2008-03-17 02:00:00 ?120 ?120 120 ? 120 ? 1263
>>> 2008-03-17 03:00:00 ?120 ?120 120 ? 120 ? 5565
>>> 2008-03-17 04:00:00 ?120 ?120 120 ? 120 ? 4081
>>> 2008-03-17 05:00:00 ?120 ?120 120 ? 120 ? 4462
>>> 2008-03-17 06:00:00 ?120 ?120 120 ? 120 ?12409
>>> 2008-03-17 07:00:00 ?120 ?120 119 ? 119 ?34083
>>> 2008-03-17 08:00:00 ?119 ?120 119 ? 120 ?34742
>>> 2008-03-17 09:00:00 ?120 ?120 120 ? 120 ?29412
>>> 2008-03-17 10:00:00 ?120 ?120 120 ? 120 ?40087
>>> 2008-03-17 11:00:00 ?120 ?120 120 ? 120 ?21834
>>> 2008-03-17 12:00:00 ?120 ?120 120 ? 120 ?26442
>>> 2008-03-17 13:00:00 ?120 ?120 120 ? 120 ?36671
>>> 2008-03-17 14:00:00 ?120 ?120 120 ? 120 ?20468
>>> 2008-03-17 15:00:00 ?120 ?120 120 ? 120 ? 8802
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>
-------------- next part --------------

R version 2.8.1 (2008-12-22)
Copyright (C) 2008 The R Foundation for Statistical Computing
ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> options(STERM='iESS', editor='emacsclient')
> ## test
> 
> rm(list=ls())
> options(digits=3,digits.sec=4)
> library(quantmod)
Loading required package: xts
Loading required package: zoo

Attaching package: 'zoo'


	The following object(s) are masked from package:base :

	 as.Date.numeric 

Loading required package: Defaults
quantmod: Quantitative Financial Modelling Framework

Version 0.3-7, Revision 461
http://www.quantmod.com

> library(TTR)
> load('test.Rdata')
> ls()
[1] "data.xmin"
> class(data.xmin)
[1] "xts" "zoo"
> dim(data.xmin)
[1] 90  5
> length(RSI(data.xmin[,"Close"]))
[1] 91
> 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: test.Rdata
Type: application/octet-stream
Size: 1340 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090428/35dadc36/attachment.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: replic.r
Type: application/octet-stream
Size: 173 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090428/35dadc36/attachment-0001.obj>

From haky.im at gmail.com  Wed Apr 29 01:59:30 2009
From: haky.im at gmail.com (Hae Kyung Im)
Date: Tue, 28 Apr 2009 18:59:30 -0500
Subject: [R-SIG-Finance] TTR's RSI
In-Reply-To: <197a5bbc0904281646y16769c7cg9a8158e36cc96bc@mail.gmail.com>
References: <197a5bbc0904281529t3c2df244m7e4a858830391e2a@mail.gmail.com>
	<8cca69990904281549t637ce0d5tf6f4bf2f2182300d@mail.gmail.com>
	<197a5bbc0904281643r375fcc88rb6248ab45be9dae@mail.gmail.com>
	<197a5bbc0904281646y16769c7cg9a8158e36cc96bc@mail.gmail.com>
Message-ID: <197a5bbc0904281659h67b86a68i28df0a0d84c97f92@mail.gmail.com>

I think I found the problem:

I seems like the function "diff" applied to an xts column returns n
elements with NA in the first one. Normally you would get n-1
elements.

Haky


On Tue, Apr 28, 2009 at 6:46 PM, Hae Kyung Im <haky.im at gmail.com> wrote:
> well... this time with the attachment
>
>
>
> On Tue, Apr 28, 2009 at 6:43 PM, Hae Kyung Im <haky.im at gmail.com> wrote:
>> Hi Josh,
>>
>> I am attaching the following files
>>
>> test.Rdata (with data.xmin, the xts object)
>> replic.r
>> replic.output.txt (output when I source replic.r)
>>
>> Thanks
>> Haky
>>
>>
>> On Tue, Apr 28, 2009 at 5:49 PM, Josh Ulrich <josh.m.ulrich at gmail.com> wrote:
>>> Hi Haky,
>>>
>>> Could you provide more information? ?I cannot replicate your results.
>>>
>>>> Lines <- " Open High Low Close Volume
>>> \"2008-03-16 17:00:00\" ?119 ?120 119 ? 119 ? 1305
>>> \"2008-03-16 18:00:00\" ?119 ?120 119 ? 120 ? 3237
>>> \"2008-03-16 19:00:00\" ?120 ?120 120 ? 120 ? 2813
>>> \"2008-03-16 20:00:00\" ?120 ?120 120 ? 120 ? 2857
>>> \"2008-03-16 21:00:00\" ?120 ?120 120 ? 120 ? 4561
>>> \"2008-03-16 22:00:00\" ?120 ?120 120 ? 120 ? 1988
>>> \"2008-03-16 23:00:00\" ?120 ?120 120 ? 120 ? ?805
>>> \"2008-03-17 00:00:00\" ?120 ?120 120 ? 120 ? 1177
>>> \"2008-03-17 01:00:00\" ?120 ?120 120 ? 120 ? 1245
>>> \"2008-03-17 02:00:00\" ?120 ?120 120 ? 120 ? 1263
>>> \"2008-03-17 03:00:00\" ?120 ?120 120 ? 120 ? 5565
>>> \"2008-03-17 04:00:00\" ?120 ?120 120 ? 120 ? 4081
>>> \"2008-03-17 05:00:00\" ?120 ?120 120 ? 120 ? 4462
>>> \"2008-03-17 06:00:00\" ?120 ?120 120 ? 120 ?12409
>>> \"2008-03-17 07:00:00\" ?120 ?120 119 ? 119 ?34083
>>> \"2008-03-17 08:00:00\" ?119 ?120 119 ? 120 ?34742
>>> \"2008-03-17 09:00:00\" ?120 ?120 120 ? 120 ?29412
>>> \"2008-03-17 10:00:00\" ?120 ?120 120 ? 120 ?40087
>>> \"2008-03-17 11:00:00\" ?120 ?120 120 ? 120 ?21834
>>> \"2008-03-17 12:00:00\" ?120 ?120 120 ? 120 ?26442
>>> \"2008-03-17 13:00:00\" ?120 ?120 120 ? 120 ?36671
>>> \"2008-03-17 14:00:00\" ?120 ?120 120 ? 120 ?20468
>>> \"2008-03-17 15:00:00\" ?120 ?120 120 ? 120 ? 8802"
>>>> data.60min <- read.table(textConnection(Lines))
>>>> dim(data.60min)
>>> [1] 23 ?5
>>>> #class(data.xmin) ?# I don't know what the 'data.xmin' object is...
>>>> class(data.60min)
>>> [1] "data.frame"
>>>> length(RSI(data.60min[,"Close"]))
>>> [1] 23
>>>> length(RSI(as.xts(data.60min)[,"Close"]))
>>> [1] 23
>>>> sessionInfo()
>>> R version 2.9.0 (2009-04-17)
>>> i386-unknown-freebsd7.1
>>>
>>> locale:
>>> C
>>>
>>> attached base packages:
>>> [1] stats ? ? graphics ?grDevices utils ? ? datasets ?methods ? base
>>>
>>> other attached packages:
>>> [1] TTR_0.2-1 xts_0.6-4 zoo_1.5-5
>>>
>>> loaded via a namespace (and not attached):
>>> [1] grid_2.9.0 ? ? ?lattice_0.17-22
>>>
>>> Best,
>>> Josh
>>> --
>>> http://quantemplation.blogspot.com
>>> http://www.fosstrading.com
>>>
>>>
>>>
>>> On Tue, Apr 28, 2009 at 5:29 PM, Hae Kyung Im <haky.im at gmail.com> wrote:
>>>> Hi,
>>>>
>>>> for some reason when I calculate RSI on a xts object of length n, I
>>>> get a result of length n+1 as in the following example:
>>>>
>>>>> dim(data.60min)
>>>> [1] 23 ?5
>>>>
>>>>> class(data.xmin)
>>>> [1] "xts" "zoo"
>>>>
>>>>> length(RSI(data.60min[,"Close"]))
>>>> [1] 24
>>>>
>>>> If I plug in a vector of length 23, RSI gives me a vector of length 23.
>>>>
>>>>> length(RSI(runif(23)))
>>>> [1] 23
>>>>
>>>> am I missing something here?
>>>>
>>>> Thanks
>>>> Haky
>>>>
>>>>
>>>> --------------------------------------------
>>>> Here is the data for your reference:
>>>>> data.60min
>>>> ? ? ? ? ? ? ? ? ? ?Open High Low Close Volume
>>>> 2008-03-16 17:00:00 ?119 ?120 119 ? 119 ? 1305
>>>> 2008-03-16 18:00:00 ?119 ?120 119 ? 120 ? 3237
>>>> 2008-03-16 19:00:00 ?120 ?120 120 ? 120 ? 2813
>>>> 2008-03-16 20:00:00 ?120 ?120 120 ? 120 ? 2857
>>>> 2008-03-16 21:00:00 ?120 ?120 120 ? 120 ? 4561
>>>> 2008-03-16 22:00:00 ?120 ?120 120 ? 120 ? 1988
>>>> 2008-03-16 23:00:00 ?120 ?120 120 ? 120 ? ?805
>>>> 2008-03-17 00:00:00 ?120 ?120 120 ? 120 ? 1177
>>>> 2008-03-17 01:00:00 ?120 ?120 120 ? 120 ? 1245
>>>> 2008-03-17 02:00:00 ?120 ?120 120 ? 120 ? 1263
>>>> 2008-03-17 03:00:00 ?120 ?120 120 ? 120 ? 5565
>>>> 2008-03-17 04:00:00 ?120 ?120 120 ? 120 ? 4081
>>>> 2008-03-17 05:00:00 ?120 ?120 120 ? 120 ? 4462
>>>> 2008-03-17 06:00:00 ?120 ?120 120 ? 120 ?12409
>>>> 2008-03-17 07:00:00 ?120 ?120 119 ? 119 ?34083
>>>> 2008-03-17 08:00:00 ?119 ?120 119 ? 120 ?34742
>>>> 2008-03-17 09:00:00 ?120 ?120 120 ? 120 ?29412
>>>> 2008-03-17 10:00:00 ?120 ?120 120 ? 120 ?40087
>>>> 2008-03-17 11:00:00 ?120 ?120 120 ? 120 ?21834
>>>> 2008-03-17 12:00:00 ?120 ?120 120 ? 120 ?26442
>>>> 2008-03-17 13:00:00 ?120 ?120 120 ? 120 ?36671
>>>> 2008-03-17 14:00:00 ?120 ?120 120 ? 120 ?20468
>>>> 2008-03-17 15:00:00 ?120 ?120 120 ? 120 ? 8802
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>
>>
>


From josh.m.ulrich at gmail.com  Wed Apr 29 02:08:05 2009
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Tue, 28 Apr 2009 19:08:05 -0500
Subject: [R-SIG-Finance] TTR's RSI
In-Reply-To: <197a5bbc0904281646y16769c7cg9a8158e36cc96bc@mail.gmail.com>
References: <197a5bbc0904281529t3c2df244m7e4a858830391e2a@mail.gmail.com>
	<8cca69990904281549t637ce0d5tf6f4bf2f2182300d@mail.gmail.com>
	<197a5bbc0904281643r375fcc88rb6248ab45be9dae@mail.gmail.com>
	<197a5bbc0904281646y16769c7cg9a8158e36cc96bc@mail.gmail.com>
Message-ID: <8cca69990904281708y49ce53b9re2f0376641525984@mail.gmail.com>

Haky,

It appears you're not using TTR_0.2.  You should not have this issue
with the newest version.

Best,
Josh
--
http://quantemplation.blogspot.com
http://www.fosstrading.com



On Tue, Apr 28, 2009 at 6:46 PM, Hae Kyung Im <haky.im at gmail.com> wrote:
> well... this time with the attachment
>
>
>
> On Tue, Apr 28, 2009 at 6:43 PM, Hae Kyung Im <haky.im at gmail.com> wrote:
>> Hi Josh,
>>
>> I am attaching the following files
>>
>> test.Rdata (with data.xmin, the xts object)
>> replic.r
>> replic.output.txt (output when I source replic.r)
>>
>> Thanks
>> Haky
>>
>>
>> On Tue, Apr 28, 2009 at 5:49 PM, Josh Ulrich <josh.m.ulrich at gmail.com> wrote:
>>> Hi Haky,
>>>
>>> Could you provide more information? ?I cannot replicate your results.
>>>
>>>> Lines <- " Open High Low Close Volume
>>> \"2008-03-16 17:00:00\" ?119 ?120 119 ? 119 ? 1305
>>> \"2008-03-16 18:00:00\" ?119 ?120 119 ? 120 ? 3237
>>> \"2008-03-16 19:00:00\" ?120 ?120 120 ? 120 ? 2813
>>> \"2008-03-16 20:00:00\" ?120 ?120 120 ? 120 ? 2857
>>> \"2008-03-16 21:00:00\" ?120 ?120 120 ? 120 ? 4561
>>> \"2008-03-16 22:00:00\" ?120 ?120 120 ? 120 ? 1988
>>> \"2008-03-16 23:00:00\" ?120 ?120 120 ? 120 ? ?805
>>> \"2008-03-17 00:00:00\" ?120 ?120 120 ? 120 ? 1177
>>> \"2008-03-17 01:00:00\" ?120 ?120 120 ? 120 ? 1245
>>> \"2008-03-17 02:00:00\" ?120 ?120 120 ? 120 ? 1263
>>> \"2008-03-17 03:00:00\" ?120 ?120 120 ? 120 ? 5565
>>> \"2008-03-17 04:00:00\" ?120 ?120 120 ? 120 ? 4081
>>> \"2008-03-17 05:00:00\" ?120 ?120 120 ? 120 ? 4462
>>> \"2008-03-17 06:00:00\" ?120 ?120 120 ? 120 ?12409
>>> \"2008-03-17 07:00:00\" ?120 ?120 119 ? 119 ?34083
>>> \"2008-03-17 08:00:00\" ?119 ?120 119 ? 120 ?34742
>>> \"2008-03-17 09:00:00\" ?120 ?120 120 ? 120 ?29412
>>> \"2008-03-17 10:00:00\" ?120 ?120 120 ? 120 ?40087
>>> \"2008-03-17 11:00:00\" ?120 ?120 120 ? 120 ?21834
>>> \"2008-03-17 12:00:00\" ?120 ?120 120 ? 120 ?26442
>>> \"2008-03-17 13:00:00\" ?120 ?120 120 ? 120 ?36671
>>> \"2008-03-17 14:00:00\" ?120 ?120 120 ? 120 ?20468
>>> \"2008-03-17 15:00:00\" ?120 ?120 120 ? 120 ? 8802"
>>>> data.60min <- read.table(textConnection(Lines))
>>>> dim(data.60min)
>>> [1] 23 ?5
>>>> #class(data.xmin) ?# I don't know what the 'data.xmin' object is...
>>>> class(data.60min)
>>> [1] "data.frame"
>>>> length(RSI(data.60min[,"Close"]))
>>> [1] 23
>>>> length(RSI(as.xts(data.60min)[,"Close"]))
>>> [1] 23
>>>> sessionInfo()
>>> R version 2.9.0 (2009-04-17)
>>> i386-unknown-freebsd7.1
>>>
>>> locale:
>>> C
>>>
>>> attached base packages:
>>> [1] stats ? ? graphics ?grDevices utils ? ? datasets ?methods ? base
>>>
>>> other attached packages:
>>> [1] TTR_0.2-1 xts_0.6-4 zoo_1.5-5
>>>
>>> loaded via a namespace (and not attached):
>>> [1] grid_2.9.0 ? ? ?lattice_0.17-22
>>>
>>> Best,
>>> Josh
>>> --
>>> http://quantemplation.blogspot.com
>>> http://www.fosstrading.com
>>>
>>>
>>>
>>> On Tue, Apr 28, 2009 at 5:29 PM, Hae Kyung Im <haky.im at gmail.com> wrote:
>>>> Hi,
>>>>
>>>> for some reason when I calculate RSI on a xts object of length n, I
>>>> get a result of length n+1 as in the following example:
>>>>
>>>>> dim(data.60min)
>>>> [1] 23 ?5
>>>>
>>>>> class(data.xmin)
>>>> [1] "xts" "zoo"
>>>>
>>>>> length(RSI(data.60min[,"Close"]))
>>>> [1] 24
>>>>
>>>> If I plug in a vector of length 23, RSI gives me a vector of length 23.
>>>>
>>>>> length(RSI(runif(23)))
>>>> [1] 23
>>>>
>>>> am I missing something here?
>>>>
>>>> Thanks
>>>> Haky
>>>>
>>>>
>>>> --------------------------------------------
>>>> Here is the data for your reference:
>>>>> data.60min
>>>> ? ? ? ? ? ? ? ? ? ?Open High Low Close Volume
>>>> 2008-03-16 17:00:00 ?119 ?120 119 ? 119 ? 1305
>>>> 2008-03-16 18:00:00 ?119 ?120 119 ? 120 ? 3237
>>>> 2008-03-16 19:00:00 ?120 ?120 120 ? 120 ? 2813
>>>> 2008-03-16 20:00:00 ?120 ?120 120 ? 120 ? 2857
>>>> 2008-03-16 21:00:00 ?120 ?120 120 ? 120 ? 4561
>>>> 2008-03-16 22:00:00 ?120 ?120 120 ? 120 ? 1988
>>>> 2008-03-16 23:00:00 ?120 ?120 120 ? 120 ? ?805
>>>> 2008-03-17 00:00:00 ?120 ?120 120 ? 120 ? 1177
>>>> 2008-03-17 01:00:00 ?120 ?120 120 ? 120 ? 1245
>>>> 2008-03-17 02:00:00 ?120 ?120 120 ? 120 ? 1263
>>>> 2008-03-17 03:00:00 ?120 ?120 120 ? 120 ? 5565
>>>> 2008-03-17 04:00:00 ?120 ?120 120 ? 120 ? 4081
>>>> 2008-03-17 05:00:00 ?120 ?120 120 ? 120 ? 4462
>>>> 2008-03-17 06:00:00 ?120 ?120 120 ? 120 ?12409
>>>> 2008-03-17 07:00:00 ?120 ?120 119 ? 119 ?34083
>>>> 2008-03-17 08:00:00 ?119 ?120 119 ? 120 ?34742
>>>> 2008-03-17 09:00:00 ?120 ?120 120 ? 120 ?29412
>>>> 2008-03-17 10:00:00 ?120 ?120 120 ? 120 ?40087
>>>> 2008-03-17 11:00:00 ?120 ?120 120 ? 120 ?21834
>>>> 2008-03-17 12:00:00 ?120 ?120 120 ? 120 ?26442
>>>> 2008-03-17 13:00:00 ?120 ?120 120 ? 120 ?36671
>>>> 2008-03-17 14:00:00 ?120 ?120 120 ? 120 ?20468
>>>> 2008-03-17 15:00:00 ?120 ?120 120 ? 120 ? 8802
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>
>>
>


From ggrothendieck at gmail.com  Wed Apr 29 11:15:56 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 29 Apr 2009 05:15:56 -0400
Subject: [R-SIG-Finance] Dates manipulation
In-Reply-To: <971536df0904270713h9999018y71b745591bc5df75@mail.gmail.com>
References: <829e6c8a0904270634j62697e2qf4878b8da943ea0d@mail.gmail.com> 
	<971536df0904270713h9999018y71b745591bc5df75@mail.gmail.com>
Message-ID: <971536df0904290215q4c31da41h5c6a0c3f866a16ed@mail.gmail.com>

Just one follow up.  Using the recent addition of the read.zoo
split= argument in the devel version of read.zoo we can
simplify this even further to:

> library(zoo)
>
> # grab devel version of read.zoo
> source("http://r-forge.r-project.org/plugins/scmsvn/viewcvs.php/*checkout*/pkg/R/read.zoo.R?rev=584&root=zoo")
>
> Lines <-
+ "Date    Ticker  Price
+ 2009-04-21      X       2.32
+ 2009-04-22      X       2.35
+ 2009-04-23      X       2.34
+ 2009-04-21      Y       10.2
+ 2009-04-23      Y       10.15 "
>
> read.zoo(textConnection(Lines), header = TRUE, split = 2)
              X     Y
2009-04-21 2.32 10.20
2009-04-22 2.35    NA
2009-04-23 2.34 10.15

In the above split = 2 causes the rows to be split according to the conditioning
variable in column 2 and then merges them back together essentially
reshaping the input from long format to wide format (using the same terminology
used by the reshape command in the core of R).

On Mon, Apr 27, 2009 at 10:13 AM, Gabor Grothendieck
<ggrothendieck at gmail.com> wrote:
> zoo objects represent time series and that is not a time series
> in its current form but you could read it in using read.table and
> then its only one line of code to transform it to one.
>
>
> library(zoo)
> Lines <-
> "Date ? ?Ticker ?Price
> 2009-04-21 ? ? ?X ? ? ? 2.32
> 2009-04-22 ? ? ?X ? ? ? 2.35
> 2009-04-23 ? ? ?X ? ? ? 2.34
> 2009-04-21 ? ? ?Y ? ? ? 10.2
> 2009-04-22 ? ? ?Y ? ? ? 10.32
> 2009-04-23 ? ? ?Y ? ? ? 10.15"
>
> DF <- read.table(textConnection(Lines), header = TRUE)
> DF$Date <- as.Date(DF$Date)
> do.call(merge, by(DF[-2], DF[2], function(d) zoo(d$Price, d$Date)))
>
>
> # Using read.zoo from the development version of zoo
> # the last line could even be reduced to this:
>
> source("http://r-forge.r-project.org/plugins/scmsvn/viewcvs.php/*checkout*/pkg/R/read.zoo.R?rev=519&root=zoo")
> do.call(merge, by(DF[-2], DF[2], read.zoo))
>
>
>
> Make sure you have a sufficiently recent version of zoo since
> read.zoo
>
>
> On Mon, Apr 27, 2009 at 9:34 AM, Manoj <manojsw at gmail.com> wrote:
>> Hi All,
>> ? I am using version 2,8,1 and working with zoo library.
>>
>> ? The sample/dummy data-set i am working off takes the following structure:
>>
>> Date ? ?Ticker ?Price
>> 2009-04-21????? X? ? ? ?2.32
>> 2009-04-22????? X? ? ? ?2.35
>> 2009-04-23????? X? ? ? ?2.34
>> 2009-04-21????? Y ? ? ? 10.2
>> 2009-04-22????? Y ? ? ? 10.32
>> 2009-04-23????? Y ? ? ? 10.15
>>
>> ? The data-frame consist of Date as a Date object, Ticker and Price.
>> Price can be uniquely identified by Date & Ticker.
>>
>> ? One of the issues with creating a zoo object with above data-set is
>> the fact that the unique key is basically a composite key so it's not
>> purely time-series data but does take the format of panel data.
>>
>> ?I want to be able to perform normal data function (lag,time etc) on
>> the above data-set for example, lag would return all the rows except
>> for ticker X&Y for 2009-04-23.
>>
>> ?Is there anyway, i can sub-class zoo object to achieve this functionality?
>>
>> ?Any suggestions would be greatly appreciated - thanks in advance.
>>
>>
>> Manoj
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>


From megh700004 at yahoo.com  Wed Apr 29 11:32:30 2009
From: megh700004 at yahoo.com (megh)
Date: Wed, 29 Apr 2009 02:32:30 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] VaR again
Message-ID: <23293725.post@talk.nabble.com>


Hi all Gurus, I have a problem to quantify the riskiness of a typical
position wherein this position is in some foreign country. Let me be more
specific on my problem.

Say I am a British investor and taken a position in NYSE, say in ATT (AT&T).
Therefore apart from the risk due to fluctuation in stock quote of that, I
am exposed of additional risk due to fluctuation in USD/GBP exchange rate. I
intend to calculate the VaR of this position in GBP. Here I used monte carlo
simulation approach to find that, which is as follows, please see the R code
:

# calculation of risk on an unit position
att <- 25.67                                                                     
# last traded price of AT&T in USD
usdgbp <- 0.68366                                                            
# last quote for USDGBP   
vcv <- matrix(c(5.33727E-05, 2.56709E-05, 2.56709E-05,0.000176556), 2)         
                                                                                       
# VCV matrix for AT&T and USDGBP

# We simulate 1-day ahead stock price and ex. rate assumig a Bi-variate
normal dist. with above VCV structure
library(mnormt)
simu <- exp(rmnorm(10000, c(log(25.67),log(0.68366)), vcv))
simu.pos.val <- apply(simu, 1, function(X) X[1]*X[2])                 #
Simulated value of my position in USD
abs(quantile(simu.pos.val, 0.05) - att*usdgbp)               # VaR (in GBP)
in terms of Maximum possible loss

Upto this point I am OK. However next thing automatically comes is that what
is contribution of Stock and Ex. rate in total risk, i.e. Dissecting the
risk. Can anyone please guide me how to do this for n-asset portfolio (of
this kind) under MCS framework?

Thanks
-- 
View this message in context: http://www.nabble.com/VaR-again-tp23293725p23293725.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From philjoubert at yahoo.com  Wed Apr 29 12:27:18 2009
From: philjoubert at yahoo.com (Phil Joubert)
Date: Wed, 29 Apr 2009 03:27:18 -0700 (PDT)
Subject: [R-SIG-Finance] RBloomberg - limit on size of return array?
Message-ID: <197118.21923.qm@web111509.mail.gq1.yahoo.com>


Hi 

I'm using RBloomberg to download timeseries data, but I'm getting an error. 

vsTickers <- unlist(blpGetData(oBbgConn, sIndex, "INDX_MEMBERS", retval="raw"))
vsTickers <- paste(vsTickers, "Equity")
dtStart <- chron("31/12/1998", format="d/m/y")
dtEnd <- chron("31/12/2008", format="d/m/y")
vdPrices <- blpGetData(oBbgConn, vsTickers, "PX_LAST", dtStart, dtEnd)

I expect this code to get the TS of the FTSE components over the last 10 years. Instead I get a zoo object with the correct number of columns, but no data, and the warning "In as.matrix.BlpCOMReturn(x) : NAs introduced by coercion".

I suspect the problem is the size of the return array. If I try to get a subset of the data (e.g. vsTickers[1:40] or [40:79], or smaller time frame) I have no problem. If I set retval="raw" the first element is "Error : Exception occurred.\n".

Any ideas? I've tried the same extraction via the VBA, but it just hangs.

thanks
Phil


From brian at braverock.com  Wed Apr 29 14:56:06 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 29 Apr 2009 07:56:06 -0500
Subject: [R-SIG-Finance] VaR again
In-Reply-To: <23293725.post@talk.nabble.com>
References: <23293725.post@talk.nabble.com>
Message-ID: <49F84E66.2000903@braverock.com>

megh wrote:
> Hi all Gurus, I have a problem to quantify the riskiness of a typical
> position wherein this position is in some foreign country. Let me be more
> specific on my problem.
>
> Say I am a British investor and taken a position in NYSE, say in ATT (AT&T).
> Therefore apart from the risk due to fluctuation in stock quote of that, I
> am exposed of additional risk due to fluctuation in USD/GBP exchange rate. I
> intend to calculate the VaR of this position in GBP. Here I used monte carlo
> simulation approach to find that, which is as follows, please see the R code
> :
>
> # calculation of risk on an unit position
> att <- 25.67                                                                     
> # last traded price of AT&T in USD
> usdgbp <- 0.68366                                                            
> # last quote for USDGBP   
> vcv <- matrix(c(5.33727E-05, 2.56709E-05, 2.56709E-05,0.000176556), 2)         
>                                                                                        
> # VCV matrix for AT&T and USDGBP
>
> # We simulate 1-day ahead stock price and ex. rate assumig a Bi-variate
> normal dist. with above VCV structure
> library(mnormt)
> simu <- exp(rmnorm(10000, c(log(25.67),log(0.68366)), vcv))
> simu.pos.val <- apply(simu, 1, function(X) X[1]*X[2])                 #
> Simulated value of my position in USD
> abs(quantile(simu.pos.val, 0.05) - att*usdgbp)               # VaR (in GBP)
> in terms of Maximum possible loss
>
> Upto this point I am OK. However next thing automatically comes is that what
> is contribution of Stock and Ex. rate in total risk, i.e. Dissecting the
> risk. Can anyone please guide me how to do this for n-asset portfolio (of
> this kind) under MCS framework?
>
> Thanks
>   

My first comment is that your Monte Carlo simulation makes the
assumption that the simulation is normally distributed.  Currency
markets are almost all much fatter tailed than that.  You might want to
consider a different VaR method that takes the fat tails into account.
Many people believe that because Monte Carlo VaR methods are
"non-parametric" that there are no distributional assumptions built in,
but a quick glance at your code should make it obvious that this is not
true, and how dangerous it is.

Next, If you want to separate the two components, and do it for a large
portfolio that may be mixed in different currencies  and instrument
types, you should be calculating VaR in percentages, using returns.
Then you calculate the market risk to the portfolio/instrument and the
currency risk to the portfolio/instrument separately. You can always
turn the aggregate number back to dollars/pounds/whatever if you
need/want to.  To get a univariate VaR number, you can aggregate the
positions by weight and calculate a univariate return for the portfolio
in a given currency, from which VaR may be taken (but see below on
portfolio VaR).

I think it is rather important to look at the actual observed (or
estimated) variance, skewness, and kurtosis of each instrument in
working out the VaR for that instrument, via simulation or any other
method.  Then, you can separately work out the currency risk to your
portfolio once you aggregate the risks of each position in a given currency.

Finally, if you are doing this in a portfolio context, the co-moments of
the instruments have a massive effect on the total portfolio risk.
There is a large literature on "Portfolio VaR" or "Component VaR" that
discusses this and its solution. Component VaR is subadditive, so you
can dissect the risk as you ask for above.

Regards,

    - Brian


-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From tariq.khan at gmail.com  Wed Apr 29 15:06:03 2009
From: tariq.khan at gmail.com (Tariq Khan)
Date: Wed, 29 Apr 2009 14:06:03 +0100
Subject: [R-SIG-Finance] quantmod: overlaying time series bar charts
In-Reply-To: <fb92d9f00904280549t4594c8a7t77bdef33c7c1cf23@mail.gmail.com>
References: <fb92d9f00904280549t4594c8a7t77bdef33c7c1cf23@mail.gmail.com>
Message-ID: <2310043c0904290606l242cc691k8a0e2cbb5476ee21@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090429/de682071/attachment.pl>

From jeff.a.ryan at gmail.com  Wed Apr 29 16:15:04 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 29 Apr 2009 09:15:04 -0500
Subject: [R-SIG-Finance] quantmod: overlaying time series bar charts
In-Reply-To: <fb92d9f00904280549t4594c8a7t77bdef33c7c1cf23@mail.gmail.com>
References: <fb92d9f00904280549t4594c8a7t77bdef33c7c1cf23@mail.gmail.com>
Message-ID: <e8e755250904290715q4f529874nbb068ea37973bba0@mail.gmail.com>

Sebastian,

At present you can't do that on one graphical region.

If you would like two plotted above and below each other, this would
be a solution:

layout(matrix(1:2, nrow=2))
chartSeries(ts1, layout=NULL, TA=NULL)
chartSeries(ts2, layout=NULL, TA=NULL)
layout(1)

The first layout call will have to be adjusted if you make use of
additional TA windows. ?layout is the place to start for that.

Just curious, as this has been requested before, what is your use
case/rationale for the overlay?

Unless the series share the same scale, the charts are bound to be
very confusing.  Even with the same scale I am not too sure you will
be deriving any information from the overlay.

Jeff

On Tue, Apr 28, 2009 at 7:49 AM, Sebastian Hauer
<sebastian.hauer at gmail.com> wrote:
> Hello,
> I was wondering if there is an easy way of overlaying two bar charts
> using quantmod?
> Here is what I am trying to do:
>
> chartSeries(ts1, type = "bars", show.grid = TRUE, up.col = "green",
> dn.col = "green")
> chartSeries(ts2, type = "bars", show.grid = TRUE, up.col = "red",
> dn.col = "red")
>
> But have ts2 show in the same plot frame or if that does not work have
> it show in a frame below the first one.
>
> Regards,
> Sebastian
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From josh.m.ulrich at gmail.com  Wed Apr 29 17:07:42 2009
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Wed, 29 Apr 2009 10:07:42 -0500
Subject: [R-SIG-Finance] TTR's RSI
In-Reply-To: <197a5bbc0904282128y62ea854evaed665e1efb598d@mail.gmail.com>
References: <197a5bbc0904281529t3c2df244m7e4a858830391e2a@mail.gmail.com>
	<8cca69990904281549t637ce0d5tf6f4bf2f2182300d@mail.gmail.com>
	<197a5bbc0904281643r375fcc88rb6248ab45be9dae@mail.gmail.com>
	<197a5bbc0904281646y16769c7cg9a8158e36cc96bc@mail.gmail.com>
	<8cca69990904281708y49ce53b9re2f0376641525984@mail.gmail.com>
	<197a5bbc0904281732r18d42a7cxbfe10cb954f14fc2@mail.gmail.com>
	<8cca69990904281741g7ad6c41bi867403478346f2a3@mail.gmail.com>
	<197a5bbc0904282128y62ea854evaed665e1efb598d@mail.gmail.com>
Message-ID: <8cca69990904290807q7177544ax8969bff66fa92b1f@mail.gmail.com>

To provide closure, updating to the latest version of R and TTR
resolved this issue.

--
http://www.fosstrading.com



On Tue, Apr 28, 2009 at 11:28 PM, Hae Kyung Im <haky.im at gmail.com> wrote:
> Hi Josh,
> thanks for your help. I tried so many different things that I am not
> exactly sure what it was that made it work. I installed the latest
> version of R and then I could install the latest version of TTR as
> well.
> Haky
>
> On Tue, Apr 28, 2009 at 7:41 PM, Josh Ulrich <josh.m.ulrich at gmail.com> wrote:
>> That's odd. ?Does your mirror have the updated Mac binary?
>>
>> You can use install.packages() to install from your local disk. ?See
>> the Examples in ?install.packages.
>>
>> Best,
>> Josh
>> --
>> http://quantemplation.blogspot.com
>> http://www.fosstrading.com
>>
>>
>>
>> On Tue, Apr 28, 2009 at 7:32 PM, Hae Kyung Im <haky.im at gmail.com> wrote:
>>> I see... Thanks.
>>>
>>> But when I install TTR using install.packages, I get version 0.14.
>>>
>>> I downloaded the new version in my hard drive but I'm not sure how to
>>> install from local drive. I am using MacOs.
>>> Do you know how to do this on a Mac?
>>>
>>> Thanks
>>> Haky
>>>
>>>
>>>
>>> On Tue, Apr 28, 2009 at 7:08 PM, Josh Ulrich <josh.m.ulrich at gmail.com> wrote:
>>>> Haky,
>>>>
>>>> It appears you're not using TTR_0.2. ?You should not have this issue
>>>> with the newest version.
>>>>
>>>> Best,
>>>> Josh
>>>> --
>>>> http://quantemplation.blogspot.com
>>>> http://www.fosstrading.com
>>>>
>>>>
>>>>
>>>> On Tue, Apr 28, 2009 at 6:46 PM, Hae Kyung Im <haky.im at gmail.com> wrote:
>>>>> well... this time with the attachment
>>>>>
>>>>>
>>>>>
>>>>> On Tue, Apr 28, 2009 at 6:43 PM, Hae Kyung Im <haky.im at gmail.com> wrote:
>>>>>> Hi Josh,
>>>>>>
>>>>>> I am attaching the following files
>>>>>>
>>>>>> test.Rdata (with data.xmin, the xts object)
>>>>>> replic.r
>>>>>> replic.output.txt (output when I source replic.r)
>>>>>>
>>>>>> Thanks
>>>>>> Haky
>>>>>>
>>>>>>
>>>>>> On Tue, Apr 28, 2009 at 5:49 PM, Josh Ulrich <josh.m.ulrich at gmail.com> wrote:
>>>>>>> Hi Haky,
>>>>>>>
>>>>>>> Could you provide more information? ?I cannot replicate your results.
>>>>>>>
>>>>>>>> Lines <- " Open High Low Close Volume
>>>>>>> \"2008-03-16 17:00:00\" ?119 ?120 119 ? 119 ? 1305
>>>>>>> \"2008-03-16 18:00:00\" ?119 ?120 119 ? 120 ? 3237
>>>>>>> \"2008-03-16 19:00:00\" ?120 ?120 120 ? 120 ? 2813
>>>>>>> \"2008-03-16 20:00:00\" ?120 ?120 120 ? 120 ? 2857
>>>>>>> \"2008-03-16 21:00:00\" ?120 ?120 120 ? 120 ? 4561
>>>>>>> \"2008-03-16 22:00:00\" ?120 ?120 120 ? 120 ? 1988
>>>>>>> \"2008-03-16 23:00:00\" ?120 ?120 120 ? 120 ? ?805
>>>>>>> \"2008-03-17 00:00:00\" ?120 ?120 120 ? 120 ? 1177
>>>>>>> \"2008-03-17 01:00:00\" ?120 ?120 120 ? 120 ? 1245
>>>>>>> \"2008-03-17 02:00:00\" ?120 ?120 120 ? 120 ? 1263
>>>>>>> \"2008-03-17 03:00:00\" ?120 ?120 120 ? 120 ? 5565
>>>>>>> \"2008-03-17 04:00:00\" ?120 ?120 120 ? 120 ? 4081
>>>>>>> \"2008-03-17 05:00:00\" ?120 ?120 120 ? 120 ? 4462
>>>>>>> \"2008-03-17 06:00:00\" ?120 ?120 120 ? 120 ?12409
>>>>>>> \"2008-03-17 07:00:00\" ?120 ?120 119 ? 119 ?34083
>>>>>>> \"2008-03-17 08:00:00\" ?119 ?120 119 ? 120 ?34742
>>>>>>> \"2008-03-17 09:00:00\" ?120 ?120 120 ? 120 ?29412
>>>>>>> \"2008-03-17 10:00:00\" ?120 ?120 120 ? 120 ?40087
>>>>>>> \"2008-03-17 11:00:00\" ?120 ?120 120 ? 120 ?21834
>>>>>>> \"2008-03-17 12:00:00\" ?120 ?120 120 ? 120 ?26442
>>>>>>> \"2008-03-17 13:00:00\" ?120 ?120 120 ? 120 ?36671
>>>>>>> \"2008-03-17 14:00:00\" ?120 ?120 120 ? 120 ?20468
>>>>>>> \"2008-03-17 15:00:00\" ?120 ?120 120 ? 120 ? 8802"
>>>>>>>> data.60min <- read.table(textConnection(Lines))
>>>>>>>> dim(data.60min)
>>>>>>> [1] 23 ?5
>>>>>>>> #class(data.xmin) ?# I don't know what the 'data.xmin' object is...
>>>>>>>> class(data.60min)
>>>>>>> [1] "data.frame"
>>>>>>>> length(RSI(data.60min[,"Close"]))
>>>>>>> [1] 23
>>>>>>>> length(RSI(as.xts(data.60min)[,"Close"]))
>>>>>>> [1] 23
>>>>>>>> sessionInfo()
>>>>>>> R version 2.9.0 (2009-04-17)
>>>>>>> i386-unknown-freebsd7.1
>>>>>>>
>>>>>>> locale:
>>>>>>> C
>>>>>>>
>>>>>>> attached base packages:
>>>>>>> [1] stats ? ? graphics ?grDevices utils ? ? datasets ?methods ? base
>>>>>>>
>>>>>>> other attached packages:
>>>>>>> [1] TTR_0.2-1 xts_0.6-4 zoo_1.5-5
>>>>>>>
>>>>>>> loaded via a namespace (and not attached):
>>>>>>> [1] grid_2.9.0 ? ? ?lattice_0.17-22
>>>>>>>
>>>>>>> Best,
>>>>>>> Josh
>>>>>>> --
>>>>>>> http://quantemplation.blogspot.com
>>>>>>> http://www.fosstrading.com
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> On Tue, Apr 28, 2009 at 5:29 PM, Hae Kyung Im <haky.im at gmail.com> wrote:
>>>>>>>> Hi,
>>>>>>>>
>>>>>>>> for some reason when I calculate RSI on a xts object of length n, I
>>>>>>>> get a result of length n+1 as in the following example:
>>>>>>>>
>>>>>>>>> dim(data.60min)
>>>>>>>> [1] 23 ?5
>>>>>>>>
>>>>>>>>> class(data.xmin)
>>>>>>>> [1] "xts" "zoo"
>>>>>>>>
>>>>>>>>> length(RSI(data.60min[,"Close"]))
>>>>>>>> [1] 24
>>>>>>>>
>>>>>>>> If I plug in a vector of length 23, RSI gives me a vector of length 23.
>>>>>>>>
>>>>>>>>> length(RSI(runif(23)))
>>>>>>>> [1] 23
>>>>>>>>
>>>>>>>> am I missing something here?
>>>>>>>>
>>>>>>>> Thanks
>>>>>>>> Haky
>>>>>>>>
>>>>>>>>
>>>>>>>> --------------------------------------------
>>>>>>>> Here is the data for your reference:
>>>>>>>>> data.60min
>>>>>>>> ? ? ? ? ? ? ? ? ? ?Open High Low Close Volume
>>>>>>>> 2008-03-16 17:00:00 ?119 ?120 119 ? 119 ? 1305
>>>>>>>> 2008-03-16 18:00:00 ?119 ?120 119 ? 120 ? 3237
>>>>>>>> 2008-03-16 19:00:00 ?120 ?120 120 ? 120 ? 2813
>>>>>>>> 2008-03-16 20:00:00 ?120 ?120 120 ? 120 ? 2857
>>>>>>>> 2008-03-16 21:00:00 ?120 ?120 120 ? 120 ? 4561
>>>>>>>> 2008-03-16 22:00:00 ?120 ?120 120 ? 120 ? 1988
>>>>>>>> 2008-03-16 23:00:00 ?120 ?120 120 ? 120 ? ?805
>>>>>>>> 2008-03-17 00:00:00 ?120 ?120 120 ? 120 ? 1177
>>>>>>>> 2008-03-17 01:00:00 ?120 ?120 120 ? 120 ? 1245
>>>>>>>> 2008-03-17 02:00:00 ?120 ?120 120 ? 120 ? 1263
>>>>>>>> 2008-03-17 03:00:00 ?120 ?120 120 ? 120 ? 5565
>>>>>>>> 2008-03-17 04:00:00 ?120 ?120 120 ? 120 ? 4081
>>>>>>>> 2008-03-17 05:00:00 ?120 ?120 120 ? 120 ? 4462
>>>>>>>> 2008-03-17 06:00:00 ?120 ?120 120 ? 120 ?12409
>>>>>>>> 2008-03-17 07:00:00 ?120 ?120 119 ? 119 ?34083
>>>>>>>> 2008-03-17 08:00:00 ?119 ?120 119 ? 120 ?34742
>>>>>>>> 2008-03-17 09:00:00 ?120 ?120 120 ? 120 ?29412
>>>>>>>> 2008-03-17 10:00:00 ?120 ?120 120 ? 120 ?40087
>>>>>>>> 2008-03-17 11:00:00 ?120 ?120 120 ? 120 ?21834
>>>>>>>> 2008-03-17 12:00:00 ?120 ?120 120 ? 120 ?26442
>>>>>>>> 2008-03-17 13:00:00 ?120 ?120 120 ? 120 ?36671
>>>>>>>> 2008-03-17 14:00:00 ?120 ?120 120 ? 120 ?20468
>>>>>>>> 2008-03-17 15:00:00 ?120 ?120 120 ? 120 ? 8802
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>>>>> -- Subscriber-posting only.
>>>>>>>> -- If you want to post, subscribe first.
>>>>>>>>
>>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>


From ctchadwick at hotmail.com  Wed Apr 29 17:51:05 2009
From: ctchadwick at hotmail.com (resident76)
Date: Wed, 29 Apr 2009 08:51:05 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] VaR again
In-Reply-To: <23293725.post@talk.nabble.com>
References: <23293725.post@talk.nabble.com>
Message-ID: <23296123.post@talk.nabble.com>


I have some code that handles the risk dissection for N assets, but it's
incorporated into some larger code so it might be confusing to just hand it
to you without trying to adapt it in a better form.  I would be glad to help
out and post it up for the other folks out there as well once I clean up the
confusion in it.  

The code doesn't address the V at R side, only the contribution of an asset's
volatility to the total volatility.  I'll take a look at your example and
try to formulate the code if that is something you might be interested in.

res


megh wrote:
> 
> Hi all Gurus, I have a problem to quantify the riskiness of a typical
> position wherein this position is in some foreign country. Let me be more
> specific on my problem.
> 
> Say I am a British investor and taken a position in NYSE, say in ATT
> (AT&T). Therefore apart from the risk due to fluctuation in stock quote of
> that, I am exposed of additional risk due to fluctuation in USD/GBP
> exchange rate. I intend to calculate the VaR of this position in GBP. Here
> I used monte carlo simulation approach to find that, which is as follows,
> please see the R code :
> 
> # calculation of risk on an unit position
> att <- 25.67                                                                     
> # last traded price of AT&T in USD
> usdgbp <- 0.68366                                                            
> # last quote for USDGBP   
> vcv <- matrix(c(5.33727E-05, 2.56709E-05, 2.56709E-05,0.000176556), 2)         
>                                                                                        
> # VCV matrix for AT&T and USDGBP
> 
> # We simulate 1-day ahead stock price and ex. rate assumig a Bi-variate
> normal dist. with above VCV structure
> library(mnormt)
> simu <- exp(rmnorm(10000, c(log(25.67),log(0.68366)), vcv))
> simu.pos.val <- apply(simu, 1, function(X) X[1]*X[2])                 #
> Simulated value of my position in USD
> abs(quantile(simu.pos.val, 0.05) - att*usdgbp)               # VaR (in
> GBP) in terms of Maximum possible loss
> 
> Upto this point I am OK. However next thing automatically comes is that
> what is contribution of Stock and Ex. rate in total risk, i.e. Dissecting
> the risk. Can anyone please guide me how to do this for n-asset portfolio
> (of this kind) under MCS framework?
> 
> Thanks
> 

-- 
View this message in context: http://www.nabble.com/VaR-again-tp23293725p23296123.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From Jiqiong.Dai at soros.com  Wed Apr 29 18:28:22 2009
From: Jiqiong.Dai at soros.com (Dai, Jiqiong)
Date: Wed, 29 Apr 2009 12:28:22 -0400
Subject: [R-SIG-Finance] quantmod: overlaying time series bar charts
In-Reply-To: <e8e755250904290715q4f529874nbb068ea37973bba0@mail.gmail.com>
References: <fb92d9f00904280549t4594c8a7t77bdef33c7c1cf23@mail.gmail.com>
	<e8e755250904290715q4f529874nbb068ea37973bba0@mail.gmail.com>
Message-ID: <78035F3A7265E94484B856D311E750E80248E21C@NYC-SOR-EXCH-06.SorosFunds.com>

In  quantmod:::chartSeries.chob, comment out:

rect(coords[1], coords[3], coords[2], coords[4], col = x at colors$area)

You will make it.



Jiqiong Dai
Soros Fund Management, LLC
888 7th Avenue, 32nd Floor
New York, New York 10106
Tel: (212) 320-5741
Fax: (646) 731-5741

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Jeff Ryan
Sent: Wednesday, April 29, 2009 10:15 AM
To: Sebastian Hauer
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] quantmod: overlaying time series bar charts

Sebastian,

At present you can't do that on one graphical region.

If you would like two plotted above and below each other, this would be
a solution:

layout(matrix(1:2, nrow=2))
chartSeries(ts1, layout=NULL, TA=NULL)
chartSeries(ts2, layout=NULL, TA=NULL)
layout(1)

The first layout call will have to be adjusted if you make use of
additional TA windows. ?layout is the place to start for that.

Just curious, as this has been requested before, what is your use
case/rationale for the overlay?

Unless the series share the same scale, the charts are bound to be very
confusing.  Even with the same scale I am not too sure you will be
deriving any information from the overlay.

Jeff

On Tue, Apr 28, 2009 at 7:49 AM, Sebastian Hauer
<sebastian.hauer at gmail.com> wrote:
> Hello,
> I was wondering if there is an easy way of overlaying two bar charts 
> using quantmod?
> Here is what I am trying to do:
>
> chartSeries(ts1, type = "bars", show.grid = TRUE, up.col = "green", 
> dn.col = "green") chartSeries(ts2, type = "bars", show.grid = TRUE, 
> up.col = "red", dn.col = "red")
>
> But have ts2 show in the same plot frame or if that does not work have

> it show in a frame below the first one.
>
> Regards,
> Sebastian
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



--
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From megh700004 at yahoo.com  Wed Apr 29 18:49:15 2009
From: megh700004 at yahoo.com (Megh Dal)
Date: Wed, 29 Apr 2009 09:49:15 -0700 (PDT)
Subject: [R-SIG-Finance] VaR again
In-Reply-To: <49F84E66.2000903@braverock.com>
Message-ID: <483354.42803.qm@web58101.mail.re3.yahoo.com>


Hi Brian, Thank you so much for this kind reply. I understand that, in VaR calculation all Kurtosis, skewness need to be incorporated and therefore some other fat-tailed distribution like t-dist can be better alternative of Normal distribution. However here my purpose is little bit different. Here my goal is not to calculate VaR most accurately using some complex distribution/modelling (however you also agree that a sophisticated model does not guarantee a better estimate) rather to get a overall view of total risk of my portfolio and contribution of each component on that.

Here you suggested to calculate VaR in percentage. however I am not sure how to do it in current scenario because, Currency exposure is not "explicit" here. Am I missing something detail here? Can you please be more specific? Any reproducible example and/or web resources will be truly helpful to me.

Thanks,


--- On Wed, 4/29/09, Brian G. Peterson <brian at braverock.com> wrote:

> From: Brian G. Peterson <brian at braverock.com>
> Subject: Re: [R-SIG-Finance] VaR again
> To: "megh" <megh700004 at yahoo.com>
> Cc: r-sig-finance at stat.math.ethz.ch
> Date: Wednesday, April 29, 2009, 6:26 PM
> megh wrote:
> > Hi all Gurus, I have a problem to quantify the
> riskiness of a typical
> > position wherein this position is in some foreign
> country. Let me be more
> > specific on my problem.
> > 
> > Say I am a British investor and taken a position in
> NYSE, say in ATT (AT&T).
> > Therefore apart from the risk due to fluctuation in
> stock quote of that, I
> > am exposed of additional risk due to fluctuation in
> USD/GBP exchange rate. I
> > intend to calculate the VaR of this position in GBP.
> Here I used monte carlo
> > simulation approach to find that, which is as follows,
> please see the R code
> > :
> > 
> > # calculation of risk on an unit position
> > att <- 25.67                                       
>                              # last traded price of AT&T
> in USD
> > usdgbp <- 0.68366                                  
>                          # last quote for USDGBP   vcv <-
> matrix(c(5.33727E-05, 2.56709E-05, 2.56709E-05,0.000176556),
> 2)                                                          
>                                      # VCV matrix for
> AT&T and USDGBP
> > 
> > # We simulate 1-day ahead stock price and ex. rate
> assumig a Bi-variate
> > normal dist. with above VCV structure
> > library(mnormt)
> > simu <- exp(rmnorm(10000,
> c(log(25.67),log(0.68366)), vcv))
> > simu.pos.val <- apply(simu, 1, function(X)
> X[1]*X[2])                 #
> > Simulated value of my position in USD
> > abs(quantile(simu.pos.val, 0.05) - att*usdgbp)        
>       # VaR (in GBP)
> > in terms of Maximum possible loss
> > 
> > Upto this point I am OK. However next thing
> automatically comes is that what
> > is contribution of Stock and Ex. rate in total risk,
> i.e. Dissecting the
> > risk. Can anyone please guide me how to do this for
> n-asset portfolio (of
> > this kind) under MCS framework?
> > 
> > Thanks
> >   
> 
> My first comment is that your Monte Carlo simulation makes
> the
> assumption that the simulation is normally distributed. 
> Currency
> markets are almost all much fatter tailed than that.  You
> might want to
> consider a different VaR method that takes the fat tails
> into account.
> Many people believe that because Monte Carlo VaR methods
> are
> "non-parametric" that there are no distributional
> assumptions built in,
> but a quick glance at your code should make it obvious that
> this is not
> true, and how dangerous it is.
> 
> Next, If you want to separate the two components, and do it
> for a large
> portfolio that may be mixed in different currencies  and
> instrument
> types, you should be calculating VaR in percentages, using
> returns.
> Then you calculate the market risk to the
> portfolio/instrument and the
> currency risk to the portfolio/instrument separately. You
> can always
> turn the aggregate number back to dollars/pounds/whatever
> if you
> need/want to.  To get a univariate VaR number, you can
> aggregate the
> positions by weight and calculate a univariate return for
> the portfolio
> in a given currency, from which VaR may be taken (but see
> below on
> portfolio VaR).
> 
> I think it is rather important to look at the actual
> observed (or
> estimated) variance, skewness, and kurtosis of each
> instrument in
> working out the VaR for that instrument, via simulation or
> any other
> method.  Then, you can separately work out the currency
> risk to your
> portfolio once you aggregate the risks of each position in
> a given currency.
> 
> Finally, if you are doing this in a portfolio context, the
> co-moments of
> the instruments have a massive effect on the total
> portfolio risk.
> There is a large literature on "Portfolio VaR" or
> "Component VaR" that
> discusses this and its solution. Component VaR is
> subadditive, so you
> can dissect the risk as you ask for above.
> 
> Regards,
> 
>    - Brian
> 
> 
> -- Brian G. Peterson
> http://braverock.com/brian/
> Ph: 773-459-4973
> IM: bgpbraverock


From ctchadwick at hotmail.com  Thu Apr 30 04:37:34 2009
From: ctchadwick at hotmail.com (resident76)
Date: Wed, 29 Apr 2009 19:37:34 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] VaR again
In-Reply-To: <23293725.post@talk.nabble.com>
References: <23293725.post@talk.nabble.com>
Message-ID: <23307093.post@talk.nabble.com>


megh,

Like you, I agree with Brian as well about the normality issue.  I have the
following code to estimate fractional contributions to risk assuming normal
returns (for calculating var-cov matrix at least).   I tried to make it
general so you can slap it in with a minimal effort.  It should work for N
assets, just as long as you define vector of weights to reflect the number
of assets...

The calculations are based on a paper by Edward Qian: On the Financial
Interpretation of Risk Contribution: Risk Budgets Do Add Up, located at:

http://papers.ssrn.com/sol3/papers.cfm?abstract_id=684221

hope that helps out.

res

#----------------------------------------------------------------------------------------------
# Standard mean-variance portfolio estimates
# Use mean returns on closing prices for covariance matrix construction

# The following code assumes you have a vector of weights such as:
#         wts <- matrix(c(0.4,0.6),ncol=2,nrow=1,byrow=T)
# and a matrix of asset prices where the columns are the individual assets
such as:
#         assets <- cbind(open, high, low, close) = cbind(O,H,L,C)
# so for multiple assets:
#         assets <- cbind(O1,H1,L1,C1, O2,H2,L2,C2, ..., On,Hn,Ln,Cn)
# then transform assets to log assets by:
# ln.assets <- log(assets)

# assets.labels is a vector of length N of the asset names:
#         assets.labels <- c("SBUX", "SPX", "C", ..., "asset-N")
# annualization is the factor used to adjust the volatility to annual
volatility
# Rf is the risk-free rate
#


ind <- 1:length(assets.labels)
assets.rtn <- ln.assets[2:N,(4*ind)] - ln.assets[1:(N-1),(4*ind)]
cov.rtn <- cov(assets.rtn); colnames(cov.rtn) = assets.labels ;
rownames(cov.rtn) = assets.labels
cor.rtn <- cor(assets.rtn); colnames(cor.rtn) = assets.labels ;
rownames(cor.rtn) = assets.labels

vol.scen <- matrix(0.0,ncol=2,nrow=nrow(wts))
vol.scen[,1] <- sqrt(wts%*%cov.rtn%*%t(wts))
vol.scen[,2] <- vol.scen[,1]*sqrt(annualization)
vol.scen <- t(vol.scen)

# Expected Returns & Sharpe Ratio
E.rtn <- sum(wts*colMeans(assets.rtn))- Rf
Srp <- E.rtn/vol.scen[1,1]

cat("Standard Portfolio E(R) & Vol Given Scenario Weights:", "\n")
print(round(vol.scen*100,2))
cat("E(R):     ", round(E.rtn*100,4), "\n")
cat("Sharpe:   ", round(Srp*100,4), "\n")
cat("Weights:  ", "\n")
print(round(wts,3))
cat("Assest Exposure:  ",sum(wts), "\n")
cat("-----------------------------------------------------------------------------------------",
"\n")


#----------------------------------------------------------------------------------------------
# Proportional Contribution to Risk Estimate
# --start with partial derivatives wrt weights--

partials <- matrix(0.0,ncol=1 ,nrow=length(wts))
for(i in 1:length(wts)){
partials[i,] <- sum(wts%*%cov.rtn[i,])
}
partials <- partials*(1/vol.scen[1,1])
frac.contrib <- t(wts)*partials*(1/vol.scen[1,1])
rownames(frac.contrib) = assets.labels ; colnames(frac.contrib) = ann.name

cat("Percentage Contribution To Total Risk:", "\n")
print(round(frac.contrib*100,2))
cat("-----------------------------------------------------------------------------------------",
"\n")
#----------------------------------------------------------------------------------------------





-- 
View this message in context: http://www.nabble.com/VaR-again-tp23293725p23307093.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From Anil.Vijendran.wg08 at wharton.upenn.edu  Thu Apr 30 15:57:25 2009
From: Anil.Vijendran.wg08 at wharton.upenn.edu (Anil Vijendran)
Date: Thu, 30 Apr 2009 06:57:25 -0700
Subject: [R-SIG-Finance] fPortfolio custom constraints
Message-ID: <f6ac578e0904300657q2d295a45u7ba4c53e6062be20@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090430/50bd3dd1/attachment.pl>

From brian at braverock.com  Thu Apr 30 17:16:53 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 30 Apr 2009 10:16:53 -0500
Subject: [R-SIG-Finance] VaR again
In-Reply-To: <23307093.post@talk.nabble.com>
References: <23293725.post@talk.nabble.com> <23307093.post@talk.nabble.com>
Message-ID: <49F9C0E5.7050709@braverock.com>

res,

We've got functions in PerformanceAnalytics that do this as well, 
including for non-normally distributed portfolio instruments.  They're 
in the current CRAN version, and improved and expanded in the version 
we're getting ready for release.

We extended Qian's two-instrument example to n-instrument portfolios in 
our paper in JoR "Estimation and Decomposition of Downside Risk for 
Portfolios with Non-Normal Returns", and added modified expected 
shortfall and its portfolio counterpart as well.

Regards,

  - Brian

resident76 wrote:
> megh,
>
> Like you, I agree with Brian as well about the normality issue.  I have the
> following code to estimate fractional contributions to risk assuming normal
> returns (for calculating var-cov matrix at least).   I tried to make it
> general so you can slap it in with a minimal effort.  It should work for N
> assets, just as long as you define vector of weights to reflect the number
> of assets...
>
> The calculations are based on a paper by Edward Qian: On the Financial
> Interpretation of Risk Contribution: Risk Budgets Do Add Up, located at:
>
> http://papers.ssrn.com/sol3/papers.cfm?abstract_id=684221
>
> hope that helps out.
>
> res
>
> #----------------------------------------------------------------------------------------------
> # Standard mean-variance portfolio estimates
> # Use mean returns on closing prices for covariance matrix construction
>
> # The following code assumes you have a vector of weights such as:
> #         wts <- matrix(c(0.4,0.6),ncol=2,nrow=1,byrow=T)
> # and a matrix of asset prices where the columns are the individual assets
> such as:
> #         assets <- cbind(open, high, low, close) = cbind(O,H,L,C)
> # so for multiple assets:
> #         assets <- cbind(O1,H1,L1,C1, O2,H2,L2,C2, ..., On,Hn,Ln,Cn)
> # then transform assets to log assets by:
> # ln.assets <- log(assets)
>
> # assets.labels is a vector of length N of the asset names:
> #         assets.labels <- c("SBUX", "SPX", "C", ..., "asset-N")
> # annualization is the factor used to adjust the volatility to annual
> volatility
> # Rf is the risk-free rate
> #
>
>
> ind <- 1:length(assets.labels)
> assets.rtn <- ln.assets[2:N,(4*ind)] - ln.assets[1:(N-1),(4*ind)]
> cov.rtn <- cov(assets.rtn); colnames(cov.rtn) = assets.labels ;
> rownames(cov.rtn) = assets.labels
> cor.rtn <- cor(assets.rtn); colnames(cor.rtn) = assets.labels ;
> rownames(cor.rtn) = assets.labels
>
> vol.scen <- matrix(0.0,ncol=2,nrow=nrow(wts))
> vol.scen[,1] <- sqrt(wts%*%cov.rtn%*%t(wts))
> vol.scen[,2] <- vol.scen[,1]*sqrt(annualization)
> vol.scen <- t(vol.scen)
>
> # Expected Returns & Sharpe Ratio
> E.rtn <- sum(wts*colMeans(assets.rtn))- Rf
> Srp <- E.rtn/vol.scen[1,1]
>
> cat("Standard Portfolio E(R) & Vol Given Scenario Weights:", "\n")
> print(round(vol.scen*100,2))
> cat("E(R):     ", round(E.rtn*100,4), "\n")
> cat("Sharpe:   ", round(Srp*100,4), "\n")
> cat("Weights:  ", "\n")
> print(round(wts,3))
> cat("Assest Exposure:  ",sum(wts), "\n")
> cat("-----------------------------------------------------------------------------------------",
> "\n")
>
>
> #----------------------------------------------------------------------------------------------
> # Proportional Contribution to Risk Estimate
> # --start with partial derivatives wrt weights--
>
> partials <- matrix(0.0,ncol=1 ,nrow=length(wts))
> for(i in 1:length(wts)){
> partials[i,] <- sum(wts%*%cov.rtn[i,])
> }
> partials <- partials*(1/vol.scen[1,1])
> frac.contrib <- t(wts)*partials*(1/vol.scen[1,1])
> rownames(frac.contrib) = assets.labels ; colnames(frac.contrib) = ann.name
>
> cat("Percentage Contribution To Total Risk:", "\n")
> print(round(frac.contrib*100,2))
> cat("-----------------------------------------------------------------------------------------",
> "\n")
> #----------------------------------------------------------------------------------------------
>
>
>
>
>
>   


-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From ymoser at gmail.com  Thu Apr 30 17:53:32 2009
From: ymoser at gmail.com (Yaakov Moser)
Date: Thu, 30 Apr 2009 18:53:32 +0300
Subject: [R-SIG-Finance] fPortfolio - portfolioFrontier - Limit on number of
	assets
Message-ID: <49F9C97C.6030801@gmail.com>

Is there a limit on the number of assets that can be entered into a 
portfolio to be optimized?
I have found that at around 60 it stops giving results.
Is this a limit on the calculation, or something due to my dataset?

Thanks

Yaakov


From haky.im at gmail.com  Thu Apr 30 22:56:52 2009
From: haky.im at gmail.com (Hae Kyung Im)
Date: Thu, 30 Apr 2009 15:56:52 -0500
Subject: [R-SIG-Finance] abline for quantmod charts
Message-ID: <197a5bbc0904301356i37892c1fta735e7b1c333b784@mail.gmail.com>

Hi,

I was trying to add vertical lines to a quantmod plot using

abline( v = somdatetime ) like in the following example

> getSymbols("YHOO")
[1] "YHOO"
> chartSeries(YHOO)
> abline( v=as.POSIXct("2008-04-01","%Y-%m-%d") )

But nothing shows up on the chart.

Any ideas on how to do this?

Thanks
Haky


From jeff.a.ryan at gmail.com  Thu Apr 30 23:16:48 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 30 Apr 2009 16:16:48 -0500
Subject: [R-SIG-Finance] abline for quantmod charts
In-Reply-To: <197a5bbc0904301356i37892c1fta735e7b1c333b784@mail.gmail.com>
References: <197a5bbc0904301356i37892c1fta735e7b1c333b784@mail.gmail.com>
Message-ID: <e8e755250904301416p5cae9771med5fd8d41207e349@mail.gmail.com>

Hi Hae,

The best way to do this at the moment is with addTA:

chartSeries(YHOO)
addTA(xts(TRUE,as.Date("2008-04-01")),on=-(1:2),col="#333333")

zoomChart('200803/200804')

You'll notice that it draws a thick line (region).  This is usually
more useful than just a line on quantmod charts, as the spacing is
between observations is padded.

I will at some point be adding access for primitive graphics function
to the whole charting process, but for now this should do what you
want.


HTH,
Jeff

On Thu, Apr 30, 2009 at 3:56 PM, Hae Kyung Im <haky.im at gmail.com> wrote:
> Hi,
>
> I was trying to add vertical lines to a quantmod plot using
>
> abline( v = somdatetime ) like in the following example
>
>> getSymbols("YHOO")
> [1] "YHOO"
>> chartSeries(YHOO)
>> abline( v=as.POSIXct("2008-04-01","%Y-%m-%d") )
>
> But nothing shows up on the chart.
>
> Any ideas on how to do this?
>
> Thanks
> Haky
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From landronimirc at gmail.com  Fri May  1 14:50:48 2009
From: landronimirc at gmail.com (Liviu Andronic)
Date: Fri, 1 May 2009 14:50:48 +0200
Subject: [R-SIG-Finance] fit NGARCH model
Message-ID: <68b1e2610905010550w61616389h2267d6c4e7062504@mail.gmail.com>

Dear all,
Can anyone suggest a library that allows for fitting Non-linear GARCH
models? Rmetrics seems not to provide this. I did find the rgarch [1]
package with fit.NGARCH(), but it is still unpublished on CRAN and is
marked "Development Status: 4 - Beta" (to be fair, same status as
Rmetrics). Perhaps someone is familiar with the package, and can hint
at its reliability.
Thank you,
Liviu

[1] http://rgarch.r-forge.r-project.org/index.html



-- 
Do you know how to read?
http://www.alienetworks.com/srtest.cfm
Do you know how to write?
http://garbl.home.comcast.net/~garbl/stylemanual/e.htm#e-mail


From Heiko-Mayer at gmx.de  Fri May  1 17:10:55 2009
From: Heiko-Mayer at gmx.de (Heiko Mayer)
Date: Fri, 01 May 2009 17:10:55 +0200
Subject: [R-SIG-Finance] Chart formats
Message-ID: <20090501151055.158760@gmx.net>

Hi all,

I am trying to get a chart with "years" shown only on the x-axis (e.g. 2005,2006,2007). However, xts and ts always show the day, month and year as shown below. I can hardly believe, the only solution is manually creating a character vector. Does anyone know a smarter idea? Or is there a library I might have overseen providing this functionality?

library(xts)
library(quantmod)
getSymbols("GS")
plot(GS[,1],major.ticks='years')

Thanks,
Heiko
--


From jeff.a.ryan at gmail.com  Fri May  1 17:15:52 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 1 May 2009 10:15:52 -0500
Subject: [R-SIG-Finance] Chart formats
In-Reply-To: <20090501151055.158760@gmx.net>
References: <20090501151055.158760@gmx.net>
Message-ID: <e8e755250905010815k6f8d93bl24b987de2ed9e281@mail.gmail.com>

Heiko,

Try:

plot(GS[,1], major.ticks="years", major.format="%Y")

HTH
Jeff

On Fri, May 1, 2009 at 10:10 AM, Heiko Mayer <Heiko-Mayer at gmx.de> wrote:
> Hi all,
>
> I am trying to get a chart with "years" shown only on the x-axis (e.g. 2005,2006,2007). However, xts and ts always show the day, month and year as shown below. I can hardly believe, the only solution is manually creating a character vector. Does anyone know a smarter idea? Or is there a library I might have overseen providing this functionality?
>
> library(xts)
> library(quantmod)
> getSymbols("GS")
> plot(GS[,1],major.ticks='years')
>
> Thanks,
> Heiko
> --
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From pdebruic at gmail.com  Fri May  1 17:22:01 2009
From: pdebruic at gmail.com (Paul DeBruicker)
Date: Fri, 1 May 2009 11:22:01 -0400
Subject: [R-SIG-Finance] RBloomberg - limit on size of return array?
In-Reply-To: <197118.21923.qm@web111509.mail.gq1.yahoo.com>
References: <197118.21923.qm@web111509.mail.gq1.yahoo.com>
Message-ID: <f2e3401f0905010822n22b64176t616d8c24c8528b9d@mail.gmail.com>

Hi,


What's the problem with doing it in two requests and gluing it
together with a rbind() or cbind()?

Also, you might benefit from specifying the two letter exchange code
before the word "Equity" in your paste() statement.  So make it "LN
Equity" or whatever it should be.



Paul



On Wed, Apr 29, 2009 at 6:27 AM, Phil Joubert <philjoubert at yahoo.com> wrote:
>
> Hi
>
> I'm using RBloomberg to download timeseries data, but I'm getting an error.
>
> vsTickers <- unlist(blpGetData(oBbgConn, sIndex, "INDX_MEMBERS", retval="raw"))
> vsTickers <- paste(vsTickers, "Equity")
> dtStart <- chron("31/12/1998", format="d/m/y")
> dtEnd <- chron("31/12/2008", format="d/m/y")
> vdPrices <- blpGetData(oBbgConn, vsTickers, "PX_LAST", dtStart, dtEnd)
>
> I expect this code to get the TS of the FTSE components over the last 10 years. Instead I get a zoo object with the correct number of columns, but no data, and the warning "In as.matrix.BlpCOMReturn(x) : NAs introduced by coercion".
>
> I suspect the problem is the size of the return array. If I try to get a subset of the data (e.g. vsTickers[1:40] or [40:79], or smaller time frame) I have no problem. If I set retval="raw" the first element is "Error : Exception occurred.\n".
>
> Any ideas? I've tried the same extraction via the VBA, but it just hangs.
>
> thanks
> Phil
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ggrothendieck at gmail.com  Fri May  1 17:47:51 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 1 May 2009 11:47:51 -0400
Subject: [R-SIG-Finance] Chart formats
In-Reply-To: <20090501151055.158760@gmx.net>
References: <20090501151055.158760@gmx.net>
Message-ID: <971536df0905010847g321b5303w62215bad0d9d902@mail.gmail.com>

In this case the following gives years:

plot(as.zoo(GS[,1]))

and if variations of it do not, you can force it:

plot(as.zoo(GS[,1]), xaxt = "n")
years <- as.Date(unique(cut(time(GS), "year")))
axis(1, at = years, lab = format(years, "%Y"))


On Fri, May 1, 2009 at 11:10 AM, Heiko Mayer <Heiko-Mayer at gmx.de> wrote:
> Hi all,
>
> I am trying to get a chart with "years" shown only on the x-axis (e.g. 2005,2006,2007). However, xts and ts always show the day, month and year as shown below. I can hardly believe, the only solution is manually creating a character vector. Does anyone know a smarter idea? Or is there a library I might have overseen providing this functionality?
>
> library(xts)
> library(quantmod)
> getSymbols("GS")
> plot(GS[,1],major.ticks='years')
>
> Thanks,
> Heiko
> --
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From philjoubert at yahoo.com  Fri May  1 20:24:54 2009
From: philjoubert at yahoo.com (Phil Joubert)
Date: Fri, 1 May 2009 11:24:54 -0700 (PDT)
Subject: [R-SIG-Finance] RBloomberg - limit on size of return array?
Message-ID: <802474.78384.qm@web111512.mail.gq1.yahoo.com>


No problem with the workaround you suggest - thats what I've done. I just think it would be cleaner to be able to do the call as originally suggested.

INDX_MEMB returns the full ticker including country / market code - eg first value is "AAL LN".

thanks
Phil

--- On Fri, 5/1/09, Paul DeBruicker <pdebruic at gmail.com> wrote:

> From: Paul DeBruicker <pdebruic at gmail.com>
> Subject: Re: [R-SIG-Finance] RBloomberg - limit on size of return array?
> To: "Phil Joubert" <philjoubert at yahoo.com>
> Cc: r-sig-finance at stat.math.ethz.ch
> Date: Friday, May 1, 2009, 6:22 PM
> Hi,
> 
> 
> What's the problem with doing it in two requests and gluing
> it
> together with a rbind() or cbind()?
> 
> Also, you might benefit from specifying the two letter
> exchange code
> before the word "Equity" in your paste() statement.?
> So make it "LN
> Equity" or whatever it should be.
> 
> 
> 
> Paul
> 
> 
> 
> On Wed, Apr 29, 2009 at 6:27 AM, Phil Joubert <philjoubert at yahoo.com>
> wrote:
> >
> > Hi
> >
> > I'm using RBloomberg to download timeseries data, but
> I'm getting an error.
> >
> > vsTickers <- unlist(blpGetData(oBbgConn, sIndex,
> "INDX_MEMBERS", retval="raw"))
> > vsTickers <- paste(vsTickers, "Equity")
> > dtStart <- chron("31/12/1998", format="d/m/y")
> > dtEnd <- chron("31/12/2008", format="d/m/y")
> > vdPrices <- blpGetData(oBbgConn, vsTickers,
> "PX_LAST", dtStart, dtEnd)
> >
> > I expect this code to get the TS of the FTSE
> components over the last 10 years. Instead I get a zoo
> object with the correct number of columns, but no data, and
> the warning "In as.matrix.BlpCOMReturn(x) : NAs introduced
> by coercion".
> >
> > I suspect the problem is the size of the return array.
> If I try to get a subset of the data (e.g. vsTickers[1:40]
> or [40:79], or smaller time frame) I have no problem. If I
> set retval="raw" the first element is "Error : Exception
> occurred.\n".
> >
> > Any ideas? I've tried the same extraction via the VBA,
> but it just hangs.
> >
> > thanks
> > Phil
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch
> mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
> 





From haky.im at gmail.com  Fri May  1 23:45:00 2009
From: haky.im at gmail.com (Hae Kyung Im)
Date: Fri, 1 May 2009 16:45:00 -0500
Subject: [R-SIG-Finance] tick data database
Message-ID: <197a5bbc0905011445r3e7ca72co41abe6a6bc315a28@mail.gmail.com>

Hi,

it may be slightly off topic but I was wondering if any of you heard
about using netCDF format (or similar) to handle tick data?

I thought kdb would be a nice option but the price seems a bit too
high for my purpose. Do you know of any good open source alternative?

Also is there any package to connect R with kdb?

Thanks
Haky


From edd at debian.org  Fri May  1 23:59:18 2009
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 1 May 2009 16:59:18 -0500
Subject: [R-SIG-Finance] tick data database
In-Reply-To: <197a5bbc0905011445r3e7ca72co41abe6a6bc315a28@mail.gmail.com>
References: <197a5bbc0905011445r3e7ca72co41abe6a6bc315a28@mail.gmail.com>
Message-ID: <18939.28854.908812.47103@ron.nulle.part>


On 1 May 2009 at 16:45, Hae Kyung Im wrote:
| it may be slightly off topic but I was wondering if any of you heard
| about using netCDF format (or similar) to handle tick data?

I know of places that use hdf5 so it likely that someone may also be using
netCDF.
 
| I thought kdb would be a nice option but the price seems a bit too
| high for my purpose. Do you know of any good open source alternative?
| 
| Also is there any package to connect R with kdb?

Yes, you can get it off the (public access, as I recall) kx.com website.  I
looked at it for a few days---and even enhanced the existing R / kdb package
with corrected support for sub-second time types between R and Kx with a
patch you find on my blog at http://dirk.eddelbuettel.com/blog---but we
decided to go with a competing product (for which I've since written an
internal R package connecting to their C++ API).  If you want to evaluate Kx,
you get a free-as-in-beer 32 bit binary that will run for two hours after
which you need to relaunch.

The is a lot of cool stuff listed at the bottom of the 'column-oriented DBMS'
page on Wikipedia.  Someone should nudge these towards open-source tick data
bases. Jeff and I talked about it but alas no free time...

Dirk 

-- 
Three out of two people have difficulties with fractions.


From windspeedo99 at gmail.com  Sat May  2 14:38:58 2009
From: windspeedo99 at gmail.com (Wind2)
Date: Sat, 2 May 2009 05:38:58 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] tick data database
In-Reply-To: <197a5bbc0905011445r3e7ca72co41abe6a6bc315a28@mail.gmail.com>
References: <197a5bbc0905011445r3e7ca72co41abe6a6bc315a28@mail.gmail.com>
Message-ID: <23345807.post@talk.nabble.com>



Hae Kyung Im wrote:
> 
> Also is there any package to connect R with kdb?
> 

https://code.kx.com/trac/wiki/Cookbook/IntegratingWithR
account/password: anonymous/anonymous
-- 
View this message in context: http://www.nabble.com/tick-data-database-tp23340204p23345807.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From us at activestocks.de  Sat May  2 16:44:11 2009
From: us at activestocks.de (Ulrich Staudinger)
Date: Sat, 2 May 2009 07:44:11 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] tick data database
In-Reply-To: <23345807.post@talk.nabble.com>
References: <197a5bbc0905011445r3e7ca72co41abe6a6bc315a28@mail.gmail.com>
	<23345807.post@talk.nabble.com>
Message-ID: <23346758.post@talk.nabble.com>




Wind2 wrote:
> 
> 
> Hae Kyung Im wrote:
>> 
>> Also is there any package to connect R with kdb?
>> 
> 
> https://code.kx.com/trac/wiki/Cookbook/IntegratingWithR
> account/password: anonymous/anonymous
> 


Hi,

sorry to crosspost, but there are two interesting threads on aq.org and on
the rsig-finance mailing list where i think it is worth they know of each
other ...

http://www.nabble.com/Thoughts-about-handling-huge-amounts-of-ticks-in-db-backend-td22999165.html
http://www.nabble.com/tick-data-database-td23340204.html


Regards,
Ulrich
-- 
View this message in context: http://www.nabble.com/tick-data-database-tp23340204p23346758.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From landronimirc at gmail.com  Sun May  3 01:45:37 2009
From: landronimirc at gmail.com (Liviu Andronic)
Date: Sun, 3 May 2009 01:45:37 +0200
Subject: [R-SIG-Finance] issues with NGARCH in rgarch package
Message-ID: <68b1e2610905021645x1135c283yed07d6bc86d018dd@mail.gmail.com>

Dear Alexios,

We are currently using rgarch from R-forge to compute basic GARCH(1,1)
and more fancy NGARCH(1,1) models. For the simple garch(1,1) we get
different results from tseries::garch(), fGarch::garchFit() and
rgarch::ugarchfit(), however fairly comparable; if you're interested,
I could post self-contained code highlighting the differences.

For the NGARCH (via fGARCH), however, we get funny results and I
believe it would be a bug in the code. If you look at the Hentschel's
"Family GARCH" model [1], NGARCH is obtained when lambda is fixed and
=2 (page 9 and 11 of the .pdf). However ugarchfit() seems to try to
estimate this guy.
A second issue, ugarchfit() will not try to estimate the assymetry
shift: `theta' in our econometrics class, `b' in the article (page 11
of the .pdf), or `fb' in the rgarch package (see `ugarchspec-methods
{rgarch}' help page). Looking at the source code (internal-fgarch.R,
line 31), it seems that fb is hard-set to 0:
	NGARCH   =list(parameters=list(lambda=2, delta=0, fb=0, fc=0, fk=1),

		indicator=c(1,0,0,0,0), garchtype = 4),
 although we believe it should be one of the freely estimated
parameters (instead of lambda, as currently it seems to be).

Would there be any easy way to work around this issue? For code
highlighting the problem, please look below.
Thank you,
Liviu

[1] http://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity#fGARCH

### Code ###
> return=vector()
> zigma=vector()
> return[1]=0
> zigma[1]=0
> normvector=rnorm(499,mean=0,sd=1)
> #simulate random returns for predefined NGARCH parameters
> for(i in 2:500)
+ {
+ zigma[i]=(0.01+0.05*(return[i-1]-2*zigma[i-1])^2+0.6*zigma[i-1]^2)^0.5
+ return[i]=zigma[i]*normvector[i-1]
+ }
> head(return)
[1]  0.00000  0.08726 -0.02434  0.26456  0.19681 -0.01039
> # Fitting Non-linear GARCH
> require(rgarch)
> spec.NGARCH <- ugarchspec(variance.model=list(model="fGARCH",
+ garchOrder=c(1,1), submodel="NGARCH"), mean.model=list(armaOrder=c(0,0),
+ include.mean=FALSE), distribution.model="snorm")
> fit.NGARCH <- ugarchfit(data=return, spec=spec.NGARCH)
> fit.NGARCH at fit$coef
  omega  alpha1   beta1  lambda
0.04276 0.06191 0.87457 0.43634
> fit.NGARCH


GARCH Model Spec
--------------------------
Model : fGARCH Sub-Model : NGARCH

Exogenous Regressors in variance equation: none

Mean Equation :
Include Mean :  FALSE
AR(FI)MA Model : (0,0,0)
Garch-in-Mean :  FALSE
Exogenous Regressors in mean equation: none
Conditional Distribution:  norm

GARCH Model Fit
--------------------------
Optimal Parameters:
        Estimate  Std. Error  t value Pr(>|t|)
omega    0.04276    0.030425   1.4054 0.159894
alpha1   0.06191    0.028462   2.1752 0.029613
beta1    0.87457    0.061289  14.2695 0.000000
lambda   0.43634    0.171291   2.5474 0.010854

Robust Standard Errors:
        Estimate  Std. Error  t value Pr(>|t|)
omega    0.04276    0.034747   1.2306 0.218473
alpha1   0.06191    0.036494   1.6965 0.089795
beta1    0.87457    0.076156  11.4839 0.000000
lambda   0.43634    0.179153   2.4356 0.014868

LogLikelihood : 62.65

Information Criteria:
Akaike       0.26660
Bayes        0.30032
Shibata      0.26648
Hannan-Quinn 0.27983


Q-Statistics on Standardized Residuals:
      statistic p-value
Lag10     6.161  0.8016
Lag15    13.434  0.5688
Lag20    21.480  0.3694

H0 : No serial correlation

Q-Statistics on Standardized Squared Residuals:
      statistic p-value
Lag10     7.032  0.7224
Lag15    10.325  0.7988
Lag20    17.254  0.6364

ARCH LM Tests:
             Statistic DoF P-Value
ARCH Lag[2]     0.1076   2  0.9476
ARCH Lag[5]     6.0089   5  0.3054
ARCH Lag[10]    8.0694  10  0.6221

Joint Statistic of the Nyblom stability test: 0.9012

Individual Nyblom Statistics:
omega  0.2906
alpha1 0.2260
beta1  0.2734
lambda 0.2845

Sign Bias Test
                   t-value    prob sig
Sign Bias           0.7199 0.47193
Negative Sign Bias  1.0525 0.29308
Positive Sign Bias  1.6822 0.09316   *
Joint Effect        5.5168 0.13764

Chisq Goodnes of Fit Test
[1] "under revision"


Elapsed time : 1.298

### End of code ###


From ymoser at gmail.com  Sun May  3 18:59:22 2009
From: ymoser at gmail.com (Yaakov Moser)
Date: Sun, 03 May 2009 19:59:22 +0300
Subject: [R-SIG-Finance] fPortfolio - Change in getWeights function
Message-ID: <49FDCD6A.3080206@gmail.com>

I recently upgraded to the latest version of fPortfolio:


> Package:            fPortfolio
> Version:            2100.77
> Revision:           4093
> Date:               2009-04-19
Has there been a change in the behaviour of the getWeights function?

In the previous versions, it used to return a matrix with the different 
assets as the columns and the frontier points as the rows.

Now it only returns a single row, which appears to be the last point 
that is found.


Do I need to change the syntax by which I call it?


Any help appreciated.


See sample code below.


Thanks


Yaakov


Data <- as.timeSeries(data(SMALLCAP.RET))
Data<-Data[,1:3]
Spec = portfolioSpec()
portfolio<-portfolioFrontier(Data,Spec)

#portfolio returns 5 out of the 50 points as we expect.

# The weights are listed as well.

portfolio

#frontierPoints returns a 2 X 50 matrix, as expected

frontierPoints(portfolio)

#getWeights returns only one row...

getWeights(portfolio)


> [1] 2.258490e-08 0.000000e+00 1.000000e+00
> attr(,"invest")
> [1] 1


From Matthias.Koberstein at hsbctrinkaus.de  Mon May  4 09:16:28 2009
From: Matthias.Koberstein at hsbctrinkaus.de (Matthias.Koberstein at hsbctrinkaus.de)
Date: Mon, 4 May 2009 09:16:28 +0200
Subject: [R-SIG-Finance] WG: Re: RBloomberg - limit on size of return array?
Message-ID: <OFF5820EE3.AF0D4B1B-ONC12575AC.00272449-C12575AC.0027F5D4@hsbctrinkaus.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090504/b3ba3338/attachment.pl>

From ymoser at gmail.com  Mon May  4 12:29:18 2009
From: ymoser at gmail.com (Yaakov Moser)
Date: Mon, 04 May 2009 13:29:18 +0300
Subject: [R-SIG-Finance] fPortfolio - Change in getWeights function
In-Reply-To: <mailman.3.1241431202.11500.r-sig-finance@stat.math.ethz.ch>
References: <mailman.3.1241431202.11500.r-sig-finance@stat.math.ethz.ch>
Message-ID: <49FEC37E.8080807@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090504/ba8e42f8/attachment.pl>

From jeff.a.ryan at gmail.com  Mon May  4 17:18:39 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Mon, 4 May 2009 10:18:39 -0500
Subject: [R-SIG-Finance] R/Finance 2009 Presentations Online
Message-ID: <e8e755250905040818y58f006d6q62f3b916a54ff25b@mail.gmail.com>

R-SIG-Finance:

For those who missed it, the slides for the R/Finance 2009 tutorials
and presentations are now available on RinFinance.com

http://www.RinFinance.com/presentations

We want to thank everyone who traveled to Chicago to make this happen.
 With nearly 200 attendees, coming from 8 countries and 20+ states in
the US, the conference exceeded all of our expectations.  A very big
thank you to our presenters for taking the time to join us, UIC's
International Center for Futures and Derivatives for hosting the
event, and our sponsors REvolution Computing and Microsoft for their
support.

We look forward to doing it again in the future.  Stay tuned...


On behalf of the committee and sponsors.


From ron_michael70 at yahoo.com  Tue May  5 08:56:16 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Mon, 4 May 2009 23:56:16 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] A question on Interest Rate
Message-ID: <23382045.post@talk.nabble.com>


Please forgive me if my question is too childish however if anyone give me
some favor on my problem I would be grateful.

I have a deposit with tenor 44 days that gives 9% interest p.a. However I
want to calculate rate of return from that deposit for 44 days. Here I
proceed as follows. Let say it is y%, then I have following equation :

$100*(1+y/100)^8 = $100*(1+9/100) -> y = 1.083044%. Is it correct? However
here my doubt is LHS covers only 44*8 = 352 days wherein a year is assumed
as 360 days. Therefore there still 8 days missing in my calculation.
Therefore I am in dilemma about the correctness of my calculation. Can
anyone please suggest if my calculation is ok? Above calculation comes from
pricing a forward contract with 44 days maturity.

Regards,
-- 
View this message in context: http://www.nabble.com/A-question-on-Interest-Rate-tp23382045p23382045.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From mdkhalidiqbal at gmail.com  Tue May  5 11:35:12 2009
From: mdkhalidiqbal at gmail.com (Khalid Iqbal)
Date: Tue, 5 May 2009 15:05:12 +0530
Subject: [R-SIG-Finance] VAR Modelling
Message-ID: <ded8d49c0905050235gd51c49j9e498504ab7c2be9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090505/12ef2f14/attachment.pl>

From singularitaet at gmx.net  Tue May  5 14:43:04 2009
From: singularitaet at gmx.net (Stefan Grosse)
Date: Tue, 5 May 2009 14:43:04 +0200
Subject: [R-SIG-Finance] VAR Modelling
In-Reply-To: <ded8d49c0905050235gd51c49j9e498504ab7c2be9@mail.gmail.com>
References: <ded8d49c0905050235gd51c49j9e498504ab7c2be9@mail.gmail.com>
Message-ID: <20090505144304.0ceeb075@gmx.net>

These questions suspiciously look like homework questions of an
introductionary econometrics class and there quite some good books to
answer them. Even a little use of google should answer them...

Please use the list if you have problems with the software. 

On Tue, 5 May 2009 15:05:12 +0530 Khalid Iqbal
<mdkhalidiqbal at gmail.com> wrote:

KI> 1) I calculated impulse response function keeping one series as
KI> impulse and other as response. I know the
KI>     theoretical interpretation of IRF but how to infer anything
KI> from the "irf" plot. Tell me as to how can I read the graph.
KI> 
KI> 2) What should we do if the model fails multivariate normality
KI> tests like JB-test?
KI> 
KI> 3) Would it be good if we take difference of two series (after
KI> adjusting them to be comparable on a common scale)
KI>     and then model the series using univariate techniques.
KI> 
KI> 4) Would it be better if I take log(Pt / Pt-1) and then compare the
KI> 2 series.


From davidr at rhotrading.com  Tue May  5 15:27:55 2009
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Tue, 5 May 2009 08:27:55 -0500
Subject: [R-SIG-Finance] [R-sig-finance] A question on Interest Rate
In-Reply-To: <23382045.post@talk.nabble.com>
References: <23382045.post@talk.nabble.com>
Message-ID: <F9F2A641C593D7408925574C05A7BE7702BF3092@rhopost.rhotrading.com>

Assuming Act/Act (this year 365/365), you should have

(1 + y/100/(365/44))^(365/44) = (1 + 9/100) => y = 8.662688075 %

David L. Reiner, PhD
Head Quant
Rho Trading Securities, LLC


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of RON70
Sent: Tuesday, May 05, 2009 1:56 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] [R-sig-finance] A question on Interest Rate


Please forgive me if my question is too childish however if anyone give
me
some favor on my problem I would be grateful.

I have a deposit with tenor 44 days that gives 9% interest p.a. However
I
want to calculate rate of return from that deposit for 44 days. Here I
proceed as follows. Let say it is y%, then I have following equation :

$100*(1+y/100)^8 = $100*(1+9/100) -> y = 1.083044%. Is it correct?
However
here my doubt is LHS covers only 44*8 = 352 days wherein a year is
assumed
as 360 days. Therefore there still 8 days missing in my calculation.
Therefore I am in dilemma about the correctness of my calculation. Can
anyone please suggest if my calculation is ok? Above calculation comes
from
pricing a forward contract with 44 days maturity.

Regards,
-- 
View this message in context:
http://www.nabble.com/A-question-on-Interest-Rate-tp23382045p23382045.ht
ml
Sent from the Rmetrics mailing list archive at Nabble.com.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From ron_michael70 at yahoo.com  Tue May  5 16:38:39 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Tue, 5 May 2009 07:38:39 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] A question on Interest Rate
In-Reply-To: <F9F2A641C593D7408925574C05A7BE7702BF3092@rhopost.rhotrading.com>
References: <23382045.post@talk.nabble.com>
	<F9F2A641C593D7408925574C05A7BE7702BF3092@rhopost.rhotrading.com>
Message-ID: <23388654.post@talk.nabble.com>


Thanks davidr-2  for your response. However I still have doubt. This means if
I deposit $100 then after 44 days I would get $(100+8.662688075), which give
rate of return 9% annually? I think you considered annualized return (not
very sure) however I want "direct/realized" return from my deposit. Will you
please clarify me?

Thanks and regards,


davidr-2 wrote:
> 
> Assuming Act/Act (this year 365/365), you should have
> 
> (1 + y/100/(365/44))^(365/44) = (1 + 9/100) => y = 8.662688075 %
> 
> David L. Reiner, PhD
> Head Quant
> Rho Trading Securities, LLC
> 
> 
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of RON70
> Sent: Tuesday, May 05, 2009 1:56 AM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] [R-sig-finance] A question on Interest Rate
> 
> 
> Please forgive me if my question is too childish however if anyone give
> me
> some favor on my problem I would be grateful.
> 
> I have a deposit with tenor 44 days that gives 9% interest p.a. However
> I
> want to calculate rate of return from that deposit for 44 days. Here I
> proceed as follows. Let say it is y%, then I have following equation :
> 
> $100*(1+y/100)^8 = $100*(1+9/100) -> y = 1.083044%. Is it correct?
> However
> here my doubt is LHS covers only 44*8 = 352 days wherein a year is
> assumed
> as 360 days. Therefore there still 8 days missing in my calculation.
> Therefore I am in dilemma about the correctness of my calculation. Can
> anyone please suggest if my calculation is ok? Above calculation comes
> from
> pricing a forward contract with 44 days maturity.
> 
> Regards,
> -- 
> View this message in context:
> http://www.nabble.com/A-question-on-Interest-Rate-tp23382045p23382045.ht
> ml
> Sent from the Rmetrics mailing list archive at Nabble.com.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/A-question-on-Interest-Rate-tp23382045p23388654.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From Zeno.Adams at ebs.edu  Tue May  5 17:02:47 2009
From: Zeno.Adams at ebs.edu (Adams, Zeno)
Date: Tue, 5 May 2009 17:02:47 +0200
Subject: [R-SIG-Finance] [R-sig-finance] A question on Interest Rate
In-Reply-To: <23388654.post@talk.nabble.com>
References: <23382045.post@talk.nabble.com><F9F2A641C593D7408925574C05A7BE7702BF3092@rhopost.rhotrading.com>
	<23388654.post@talk.nabble.com>
Message-ID: <9064522880125945B98983BBAECBA1CC985567@exchsrv001.ebs.local>

I think that Reiner is basically right:

Instead of using the power 8 (which falls short of the 356 days) you would have to use the power 365/44 = 8.295 if you want to have it exact.

Thus, you would have (1+0.01*y)^(356/44) = 1.09  => y = 1.010442692% which is slightly lower than your 1.083044% (as expected).


-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von RON70
Gesendet: Dienstag, 5. Mai 2009 16:39
An: r-sig-finance at stat.math.ethz.ch
Betreff: Re: [R-SIG-Finance] [R-sig-finance] A question on Interest Rate


Thanks davidr-2  for your response. However I still have doubt. This means if
I deposit $100 then after 44 days I would get $(100+8.662688075), which give
rate of return 9% annually? I think you considered annualized return (not
very sure) however I want "direct/realized" return from my deposit. Will you
please clarify me?

Thanks and regards,


davidr-2 wrote:
> 
> Assuming Act/Act (this year 365/365), you should have
> 
> (1 + y/100/(365/44))^(365/44) = (1 + 9/100) => y = 8.662688075 %
> 
> David L. Reiner, PhD
> Head Quant
> Rho Trading Securities, LLC
> 
> 
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of RON70
> Sent: Tuesday, May 05, 2009 1:56 AM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] [R-sig-finance] A question on Interest Rate
> 
> 
> Please forgive me if my question is too childish however if anyone give
> me
> some favor on my problem I would be grateful.
> 
> I have a deposit with tenor 44 days that gives 9% interest p.a. However
> I
> want to calculate rate of return from that deposit for 44 days. Here I
> proceed as follows. Let say it is y%, then I have following equation :
> 
> $100*(1+y/100)^8 = $100*(1+9/100) -> y = 1.083044%. Is it correct?
> However
> here my doubt is LHS covers only 44*8 = 352 days wherein a year is
> assumed
> as 360 days. Therefore there still 8 days missing in my calculation.
> Therefore I am in dilemma about the correctness of my calculation. Can
> anyone please suggest if my calculation is ok? Above calculation comes
> from
> pricing a forward contract with 44 days maturity.
> 
> Regards,
> -- 
> View this message in context:
> http://www.nabble.com/A-question-on-Interest-Rate-tp23382045p23382045.ht
> ml
> Sent from the Rmetrics mailing list archive at Nabble.com.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/A-question-on-Interest-Rate-tp23382045p23388654.html
Sent from the Rmetrics mailing list archive at Nabble.com.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

EBS European Business School gemeinnuetzige GmbH - Sitz der Gesellschaft: Wiesbaden, Amtsgericht Wiesbaden HRB 19951 - Umsatzsteuer-ID DE 113891213 Geschaeftsfuehrer: Prof. Dr. Christopher Jahns,  Praesident; Dr. Reimar Palte,  Kanzler/CFO;  Sabine Fuchs, CMO; Aufsichtsrat: Dr. Hellmut K. Albrecht, Vorsitzender

From Zeno.Adams at ebs.edu  Tue May  5 17:09:55 2009
From: Zeno.Adams at ebs.edu (Adams, Zeno)
Date: Tue, 5 May 2009 17:09:55 +0200
Subject: [R-SIG-Finance] [R-sig-finance] A question on Interest Rate
In-Reply-To: <23388654.post@talk.nabble.com>
References: <23382045.post@talk.nabble.com><F9F2A641C593D7408925574C05A7BE7702BF3092@rhopost.rhotrading.com>
	<23388654.post@talk.nabble.com>
Message-ID: <9064522880125945B98983BBAECBA1CC985568@exchsrv001.ebs.local>

Small typo: 356 should be of course 365 days. Sorry for that


EBS European Business School gemeinnuetzige GmbH - Sitz der Gesellschaft: Wiesbaden, Amtsgericht Wiesbaden HRB 19951 - Umsatzsteuer-ID DE 113891213 Geschaeftsfuehrer: Prof. Dr. Christopher Jahns,  Praesident; Dr. Reimar Palte,  Kanzler/CFO;  Sabine Fuchs, CMO; Aufsichtsrat: Dr. Hellmut K. Albrecht, Vorsitzender

From ltorgo at inescporto.pt  Tue May  5 19:46:59 2009
From: ltorgo at inescporto.pt (Luis Torgo)
Date: Tue, 05 May 2009 18:46:59 +0100
Subject: [R-SIG-Finance] Problem with subsetting in xts package
Message-ID: <4A007B93.609@dcc.fc.up.pt>

Dear All,

I met with a problem that I think it's being caused by a problem in the 
sub-setting methods in xts.

I found this problem because I was using the Next() function from 
quantmod as follows:

 > x <- GSPC$GSPC.Adjusted["200904/"]
 > Delt(x)
           Delt.1.arithmetic
2009-04-01                NA
2009-04-02       0.028727129
2009-04-03       0.009731777
2009-04-06      -0.008332344
2009-04-07      -0.023854551
...
 > Next(x)
Error in `[.xts`(x, -(0:k), ) :
  only zeros may be mixed with negative subscripts

I found it odd as k is by default 1 and thus "-(0:k)" should be
 > -(0:1)
[1]  0 -1

After a bit of debugging I think I've isolated the problem at the 
`[.xts` code namely,

`.subset.xts` <- `[.xts` <-
function(x, i, j, drop = FALSE, ...)
   ...
   # test for negative subscripting in i
    if (!missing(i) && is.numeric(i) ) {
      if(any(i < 0)) {
        if(!all(i < 0))
          stop('only zeros may be mixed with negative subscripts')
   ...
in file "xts.methods.R".

I think it should be:
        if(!all(i <= 0))
instead...

Or am I missing something?


My sysinfo:
 > R.version
               _                          
platform       i486-pc-linux-gnu          
arch           i486                       
os             linux-gnu                  
system         i486, linux-gnu            
status                                    
major          2                          
minor          9.0                        
year           2009                       
month          04                         
day            17                         
svn rev        48333                      
language       R                          
version.string R version 2.9.0 (2009-04-17)

The xts version is 0.6-4
Thanks,
Luis

-- 
Luis Torgo
   FC/LIAAD - INESC Porto, LA    Phone : (+351) 22 339 20 93
   University of Porto           Fax   : (+351) 22 339 20 99
   R. de Ceuta, 118, 6o          email : ltorgo at liaad.up.pt
   4050-190 PORTO - PORTUGAL     WWW   : http://www.liaad.up.pt/~ltorgo


From haky.im at gmail.com  Tue May  5 19:55:49 2009
From: haky.im at gmail.com (Hae Kyung Im)
Date: Tue, 5 May 2009 12:55:49 -0500
Subject: [R-SIG-Finance] abline for quantmod charts
In-Reply-To: <e8e755250904301416p5cae9771med5fd8d41207e349@mail.gmail.com>
References: <197a5bbc0904301356i37892c1fta735e7b1c333b784@mail.gmail.com>
	<e8e755250904301416p5cae9771med5fd8d41207e349@mail.gmail.com>
Message-ID: <197a5bbc0905051055y20946045gf7a83f39b545da0e@mail.gmail.com>

Hi Jeff

I had to change the argument "on" to 1 for it to show the vertical
line (I have quantmod 0.3-8)
addTA(xts(TRUE,as.Date("2008-04-01")),on=1,col="#333333")


Now I wanted to make it easier to call including the call to a function:

addVLine = function(dtlist)
  {
    addTA(xts( rep(TRUE, NROW(dtlist)), dtlist), on=1, col="#333333")
  }

where dtlist is a vector with datetime type. But when the lines don't
show up. It does plot the vertical lines when I run the command
directly but not within the function. This is what I get on the
command line:

> addVLine(index(data.xmin)[te])
<chobTA object: addTA>


Do you know how to solve this?

Thanks
Haky




On Thu, Apr 30, 2009 at 4:16 PM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> Hi Hae,
>
> The best way to do this at the moment is with addTA:
>
> chartSeries(YHOO)
> addTA(xts(TRUE,as.Date("2008-04-01")),on=-(1:2),col="#333333")
>
> zoomChart('200803/200804')
>
> You'll notice that it draws a thick line (region). ?This is usually
> more useful than just a line on quantmod charts, as the spacing is
> between observations is padded.
>
> I will at some point be adding access for primitive graphics function
> to the whole charting process, but for now this should do what you
> want.
>
>
> HTH,
> Jeff
>
> On Thu, Apr 30, 2009 at 3:56 PM, Hae Kyung Im <haky.im at gmail.com> wrote:
>> Hi,
>>
>> I was trying to add vertical lines to a quantmod plot using
>>
>> abline( v = somdatetime ) like in the following example
>>
>>> getSymbols("YHOO")
>> [1] "YHOO"
>>> chartSeries(YHOO)
>>> abline( v=as.POSIXct("2008-04-01","%Y-%m-%d") )
>>
>> But nothing shows up on the chart.
>>
>> Any ideas on how to do this?
>>
>> Thanks
>> Haky
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> --
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com
>


From jeff.a.ryan at gmail.com  Tue May  5 20:11:46 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Tue, 5 May 2009 13:11:46 -0500
Subject: [R-SIG-Finance] Problem with subsetting in xts package
In-Reply-To: <4A007B93.609@dcc.fc.up.pt>
References: <4A007B93.609@dcc.fc.up.pt>
Message-ID: <e8e755250905051111i1ec785caqe92ed5b67e0a3b6d@mail.gmail.com>

Luis,

Thanks for the report.  That is indeed a bug.  I have fixed on R-forge.

While Next and Lag still work, I am moving to do away with them now
that there is a specific lag.xts method in xts.

lag(x, -1) does the same as Next(x), and is a bit more R-like (which
is a very good thing).

Thanks
Jeff


On Tue, May 5, 2009 at 12:46 PM, Luis Torgo <ltorgo at inescporto.pt> wrote:
> Dear All,
>
> I met with a problem that I think it's being caused by a problem in the
> sub-setting methods in xts.
>
> I found this problem because I was using the Next() function from quantmod
> as follows:
>
>> x <- GSPC$GSPC.Adjusted["200904/"]
>> Delt(x)
> ? ? ? ? ?Delt.1.arithmetic
> 2009-04-01 ? ? ? ? ? ? ? ?NA
> 2009-04-02 ? ? ? 0.028727129
> 2009-04-03 ? ? ? 0.009731777
> 2009-04-06 ? ? ?-0.008332344
> 2009-04-07 ? ? ?-0.023854551
> ...
>> Next(x)
> Error in `[.xts`(x, -(0:k), ) :
> ?only zeros may be mixed with negative subscripts
>
> I found it odd as k is by default 1 and thus "-(0:k)" should be
>> -(0:1)
> [1] ?0 -1
>
> After a bit of debugging I think I've isolated the problem at the `[.xts`
> code namely,
>
> `.subset.xts` <- `[.xts` <-
> function(x, i, j, drop = FALSE, ...)
> ?...
> ?# test for negative subscripting in i
> ? if (!missing(i) && is.numeric(i) ) {
> ? ? if(any(i < 0)) {
> ? ? ? if(!all(i < 0))
> ? ? ? ? stop('only zeros may be mixed with negative subscripts')
> ?...
> in file "xts.methods.R".
>
> I think it should be:
> ? ? ? if(!all(i <= 0))
> instead...
>
> Or am I missing something?
>
>
> My sysinfo:
>> R.version
> ? ? ? ? ? ? ?_ ? ? ? ? ? ? ? ? ? ? ? ? ?platform ? ? ? i486-pc-linux-gnu
> ? ? ?arch ? ? ? ? ? i486 ? ? ? ? ? ? ? ? ? ? ? os ? ? ? ? ? ? linux-gnu
> ? ? ? ? ? ? ?system ? ? ? ? i486, linux-gnu ? ? ? ? ? ?status
> ? ? ? ? ? ? ? ? ? ? ?major ? ? ? ? ?2 ? ? ? ? ? ? ? ? ? ? ? ? ?minor
> ?9.0 ? ? ? ? ? ? ? ? ? ? ? ?year ? ? ? ? ? 2009 ? ? ? ? ? ? ? ? ? ? ? month
> ? ? ? ? ?04 ? ? ? ? ? ? ? ? ? ? ? ? day ? ? ? ? ? ?17
> ? svn rev ? ? ? ?48333 ? ? ? ? ? ? ? ? ? ? ?language ? ? ? R
> ? ? ? ? ?version.string R version 2.9.0 (2009-04-17)
>
> The xts version is 0.6-4
> Thanks,
> Luis
>
> --
> Luis Torgo
> ?FC/LIAAD - INESC Porto, LA ? ?Phone : (+351) 22 339 20 93
> ?University of Porto ? ? ? ? ? Fax ? : (+351) 22 339 20 99
> ?R. de Ceuta, 118, 6o ? ? ? ? ?email : ltorgo at liaad.up.pt
> ?4050-190 PORTO - PORTUGAL ? ? WWW ? : http://www.liaad.up.pt/~ltorgo
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From jeff.a.ryan at gmail.com  Tue May  5 20:14:30 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Tue, 5 May 2009 13:14:30 -0500
Subject: [R-SIG-Finance] abline for quantmod charts
In-Reply-To: <197a5bbc0905051055y20946045gf7a83f39b545da0e@mail.gmail.com>
References: <197a5bbc0904301356i37892c1fta735e7b1c333b784@mail.gmail.com>
	<e8e755250904301416p5cae9771med5fd8d41207e349@mail.gmail.com>
	<197a5bbc0905051055y20946045gf7a83f39b545da0e@mail.gmail.com>
Message-ID: <e8e755250905051114q49c53982of52b62a8326cb6c8@mail.gmail.com>

Hi Haky,

wrap addTA with plot().  It is a function of being able to use the
same function from the command line, and embedded in a chartSeries
call.  Internally there is a check to see where it originates from to
take a guess as to draw it immediately or not.

addVLine = function(dtlist)
 {
   plot(addTA(xts( rep(TRUE, NROW(dtlist)), dtlist), on=1, col="#333333"))
 }

HTH
Jeff

On Tue, May 5, 2009 at 12:55 PM, Hae Kyung Im <haky.im at gmail.com> wrote:
> Hi Jeff
>
> I had to change the argument "on" to 1 for it to show the vertical
> line (I have quantmod 0.3-8)
> addTA(xts(TRUE,as.Date("2008-04-01")),on=1,col="#333333")
>
>
> Now I wanted to make it easier to call including the call to a function:
>
> addVLine = function(dtlist)
> ?{
> ? ?addTA(xts( rep(TRUE, NROW(dtlist)), dtlist), on=1, col="#333333")
> ?}
>
> where dtlist is a vector with datetime type. But when the lines don't
> show up. It does plot the vertical lines when I run the command
> directly but not within the function. This is what I get on the
> command line:
>
>> addVLine(index(data.xmin)[te])
> <chobTA object: addTA>
>
>
> Do you know how to solve this?
>
> Thanks
> Haky
>
>
>
>
> On Thu, Apr 30, 2009 at 4:16 PM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
>> Hi Hae,
>>
>> The best way to do this at the moment is with addTA:
>>
>> chartSeries(YHOO)
>> addTA(xts(TRUE,as.Date("2008-04-01")),on=-(1:2),col="#333333")
>>
>> zoomChart('200803/200804')
>>
>> You'll notice that it draws a thick line (region). ?This is usually
>> more useful than just a line on quantmod charts, as the spacing is
>> between observations is padded.
>>
>> I will at some point be adding access for primitive graphics function
>> to the whole charting process, but for now this should do what you
>> want.
>>
>>
>> HTH,
>> Jeff
>>
>> On Thu, Apr 30, 2009 at 3:56 PM, Hae Kyung Im <haky.im at gmail.com> wrote:
>>> Hi,
>>>
>>> I was trying to add vertical lines to a quantmod plot using
>>>
>>> abline( v = somdatetime ) like in the following example
>>>
>>>> getSymbols("YHOO")
>>> [1] "YHOO"
>>>> chartSeries(YHOO)
>>>> abline( v=as.POSIXct("2008-04-01","%Y-%m-%d") )
>>>
>>> But nothing shows up on the chart.
>>>
>>> Any ideas on how to do this?
>>>
>>> Thanks
>>> Haky
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>>
>>
>> --
>> Jeffrey Ryan
>> jeffrey.ryan at insightalgo.com
>>
>> ia: insight algorithmics
>> www.insightalgo.com
>>
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From ltorgo at inescporto.pt  Wed May  6 00:00:34 2009
From: ltorgo at inescporto.pt (Luis Torgo)
Date: Tue, 05 May 2009 23:00:34 +0100
Subject: [R-SIG-Finance] Problem with Delt() from quantmod
Message-ID: <4A00B702.502@dcc.fc.up.pt>

Dear All,

I'm having a problem with using Delt() on a xts object with a vector of
k values. With a single k value I have no problems and with vector of
values instead of a xts object I'm also able to get what I want. Let's
see illustrative examples:
> x
           GSPC.Adjusted
2009-04-01        811.08
2009-04-02        834.38
2009-04-03        842.50
2009-04-06        835.48
2009-04-07        815.55
2009-04-08        825.16
...
> class(x)
[1] "xts" "zoo"
> Delt(x)
           Delt.1.arithmetic
2009-04-01                NA
2009-04-02       0.028727129
2009-04-03       0.009731777
2009-04-06      -0.008332344
...
> Delt(x,k=1:3)
Error in attributes(.Data) <- c(attributes(.Data), attrib) :
  length of 'dimnames' [1] not equal to array extent

Any help is most welcome.

Thanks,
Luis

-- 
Luis Torgo
   FC/LIAAD - INESC Porto, LA    Phone : (+351) 22 339 20 93
   University of Porto           Fax   : (+351) 22 339 20 99
   R. de Ceuta, 118, 6o          email : ltorgo at liaad.up.pt
   4050-190 PORTO - PORTUGAL     WWW   : http://www.liaad.up.pt/~ltorgo


From seancarmody at gmail.com  Wed May  6 00:50:19 2009
From: seancarmody at gmail.com (Sean Carmody)
Date: Wed, 6 May 2009 08:50:19 +1000
Subject: [R-SIG-Finance] [R-sig-finance] A question on Interest Rate
In-Reply-To: <9064522880125945B98983BBAECBA1CC985567@exchsrv001.ebs.local>
References: <23382045.post@talk.nabble.com>
	<F9F2A641C593D7408925574C05A7BE7702BF3092@rhopost.rhotrading.com>
	<23388654.post@talk.nabble.com>
	<9064522880125945B98983BBAECBA1CC985567@exchsrv001.ebs.local>
Message-ID: <ce6bbb9d0905051550j41486933k8cb4fa8e2deb5f38@mail.gmail.com>

Note also that since there are a number of conventions to calculate
for a fraction of a year (e.g. actual/360. actual/365, etc), simply
quoting the interest rate of 9% doesn't provide enough information to
perform the calculation, so as he noted, Reiner had to make an
additional assumption. In this case he assumed an actual/actual
convention.

Sean.

On Wed, May 6, 2009 at 1:02 AM, Adams, Zeno <Zeno.Adams at ebs.edu> wrote:
> I think that Reiner is basically right:
>
> Instead of using the power 8 (which falls short of the 356 days) you would have to use the power 365/44 = 8.295 if you want to have it exact.
>
> Thus, you would have (1+0.01*y)^(356/44) = 1.09 ?=> y = 1.010442692% which is slightly lower than your 1.083044% (as expected).
>
>
> -----Urspr?ngliche Nachricht-----
> Von: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von RON70
> Gesendet: Dienstag, 5. Mai 2009 16:39
> An: r-sig-finance at stat.math.ethz.ch
> Betreff: Re: [R-SIG-Finance] [R-sig-finance] A question on Interest Rate
>
>
> Thanks davidr-2 ?for your response. However I still have doubt. This means if
> I deposit $100 then after 44 days I would get $(100+8.662688075), which give
> rate of return 9% annually? I think you considered annualized return (not
> very sure) however I want "direct/realized" return from my deposit. Will you
> please clarify me?
>
> Thanks and regards,
>
>
> davidr-2 wrote:
>>
>> Assuming Act/Act (this year 365/365), you should have
>>
>> (1 + y/100/(365/44))^(365/44) = (1 + 9/100) => y = 8.662688075 %
>>
>> David L. Reiner, PhD
>> Head Quant
>> Rho Trading Securities, LLC
>>
>>
>> -----Original Message-----
>> From: r-sig-finance-bounces at stat.math.ethz.ch
>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of RON70
>> Sent: Tuesday, May 05, 2009 1:56 AM
>> To: r-sig-finance at stat.math.ethz.ch
>> Subject: [R-SIG-Finance] [R-sig-finance] A question on Interest Rate
>>
>>
>> Please forgive me if my question is too childish however if anyone give
>> me
>> some favor on my problem I would be grateful.
>>
>> I have a deposit with tenor 44 days that gives 9% interest p.a. However
>> I
>> want to calculate rate of return from that deposit for 44 days. Here I
>> proceed as follows. Let say it is y%, then I have following equation :
>>
>> $100*(1+y/100)^8 = $100*(1+9/100) -> y = 1.083044%. Is it correct?
>> However
>> here my doubt is LHS covers only 44*8 = 352 days wherein a year is
>> assumed
>> as 360 days. Therefore there still 8 days missing in my calculation.
>> Therefore I am in dilemma about the correctness of my calculation. Can
>> anyone please suggest if my calculation is ok? Above calculation comes
>> from
>> pricing a forward contract with 44 days maturity.
>>
>> Regards,
>> --
>> View this message in context:
>> http://www.nabble.com/A-question-on-Interest-Rate-tp23382045p23382045.ht
>> ml
>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>
>
> --
> View this message in context: http://www.nabble.com/A-question-on-Interest-Rate-tp23382045p23388654.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
> EBS European Business School gemeinnuetzige GmbH - Sitz der Gesellschaft: Wiesbaden, Amtsgericht Wiesbaden HRB 19951 - Umsatzsteuer-ID DE 113891213 Geschaeftsfuehrer: Prof. Dr. Christopher Jahns, ?Praesident; Dr. Reimar Palte, ?Kanzler/CFO; ?Sabine Fuchs, CMO; Aufsichtsrat: Dr. Hellmut K. Albrecht, Vorsitzender
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Sean Carmody

The Stubborn Mule
http://www.stubbornmule.net
http://twitter.com/seancarmody


From jeff.a.ryan at gmail.com  Wed May  6 05:25:23 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Tue, 5 May 2009 22:25:23 -0500
Subject: [R-SIG-Finance] Problem with Delt() from quantmod
In-Reply-To: <4A00B702.502@dcc.fc.up.pt>
References: <4A00B702.502@dcc.fc.up.pt>
Message-ID: <e8e755250905052025n5308fa72hbed62da7f275fe7e@mail.gmail.com>

Hi Luis,

The documentation for Delt indicates that k with a vector of values
*should* work.  Clearly that was broken somewhere by me.

I have now made the necessary changes to allow for k to be a vector.
They are reflected in the R-forge version.

Thanks,
Jeff

On Tue, May 5, 2009 at 5:00 PM, Luis Torgo <ltorgo at inescporto.pt> wrote:
> Dear All,
>
> I'm having a problem with using Delt() on a xts object with a vector of
> k values. With a single k value I have no problems and with vector of
> values instead of a xts object I'm also able to get what I want. Let's
> see illustrative examples:
>>
>> x
>
> ? ? ? ? ?GSPC.Adjusted
> 2009-04-01 ? ? ? ?811.08
> 2009-04-02 ? ? ? ?834.38
> 2009-04-03 ? ? ? ?842.50
> 2009-04-06 ? ? ? ?835.48
> 2009-04-07 ? ? ? ?815.55
> 2009-04-08 ? ? ? ?825.16
> ...
>>
>> class(x)
>
> [1] "xts" "zoo"
>>
>> Delt(x)
>
> ? ? ? ? ?Delt.1.arithmetic
> 2009-04-01 ? ? ? ? ? ? ? ?NA
> 2009-04-02 ? ? ? 0.028727129
> 2009-04-03 ? ? ? 0.009731777
> 2009-04-06 ? ? ?-0.008332344
> ...
>>
>> Delt(x,k=1:3)
>
> Error in attributes(.Data) <- c(attributes(.Data), attrib) :
> ?length of 'dimnames' [1] not equal to array extent
>
> Any help is most welcome.
>
> Thanks,
> Luis
>
> --
> Luis Torgo
> ?FC/LIAAD - INESC Porto, LA ? ?Phone : (+351) 22 339 20 93
> ?University of Porto ? ? ? ? ? Fax ? : (+351) 22 339 20 99
> ?R. de Ceuta, 118, 6o ? ? ? ? ?email : ltorgo at liaad.up.pt
> ?4050-190 PORTO - PORTUGAL ? ? WWW ? : http://www.liaad.up.pt/~ltorgo
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From arupsarkar at yahoo.com  Wed May  6 05:40:02 2009
From: arupsarkar at yahoo.com (Sarkar, Arup)
Date: Tue, 5 May 2009 23:40:02 -0400
Subject: [R-SIG-Finance] positions in timeSeries object
Message-ID: <00dc01c9cdfc$56eec0f0$04cc42d0$@com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090505/1233cbd6/attachment.pl>

From ron_michael70 at yahoo.com  Wed May  6 10:12:02 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Wed, 6 May 2009 01:12:02 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Domestic risk free rate in FX option
Message-ID: <23401986.post@talk.nabble.com>


In CME, option on forex is traded on EUR/GBP. If I want to price this option
using some pricing formula then as Domestic risk free interest rate what
should I take? Shouldn't risk free rate in UK be appropriate? I am asking
this because as CME is in US, domestic currency is USD. Your suggestion
appreciated.
-- 
View this message in context: http://www.nabble.com/Domestic-risk-free-rate-in-FX-option-tp23401986p23401986.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From wuertz at itp.phys.ethz.ch  Thu May  7 10:04:01 2009
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Thu, 07 May 2009 10:04:01 +0200
Subject: [R-SIG-Finance] NERC holiday calendar
In-Reply-To: <45CE5EF6.2040004@aol.com>
References: <45CE5EF6.2040004@aol.com>
Message-ID: <4A0295F1.10805@itp.phys.ethz.ch>

Joe W. Byers wrote:

Thanks to Joe for his contribution!

The NERC calendar has been added to the timeDate package of Rmetrics.
The function is available on R-forge and will be submitted to CRAN with 
the next
version of timeDate.

Diethelm

> Spencer, Diethelm, and rmetrics followers,
>
> I have created a calendar function to build out the NERC holidays for
> power prices.  NERC: North American Reliability Council.  I started this
> because I needed a NYMEX calendar as the text at the bottom explains,
> but I am still waiting on NYMEX to reply on my questions.
>
> The NERC holiday function is based on teh holidayNYSE function from
> rmetrics and the NERC website rules found at
> http://www.nerc.com/~oc/offpeaks.html
>
> I would appreciate any help in verifying this function.  It returns the
> correct holidays as shown on their website.  I am not sure if the
> historical holidays are correct, especially prior to 1971 when Memorial
> day was on 5/30.  I have sent the NERC contact an email with this 
> question.
>
> I did add a new argument to the function for the FinCenter of the
> timedate class because NERC covers all North America.  User will want to
> set this to the FinCenter appropriate for their use (Chicago,
> Pacific,...).  It would be cool to add NERC regions to the timedate zone
> or fincenter lists, but this is another discussion.
>
> I would be honored if rmetrics would consider adding this function to
> the fCalendar package.
>
> The function is:
> #North American Energy Reliabibity Council Holidays calendar
> # for determining days when the 16 hour on peak hours of a day
> # for the region are recognized as off peak hours.
> holidayNERC<-function (year = currentYear,FinCenter = "Eastern")
> {
>    holidays = NULL
>    for (y in year) {
>        if (y >= 1885)
>            holidays = c(holidays, as.character(USNewYearsDay(y)))
>        if (y >= 1885)
>            holidays = c(holidays, as.character(USIndependenceDay(y)))
>        if (y >= 1885)
>            holidays = c(holidays, as.character(USThanksgivingDay(y)))
>        if (y >= 1885)
>            holidays = c(holidays, as.character(USChristmasDay(y)))
>        if (y >= 1887)
>            holidays = c(holidays, as.character(USLaborDay(y)))
>        if (y <= 1970)
>            holidays = c(holidays,
> as.character(USDecorationMemorialDay(y)))
>        if (y >= 1971)
>            holidays = c(holidays, as.character(USMemorialDay(y)))
>    }
>    holidays = sort(holidays)
>    ans = timeDate(holidays)
>    ans = ans + as.integer(as.POSIXlt(ans at Data)$wday == 0) *
>        24 * 3600
>    posix = as.POSIXlt(ans at Data)
>    y = posix$year + 1900
>    m = posix$mon + 1
>    lastday = as.POSIXlt((timeCalendar(y = y + (m + 1)%/%13,
>        m = m + 1 - (m + 1)%/%13 * 12, d = 1) - 24 * 3600)@Data)$mday
>    ExceptOnLastFriday = timeDate(as.character(.last.of.nday(year = y,
>        month = m, lastday = lastday, nday = 5)))
>    ans = ans - as.integer(ans >= timeDate("1959-07-03") &
> as.POSIXlt(ans at Data)$wday ==
>        0 & ans != ExceptOnLastFriday) * 24 * 3600
>    ans = ans[!(as.POSIXlt(ans at Data)$wday == 0 |
> as.POSIXlt(ans at Data)$wday ==
>        6)]
>    ans at FinCenter = FinCenter
>    ans
> }
> #########################################################################
>
> Thank you
> Joe Wayne Byers, Ph.D.
>
> Spencer Graves wrote:
> >       What level of help do you need?  Have you tried listing the
> > function 'holidayNYSE', making a local copy, then editing it do what 
> you
> > want?  After you've tried that, please send the results to Diethelm
> > Wuertz and me.  If you can't make it work, please explain the problem.
> >
> >       Hope this helps.
> >       Spencer Graves
> >
> > Joe W. Byers wrote:
> >> Rmetrics
> >>
> >> Can anyone help me create a new holidays calendar method like
> >> holidaysNYSE() for NYMEX holidays.  The only difference is the 
> holidays
> >> for Independence day and thanks giving.
> >> Independence Day
> >>  Monday, July 3*
> >>  Tuesday, July 4
> >>  (Electronic trading closed Sunday and Monday, July 2 and 3; reopens
> >> 6:00 PM, July 4)
> >> Thanksgiving
> >>  Thursday, November 23
> >>  Friday, November 24
> >>  (NYMEX ClearPort? and CME Globex? open both days)
> >>
> >> NYMEX is closed on Monday July 3rd this year since the 4th is on a
> >> Tuesday and Friday the day following Thanksgiving.  This will end in
> >> 2007 according to NYMEX.  The Independence day holiday I think will
> >> observe Friday as a holiday when the 4th is on a Thurs but, 
> according to
> >> NYMEX they are not sure for 2007.  Also the webpage
> >> http://www.nymex.com/holida_schedu.aspx notes that Christmas eve will
> >> become a holiday in 2007 but expirations will not change.  I am 
> sending
> >> their customer service a request to post a document with the holiday
> >> rules as well as these tables for those of us who worry about a
> >> derivatives calendar.
> >>
> >> Thank you
> >> Joe
> >>
> >> _______________________________________________
> >> R-SIG-Finance at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >>
> >
>
>


From neilt at neiltiffin.com  Thu May  7 16:57:20 2009
From: neilt at neiltiffin.com (Neil Tiffin)
Date: Thu, 7 May 2009 09:57:20 -0500
Subject: [R-SIG-Finance] Intro Stock Market Time Series Questions
Message-ID: <19958F86-A954-45CB-8463-875131B0F26C@neiltiffin.com>

I am starting to learn more about using statistics and time series to  
augment my trading activities and need a few pointers to get started  
in the right direction.  Being an engineer, I have a background in  
programming and math, at the Calculus and Laplace Transform level, but  
only intro level statistics.  I have the books "Analysis of Financial  
Time Series" by Tsay,  "Intro Statistics with R" by Dalgarrd, and  
"Data Analysis and Graphics Using R" by Maindonald and Braun.

My first interest is determine what price changes are due to the  
market in general and what are due to underlying issues with a  
particular stock.  For this I would like to compare two time series  
and determine if the security is moving with the market (S&P500) or is  
something else effecting it.  I do not need to know what is effecting  
the price movement, but I am interested in understand if the effect is  
weak or strong compared to historical movements and if the movement is  
sympathetic or counter to the general market.  I have access to real  
time data and am interested in shorter term analysis that can provide  
information to support other trading decisions.

If someone would point me to specific functions or reference material  
I am more than happy to read and learn.  I am however, overwhelmed  
with the subject and just need a bit of practical advice as to how  
others are doing this day to day.

Thank you.

Neil Tiffin
Chicago


From cwrward at gmail.com  Thu May  7 18:26:47 2009
From: cwrward at gmail.com (Charles Ward)
Date: Thu, 7 May 2009 17:26:47 +0100
Subject: [R-SIG-Finance] Intro Stock Market Time Series Questions
In-Reply-To: <19958F86-A954-45CB-8463-875131B0F26C@neiltiffin.com>
References: <19958F86-A954-45CB-8463-875131B0F26C@neiltiffin.com>
Message-ID: <bd9aa36b0905070926o40b136d0m5904b38a8c4510f5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090507/5b8e5b6e/attachment.pl>

From m.breman at yahoo.com  Thu May  7 22:24:07 2009
From: m.breman at yahoo.com (Mark Breman)
Date: Thu, 7 May 2009 13:24:07 -0700 (PDT)
Subject: [R-SIG-Finance] Intro Stock Market Time Series Questions
In-Reply-To: <19958F86-A954-45CB-8463-875131B0F26C@neiltiffin.com>
References: <19958F86-A954-45CB-8463-875131B0F26C@neiltiffin.com>
Message-ID: <567978.95915.qm@web38606.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090507/a70dbb29/attachment.pl>

From neilt at neiltiffin.com  Thu May  7 22:46:54 2009
From: neilt at neiltiffin.com (Neil Tiffin)
Date: Thu, 7 May 2009 15:46:54 -0500
Subject: [R-SIG-Finance] Intro Stock Market Time Series Questions
In-Reply-To: <567978.95915.qm@web38606.mail.mud.yahoo.com>
References: <19958F86-A954-45CB-8463-875131B0F26C@neiltiffin.com>
	<567978.95915.qm@web38606.mail.mud.yahoo.com>
Message-ID: <FF53CE81-8751-4726-95EB-C05E5AA7F2B1@neiltiffin.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090507/76c4e2f8/attachment.pl>

From landronimirc at gmail.com  Thu May  7 23:00:13 2009
From: landronimirc at gmail.com (Liviu Andronic)
Date: Thu, 7 May 2009 23:00:13 +0200
Subject: [R-SIG-Finance] issues with NGARCH in rgarch package
In-Reply-To: <68b1e2610905021645x1135c283yed07d6bc86d018dd@mail.gmail.com>
References: <68b1e2610905021645x1135c283yed07d6bc86d018dd@mail.gmail.com>
Message-ID: <68b1e2610905071400i165f1489yea7f4519110b246d@mail.gmail.com>

In a very helpful discussion off-list, Alexios helped us identify our issue.

There seems to be a (general) confusion concerning the "NGARCH" and
"NAGARCH" denominations:
- The NGARCH (Nonlinear) also known as NAGARCH (nonlinear-in-mean,
asymmetric) model introduced by Engle & Ng (1993). For this use the
fit.NAGARCH example [1].
- The NARCH, and NGARCH extension, introduced by Higgins & Bera
(1992). For this use the fit.NGARCH example [1].

Liviu
[1] http://rgarch.r-forge.r-project.org/example3.html


On Sun, May 3, 2009 at 1:45 AM, Liviu Andronic <landronimirc at gmail.com> wrote:
> Dear Alexios,
>
> We are currently using rgarch from R-forge to compute basic GARCH(1,1)
> and more fancy NGARCH(1,1) models. For the simple garch(1,1) we get
> different results from tseries::garch(), fGarch::garchFit() and
> rgarch::ugarchfit(), however fairly comparable; if you're interested,
> I could post self-contained code highlighting the differences.
>
> For the NGARCH (via fGARCH), however, we get funny results and I
> believe it would be a bug in the code. If you look at the Hentschel's
> "Family GARCH" model [1], NGARCH is obtained when lambda is fixed and
> =2 (page 9 and 11 of the .pdf). However ugarchfit() seems to try to
> estimate this guy.
> A second issue, ugarchfit() will not try to estimate the assymetry
> shift: `theta' in our econometrics class, `b' in the article (page 11
> of the .pdf), or `fb' in the rgarch package (see `ugarchspec-methods
> {rgarch}' help page). Looking at the source code (internal-fgarch.R,
> line 31), it seems that fb is hard-set to 0:
> ? ? ? ?NGARCH ? =list(parameters=list(lambda=2, delta=0, fb=0, fc=0, fk=1),
>
> ? ? ? ? ? ? ? ?indicator=c(1,0,0,0,0), garchtype = 4),
> ?although we believe it should be one of the freely estimated
> parameters (instead of lambda, as currently it seems to be).
>
> Would there be any easy way to work around this issue? For code
> highlighting the problem, please look below.
> Thank you,
> Liviu
>
> [1] http://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity#fGARCH
>
> ### Code ###
>> return=vector()
>> zigma=vector()
>> return[1]=0
>> zigma[1]=0
>> normvector=rnorm(499,mean=0,sd=1)
>> #simulate random returns for predefined NGARCH parameters
>> for(i in 2:500)
> + {
> + zigma[i]=(0.01+0.05*(return[i-1]-2*zigma[i-1])^2+0.6*zigma[i-1]^2)^0.5
> + return[i]=zigma[i]*normvector[i-1]
> + }
>> head(return)
> [1] ?0.00000 ?0.08726 -0.02434 ?0.26456 ?0.19681 -0.01039
>> # Fitting Non-linear GARCH
>> require(rgarch)
>> spec.NGARCH <- ugarchspec(variance.model=list(model="fGARCH",
> + garchOrder=c(1,1), submodel="NGARCH"), mean.model=list(armaOrder=c(0,0),
> + include.mean=FALSE), distribution.model="snorm")
>> fit.NGARCH <- ugarchfit(data=return, spec=spec.NGARCH)
>> fit.NGARCH at fit$coef
> ?omega ?alpha1 ? beta1 ?lambda
> 0.04276 0.06191 0.87457 0.43634
>> fit.NGARCH
>
>
> GARCH Model Spec
> --------------------------
> Model : fGARCH Sub-Model : NGARCH
>
> Exogenous Regressors in variance equation: none
>
> Mean Equation :
> Include Mean : ?FALSE
> AR(FI)MA Model : (0,0,0)
> Garch-in-Mean : ?FALSE
> Exogenous Regressors in mean equation: none
> Conditional Distribution: ?norm
>
> GARCH Model Fit
> --------------------------
> Optimal Parameters:
> ? ? ? ?Estimate ?Std. Error ?t value Pr(>|t|)
> omega ? ?0.04276 ? ?0.030425 ? 1.4054 0.159894
> alpha1 ? 0.06191 ? ?0.028462 ? 2.1752 0.029613
> beta1 ? ?0.87457 ? ?0.061289 ?14.2695 0.000000
> lambda ? 0.43634 ? ?0.171291 ? 2.5474 0.010854
>
> Robust Standard Errors:
> ? ? ? ?Estimate ?Std. Error ?t value Pr(>|t|)
> omega ? ?0.04276 ? ?0.034747 ? 1.2306 0.218473
> alpha1 ? 0.06191 ? ?0.036494 ? 1.6965 0.089795
> beta1 ? ?0.87457 ? ?0.076156 ?11.4839 0.000000
> lambda ? 0.43634 ? ?0.179153 ? 2.4356 0.014868
>
> LogLikelihood : 62.65
>
> Information Criteria:
> Akaike ? ? ? 0.26660
> Bayes ? ? ? ?0.30032
> Shibata ? ? ?0.26648
> Hannan-Quinn 0.27983
>
>
> Q-Statistics on Standardized Residuals:
> ? ? ?statistic p-value
> Lag10 ? ? 6.161 ?0.8016
> Lag15 ? ?13.434 ?0.5688
> Lag20 ? ?21.480 ?0.3694
>
> H0 : No serial correlation
>
> Q-Statistics on Standardized Squared Residuals:
> ? ? ?statistic p-value
> Lag10 ? ? 7.032 ?0.7224
> Lag15 ? ?10.325 ?0.7988
> Lag20 ? ?17.254 ?0.6364
>
> ARCH LM Tests:
> ? ? ? ? ? ? Statistic DoF P-Value
> ARCH Lag[2] ? ? 0.1076 ? 2 ?0.9476
> ARCH Lag[5] ? ? 6.0089 ? 5 ?0.3054
> ARCH Lag[10] ? ?8.0694 ?10 ?0.6221
>
> Joint Statistic of the Nyblom stability test: 0.9012
>
> Individual Nyblom Statistics:
> omega ?0.2906
> alpha1 0.2260
> beta1 ?0.2734
> lambda 0.2845
>
> Sign Bias Test
> ? ? ? ? ? ? ? ? ? t-value ? ?prob sig
> Sign Bias ? ? ? ? ? 0.7199 0.47193
> Negative Sign Bias ?1.0525 0.29308
> Positive Sign Bias ?1.6822 0.09316 ? *
> Joint Effect ? ? ? ?5.5168 0.13764
>
> Chisq Goodnes of Fit Test
> [1] "under revision"
>
>
> Elapsed time : 1.298
>
> ### End of code ###
>



-- 
Do you know how to read?
http://www.alienetworks.com/srtest.cfm
Do you know how to write?
http://garbl.home.comcast.net/~garbl/stylemanual/e.htm#e-mail


From tom at limepepper.co.uk  Fri May  8 05:37:05 2009
From: tom at limepepper.co.uk (Tom H)
Date: Fri, 08 May 2009 04:37:05 +0100
Subject: [R-SIG-Finance] quantmod and intraday time periods
Message-ID: <1241753825.20945.1.camel@localhost.localdomain>


Hi r-sig-finance collective,

It seems to me that quantmod does not support intraday time periods like
minute, hour etc, is that correct? or am I missing something?

Thanks,

#T


From josh.m.ulrich at gmail.com  Fri May  8 05:54:57 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Thu, 7 May 2009 22:54:57 -0500
Subject: [R-SIG-Finance] quantmod and intraday time periods
In-Reply-To: <1241753825.20945.1.camel@localhost.localdomain>
References: <1241753825.20945.1.camel@localhost.localdomain>
Message-ID: <8cca69990905072054s2c58525bx2545aae0503f4353@mail.gmail.com>

Tom,

It would really help if you were more specific about which of the 150+
functions in quantmod do not seem to support intraday data.

Best,
Josh
--
http://www.fosstrading.com



On Thu, May 7, 2009 at 10:37 PM, Tom H <tom at limepepper.co.uk> wrote:
>
> Hi r-sig-finance collective,
>
> It seems to me that quantmod does not support intraday time periods like
> minute, hour etc, is that correct? or am I missing something?
>
> Thanks,
>
> #T
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From tom at limepepper.co.uk  Fri May  8 06:05:22 2009
From: tom at limepepper.co.uk (Tom H)
Date: Fri, 08 May 2009 05:05:22 +0100
Subject: [R-SIG-Finance] quantmod and intraday time periods
In-Reply-To: <8cca69990905072054s2c58525bx2545aae0503f4353@mail.gmail.com>
References: <1241753825.20945.1.camel@localhost.localdomain>
	<8cca69990905072054s2c58525bx2545aae0503f4353@mail.gmail.com>
Message-ID: <1241755522.21653.2.camel@localhost.localdomain>

On Thu, 2009-05-07 at 22:54 -0500, Joshua Ulrich wrote:
> Tom,
> 
> It would really help if you were more specific about which of the 150+
> functions in quantmod do not seem to support intraday data.

Ah, apologies for being vague and also a newbie.

I guess my point was that I don't seem to be able to specify a time
period, so the open, high, low etc, have to be of a default time period
- which I am probably wrong in guessing is a day.

Thanks,

#T


From brian at braverock.com  Fri May  8 11:21:09 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 08 May 2009 04:21:09 -0500
Subject: [R-SIG-Finance] quantmod and intraday time periods
In-Reply-To: <1241755522.21653.2.camel@localhost.localdomain>
References: <1241753825.20945.1.camel@localhost.localdomain>	<8cca69990905072054s2c58525bx2545aae0503f4353@mail.gmail.com>
	<1241755522.21653.2.camel@localhost.localdomain>
Message-ID: <4A03F985.7020303@braverock.com>

Tom H wrote:
> On Thu, 2009-05-07 at 22:54 -0500, Joshua Ulrich wrote:
>   
>> Tom,
>>
>> It would really help if you were more specific about which of the 150+
>> functions in quantmod do not seem to support intraday data.
>>     
>
> Ah, apologies for being vague and also a newbie.
>
> I guess my point was that I don't seem to be able to specify a time
> period, so the open, high, low etc, have to be of a default time period
> - which I am probably wrong in guessing is a day.
>   
Tom,

Per the posting guide and Josh's request, could you provide a little 
example code on what you're trying to do that you can't figure out how 
to make work with intraday data?  Your follow up post was not much less 
vague. 

In general, most/all things in quantmod should work for intraday regular 
time series data and even tick data.  quantmod sits on top of xts and 
the to.period functionality in xts, so irregular tick data would 
typically be turned into some regular series (15 sec, 1 min, 10 min, 
etc.) OHLC data using to.period. Then you could apply anything else you 
needed from quantmod.

If you provide some example code of what you're trying to do, someone 
here can probably sort it out pretty quickly.

Regards,

     - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From mschulman at lightboxcap.com  Fri May  8 17:48:36 2009
From: mschulman at lightboxcap.com (Michael Schulman)
Date: Fri, 8 May 2009 15:48:36 +0000
Subject: [R-SIG-Finance] maxratioPortfolio
Message-ID: <E8AAF3671DF8FD48B55D9CC7A88B9516783B63F324@MBX4.EXCHPROD.USA.NET>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090508/7bc99bac/attachment.pl>

From fjpcaballero at gmail.com  Fri May  8 20:28:28 2009
From: fjpcaballero at gmail.com (Francisco Javier Perez Caballero)
Date: Fri, 8 May 2009 14:28:28 -0400
Subject: [R-SIG-Finance] Quantmod getFinancials
Message-ID: <4a0479d0.09b6660a.53b9.ffffa8e3@mx.google.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090508/8ad79131/attachment.pl>

From josh.m.ulrich at gmail.com  Fri May  8 22:49:16 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Fri, 8 May 2009 15:49:16 -0500
Subject: [R-SIG-Finance] Quantmod getFinancials
In-Reply-To: <4a0479d0.09b6660a.53b9.ffffa8e3@mx.google.com>
References: <4a0479d0.09b6660a.53b9.ffffa8e3@mx.google.com>
Message-ID: <8cca69990905081349g472e6075la629a443b606cec0@mail.gmail.com>

Google must have changed their pages again.

The function tries to grep for "Ending" in the HTML, but the word is
no longer in initial caps.  I have patched the r-forge version of
quantmod.  You can either download it, or change "Ending" to "ending"
in your source and rebuild.

HTH,
Josh
--
http://www.fosstrading.com



On Fri, May 8, 2009 at 1:28 PM, Francisco Javier Perez Caballero
<fjpcaballero at gmail.com> wrote:
> I'm getting the following error (R 2.9.0, quantmod 0.3-7, Revision 461):
>
>> AAPL <- getFinancials('AAPL')
> Error in colnamesISCF[[2]] : subscript out of bounds
>
> Any idea of what may be going on?
>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From dmhutch at gmail.com  Sat May  9 21:18:13 2009
From: dmhutch at gmail.com (dmhutch at gmail.com)
Date: Sat, 9 May 2009 19:18:13 +0000
Subject: [R-SIG-Finance] R-SIG-Finance Digest, Vol 60, Issue 9
In-Reply-To: <mailman.1.1241863201.9928.r-sig-finance@stat.math.ethz.ch>
References: <mailman.1.1241863201.9928.r-sig-finance@stat.math.ethz.ch>
Message-ID: <1524752910-1241896696-cardhu_decombobulator_blackberry.rim.net-362275213-@bxe1236.bisx.prod.on.blackberry>

Dee
Sent from my BlackBerry device on the Rogers Wireless Network

-----Original Message-----
From: r-sig-finance-request at stat.math.ethz.ch

Date: Sat, 09 May 2009 12:00:01 
To: <r-sig-finance at stat.math.ethz.ch>
Subject: R-SIG-Finance Digest, Vol 60, Issue 9


Send R-SIG-Finance mailing list submissions to
	r-sig-finance at stat.math.ethz.ch

To subscribe or unsubscribe via the World Wide Web, visit
	https://stat.ethz.ch/mailman/listinfo/r-sig-finance
or, via email, send a message with subject or body 'help' to
	r-sig-finance-request at stat.math.ethz.ch

You can reach the person managing the list at
	r-sig-finance-owner at stat.math.ethz.ch

When replying, please edit your Subject line so it is more specific
than "Re: Contents of R-SIG-Finance digest..."


Today's Topics:

   1. maxratioPortfolio (Michael Schulman)
   2. Quantmod getFinancials (Francisco Javier Perez Caballero)
   3. Re: Quantmod getFinancials (Joshua Ulrich)


----------------------------------------------------------------------

Message: 1
Date: Fri, 8 May 2009 15:48:36 +0000
From: Michael Schulman <mschulman at lightboxcap.com>
Subject: [R-SIG-Finance] maxratioPortfolio
To: "R-SIG-Finance at stat.math.ethz.ch"
	<R-SIG-Finance at stat.math.ethz.ch>
Message-ID:
	<E8AAF3671DF8FD48B55D9CC7A88B9516783B63F324 at MBX4.EXCHPROD.USA.NET>
Content-Type: text/plain

Hi,
  First wanted to thank everyone for a lot of very good hard work done on R and the subsequent packages.

  So I've been looking at the maxratioPortfolio code in fPortfolio.  One thing I notice is how the range is decided for the search.  It looks like the range is decided by passing "interval = range(getMu(Data))"  While this works well in a completely unrestrained situation it does not work when there are constraints.  To give an example.  I am trying to optimize a portfolio with 3 assets with the following constraints :

c("minW[1:nAssets] = -0.5", "maxW[1:nAssets] = 0.5", "eqsumW[1:nAssets] = 1e-7", "Partial")

In other words I want a dollar neutral portfolio with no weight greater or less than .5, -.5 respectively.  As a side note having to pass in 1e-7 is somewhat counterintuitive but I could not find another way as passing in 0, removed that constraint.

My mu and sigma are :

$mu
           S           VZ            T
 0.007075173 -0.001899547 -0.001357274

$Sigma
             S           VZ            T
S  0.008534089 0.0015257184 0.0015642080
VZ 0.001525718 0.0009147493 0.0008163796
T  0.001564208 0.0008163796 0.0009165100


Now just taking the min and max of my mu as the range is wrong.  They are not feasible portfolios as I can not allocate 100% to that.  In general in other optimizers I've seen the range is decided by solving for the max return given the constraints and no constraint on the variance.  Its usually a simple linear programming problem.

In my case the weights would be 0.5, -0.5, 0 to get me the maximum returns within my portfolio and within my constraints.

Sorry if anything above is incorrect, out of context, or just stupid.   I'm fairly new to R so I have no doubt I'm mistaken on many things.


Thanks... mike


DISCLAIMER: The information contained herein is to be co...{{dropped:13}}



------------------------------

Message: 2
Date: Fri, 8 May 2009 14:28:28 -0400
From: Francisco Javier Perez Caballero <fjpcaballero at gmail.com>
Subject: [R-SIG-Finance] Quantmod getFinancials
To: <r-sig-finance at stat.math.ethz.ch>
Message-ID: <4a0479d0.09b6660a.53b9.ffffa8e3 at mx.google.com>
Content-Type: text/plain

I'm getting the following error (R 2.9.0, quantmod 0.3-7, Revision 461):

> AAPL <- getFinancials('AAPL')
Error in colnamesISCF[[2]] : subscript out of bounds

Any idea of what may be going on?


	[[alternative HTML version deleted]]



------------------------------

Message: 3
Date: Fri, 8 May 2009 15:49:16 -0500
From: Joshua Ulrich <josh.m.ulrich at gmail.com>
Subject: Re: [R-SIG-Finance] Quantmod getFinancials
To: Francisco Javier Perez Caballero <fjpcaballero at gmail.com>
Cc: r-sig-finance at stat.math.ethz.ch
Message-ID:
	<8cca69990905081349g472e6075la629a443b606cec0 at mail.gmail.com>
Content-Type: text/plain; charset=ISO-8859-1

Google must have changed their pages again.

The function tries to grep for "Ending" in the HTML, but the word is
no longer in initial caps.  I have patched the r-forge version of
quantmod.  You can either download it, or change "Ending" to "ending"
in your source and rebuild.

HTH,
Josh
--
http://www.fosstrading.com



On Fri, May 8, 2009 at 1:28 PM, Francisco Javier Perez Caballero
<fjpcaballero at gmail.com> wrote:
> I'm getting the following error (R 2.9.0, quantmod 0.3-7, Revision 461):
>
>> AAPL <- getFinancials('AAPL')
> Error in colnamesISCF[[2]] : subscript out of bounds
>
> Any idea of what may be going on?
>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



------------------------------

_______________________________________________
R-SIG-Finance mailing list
R-SIG-Finance at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


End of R-SIG-Finance Digest, Vol 60, Issue 9
********************************************

From spencer.graves at prodsyse.com  Sat May  9 22:17:21 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Sat, 09 May 2009 13:17:21 -0700
Subject: [R-SIG-Finance] positions in timeSeries object
In-Reply-To: <00dc01c9cdfc$56eec0f0$04cc42d0$@com>
References: <00dc01c9cdfc$56eec0f0$04cc42d0$@com>
Message-ID: <4A05E4D1.8010508@prodsyse.com>

      First you need to examine the results of each step to isolate the 
problem. 


      When I tried to replicate your problem, I first replaced 
"as.data.frame(scan(...))" with the following: 


DF <- read.table('ibm1.txt', header=TRUE, sep='|')


      I also replaced "df" by "DF", because "df" is the function for the 
density of an F distribution, and I'd prefer not to mask that with a 
data.frame. 


      After that, I solved the problem by studying the examples in the 
help file for "timeDate". 


      To get there, however, I found that there were two functions 
called "timeDate", in packages "timeDate" and "fCalendar".  I found this 
as follows: 


library(RSiteSearch)
tD <- RSiteSearch.function('timeDate')
sum(tD$Function=='timeDate')
HTML(tD[tD$Function=='timeDate', ])


     These two functions are probably identical, but I don't know that. 
     
      Hope this helps. 
      Spencer Graves

Sarkar, Arup wrote:
> Hi: I am trying the following code to incorporate positions in the time
> series object. Can some please help me.
>
>  
>
> Data File Format:
>
>  
>
> SYMBOL|DATE|EX|TIME|PRICE|SIZE|COND|CORR|G127
>
> IBM|11/03/2008|N|9:30:07|93.0800|73600|@|0|0
>
> IBM|11/03/2008|N|9:30:07|92.9700|500|@|0|0
>
> IBM|11/03/2008|N|9:30:07|93.1100|100|@|0|0
>
> IBM|11/03/2008|N|9:30:07|92.9700|100|@|0|0
>
> IBM|11/03/2008|N|9:30:07|92.8500|200|@|0|0
>
> IBM|11/03/2008|N|9:30:07|92.8200|100|@|0|0
>
> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>
> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>
> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>
> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>
> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>
> IBM|11/03/2008|N|9:30:08|93.0000|100|@|0|0
>
>  
>
> R script:
>
>  
>
> ## Columns Names
>
> fields.list =
> list(Symbol="",Date="",Ex="",Time="",Price=0,Size=0,Cond="",Corr=0,G127=0)
>
> #create a data frame
>
> df =
> as.data.frame(scan(file="c:/ibm_1.txt",what=fields.list,sep="|",skip=1,multi
> .line=TRUE,strip.white=TRUE),stringsAsFactors=F)
>
> dates.tmp = timeDate(charvec = df[, "Date"])
>
> times.tmp = df[, "Time"]
>
> td.tmp = paste(dates.tmp,times.tmp, sep=" ")
>
> ans = timeSeries(data = df[, setdiff(colIds(df), c("Date","Time"))],pos =
> td.tmp)
>
>  
>
> I am getting the following output when I am using the following command
> ans[1:5, ]
>
>  
>
>        Symbol Ex  Price   Size      Cond Corr G127
>
>      1 "IBM"  "N" "93.08" "  73600" "@"  "0"  "0" 
>
>      2 "IBM"  "N" "92.97" "    500" "@"  "0"  "0" 
>
>      3 "IBM"  "N" "93.11" "    100" "@"  "0"  "0" 
>
>      4 "IBM"  "N" "92.97" "    100" "@"  "0"  "0" 
>
>      5 "IBM"  "N" "92.85" "    200" "@"  "0"  "0"
>
>  
>
> I want to have the following output, how can I achieve it. Any help is
> highly appreciated.
>
>  
>
>      Positions  Symbol Ex  Price   Size      Cond Corr G127
>
>       "11/03/2008 9:30:07" "IBM"  "N" "93.08" "  73600" "@"  "0"  "0" 
>
>      "11/03/2008 9:30:07" "IBM"  "N" "92.97" "    500" "@"  "0"  "0" 
>
>      "11/03/2008 9:30:07" "IBM"  "N" "93.11" "    100" "@"  "0"  "0" 
>
>      "11/03/2008 9:30:07" "IBM"  "N" "92.97" "    100" "@"  "0"  "0" 
>
>      "11/03/2008 9:30:07" "IBM"  "N" "92.85" "    200" "@"  "0"  "0"
>
>  
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From elise at predictionimpact.com  Sun May 10 08:52:14 2009
From: elise at predictionimpact.com (Elise Johnson)
Date: Sat, 9 May 2009 23:52:14 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Predictive Analytics Seminar: May
 27-28, New York City
Message-ID: <23467607.post@talk.nabble.com>


Hi all, 

I wanted to let you know about our training seminar on predictive analytics
- coming May, Oct, and Nov in NYC, Stockholm, DC and other cities.  This is
intensive training for marketers, managers and business professionals to
make actionable sense of customer data by predicting buying behavior, churn,
etc.  Past attendees provided rave reviews.

Here's more info:
----------------------

Training Program: Predictive Analytics for Business, Marketing and Web

A two-day intensive seminar brought to you by Prediction Impact, Inc. 

Dates: May 27-28, Oct 14-15, Oct 18-19, and Nov 11-12, 2009
Locations: NYC (May), Stockholm (Oct), DC (Oct), San Francisco (Nov) 

93% rate this program Excellent or Very Good. 
**The official training program of Predictive Analytics World**
**Offered in conjunction with eMetrics events** 

Also see our Online Training: Predictive Analytics Applied - immediate
access at any time:
www.predictionimpact.com/predictive-analytics-online-training.html


ABOUT THIS SEMINAR:

Business metrics do a great job summarizing the past. But if you want to
predict how customers will respond in the future, there is one place to
turn--predictive analytics. By learning from your abundant historical data,
predictive analytics provides the marketer something beyond standard
business reports and sales forecasts: actionable predictions for each
customer. These predictions encompass all channels, both online and off,
foreseeing which customers will buy, click, respond, convert or cancel. If
you predict it, you own it. 

The customer predictions generated by predictive analytics deliver more
relevant content to each customer, improving response rates, click rates,
buying behavior, retention and overall profit. For online applications such
as e-marketing and customer care recommendations, predictive analytics acts
in real-time, dynamically selecting the ad, web content or cross-sell
product each visitor is most likely to click on or respond to, according to
that visitor's profile. This is AB selection, rather than just AB testing. 

Predictive Analytics for Business, Marketing and Web is a concentrated
training program that includes interactive breakout sessions and a brief
hands-on exercise. In two days we cover: 

- The techniques, tips and pointers you need in order to run a successful
predictive analytics and data mining initiative

- How to strategically position and tactically deploy predictive analytics
and data mining at your company

- How to bridge the prevalent gap between technical understanding and
practical use

- How a predictive model works, how it's created and how much revenue it
generates

- Several detailed case studies that demonstrate predictive analytics in
action and make the concepts concrete 

- NEW TOPIC: Five Ways to Lower Costs with Predictive Analytics 


No background in statistics or modeling is required. The only specific
knowledge assumed for this training program is moderate experience with
Microsoft Excel or equivalent. 

For more information, visit
www.predictionimpact.com/predictive-analytics-training.html, or e-mail us at
training at predictionimpact.com.  You may also call (415) 683-1146. 

Cross-Registration Special: Attendees earn $250 off the Predictive Analytics
World Conference 

SNEAK PREVIEW VIDEO:
www.predictionimpact.com/predictive-analytics-times.html 

$100 off early registration, 3 weeks ahead

-- 
View this message in context: http://www.nabble.com/Predictive-Analytics-Seminar%3A-May-27-28%2C-New-York-City-tp23467607p23467607.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From edd at debian.org  Sun May 10 16:04:09 2009
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 10 May 2009 09:04:09 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Predictive Analytics Seminar:
 May 27-28, New York City
In-Reply-To: <23467607.post@talk.nabble.com>
References: <23467607.post@talk.nabble.com>
Message-ID: <18950.57049.518316.666223@ron.nulle.part>


On 9 May 2009 at 23:52, Elise Johnson wrote:
| Training Program: Predictive Analytics for Business, Marketing and Web

'Business, Marketing and Web' is clearly off-topic for the r-sig-finance
list. Please do not post this, or similarly irrelevant material here again.

Dirk (wearing the listmaster hat)

-- 
Three out of two people have difficulties with fractions.


From wuertz at itp.phys.ethz.ch  Mon May 11 03:08:18 2009
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Mon, 11 May 2009 03:08:18 +0200
Subject: [R-SIG-Finance] maxratioPortfolio
In-Reply-To: <E8AAF3671DF8FD48B55D9CC7A88B9516783B63F324@MBX4.EXCHPROD.USA.NET>
References: <E8AAF3671DF8FD48B55D9CC7A88B9516783B63F324@MBX4.EXCHPROD.USA.NET>
Message-ID: <4A077A82.6020309@itp.phys.ethz.ch>

Michael Schulman wrote:
> Hi,
>   First wanted to thank everyone for a lot of very good hard work done on R and the subsequent packages.
>
>   So I've been looking at the maxratioPortfolio code in fPortfolio.  One thing I notice is how the range is decided for the search.  It looks like the range is decided by passing "interval = range(getMu(Data))"  While this works well in a completely unrestrained situation it does not work when there are constraints.  To give an example.  I am trying to optimize a portfolio with 3 assets with the following constraints :
>
> c("minW[1:nAssets] = -0.5", "maxW[1:nAssets] = 0.5", "eqsumW[1:nAssets] = 1e-7", "Partial")
>
> In other words I want a dollar neutral portfolio with no weight greater or less than .5, -.5 respectively.  As a side note having to pass in 1e-7 is somewhat counterintuitive but I could not find another way as passing in 0, removed that constraint.
>
> My mu and sigma are :
>
> $mu
>            S           VZ            T
>  0.007075173 -0.001899547 -0.001357274
>
> $Sigma
>              S           VZ            T
> S  0.008534089 0.0015257184 0.0015642080
> VZ 0.001525718 0.0009147493 0.0008163796
> T  0.001564208 0.0008163796 0.0009165100
>
>
> Now just taking the min and max of my mu as the range is wrong.  They are not feasible portfolios as I can not allocate 100% to that.  In general in other optimizers I've seen the range is decided by solving for the max return given the constraints and no constraint on the variance.  Its usually a simple linear programming problem.
>   
Also in the most generic case for a non linear objective? I think no.

> In my case the weights would be 0.5, -0.5, 0 to get me the maximum returns within my portfolio and within my constraints.
>
> Sorry if anything above is incorrect, out of context, or just stupid.   I'm fairly new to R so I have no doubt I'm mistaken on many things.
>   

For the moment, a quick and dirty fix in this special case  (when we 
have a unique solution for the max ratio) is to replace the line

 portfolio = optimize(f = ratioFun, interval = range(getMu(Data)),
        maximum = TRUE, data = Data, spec = spec, constraints = constraints)

by

portfolio = optimize(f = ratioFun, interval = c(-BIG, BIG),
        maximum = TRUE, data = Data, spec = spec, constraints = constraints)

with BIG big enough.


We will check this and take care for a better solution.


Thanks a lot, such examples are really helpful for us.


Diethelm Wuertz



> Thanks... mike
>
>
> DISCLAIMER: The information contained herein is to be co...{{dropped:13}}
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From spencer.graves at prodsyse.com  Mon May 11 03:12:50 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Sun, 10 May 2009 18:12:50 -0700
Subject: [R-SIG-Finance] [R-sig-finance] Domestic risk free rate in FX
 option
In-Reply-To: <23401986.post@talk.nabble.com>
References: <23401986.post@talk.nabble.com>
Message-ID: <4A077B92.2020202@prodsyse.com>

      This is a deep question for which I do not have an answer.  
However, I will contribute some comments, inviting others to comment 
further. 


      A few years ago, I considered making foreign investments.  I was 
advised against it, because I plan to spend most of my money for the 
rest of my life in my local currency.  In essence, foreign investments 
add the risk of exchange rates to the risk of the investment itself. 


      This suggests that the "domestic risk free interest rate" should 
be be local to the investor.  If you live in the US and you expect the 
vast majority of your expenses today and in the future to be in US 
dollars, then your "domestic risk free interest rate" would likely be 
based on US Treasuries, and your estimates of volatility need to include 
the volatility in the exchange rates between the dollar and the currency 
in which the forex option is traded. 


      Similarly, if you live in Timbuktu and most of your financial 
dealings today and in the future are in the local currency, CFA Francs, 
then maybe you need a "risk free rate" somehow tied to CFA Francs. 


      I doubt if this answers your question, but I hope it helps. 

      Best Wishes,
      Spencer Graves


RON70 wrote:
> In CME, option on forex is traded on EUR/GBP. If I want to price this option
> using some pricing formula then as Domestic risk free interest rate what
> should I take? Shouldn't risk free rate in UK be appropriate? I am asking
> this because as CME is in US, domestic currency is USD. Your suggestion
> appreciated.
>


From heshriti at gmail.com  Mon May 11 03:53:40 2009
From: heshriti at gmail.com (Mahesh Krishnan)
Date: Sun, 10 May 2009 18:53:40 -0700
Subject: [R-SIG-Finance] [R-sig-finance] Domestic risk free rate in FX
	option
In-Reply-To: <23401986.post@talk.nabble.com>
References: <23401986.post@talk.nabble.com>
Message-ID: <33f6aa6f0905101853w4b1b488bsbe19983da93ea1fe@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090510/17de2399/attachment.pl>

From kriskumar at earthlink.net  Tue May 12 03:04:35 2009
From: kriskumar at earthlink.net (Kris)
Date: Mon, 11 May 2009 21:04:35 -0400 (GMT-04:00)
Subject: [R-SIG-Finance] [R-sig-finance] Domestic risk free rate in
	FX	option
Message-ID: <2255646.1242090275363.JavaMail.root@elwamui-hound.atl.sa.earthlink.net>

To add my two cents to this.. Black-76 formula is really Black-scholes with the substitution F=S*exp((r-q)T) and  is the formula used to price options in currency land on options on forwards in general.  The black-76 formula gives you a value in term pips so in the case of EURGBP the value that comes out of the formula is in GBP pips. In order to convert this as a % of EUR notional you divide by Spot. If you are dealing with a GBP notional then you divide the result from the formula by strike to get this as a % of GBP notional. Typical quoting convention is to quote as a % of BASE (EUR) notional.

In general it is useful to think of the equivalence to stocks when IBM is quoted as 102$ it is really IBMUSD  => the amount of USD you need for one unit of IBM. So here IBM is the base or foreign ASSET and USD is the TERM or domestic asset in whose units the price is quoted. So when you price an option on IBM the value you get is in term (USD) units


Hope this helps,

Cheers
Krishna

-----Original Message-----
>From: Mahesh Krishnan <heshriti at gmail.com>
>Sent: May 10, 2009 9:53 PM
>To: RON70 <ron_michael70 at yahoo.com>
>Cc: r-sig-finance at stat.math.ethz.ch
>Subject: Re: [R-SIG-Finance] [R-sig-finance] Domestic risk free rate in FX	option
>
>Ron,
>
>Ultimately, currency options calculations depend on what you take as
>numeraire- the domestic currency, and what you take as the foreign currency.
>In the case of CME, EUR/GBP is quoted as pounds per euro, i.e. the domestic
>currency is pounds and foreign currency is euro.
>
>So if you were to price options on currencies using standard Merton's stock
>formula, you use the risk free rate of UK as domestic, and risk free rate of
>Euro zone as your "dividend yield".
>
>To my knowledge, CME only has options on futures, not spot currency. And if
>you are trying to price that, you basically plug in the risk free rate of UK
>in the futures-options model, and you get the option premium in pounds. You
>need to verify that CME option price is quoted it in pounds, I beleive it
>does.
>
>Mahesh
>
>
>
>On Wed, May 6, 2009 at 1:12 AM, RON70 <ron_michael70 at yahoo.com> wrote:
>
>>
>> In CME, option on forex is traded on EUR/GBP. If I want to price this
>> option
>> using some pricing formula then as Domestic risk free interest rate what
>> should I take? Shouldn't risk free rate in UK be appropriate? I am asking
>> this because as CME is in US, domestic currency is USD. Your suggestion
>> appreciated.
>> --
>> View this message in context:
>> http://www.nabble.com/Domestic-risk-free-rate-in-FX-option-tp23401986p23401986.html
>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
>-- 
>Mahesh Krishnan, Ph.D
>
>	[[alternative HTML version deleted]]
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only.
>-- If you want to post, subscribe first.


From arupsarkar at yahoo.com  Tue May 12 05:04:41 2009
From: arupsarkar at yahoo.com (Sarkar, Arup)
Date: Mon, 11 May 2009 23:04:41 -0400
Subject: [R-SIG-Finance] positions in timeSeries object
In-Reply-To: <4A05E4D1.8010508@prodsyse.com>
References: <00dc01c9cdfc$56eec0f0$04cc42d0$@com>
	<4A05E4D1.8010508@prodsyse.com>
Message-ID: <013c01c9d2ae$65292eb0$2f7b8c10$@com>

Spencer: Thanks very much for the response, I did install RSiteSearch,
however when I am trying out 
tD <- RSiteSearch.function('timeDate') command I am getting the following
error.

> tD <- RSiteSearch.function('timeDate')
Error: could not find function "RSiteSearch.function"
 Do I have to install anything else?

Regards
Arup

-----Original Message-----
From: spencerg [mailto:spencer.graves at prodsyse.com] 
Sent: Saturday, May 09, 2009 4:17 PM
To: Sarkar, Arup
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] positions in timeSeries object

      First you need to examine the results of each step to isolate the 
problem. 


      When I tried to replicate your problem, I first replaced 
"as.data.frame(scan(...))" with the following: 


DF <- read.table('ibm1.txt', header=TRUE, sep='|')


      I also replaced "df" by "DF", because "df" is the function for the 
density of an F distribution, and I'd prefer not to mask that with a 
data.frame. 


      After that, I solved the problem by studying the examples in the 
help file for "timeDate". 


      To get there, however, I found that there were two functions 
called "timeDate", in packages "timeDate" and "fCalendar".  I found this 
as follows: 


library(RSiteSearch)
tD <- RSiteSearch.function('timeDate')
sum(tD$Function=='timeDate')
HTML(tD[tD$Function=='timeDate', ])


     These two functions are probably identical, but I don't know that. 
     
      Hope this helps. 
      Spencer Graves

Sarkar, Arup wrote:
> Hi: I am trying the following code to incorporate positions in the time
> series object. Can some please help me.
>
>  
>
> Data File Format:
>
>  
>
> SYMBOL|DATE|EX|TIME|PRICE|SIZE|COND|CORR|G127
>
> IBM|11/03/2008|N|9:30:07|93.0800|73600|@|0|0
>
> IBM|11/03/2008|N|9:30:07|92.9700|500|@|0|0
>
> IBM|11/03/2008|N|9:30:07|93.1100|100|@|0|0
>
> IBM|11/03/2008|N|9:30:07|92.9700|100|@|0|0
>
> IBM|11/03/2008|N|9:30:07|92.8500|200|@|0|0
>
> IBM|11/03/2008|N|9:30:07|92.8200|100|@|0|0
>
> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>
> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>
> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>
> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>
> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>
> IBM|11/03/2008|N|9:30:08|93.0000|100|@|0|0
>
>  
>
> R script:
>
>  
>
> ## Columns Names
>
> fields.list =
> list(Symbol="",Date="",Ex="",Time="",Price=0,Size=0,Cond="",Corr=0,G127=0)
>
> #create a data frame
>
> df =
>
as.data.frame(scan(file="c:/ibm_1.txt",what=fields.list,sep="|",skip=1,multi
> .line=TRUE,strip.white=TRUE),stringsAsFactors=F)
>
> dates.tmp = timeDate(charvec = df[, "Date"])
>
> times.tmp = df[, "Time"]
>
> td.tmp = paste(dates.tmp,times.tmp, sep=" ")
>
> ans = timeSeries(data = df[, setdiff(colIds(df), c("Date","Time"))],pos =
> td.tmp)
>
>  
>
> I am getting the following output when I am using the following command
> ans[1:5, ]
>
>  
>
>        Symbol Ex  Price   Size      Cond Corr G127
>
>      1 "IBM"  "N" "93.08" "  73600" "@"  "0"  "0" 
>
>      2 "IBM"  "N" "92.97" "    500" "@"  "0"  "0" 
>
>      3 "IBM"  "N" "93.11" "    100" "@"  "0"  "0" 
>
>      4 "IBM"  "N" "92.97" "    100" "@"  "0"  "0" 
>
>      5 "IBM"  "N" "92.85" "    200" "@"  "0"  "0"
>
>  
>
> I want to have the following output, how can I achieve it. Any help is
> highly appreciated.
>
>  
>
>      Positions  Symbol Ex  Price   Size      Cond Corr G127
>
>       "11/03/2008 9:30:07" "IBM"  "N" "93.08" "  73600" "@"  "0"  "0" 
>
>      "11/03/2008 9:30:07" "IBM"  "N" "92.97" "    500" "@"  "0"  "0" 
>
>      "11/03/2008 9:30:07" "IBM"  "N" "93.11" "    100" "@"  "0"  "0" 
>
>      "11/03/2008 9:30:07" "IBM"  "N" "92.97" "    100" "@"  "0"  "0" 
>
>      "11/03/2008 9:30:07" "IBM"  "N" "92.85" "    200" "@"  "0"  "0"
>
>  
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From johnni.nielsen2 at gmail.com  Tue May 12 10:43:41 2009
From: johnni.nielsen2 at gmail.com (TipTop)
Date: Tue, 12 May 2009 01:43:41 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Interfacing R with Interactive
	Brokers
Message-ID: <23498533.post@talk.nabble.com>


Hey,

has anyone here been able to succesfully interface R with a Interactive
Brokers account?
What is your experience and recommendations?

Regards

Johnni
-- 
View this message in context: http://www.nabble.com/Interfacing-R-with-Interactive-Brokers-tp23498533p23498533.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From josemariarodriguez1976 at gmail.com  Tue May 12 13:12:55 2009
From: josemariarodriguez1976 at gmail.com (=?ISO-8859-1?Q?jos=E9_maria_Rodriguez?=)
Date: Tue, 12 May 2009 13:12:55 +0200
Subject: [R-SIG-Finance] R^2 extraction and
	autocorrelation/heterokedasticity on TSLS regression
Message-ID: <865aedfa0905120412q7a732675kc3111dccbd8c4e46@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090512/1ce0c658/attachment.pl>

From brian at braverock.com  Tue May 12 13:13:50 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 12 May 2009 06:13:50 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Interfacing R with Interactive
 Brokers
In-Reply-To: <23498533.post@talk.nabble.com>
References: <23498533.post@talk.nabble.com>
Message-ID: <4A0959EE.5010800@braverock.com>

TipTop wrote:
> has anyone here been able to succesfully interface R with a Interactive
> Brokers account?
> What is your experience and recommendations?
>   
Try searching a bit before posting? 

The IBrokers package implements an interface to R.

http://cran.r-project.org/web/packages/IBrokers/index.html

Regards,

   - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From josemariarodriguez1976 at gmail.com  Tue May 12 14:11:03 2009
From: josemariarodriguez1976 at gmail.com (=?ISO-8859-1?Q?jos=E9_maria_Rodriguez?=)
Date: Tue, 12 May 2009 14:11:03 +0200
Subject: [R-SIG-Finance] R^2 extraction and
	autocorrelation/heterokedasticity tests on tsls regression
Message-ID: <865aedfa0905120511q46d9131cr8837cebf3821e163@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090512/fcce1e93/attachment.pl>

From jeff.a.ryan at gmail.com  Tue May 12 14:16:33 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Tue, 12 May 2009 07:16:33 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Interfacing R with Interactive
	Brokers
In-Reply-To: <23498533.post@talk.nabble.com>
References: <23498533.post@talk.nabble.com>
Message-ID: <e8e755250905120516n4cd900d9lb8c04f4a40740a4a@mail.gmail.com>

As Brian pointed out, the only CRAN version is via IBrokers.

This is implemented entirely in R code, so it is very portable and
very "R"-like with respect to managing events to/from the TWS (Trader
Workstation).  There are two vignettes, though both are not quite
synched with the current release to CRAN (a brand new 0.2-0 as of
yesterday). The second vignette isn't complete yet either.

The package is an entirely re-implemented API based off the
Interactive Brokers sources.  There is no dependence on the 'official'
API.  Licensing or otherwise (IBrokers is GPL'd).

There are of course numerous other ways to manage a connection.  You
can use the Java API and something like RJava (look at RIB on Rforge).
 Or Rpy and the IbPy interface for Python.  The trading-shim is also a
viable option.  You could even write it at the C/C++ level and call R
from there when needed.  Keep in mind there are many technical issues
once you add an additional layer into the mix.  There may also be
licensing issues given that the official API has no visible license...

One of the major benefits of "IBrokers" is that you get a lot more
design than just a simple wrapper to the underlying API/connection.
Through the use of eWrapper closures and a customized twsCALLBACK you
can quickly build live trading systems entirely in R.  Order handling
isn't quite ready yet (though is available in the technical/alpha
sense), but should be very soon.

If you give it a try, please pass along good and bad feedback to the
author (me ;) )

Thanks,
Jeff

On Tue, May 12, 2009 at 3:43 AM, TipTop <johnni.nielsen2 at gmail.com> wrote:
>
> Hey,
>
> has anyone here been able to succesfully interface R with a Interactive
> Brokers account?
> What is your experience and recommendations?
>
> Regards
>
> Johnni
> --
> View this message in context: http://www.nabble.com/Interfacing-R-with-Interactive-Brokers-tp23498533p23498533.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From josemariarodriguez1976 at gmail.com  Tue May 12 14:43:32 2009
From: josemariarodriguez1976 at gmail.com (=?ISO-8859-1?Q?jos=E9_maria_Rodriguez?=)
Date: Tue, 12 May 2009 14:43:32 +0200
Subject: [R-SIG-Finance] TSLS: R^2 extraction and autocorrelation and
	heterokedasticity tests
Message-ID: <865aedfa0905120543n6ac61ddei714230c48e42503a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090512/fcaafe6f/attachment.pl>

From johnni.nielsen2 at gmail.com  Tue May 12 14:53:05 2009
From: johnni.nielsen2 at gmail.com (TipTop)
Date: Tue, 12 May 2009 05:53:05 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Interfacing R with Interactive
	Brokers
In-Reply-To: <e8e755250905120516n4cd900d9lb8c04f4a40740a4a@mail.gmail.com>
References: <23498533.post@talk.nabble.com>
	<e8e755250905120516n4cd900d9lb8c04f4a40740a4a@mail.gmail.com>
Message-ID: <23501989.post@talk.nabble.com>


Thanks for the reply Jeff! part of my motivation behind the request, was also
not to get in touch with people who has implemented this, and is using it on
a daily basis. Im new to R and have some basis knowledge in Matlab, who as
you know also interface to R. However as an individual investor i feel R and
its open source platform may be a better longterm bet for me. 

I shall start my journey into the implementation of the package and thanks
for your effort on this and also to Brian for the performanceanalytics
package.

Johnni


Jeff Ryan wrote:
> 
> As Brian pointed out, the only CRAN version is via IBrokers.
> 
> This is implemented entirely in R code, so it is very portable and
> very "R"-like with respect to managing events to/from the TWS (Trader
> Workstation).  There are two vignettes, though both are not quite
> synched with the current release to CRAN (a brand new 0.2-0 as of
> yesterday). The second vignette isn't complete yet either.
> 
> The package is an entirely re-implemented API based off the
> Interactive Brokers sources.  There is no dependence on the 'official'
> API.  Licensing or otherwise (IBrokers is GPL'd).
> 
> There are of course numerous other ways to manage a connection.  You
> can use the Java API and something like RJava (look at RIB on Rforge).
>  Or Rpy and the IbPy interface for Python.  The trading-shim is also a
> viable option.  You could even write it at the C/C++ level and call R
> from there when needed.  Keep in mind there are many technical issues
> once you add an additional layer into the mix.  There may also be
> licensing issues given that the official API has no visible license...
> 
> One of the major benefits of "IBrokers" is that you get a lot more
> design than just a simple wrapper to the underlying API/connection.
> Through the use of eWrapper closures and a customized twsCALLBACK you
> can quickly build live trading systems entirely in R.  Order handling
> isn't quite ready yet (though is available in the technical/alpha
> sense), but should be very soon.
> 
> If you give it a try, please pass along good and bad feedback to the
> author (me ;) )
> 
> Thanks,
> Jeff
> 
> On Tue, May 12, 2009 at 3:43 AM, TipTop <johnni.nielsen2 at gmail.com> wrote:
>>
>> Hey,
>>
>> has anyone here been able to succesfully interface R with a Interactive
>> Brokers account?
>> What is your experience and recommendations?
>>
>> Regards
>>
>> Johnni
>> --
>> View this message in context:
>> http://www.nabble.com/Interfacing-R-with-Interactive-Brokers-tp23498533p23498533.html
>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
> 
> 
> 
> -- 
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
> 
> ia: insight algorithmics
> www.insightalgo.com
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/Interfacing-R-with-Interactive-Brokers-tp23498533p23501989.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From bearxu83 at gmail.com  Tue May 12 15:52:38 2009
From: bearxu83 at gmail.com (BearXu)
Date: Tue, 12 May 2009 14:52:38 +0100
Subject: [R-SIG-Finance] [R-sig-finance] Domestic risk free rate in FX
	option
In-Reply-To: <2255646.1242090275363.JavaMail.root@elwamui-hound.atl.sa.earthlink.net>
References: <2255646.1242090275363.JavaMail.root@elwamui-hound.atl.sa.earthlink.net>
Message-ID: <82527b5d0905120652w3e9ec3f3i785f13f255fc20ae@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090512/c65f1e5c/attachment.pl>

From bearxu83 at gmail.com  Tue May 12 16:22:14 2009
From: bearxu83 at gmail.com (BearXu)
Date: Tue, 12 May 2009 15:22:14 +0100
Subject: [R-SIG-Finance] the payoff of an call option
Message-ID: <82527b5d0905120722u79aa550flcb87fe883310dea9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090512/8a6e0ca6/attachment.pl>

From davidr at rhotrading.com  Tue May 12 16:24:19 2009
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Tue, 12 May 2009 09:24:19 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Domestic risk free rate in
	FXoption
In-Reply-To: <82527b5d0905120652w3e9ec3f3i785f13f255fc20ae@mail.gmail.com>
References: <2255646.1242090275363.JavaMail.root@elwamui-hound.atl.sa.earthlink.net>
	<82527b5d0905120652w3e9ec3f3i785f13f255fc20ae@mail.gmail.com>
Message-ID: <1714D297FE4041D98F1332A73FB4EC5E@rhotrading.com>

NO, the price is in GBP per EUR.
Look at http://www.cmegroup.com/rulebook/CME/III/300/301A/301A.pdf.

Please check your facts before posting.
-- David


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of BearXu
Sent: Tuesday, May 12, 2009 8:53 AM
To: Kris
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] [R-sig-finance] Domestic risk free rate in
FXoption

In my opinion, the risk free rate may still be the US risk free rate
because
the price of EUROGBP is in  US dollar.

But the derivative is changed in terms of its underlying, so whether the
Model of EUROGBP is Brown Motion or not will affect your price formula
much.

2009/5/12 Kris <kriskumar at earthlink.net>

> To add my two cents to this.. Black-76 formula is really Black-scholes
with
> the substitution F=S*exp((r-q)T) and  is the formula used to price
options
> in currency land on options on forwards in general.  The black-76
formula
> gives you a value in term pips so in the case of EURGBP the value that
comes
> out of the formula is in GBP pips. In order to convert this as a % of
EUR
> notional you divide by Spot. If you are dealing with a GBP notional
then you
> divide the result from the formula by strike to get this as a % of GBP
> notional. Typical quoting convention is to quote as a % of BASE (EUR)
> notional.
>
> In general it is useful to think of the equivalence to stocks when IBM
is
> quoted as 102$ it is really IBMUSD  => the amount of USD you need for
one
> unit of IBM. So here IBM is the base or foreign ASSET and USD is the
TERM or
> domestic asset in whose units the price is quoted. So when you price
an
> option on IBM the value you get is in term (USD) units
>
>
> Hope this helps,
>
> Cheers
> Krishna
>
> -----Original Message-----
> >From: Mahesh Krishnan <heshriti at gmail.com>
> >Sent: May 10, 2009 9:53 PM
> >To: RON70 <ron_michael70 at yahoo.com>
> >Cc: r-sig-finance at stat.math.ethz.ch
> >Subject: Re: [R-SIG-Finance] [R-sig-finance] Domestic risk free rate
in FX
>     option
> >
> >Ron,
> >
> >Ultimately, currency options calculations depend on what you take as
> >numeraire- the domestic currency, and what you take as the foreign
> currency.
> >In the case of CME, EUR/GBP is quoted as pounds per euro, i.e. the
> domestic
> >currency is pounds and foreign currency is euro.
> >
> >So if you were to price options on currencies using standard Merton's
> stock
> >formula, you use the risk free rate of UK as domestic, and risk free
rate
> of
> >Euro zone as your "dividend yield".
> >
> >To my knowledge, CME only has options on futures, not spot currency.
And
> if
> >you are trying to price that, you basically plug in the risk free
rate of
> UK
> >in the futures-options model, and you get the option premium in
pounds.
> You
> >need to verify that CME option price is quoted it in pounds, I
beleive it
> >does.
> >
> >Mahesh
> >
> >
> >
> >On Wed, May 6, 2009 at 1:12 AM, RON70 <ron_michael70 at yahoo.com>
wrote:
> >
> >>
> >> In CME, option on forex is traded on EUR/GBP. If I want to price
this
> >> option
> >> using some pricing formula then as Domestic risk free interest rate
what
> >> should I take? Shouldn't risk free rate in UK be appropriate? I am
> asking
> >> this because as CME is in US, domestic currency is USD. Your
suggestion
> >> appreciated.
> >> --
> >> View this message in context:
> >>
>
http://www.nabble.com/Domestic-risk-free-rate-in-FX-option-tp23401986p23
401986.html
> >> Sent from the Rmetrics mailing list archive at Nabble.com.
> >>
> >> _______________________________________________
> >> R-SIG-Finance at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >> -- Subscriber-posting only.
> >> -- If you want to post, subscribe first.
> >>
> >
> >
> >
> >--
> >Mahesh Krishnan, Ph.D
> >
> >       [[alternative HTML version deleted]]
> >
> >_______________________________________________
> >R-SIG-Finance at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >-- Subscriber-posting only.
> >-- If you want to post, subscribe first.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

This e-mail and any materials attached hereto, including, without limitation, all content hereof and thereof (collectively, "Rho Content") are confidential and proprietary to Rho Trading Securities, LLC ("Rho") and/or its affiliates, and are protected by intellectual property laws.  Without the prior written consent of Rho, the Rho Content may not (i) be disclosed to any third party or (ii) be reproduced or otherwise used by anyone other than current employees of Rho or its affiliates, on behalf of Rho or its affiliates.

THE RHO CONTENT IS PROVIDED AS IS, WITHOUT REPRESENTATIONS OR WARRANTIES OF ANY KIND.  TO THE MAXIMUM EXTENT PERMISSIBLE UNDER APPLICABLE LAW, RHO HEREBY DISCLAIMS ANY AND ALL WARRANTIES, EXPRESS AND IMPLIED, RELATING TO THE RHO CONTENT, AND NEITHER RHO NOR ANY OF ITS AFFILIATES SHALL IN ANY EVENT BE LIABLE FOR ANY DAMAGES OF ANY NATURE WHATSOEVER, INCLUDING, BUT NOT LIMITED TO, DIRECT, INDIRECT, CONSEQUENTIAL, SPECIAL AND PUNITIVE DAMAGES, LOSS OF PROFITS AND TRADING LOSSES, RESULTING FROM ANY PERSON?S USE OR RELIANCE UPON, OR INABILITY TO USE, ANY RHO CONTENT, EVEN IF RHO IS ADVISED OF THE POSSIBILITY OF SUCH DAMAGES OR IF SUCH DAMAGES WERE FORESEEABLE.


From ron_michael70 at yahoo.com  Tue May 12 16:29:46 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Tue, 12 May 2009 07:29:46 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Domestic risk free rate in FX
	option
In-Reply-To: <82527b5d0905120652w3e9ec3f3i785f13f255fc20ae@mail.gmail.com>
References: <2255646.1242090275363.JavaMail.root@elwamui-hound.atl.sa.earthlink.net>
	<82527b5d0905120652w3e9ec3f3i785f13f255fc20ae@mail.gmail.com>
Message-ID: <23503701.post@talk.nabble.com>


"the price of EUROGBP is in  US dollar" : from the specification given in CME
it is in GBP. Please see this :
http://www.cmegroup.com/trading/fx/fx/euro-fx-british-pound_contractSpecs_options.html



BearXu wrote:
> 
> In my opinion, the risk free rate may still be the US risk free rate
> because
> the price of EUROGBP is in  US dollar.
> 
> But the derivative is changed in terms of its underlying, so whether the
> Model of EUROGBP is Brown Motion or not will affect your price formula
> much.
> 
> 2009/5/12 Kris <kriskumar at earthlink.net>
> 
>> To add my two cents to this.. Black-76 formula is really Black-scholes
>> with
>> the substitution F=S*exp((r-q)T) and  is the formula used to price
>> options
>> in currency land on options on forwards in general.  The black-76 formula
>> gives you a value in term pips so in the case of EURGBP the value that
>> comes
>> out of the formula is in GBP pips. In order to convert this as a % of EUR
>> notional you divide by Spot. If you are dealing with a GBP notional then
>> you
>> divide the result from the formula by strike to get this as a % of GBP
>> notional. Typical quoting convention is to quote as a % of BASE (EUR)
>> notional.
>>
>> In general it is useful to think of the equivalence to stocks when IBM is
>> quoted as 102$ it is really IBMUSD  => the amount of USD you need for one
>> unit of IBM. So here IBM is the base or foreign ASSET and USD is the TERM
>> or
>> domestic asset in whose units the price is quoted. So when you price an
>> option on IBM the value you get is in term (USD) units
>>
>>
>> Hope this helps,
>>
>> Cheers
>> Krishna
>>
>> -----Original Message-----
>> >From: Mahesh Krishnan <heshriti at gmail.com>
>> >Sent: May 10, 2009 9:53 PM
>> >To: RON70 <ron_michael70 at yahoo.com>
>> >Cc: r-sig-finance at stat.math.ethz.ch
>> >Subject: Re: [R-SIG-Finance] [R-sig-finance] Domestic risk free rate in
>> FX
>>     option
>> >
>> >Ron,
>> >
>> >Ultimately, currency options calculations depend on what you take as
>> >numeraire- the domestic currency, and what you take as the foreign
>> currency.
>> >In the case of CME, EUR/GBP is quoted as pounds per euro, i.e. the
>> domestic
>> >currency is pounds and foreign currency is euro.
>> >
>> >So if you were to price options on currencies using standard Merton's
>> stock
>> >formula, you use the risk free rate of UK as domestic, and risk free
>> rate
>> of
>> >Euro zone as your "dividend yield".
>> >
>> >To my knowledge, CME only has options on futures, not spot currency. And
>> if
>> >you are trying to price that, you basically plug in the risk free rate
>> of
>> UK
>> >in the futures-options model, and you get the option premium in pounds.
>> You
>> >need to verify that CME option price is quoted it in pounds, I beleive
>> it
>> >does.
>> >
>> >Mahesh
>> >
>> >
>> >
>> >On Wed, May 6, 2009 at 1:12 AM, RON70 <ron_michael70 at yahoo.com> wrote:
>> >
>> >>
>> >> In CME, option on forex is traded on EUR/GBP. If I want to price this
>> >> option
>> >> using some pricing formula then as Domestic risk free interest rate
>> what
>> >> should I take? Shouldn't risk free rate in UK be appropriate? I am
>> asking
>> >> this because as CME is in US, domestic currency is USD. Your
>> suggestion
>> >> appreciated.
>> >> --
>> >> View this message in context:
>> >>
>> http://www.nabble.com/Domestic-risk-free-rate-in-FX-option-tp23401986p23401986.html
>> >> Sent from the Rmetrics mailing list archive at Nabble.com.
>> >>
>> >> _______________________________________________
>> >> R-SIG-Finance at stat.math.ethz.ch mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> >> -- Subscriber-posting only.
>> >> -- If you want to post, subscribe first.
>> >>
>> >
>> >
>> >
>> >--
>> >Mahesh Krishnan, Ph.D
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> >_______________________________________________
>> >R-SIG-Finance at stat.math.ethz.ch mailing list
>> >https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> >-- Subscriber-posting only.
>> >-- If you want to post, subscribe first.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/Re%3A--R-sig-finance--Domestic-risk-free-rate-in-FX%09option-tp23494513p23503701.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From bearxu83 at gmail.com  Tue May 12 16:41:32 2009
From: bearxu83 at gmail.com (BearXu)
Date: Tue, 12 May 2009 15:41:32 +0100
Subject: [R-SIG-Finance] [R-sig-finance] Domestic risk free rate in FX
	option
In-Reply-To: <23503701.post@talk.nabble.com>
References: <2255646.1242090275363.JavaMail.root@elwamui-hound.atl.sa.earthlink.net>
	<82527b5d0905120652w3e9ec3f3i785f13f255fc20ae@mail.gmail.com> 
	<23503701.post@talk.nabble.com>
Message-ID: <82527b5d0905120741t540ae439s22ebd1f8641d8279@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090512/561d6ba8/attachment.pl>

From bearxu83 at gmail.com  Tue May 12 17:52:12 2009
From: bearxu83 at gmail.com (BearXu)
Date: Tue, 12 May 2009 16:52:12 +0100
Subject: [R-SIG-Finance] the payoff of an call option
In-Reply-To: <82527b5d0905120722u79aa550flcb87fe883310dea9@mail.gmail.com>
References: <82527b5d0905120722u79aa550flcb87fe883310dea9@mail.gmail.com>
Message-ID: <82527b5d0905120852p54ae7609h714a8047ca88fec7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090512/f6b3a784/attachment.pl>

From pdebruic at gmail.com  Tue May 12 20:57:56 2009
From: pdebruic at gmail.com (Paul DeBruicker)
Date: Tue, 12 May 2009 14:57:56 -0400
Subject: [R-SIG-Finance] the payoff of an call option
In-Reply-To: <82527b5d0905120852p54ae7609h714a8047ca88fec7@mail.gmail.com>
References: <82527b5d0905120722u79aa550flcb87fe883310dea9@mail.gmail.com>
	<82527b5d0905120852p54ae7609h714a8047ca88fec7@mail.gmail.com>
Message-ID: <4A09C6B4.1030000@gmail.com>

BearXu wrote:
> A more detail example.
> 
> If there are two persons: A and B, A wants to buy a stock; B wants to buy an
> option of this stock and the strike price of it is 90$.Now they both have
> 100$. Suppose the stock price now is 100$.
> 
> So A use his 100$ buy a stock, and B use a little money C <100$ buy an
> option and save other money into bank.
> 
> One year later, the stock price is 110$, so the profit of A is [110-100]$
> and B took his money out from the bank 90$ to buy the stock and his profit
> from this trade is [110-(90+C)]$, but he still got another profit from the
> bank interests during the time because he saved the money in it. So his
> total profit is [110+r-(90+C)]$.
> 
> 
That actually looks like the payoff of a long call position and a long 
cash position.  There's nothing that says the investor has to put the 
cash they didn't use to buy the stock when they opened the call position 
into cash.  There are no margin requirements for buying call options. 
The investor could have taken out the option position C without having 
cash to buy the stock ever.  If you exercise and sell at expiration all 
the cash you ever need is the premium paid at initiation.  The 
settlement dates on the transactions can be the same so you wouldn't 
have to borrow the cash to exercise.  And really, in most cases you 
wouldn't exercise, you'd just sell it back.

Regardless, I'm not sure what any of this has to do with the 
intersection of R and Finance.

Paul


From Zeno.Adams at ebs.edu  Wed May 13 11:23:09 2009
From: Zeno.Adams at ebs.edu (Adams, Zeno)
Date: Wed, 13 May 2009 11:23:09 +0200
Subject: [R-SIG-Finance] the payoff of an call option
References: <82527b5d0905120722u79aa550flcb87fe883310dea9@mail.gmail.com><82527b5d0905120852p54ae7609h714a8047ca88fec7@mail.gmail.com>
	<4A09C6B4.1030000@gmail.com>
Message-ID: <9064522880125945B98983BBAECBA1CC21CC4E@exchsrv001.ebs.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090513/dee45774/attachment.pl>

From bearxu83 at gmail.com  Wed May 13 14:29:56 2009
From: bearxu83 at gmail.com (BearXu)
Date: Wed, 13 May 2009 13:29:56 +0100
Subject: [R-SIG-Finance] the payoff of an call option
In-Reply-To: <9064522880125945B98983BBAECBA1CC21CC4E@exchsrv001.ebs.local>
References: <82527b5d0905120722u79aa550flcb87fe883310dea9@mail.gmail.com> 
	<82527b5d0905120852p54ae7609h714a8047ca88fec7@mail.gmail.com> 
	<4A09C6B4.1030000@gmail.com>
	<9064522880125945B98983BBAECBA1CC21CC4E@exchsrv001.ebs.local>
Message-ID: <82527b5d0905130529h3a1c687as94ab06c66fea87ea@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090513/08dc76ed/attachment.pl>

From swisdom at gmail.com  Wed May 13 19:50:16 2009
From: swisdom at gmail.com (Steve Wisdom)
Date: Wed, 13 May 2009 13:50:16 -0400
Subject: [R-SIG-Finance] "Next" in quantmod
Message-ID: <8890ce50905131050s55c2b162u8662e5aa8d4dcf12@mail.gmail.com>

Not sure is this is a bug, or a feature, or just a brain-cramp on my end:

> require(quantmod)
Loading required package: quantmod
Loading required package: xts
Loading required package: zoo

Attaching package: 'zoo'

        The following object(s) are masked from package:base :

         as.Date.numeric

Loading required package: Defaults
quantmod: Quantitative Financial Modelling Framework

Version 0.3-7, Revision 461
http://www.quantmod.com

> getSymbols("CSCO")
[1] "CSCO"
> head(Lag(Cl(CSCO)))
           Lag.1
2007-01-03    NA
2007-01-04 27.73
2007-01-05 28.46
2007-01-08 28.47
2007-01-09 28.63
2007-01-10 28.47
> head(Next(Cl(CSCO)))                                <--------------
Error in `[.xts`(x, -(0:k), ) :
  only zeros may be mixed with negative subscripts
> R.version.string


From jeff.a.ryan at gmail.com  Wed May 13 19:57:11 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 13 May 2009 12:57:11 -0500
Subject: [R-SIG-Finance] "Next" in quantmod
In-Reply-To: <8890ce50905131050s55c2b162u8662e5aa8d4dcf12@mail.gmail.com>
References: <8890ce50905131050s55c2b162u8662e5aa8d4dcf12@mail.gmail.com>
Message-ID: <e8e755250905131057g6d71f6c3h50016685523b35f@mail.gmail.com>

A bug.  :)

It is fixed on R-forge, and will be headed to CRAN soon.

> install.packages("quantmod", repos="http://r-forge.r-project.org")
trying URL 'http://r-forge.r-project.org/src/contrib/quantmod_0.3-9.tar.gz'
Content type 'application/x-gzip' length 100132 bytes (97 Kb)
opened URL
==================================================
downloaded 97 Kb

* Installing *source* package 'quantmod' ...
** R
** demo
....
> library(quantmod)

> getSymbols("CSCO")
[1] "CSCO"
> head(Next(Cl(CSCO)))
            Next
2007-01-03 28.46
2007-01-04 28.47
2007-01-05 28.63
2007-01-08 28.47
2007-01-09 28.68
2007-01-10 28.69


Thanks,
Jeff

On Wed, May 13, 2009 at 12:50 PM, Steve Wisdom <swisdom at gmail.com> wrote:
> Not sure is this is a bug, or a feature, or just a brain-cramp on my end:
>
>> require(quantmod)
> Loading required package: quantmod
> Loading required package: xts
> Loading required package: zoo
>
> Attaching package: 'zoo'
>
> ? ? ? ?The following object(s) are masked from package:base :
>
> ? ? ? ? as.Date.numeric
>
> Loading required package: Defaults
> quantmod: Quantitative Financial Modelling Framework
>
> Version 0.3-7, Revision 461
> http://www.quantmod.com
>
>> getSymbols("CSCO")
> [1] "CSCO"
>> head(Lag(Cl(CSCO)))
> ? ? ? ? ? Lag.1
> 2007-01-03 ? ?NA
> 2007-01-04 27.73
> 2007-01-05 28.46
> 2007-01-08 28.47
> 2007-01-09 28.63
> 2007-01-10 28.47
>> head(Next(Cl(CSCO))) ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?<--------------
> Error in `[.xts`(x, -(0:k), ) :
> ?only zeros may be mixed with negative subscripts
>> R.version.string
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From etyurin at skipstonellc.com  Thu May 14 15:35:42 2009
From: etyurin at skipstonellc.com (Eugene Tyurin)
Date: Thu, 14 May 2009 09:35:42 -0400
Subject: [R-SIG-Finance] trying to plot coincident time series in quantmod...
Message-ID: <e4adf3900905140635o72c79757vd56c59421ac25a27@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090514/e2695330/attachment.pl>

From jeff.a.ryan at gmail.com  Thu May 14 16:24:08 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 14 May 2009 09:24:08 -0500
Subject: [R-SIG-Finance] trying to plot coincident time series in
	quantmod...
In-Reply-To: <e4adf3900905140635o72c79757vd56c59421ac25a27@mail.gmail.com>
References: <e4adf3900905140635o72c79757vd56c59421ac25a27@mail.gmail.com>
Message-ID: <e8e755250905140724t32079996pe77567c3467b38cc@mail.gmail.com>

Hi Eugene,

addPoints isn't very useful.  I intend on adding wrapper functions to
the underlying base graphics functions in R, that will work with
quantmod.  Until then 'addTA' will do almost everything you would want
for simple additions.

Both of these work:

addTA(dat_[,'5x16'],type='p',col='red',pch=25,on=1)
addTA(dat_[,'7x24'],type='p',col='red',pch=25,on=1)


Key points:

** Don't convert dat_ from xts.  The timestamps are important.
** the "on" argument is where to draw.  In this case "1" is the main
window you want.

HTH
Jeff

On Thu, May 14, 2009 at 8:35 AM, Eugene Tyurin <etyurin at skipstonellc.com> wrote:
> Hello!
>
> I am trying to plot more than one time series on one chart using quantmod's
> functions. However, my ability to parse the source code is lacking...
>
> For the life of me I can't figure out a way to use addPoints(). Simple
> points() function breaks down due to na.omit() in chartSeries().
>
> What is the "proper way" to do something like this? ?I attached a simple
> example below:
>
> ix<-as.Date('2009-05-01')+seq(0,6)
> dat_<-xts(cbind(sin(1:7),sin(1:7)),order.by=ix)
> colnames(dat_)<-c('7x24','5x16')
> dat_[2:3,'5x16']<-NA
>
> # This works fine:
> lineChart(dat_[,'7x24'])
> points(as.vector(dat_[,'5x16']),col='red',type='p',pch=25)
>
> # This does not:
> lineChart(dat_[,'5x16'])
> points(as.vector(dat_[,'7x24']),col='red',type='p',pch=25)
>
> Thanks!
> -- Eugene Tyurin.
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From etyurin at skipstonellc.com  Thu May 14 18:35:51 2009
From: etyurin at skipstonellc.com (Eugene Tyurin)
Date: Thu, 14 May 2009 12:35:51 -0400
Subject: [R-SIG-Finance] trying to plot coincident time series in
	quantmod...
In-Reply-To: <e8e755250905140724t32079996pe77567c3467b38cc@mail.gmail.com>
References: <e4adf3900905140635o72c79757vd56c59421ac25a27@mail.gmail.com>
	<e8e755250905140724t32079996pe77567c3467b38cc@mail.gmail.com>
Message-ID: <e4adf3900905140935p556cad9cp5a954f112a31e623@mail.gmail.com>

Jeff,

Thank you very much - addTA really does what I need. However, there's
a wrinkle - addTA somehow does not work inside a function!  Code
sample attached:

ix<-as.Date('2009-05-01')+seq(0:19)
x<-sin(1:20)
y<-cos(1:20)
dat_<-xts(cbind(x,y),order.by=ix)

# You see both lines:
lineChart(dat_$x)
addTA(dat_$y,on=1,col='red')

z <- function(a) {
	lineChart(a[,1],theme='white')
	addTA(a[,2],on=1,col='blue') # <--- this one will not be displayed
}

# You see only one line:
z(dat_)


On Thu, May 14, 2009 at 10:24 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
>
> Hi Eugene,
>
> addPoints isn't very useful. ?I intend on adding wrapper functions to
> the underlying base graphics functions in R, that will work with
> quantmod. ?Until then 'addTA' will do almost everything you would want
> for simple additions.
>
> Both of these work:
>
> addTA(dat_[,'5x16'],type='p',col='red',pch=25,on=1)
> addTA(dat_[,'7x24'],type='p',col='red',pch=25,on=1)
>
>
> Key points:
>
> ** Don't convert dat_ from xts. ?The timestamps are important.
> ** the "on" argument is where to draw. ?In this case "1" is the main
> window you want.
>
> HTH
> Jeff
>
> On Thu, May 14, 2009 at 8:35 AM, Eugene Tyurin <etyurin at skipstonellc.com> wrote:
> > Hello!
> >
> > I am trying to plot more than one time series on one chart using quantmod's
> > functions. However, my ability to parse the source code is lacking...
> >
> > For the life of me I can't figure out a way to use addPoints(). Simple
> > points() function breaks down due to na.omit() in chartSeries().
> >
> > What is the "proper way" to do something like this? ?I attached a simple
> > example below:
> >
> > ix<-as.Date('2009-05-01')+seq(0,6)
> > dat_<-xts(cbind(sin(1:7),sin(1:7)),order.by=ix)
> > colnames(dat_)<-c('7x24','5x16')
> > dat_[2:3,'5x16']<-NA
> >
> > # This works fine:
> > lineChart(dat_[,'7x24'])
> > points(as.vector(dat_[,'5x16']),col='red',type='p',pch=25)
> >
> > # This does not:
> > lineChart(dat_[,'5x16'])
> > points(as.vector(dat_[,'7x24']),col='red',type='p',pch=25)
> >
> > Thanks!
> > -- Eugene Tyurin.
> >
> > ? ? ? ?[[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>
>
>
> --
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com


From jeff.a.ryan at gmail.com  Thu May 14 18:49:46 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 14 May 2009 11:49:46 -0500
Subject: [R-SIG-Finance] trying to plot coincident time series in
	quantmod...
In-Reply-To: <e4adf3900905140935p556cad9cp5a954f112a31e623@mail.gmail.com>
References: <e4adf3900905140635o72c79757vd56c59421ac25a27@mail.gmail.com>
	<e8e755250905140724t32079996pe77567c3467b38cc@mail.gmail.com>
	<e4adf3900905140935p556cad9cp5a954f112a31e623@mail.gmail.com>
Message-ID: <e8e755250905140949j4f3ffa92t729811c02e069d97@mail.gmail.com>

Try wrapping the addTA call in plot()

 z <- function(a) {
        lineChart(a[,1],theme='white')
        plot(addTA(a[,2],on=1,col='blue'))
 }

HTH
Jeff

On Thu, May 14, 2009 at 11:35 AM, Eugene Tyurin
<etyurin at skipstonellc.com> wrote:
> Jeff,
>
> Thank you very much - addTA really does what I need. However, there's
> a wrinkle - addTA somehow does not work inside a function! ?Code
> sample attached:
>
> ix<-as.Date('2009-05-01')+seq(0:19)
> x<-sin(1:20)
> y<-cos(1:20)
> dat_<-xts(cbind(x,y),order.by=ix)
>
> # You see both lines:
> lineChart(dat_$x)
> addTA(dat_$y,on=1,col='red')
>
> z <- function(a) {
> ? ? ? ?lineChart(a[,1],theme='white')
> ? ? ? ?addTA(a[,2],on=1,col='blue') # <--- this one will not be displayed
> }
>
> # You see only one line:
> z(dat_)
>
>
> On Thu, May 14, 2009 at 10:24 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
>>
>> Hi Eugene,
>>
>> addPoints isn't very useful. ?I intend on adding wrapper functions to
>> the underlying base graphics functions in R, that will work with
>> quantmod. ?Until then 'addTA' will do almost everything you would want
>> for simple additions.
>>
>> Both of these work:
>>
>> addTA(dat_[,'5x16'],type='p',col='red',pch=25,on=1)
>> addTA(dat_[,'7x24'],type='p',col='red',pch=25,on=1)
>>
>>
>> Key points:
>>
>> ** Don't convert dat_ from xts. ?The timestamps are important.
>> ** the "on" argument is where to draw. ?In this case "1" is the main
>> window you want.
>>
>> HTH
>> Jeff
>>
>> On Thu, May 14, 2009 at 8:35 AM, Eugene Tyurin <etyurin at skipstonellc.com> wrote:
>> > Hello!
>> >
>> > I am trying to plot more than one time series on one chart using quantmod's
>> > functions. However, my ability to parse the source code is lacking...
>> >
>> > For the life of me I can't figure out a way to use addPoints(). Simple
>> > points() function breaks down due to na.omit() in chartSeries().
>> >
>> > What is the "proper way" to do something like this? ?I attached a simple
>> > example below:
>> >
>> > ix<-as.Date('2009-05-01')+seq(0,6)
>> > dat_<-xts(cbind(sin(1:7),sin(1:7)),order.by=ix)
>> > colnames(dat_)<-c('7x24','5x16')
>> > dat_[2:3,'5x16']<-NA
>> >
>> > # This works fine:
>> > lineChart(dat_[,'7x24'])
>> > points(as.vector(dat_[,'5x16']),col='red',type='p',pch=25)
>> >
>> > # This does not:
>> > lineChart(dat_[,'5x16'])
>> > points(as.vector(dat_[,'7x24']),col='red',type='p',pch=25)
>> >
>> > Thanks!
>> > -- Eugene Tyurin.
>> >
>> > ? ? ? ?[[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> > -- Subscriber-posting only.
>> > -- If you want to post, subscribe first.
>> >
>>
>>
>>
>> --
>> Jeffrey Ryan
>> jeffrey.ryan at insightalgo.com
>>
>> ia: insight algorithmics
>> www.insightalgo.com
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From cevans at chyden.net  Thu May 14 19:49:49 2009
From: cevans at chyden.net (Charles Evans)
Date: Thu, 14 May 2009 13:49:49 -0400
Subject: [R-SIG-Finance] ca.jo help
Message-ID: <1623B880-E31D-420B-B576-DA28490EE33E@chyden.net>

This is in regard to the ca.jo() function in the urca package in R.

I am a PhD candidate in Finance at Florida Atlantic University in Boca  
Raton, FL, USA.  As part of my research I must perform cointegration  
tests on the price and NAV time series of 51 ETFs.  I am able to run  
Augmented Dickey-Fuller and Phillips-Perron tests in R.  However, I am  
having a devil of a time getting ca.jo() to work.  If anyone can  
provide guidance, I would be most appreciative.

I suspect that the problem is related to how I have my data  
organized.  I have one set of price data (called pt) for 51 ETFs with  
one ETF daily price series per column and T+1 rows; the first row  
contains headers.  The first column contains dates.  I have another  
set of NAV data (called nt) organized in the same way.  The ETFs have  
different incept dates, and the tables contain NA values.

I want to test for price/NAV cointegration using the Johansen  
procedures ? both eignevalue and trace test ? for each of the 51  
pairs.  When I create a time series for, e.g., price[,49] and NAV[,49]  
using the cbind() function I get:



 > johansen_pn <- ts(cbind(pt[,49],nt[,49]))
 > ca.jo(johansen_pn, type = c("trace"), ecdet = c("const"), K = 2,  
spec=c("longrun"), season = NULL, dumvar = NULL)

Error in dimnames(x) <- dn :
length of 'dimnames' [2] not equal to array extent



I have tried n different ways to construct the "x" (here, johansen_pn)  
variable, and I get the same dimnames error every time.

Can anyone suggest a remedy or a cookbook explanation of how to use  
ca.jo() properly?

Charles Evans
cevans at chyden.net


From spencer.graves at prodsyse.com  Fri May 15 01:19:45 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Thu, 14 May 2009 16:19:45 -0700
Subject: [R-SIG-Finance] TSLS: R^2 extraction and autocorrelation and
 heterokedasticity tests
In-Reply-To: <865aedfa0905120543n6ac61ddei714230c48e42503a@mail.gmail.com>
References: <865aedfa0905120543n6ac61ddei714230c48e42503a@mail.gmail.com>
Message-ID: <4A0CA711.6070708@prodsyse.com>

  What code did you use to produce "output"?


Have you tried "str(output)" and "names(output)"?


If this is not enough to help you answer your question, you might try 
rephrasing your question using commented, minimal, self-contained, 
reproducible code, as suggested in the posting guide 
"http://www.R-project.org/posting-guide.html".


Spencer Graves


jos? maria Rodriguez wrote:
> Hi,
>
> I'm actually I?m performing a TSLS linear multiple regression on annually
> data which go from 1971 to 1997. After performing the TSLS regression, I
> tried to extract the R squared value using ?output$r.squared? function and
> to perform autocorrelation (Durbin Watson and Breush Godfrey) and
> heterokedasticity tests (Breush-pagan and Goldfeld Quandt)  but I have
> errors messages. More specifically, this is function that I write to R and
> below its response :
> for R^2 :
>   
>> output$r.squared
>>     
> NULL
> for heterokedasticity tests :
>   
>> bptest(reg1)
>>     
> Error in terms.default(formula) : no terms component
> and for autocorrelation test, when I try :
> durbin.watson(reg1$residuals, max.lag=10)
>  [1] 1.509 2.520 2.247 2.001 1.743 1.092 1.392 1.439 1.468 1.035
> this give me only the durbin watson value and not the probabilities
> (p-value)
> When performing these tests on lm object I have no problem. So my question
> is how to extract R^2 from a tsls regression (object) and how to perform
> autocorrelation and heterokedasticity tests on tsls regression. I looked at
> the sem package but I found no answer to my questions. So please is there
> any person who can help me.
>
> Think you in advance
>
> 	[[alternative HTML version deleted]]
>
>   
> ------------------------------------------------------------------------
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From spencer.graves at prodsyse.com  Fri May 15 05:50:00 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Thu, 14 May 2009 20:50:00 -0700
Subject: [R-SIG-Finance] TSLS: R^2 extraction and autocorrelation and
 heterokedasticity tests
In-Reply-To: <Pine.LNX.4.64.0905150209090.29766@paninaro.stat-math.wu-wien.ac.at>
References: <865aedfa0905120543n6ac61ddei714230c48e42503a@mail.gmail.com>
	<4A0CA711.6070708@prodsyse.com>
	<Pine.LNX.4.64.0905150209090.29766@paninaro.stat-math.wu-wien.ac.at>
Message-ID: <4A0CE668.1000006@prodsyse.com>

Dear Achim:  Thanks for the clarification.  sg

Achim Zeileis wrote:
> On Thu, 14 May 2009, spencerg wrote:
>
>> What code did you use to produce "output"?
>
> He probably has used tsls() from "sem". ivreg() in "AER" also provides
> two-stage least squares but along with a few more methods, such as 
> terms(), model.matrix() etc.
>
> This will be sufficient to call bptest() etc. from "lmtest" without 
> error. However, this might be misleading. Internally, many "lmtest" 
> functions re-fit the linear model and it depends on the test function 
> and how the ivreg model was fitted whether what happens. Some 
> functions internally re-fit the right model, others might re-fit the 
> usual OLS model. I have never thought about using the "lmtest" tests 
> with "ivreg" objects, I'll try to incorporate that in future updates.
>
> In the meantime, I recommend that you re-fit the second stage of the 
> 2SLS "by hand" and then call the lmtest functionality. That way you 
> can be sure that the second stage model is really used. In pseudo-code:
>   fm_2sls <- ivreg(y ~ x1 + x2 | z1 + z2 + z3, data = mydata)
>   fm_aux <- lm(model.response(model.frame(fm_2sls)) ~
>     model.matrix(fm_2sls)[,-1])
>   bptest(fm_aux)
>
> etc.
> Z
>
>>
>> Have you tried "str(output)" and "names(output)"?
>>
>>
>> If this is not enough to help you answer your question, you might try 
>> rephrasing your question using commented, minimal, self-contained, 
>> reproducible code, as suggested in the posting guide 
>> "http://www.R-project.org/posting-guide.html".
>>
>>
>> Spencer Graves
>>
>>
>> jos? maria Rodriguez wrote:
>>> Hi,
>>>
>>> I'm actually I?m performing a TSLS linear multiple regression on 
>>> annually
>>> data which go from 1971 to 1997. After performing the TSLS 
>>> regression, I
>>> tried to extract the R squared value using ?output$r.squared? 
>>> function and
>>> to perform autocorrelation (Durbin Watson and Breush Godfrey) and
>>> heterokedasticity tests (Breush-pagan and Goldfeld Quandt)  but I have
>>> errors messages. More specifically, this is function that I write to 
>>> R and
>>> below its response :
>>> for R^2 :
>>>
>>>> output$r.squared
>>>>
>>> NULL
>>> for heterokedasticity tests :
>>>
>>>> bptest(reg1)
>>>>
>>> Error in terms.default(formula) : no terms component
>>> and for autocorrelation test, when I try :
>>> durbin.watson(reg1$residuals, max.lag=10)
>>>  [1] 1.509 2.520 2.247 2.001 1.743 1.092 1.392 1.439 1.468 1.035
>>> this give me only the durbin watson value and not the probabilities
>>> (p-value)
>>> When performing these tests on lm object I have no problem. So my 
>>> question
>>> is how to extract R^2 from a tsls regression (object) and how to 
>>> perform
>>> autocorrelation and heterokedasticity tests on tsls regression. I 
>>> looked at
>>> the sem package but I found no answer to my questions. So please is 
>>> there
>>> any person who can help me.
>>>
>>> Think you in advance
>>>
>>>     [[alternative HTML version deleted]]
>>>
>>>   
>>> ------------------------------------------------------------------------ 
>>>
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>


From Heiko-Mayer at gmx.de  Fri May 15 08:25:47 2009
From: Heiko-Mayer at gmx.de (Heiko Mayer)
Date: Fri, 15 May 2009 08:25:47 +0200
Subject: [R-SIG-Finance] BLCOP / Idzorek working paper
Message-ID: <20090515062547.35460@gmx.net>

Hi all,

I have read the working paper from Idzorek regarding Black Litterman as shown below. As the examples of the BLCOP manuals are based on that paper, I have expected equal results. Posterior looks good (beside bonds) , but the optimized portfolio weights (Idzorek: page 17, table 6) are completly different. No bonds are allocated. Beside long only, I cannot see any constraints, therefore I assume "solve.QP" used in "optimalPortfolios" should work. Any ideas?
As markets tend to overreact significantly sometimes, it might not the best idea using market cap. Would it be a good idea to start with risk (sigma) adjusted weights and re-optimize these to get the prior returns?

library(BLCOP)
## example from Thomas M. Idzorek's paper "A STEP-BY-STEP GUIDE TO THE BLACK-LITTERMAN MODEL"
# http://corporate.morningstar.com/ib/documents/MethodologyDocuments/IBBAssociates/BlackLitterman.pdf
x <- c(0.001005,0.001328,-0.000579,-0.000675,0.000121,0.000128,-0.000445,-0.000437 ,
0.001328,0.007277,-0.001307,-0.000610,-0.002237,-0.000989,0.001442,-0.001535 ,
-0.000579,-0.001307,0.059852,0.027588,0.063497,0.023036,0.032967,0.048039 ,
-0.000675,-0.000610,0.027588,0.029609,0.026572,0.021465,0.020697,0.029854 ,
0.000121,-0.002237,0.063497,0.026572,0.102488,0.042744,0.039943,0.065994 ,
0.000128,-0.000989,0.023036,0.021465,0.042744,0.032056,0.019881,0.032235 ,
-0.000445,0.001442,0.032967,0.020697,0.039943,0.019881,0.028355,0.035064 ,
-0.000437,-0.001535,0.048039,0.029854,0.065994,0.032235,0.035064,0.079958 )
varCov <- matrix(x, ncol = 8, nrow = 8)
mu <- c(0.08, 0.67,6.41, 4.08, 7.43, 3.70, 4.80, 6.60) / 100
pick <- matrix(0, ncol = 8, nrow = 3, dimnames = list(NULL, letters[1:8]))
pick[1,7] <- 1
pick[2,1] <- -1; pick[2,2] <- 1
pick[3, 3:6] <- c(0.9, -0.9, .1, -.1)
confidences <- 1 / c(0.000709, 0.000141, 0.000866) # Replaced wrong value "0.00709". Zero was missing.
myViews <- BLViews(pick, c(0.0525, 0.0025, 0.02), confidences, letters[1:8])
myPosterior <- posteriorEst(myViews, tau = 0.025, mu, varCov )
optimalPortfolios(myPosterior)

Thanks,
Heiko
--


From fgochez at mango-solutions.com  Fri May 15 15:55:16 2009
From: fgochez at mango-solutions.com (Francisco Gochez)
Date: Fri, 15 May 2009 14:55:16 +0100
Subject: [R-SIG-Finance] BLCOP / Idzorek working paper
In-Reply-To: <20090515062547.35460@gmx.net>
References: <20090515062547.35460@gmx.net>
Message-ID: <3CBFCFB1FEFFA841BA83ADF2F2A9C6FA35A6AE@mango-data1.Mango.local>

 
Hi Heiko,

Thanks for bringing this to my attention.  I will look into the matter
over the weekend to see what might be wrong.  In reality,
"optimalPortfolios" is more or less a "toy" to experiment with, and not
meant to be used as a serious tool.  The next version of the package,
which will be out soon, will interface with some of Rmetric's portfolio
optimization tools.  I am in the process of discussing how best to do
this with Prof. Diethelm Wuertz at the moment.

Kind regards,

Francisco

mango solutions

S & R Consulting and Training

+44 (0)1249 767 700


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Heiko
Mayer
Sent: 15 May 2009 07:26
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] BLCOP / Idzorek working paper

Hi all,

I have read the working paper from Idzorek regarding Black Litterman as
shown below. As the examples of the BLCOP manuals are based on that
paper, I have expected equal results. Posterior looks good (beside
bonds) , but the optimized portfolio weights (Idzorek: page 17, table 6)
are completly different. No bonds are allocated. Beside long only, I
cannot see any constraints, therefore I assume "solve.QP" used in
"optimalPortfolios" should work. Any ideas?
As markets tend to overreact significantly sometimes, it might not the
best idea using market cap. Would it be a good idea to start with risk
(sigma) adjusted weights and re-optimize these to get the prior returns?

library(BLCOP)
## example from Thomas M. Idzorek's paper "A STEP-BY-STEP GUIDE TO THE
BLACK-LITTERMAN MODEL"
#
http://corporate.morningstar.com/ib/documents/MethodologyDocuments/IBBAs
sociates/BlackLitterman.pdf
x <-
c(0.001005,0.001328,-0.000579,-0.000675,0.000121,0.000128,-0.000445,-0.0
00437 ,
0.001328,0.007277,-0.001307,-0.000610,-0.002237,-0.000989,0.001442,-0.00
1535 ,
-0.000579,-0.001307,0.059852,0.027588,0.063497,0.023036,0.032967,0.04803
9 ,
-0.000675,-0.000610,0.027588,0.029609,0.026572,0.021465,0.020697,0.02985
4 ,
0.000121,-0.002237,0.063497,0.026572,0.102488,0.042744,0.039943,0.065994
,
0.000128,-0.000989,0.023036,0.021465,0.042744,0.032056,0.019881,0.032235
,
-0.000445,0.001442,0.032967,0.020697,0.039943,0.019881,0.028355,0.035064
,
-0.000437,-0.001535,0.048039,0.029854,0.065994,0.032235,0.035064,0.07995
8 ) varCov <- matrix(x, ncol = 8, nrow = 8) mu <- c(0.08, 0.67,6.41,
4.08, 7.43, 3.70, 4.80, 6.60) / 100 pick <- matrix(0, ncol = 8, nrow =
3, dimnames = list(NULL, letters[1:8])) pick[1,7] <- 1 pick[2,1] <- -1;
pick[2,2] <- 1 pick[3, 3:6] <- c(0.9, -0.9, .1, -.1) confidences <- 1 /
c(0.000709, 0.000141, 0.000866) # Replaced wrong value "0.00709". Zero
was missing.
myViews <- BLViews(pick, c(0.0525, 0.0025, 0.02), confidences,
letters[1:8]) myPosterior <- posteriorEst(myViews, tau = 0.025, mu,
varCov )
optimalPortfolios(myPosterior)

Thanks,
Heiko
--

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From nands31 at gmail.com  Fri May 15 17:09:59 2009
From: nands31 at gmail.com (Subhrangshu Nandi)
Date: Fri, 15 May 2009 10:09:59 -0500
Subject: [R-SIG-Finance] Convert Daily PnL to Returns
Message-ID: <de69a2b90905150809p3b6a272drcd24a8a620aebc77@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090515/d7f0b5d0/attachment.pl>

From brian at braverock.com  Fri May 15 19:03:20 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 15 May 2009 12:03:20 -0500
Subject: [R-SIG-Finance] Convert Daily PnL to Returns
In-Reply-To: <de69a2b90905150809p3b6a272drcd24a8a620aebc77@mail.gmail.com>
References: <de69a2b90905150809p3b6a272drcd24a8a620aebc77@mail.gmail.com>
Message-ID: <4A0DA058.2090809@braverock.com>

Convert them to a wealth index. Add 1 to your first PnL number, and 
cumsum. This creates a fake "price series" that can then be turned into 
a return series for all the other analysis you need to do.

if 'PnL' is your data variable:

cumsum(PnL+1)

Regards,

- Brian

Subhrangshu Nandi wrote:
> I'm using R to optimize our portfolio. However, I do not have a return
> series, as required in most of the portfolio optimization packages. I have
> daily PnLs (regular profit/loss numbers) of several products. In order to be
> able to use PerformanceAnalytics or RMetrics, how should I about converting
> them to returns? Also, my objective of optimizing the portfolio is to
> maximize the return, constrained on some risk parameter like VAR/CVAR/etc
> and then decide how much to trade each product.
>
> Any headstart in solving this problem will be helpful.
>
> Thanks a lot,
> -Nandi
>
>   


-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From brian at braverock.com  Fri May 15 19:12:38 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 15 May 2009 12:12:38 -0500
Subject: [R-SIG-Finance] Convert Daily PnL to Returns
In-Reply-To: <4A0DA058.2090809@braverock.com>
References: <de69a2b90905150809p3b6a272drcd24a8a620aebc77@mail.gmail.com>
	<4A0DA058.2090809@braverock.com>
Message-ID: <4A0DA286.8020902@braverock.com>

Oops, my code line is wrong. Sorry.

I wasn't thinking clearly about what you said. You need to add the 
capital number to the series.

if you have $1000, and your PnL series is in $, then you would do

cumsum(PnL)+1000

to create your wealth index from which you can calculate returns.

if the capital changes, you have to take into account the addition or 
withdrawal from the capital account in your series.

Sorry for the confusion,

- Brian

Brian G. Peterson wrote:
> Convert them to a wealth index. Add 1 to your first PnL number, and 
> cumsum. This creates a fake "price series" that can then be turned 
> into a return series for all the other analysis you need to do.
>
> if 'PnL' is your data variable:
>
> cumsum(PnL+1)
>
> Regards,
>
> - Brian
>
> Subhrangshu Nandi wrote:
>> I'm using R to optimize our portfolio. However, I do not have a return
>> series, as required in most of the portfolio optimization packages. I 
>> have
>> daily PnLs (regular profit/loss numbers) of several products. In 
>> order to be
>> able to use PerformanceAnalytics or RMetrics, how should I about 
>> converting
>> them to returns? Also, my objective of optimizing the portfolio is to
>> maximize the return, constrained on some risk parameter like 
>> VAR/CVAR/etc
>> and then decide how much to trade each product.
>>
>> Any headstart in solving this problem will be helpful.
>>
>> Thanks a lot,
>> -Nandi
>>
>
>


-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From nands31 at gmail.com  Fri May 15 21:22:01 2009
From: nands31 at gmail.com (Subhrangshu Nandi)
Date: Fri, 15 May 2009 14:22:01 -0500
Subject: [R-SIG-Finance] Non-parametric tests in R
Message-ID: <de69a2b90905151222x11a518dcp6ab18ae092184cb3@mail.gmail.com>

Lets say I have a population of 500 datapoints, which is clearly not normal.
(Please see attached text file for an example population). These datapoints
can be assumed to be IID. Now, I get 30 more datapoints and want to test if
these have been drawn (with replacement) from the population. I'm doing the
followin:

1. Since the population is non-normal, I'm unable to use regular t-tests. I
am trying to use Wilcoxon's rank sum test in R. I'm not sure if it only
compares the new sample with the median of the population or does it also
compare the central tendency of the dataset around the median.

2. I'm bootstrapping 5000 samples (resampling), of size 30 from the
population and recording their means and standard deviations and trying to
infer if my sample mean is within acceptable range. I'm using the concept of
Law of Large Numbers, however, I'm not sure if this is an acceptable
methodology.

Any thoughts on this will be great.
-Nandi

-- 
I'm a great believer in luck, and I find the harder I work the more I have
of it.  ~Thomas Jefferson

Subhrangshu Nandi
High Frequency Trading
Greater Chicago Area
Office:(312) 601-8096
EFax: (703) 852-7405
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090515/f1ca4d4c/attachment.html>
-------------- next part --------------
Date	DailyNetProfit
1/2/2007	-51
1/3/2007	-221
1/4/2007	325
1/5/2007	433
1/8/2007	397
1/9/2007	750
1/10/2007	152
1/11/2007	519
1/12/2007	-272
1/16/2007	296
1/17/2007	412
1/18/2007	-45
1/19/2007	-59
1/22/2007	315
1/23/2007	174
1/24/2007	233
1/25/2007	214
1/26/2007	143
1/29/2007	252
1/30/2007	316
1/31/2007	356
2/1/2007	151
2/2/2007	148
2/5/2007	180
2/6/2007	179
2/7/2007	50
2/8/2007	262
2/9/2007	-882
2/12/2007	-252
2/13/2007	81
2/14/2007	-446
2/15/2007	151
2/16/2007	32
2/20/2007	181
2/21/2007	254
2/22/2007	329
2/23/2007	266
2/26/2007	119
2/27/2007	28
2/28/2007	470
3/1/2007	375
3/2/2007	393
3/5/2007	126
3/6/2007	441
3/7/2007	527
3/8/2007	-1187
3/9/2007	369
3/12/2007	-334
3/13/2007	235
3/14/2007	-1022
3/15/2007	238
3/16/2007	-77
3/19/2007	23
3/20/2007	40
3/21/2007	32
3/22/2007	-1147
3/23/2007	389
3/26/2007	-718
3/27/2007	222
3/28/2007	-68
3/29/2007	433
3/30/2007	-144
4/2/2007	235
4/3/2007	438
4/4/2007	342
4/5/2007	-203
4/10/2007	212
4/11/2007	48
4/12/2007	496
4/13/2007	144
4/16/2007	-226
4/17/2007	59
4/18/2007	104
4/19/2007	459
4/20/2007	460
4/23/2007	134
4/24/2007	33
4/25/2007	149
4/26/2007	101
4/27/2007	451
4/30/2007	137
5/2/2007	114
5/3/2007	-41
5/4/2007	-114
5/7/2007	166
5/8/2007	-43
5/9/2007	-96
5/10/2007	434
5/11/2007	8
5/14/2007	137
5/15/2007	319
5/16/2007	349
5/17/2007	294
5/18/2007	207
5/21/2007	-197
5/22/2007	194
5/23/2007	158
5/24/2007	-37
5/25/2007	406
5/29/2007	216
5/30/2007	179
5/31/2007	24
6/1/2007	294
6/4/2007	163
6/5/2007	199
6/6/2007	821
6/7/2007	464
6/8/2007	36
6/11/2007	354
6/12/2007	201
6/13/2007	598
6/14/2007	243
6/15/2007	0
6/18/2007	231
6/19/2007	235
6/20/2007	246
6/21/2007	528
6/22/2007	-44
6/25/2007	351
6/26/2007	75
6/27/2007	507
6/28/2007	100
6/29/2007	235
7/2/2007	-199
7/3/2007	125
7/5/2007	225
7/6/2007	215
7/9/2007	-11
7/10/2007	567
7/11/2007	4
7/12/2007	101
7/13/2007	120
7/16/2007	207
7/17/2007	378
7/18/2007	102
7/19/2007	442
7/20/2007	-253
7/23/2007	309
7/24/2007	323
7/25/2007	190
7/26/2007	334
7/27/2007	207
7/30/2007	150
7/31/2007	-264
8/1/2007	-43
8/2/2007	49
8/3/2007	-76
8/6/2007	22
8/7/2007	468
8/8/2007	353
8/9/2007	347
8/10/2007	138
8/13/2007	278
8/14/2007	462
8/15/2007	378
8/16/2007	126
8/17/2007	877
8/20/2007	-300
8/21/2007	433
8/22/2007	294
8/23/2007	204
8/24/2007	-261
8/27/2007	65
8/28/2007	-1066
8/29/2007	494
8/30/2007	-22
8/31/2007	239
9/4/2007	-253
9/5/2007	347
9/6/2007	453
9/7/2007	71
9/10/2007	-396
9/11/2007	516
9/12/2007	300
9/13/2007	-13
9/14/2007	425
9/17/2007	291
9/18/2007	-2539
9/19/2007	204
9/20/2007	-911
9/21/2007	1
9/24/2007	260
9/25/2007	652
9/26/2007	-36
9/27/2007	315
9/28/2007	107
10/1/2007	381
10/2/2007	-265
10/3/2007	-240
10/4/2007	189
10/5/2007	2
10/8/2007	142
10/9/2007	52
10/10/2007	90
10/11/2007	268
10/12/2007	-24
10/15/2007	46
10/16/2007	121
10/17/2007	-80
10/18/2007	292
10/19/2007	-661
10/22/2007	63
10/23/2007	-269
10/24/2007	-937
10/25/2007	-91
10/26/2007	163
10/29/2007	-46
10/30/2007	174
10/31/2007	257
11/1/2007	-167
11/2/2007	78
11/5/2007	346
11/6/2007	324
11/7/2007	-1420
11/8/2007	-595
11/9/2007	-392
11/12/2007	-494
11/13/2007	-616
11/14/2007	-170
11/15/2007	-908
11/16/2007	84
11/19/2007	72
11/20/2007	-322
11/21/2007	370
11/23/2007	381
11/26/2007	591
11/27/2007	-119
11/28/2007	70
11/29/2007	245
11/30/2007	-673
12/3/2007	289
12/4/2007	556
12/5/2007	-563
12/6/2007	300
12/7/2007	275
12/10/2007	369
12/11/2007	-154
12/12/2007	976
12/13/2007	575
12/14/2007	463
12/17/2007	335
12/18/2007	202
12/19/2007	498
12/20/2007	-162
12/21/2007	-428
12/24/2007	0
12/27/2007	584
12/28/2007	-26
12/31/2007	0
1/2/2008	-66
1/3/2008	312
1/4/2008	-480
1/7/2008	91
1/8/2008	813
1/9/2008	709
1/10/2008	427
1/11/2008	149
1/14/2008	339
1/15/2008	238
1/16/2008	632
1/17/2008	831
1/18/2008	929
1/22/2008	1320
1/23/2008	1636
1/24/2008	440
1/25/2008	507
1/28/2008	389
1/29/2008	519
1/30/2008	354
1/31/2008	413
2/1/2008	820
2/4/2008	613
2/5/2008	644
2/6/2008	448
2/7/2008	96
2/8/2008	417
2/11/2008	-528
2/12/2008	703
2/13/2008	493
2/14/2008	408
2/15/2008	-678
2/18/2008	169
2/19/2008	-182
2/20/2008	-521
2/21/2008	-16
2/22/2008	260
2/25/2008	614
2/26/2008	537
2/27/2008	893
2/28/2008	-217
2/29/2008	191
3/3/2008	104
3/4/2008	-367
3/5/2008	394
3/6/2008	-155
3/7/2008	500
3/10/2008	702
3/11/2008	183
3/12/2008	459
3/13/2008	-295
3/14/2008	1141
3/17/2008	-378
3/18/2008	368
3/19/2008	246
3/20/2008	100
3/25/2008	-28
3/26/2008	-70
3/27/2008	-241
3/28/2008	189
3/31/2008	180
4/1/2008	477
4/2/2008	104
4/3/2008	-92
4/4/2008	-35
4/7/2008	339
4/8/2008	317
4/9/2008	-50
4/10/2008	296
4/11/2008	117
4/14/2008	139
4/15/2008	-1
4/16/2008	246
4/17/2008	497
4/18/2008	-45
4/21/2008	362
4/22/2008	22
4/23/2008	150
4/24/2008	-120
4/25/2008	173
4/28/2008	341
4/29/2008	172
4/30/2008	-484
5/2/2008	165
5/5/2008	388
5/6/2008	-330
5/7/2008	2
5/8/2008	398
5/9/2008	400
5/12/2008	98
5/13/2008	-99
5/14/2008	387
5/15/2008	116
5/16/2008	-384
5/19/2008	251
5/20/2008	-3479
5/21/2008	508
5/22/2008	411
5/23/2008	389
5/26/2008	204
5/27/2008	-908
5/28/2008	364
5/29/2008	62
5/30/2008	92
6/2/2008	-511
6/3/2008	300
6/4/2008	-80
6/5/2008	-188
6/6/2008	711
6/9/2008	49
6/10/2008	-1335
6/11/2008	252
6/12/2008	196
6/13/2008	128
6/16/2008	434
6/17/2008	59
6/18/2008	201
6/19/2008	203
6/20/2008	303
6/23/2008	152
6/24/2008	380
6/25/2008	-122
6/26/2008	656
6/27/2008	551
6/30/2008	113
7/1/2008	744
7/2/2008	386
7/3/2008	494
7/4/2008	258
7/7/2008	509
7/8/2008	330
7/9/2008	-47
7/10/2008	983
7/11/2008	490
7/14/2008	701
7/15/2008	366
7/16/2008	321
7/17/2008	313
7/18/2008	329
7/21/2008	82
7/22/2008	381
7/23/2008	291
7/24/2008	133
7/25/2008	571
7/28/2008	-140
7/29/2008	255
7/30/2008	321
7/31/2008	-3
8/1/2008	641
8/4/2008	450
8/5/2008	191
8/6/2008	328
8/7/2008	62
8/8/2008	19
8/11/2008	86
8/12/2008	397
8/13/2008	178
8/14/2008	-76
8/15/2008	126
8/18/2008	368
8/19/2008	388
8/20/2008	156
8/21/2008	91
8/22/2008	415
8/25/2008	-544
8/26/2008	-785
8/27/2008	-19
8/28/2008	120
8/29/2008	-453
9/1/2008	221
9/2/2008	378
9/3/2008	185
9/4/2008	260
9/5/2008	538
9/8/2008	173
9/9/2008	126
9/10/2008	488
9/11/2008	101
9/12/2008	353
9/15/2008	757
9/16/2008	1542
9/17/2008	445
9/18/2008	-648
9/19/2008	753
9/22/2008	448
9/23/2008	594
9/24/2008	565
9/25/2008	129
9/26/2008	406
9/29/2008	1643
9/30/2008	-2413
10/1/2008	491
10/2/2008	120
10/3/2008	1032
10/6/2008	-465
10/7/2008	759
10/8/2008	2160
10/9/2008	1559
10/10/2008	-7
10/13/2008	1841
10/14/2008	753
10/15/2008	1017
10/16/2008	432
10/17/2008	-335
10/20/2008	354
10/21/2008	-272
10/22/2008	-166
10/23/2008	-189
10/24/2008	-3117
10/27/2008	2233
10/28/2008	-486
10/29/2008	1123
10/30/2008	268
10/31/2008	-243
11/3/2008	152
11/4/2008	-422
11/5/2008	0
11/6/2008	-726
11/7/2008	764
11/10/2008	254
11/11/2008	-991
11/12/2008	-2547
11/13/2008	1120
11/14/2008	1078
11/17/2008	387
11/18/2008	773
11/19/2008	166
11/20/2008	683
11/21/2008	1013
11/24/2008	-805
11/25/2008	699
11/26/2008	244
11/27/2008	421
11/28/2008	833
12/1/2008	-1611
12/2/2008	976
12/3/2008	-645
12/4/2008	857
12/5/2008	510
12/8/2008	864
12/9/2008	399
12/10/2008	585
12/11/2008	416
12/12/2008	781
12/15/2008	965
12/16/2008	857
12/18/2008	-210
12/19/2008	939
12/22/2008	184
12/23/2008	-354
12/24/2008	0
12/29/2008	-932
12/30/2008	-1
12/31/2008	0
1/2/2009	426
1/5/2009	550
1/6/2009	913
1/7/2009	639
1/8/2009	92
1/9/2009	-30
1/12/2009	-256
1/13/2009	149
1/14/2009	199
1/15/2009	307
1/16/2009	-445
1/19/2009	421
1/20/2009	606
1/21/2009	-375
1/22/2009	-142
1/23/2009	107
1/26/2009	925
1/27/2009	426
1/28/2009	210
1/29/2009	401
1/30/2009	476
2/2/2009	491
2/3/2009	356
2/4/2009	233
2/5/2009	-723
2/6/2009	-969
2/9/2009	-76
2/10/2009	-7
2/11/2009	568
2/12/2009	424
2/13/2009	393
2/16/2009	164
2/17/2009	72
2/18/2009	517
2/19/2009	465
2/20/2009	482
2/23/2009	-798
2/24/2009	227
2/25/2009	227
2/26/2009	545
2/27/2009	483
3/2/2009	106
3/3/2009	14
3/4/2009	187
3/5/2009	-70
3/6/2009	98
3/9/2009	15
3/10/2009	-218
3/11/2009	466
3/12/2009	-108
3/13/2009	545
3/16/2009	45
3/17/2009	382
3/18/2009	412
3/19/2009	125
3/20/2009	400
3/23/2009	68
3/24/2009	499
3/25/2009	-329
3/26/2009	437
3/27/2009	-349
3/30/2009	209
3/31/2009	289
4/1/2009	271
4/2/2009	-66
4/3/2009	-737
4/6/2009	222
4/7/2009	352
4/8/2009	-12
4/14/2009	59
4/15/2009	221
4/16/2009	372
4/17/2009	436
4/20/2009	-53
4/22/2009	-221
4/24/2009	-141
4/27/2009	95
4/28/2009	-9
4/29/2009	20

From just.tawfiq at gmail.com  Sat May 16 08:07:04 2009
From: just.tawfiq at gmail.com (tawfiq just)
Date: Sat, 16 May 2009 01:07:04 -0500
Subject: [R-SIG-Finance] extract parameters from fitCopula outputs !!
Message-ID: <eb0669d70905152307g5090b2deibd442bd044975af1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090516/4ef5337c/attachment.pl>

From spencer.graves at prodsyse.com  Sun May 17 03:00:18 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Sat, 16 May 2009 18:00:18 -0700
Subject: [R-SIG-Finance] ca.jo help
In-Reply-To: <1623B880-E31D-420B-B576-DA28490EE33E@chyden.net>
References: <1623B880-E31D-420B-B576-DA28490EE33E@chyden.net>
Message-ID: <4A0F61A2.4090007@prodsyse.com>

      1.  Your sample code is not self contained, which means that I can 
not replicate what you did. 


      2.  Have you looked at Pfaff (2008) Analysis of Integrated and 
Cointegrated Time Series with R, 2nd ed. (Springer)?  I don't know if 
Pfaff discusses "ca.jo" in this book, but it might be worth reviewing -- 
for more than just this. 


      3.  Have you asked Pfaff directly?  He is listed as the maintainer 
of that package, and maintainers are often available to answer questions 
like this. 


      4.  What have you done to compare your example with the examples 
on the "ca.jo" help page?  By comparing "str(johansen_pn)" with "str" of 
the objects in the two examples, you might be able to see something. 


      5.  Have you tried "traceback()" after receiving the error?  This 
often does not work, but it works often enough to make it worthwhile. 


      6.  If none of the above produce enlightenment, you could try 
"debug(ca.jo)", then issue the command below that produced the error.  
When you then repeat the command, it will put you in the environment of 
the function and stop.  From there, you can walk through the code line 
by line looking at the objects created, etc., until you find the error. 
This has worked for me. 


      Hope this helps. 
      Spencer Graves

Charles Evans wrote:
> This is in regard to the ca.jo() function in the urca package in R.
>
> I am a PhD candidate in Finance at Florida Atlantic University in Boca 
> Raton, FL, USA.  As part of my research I must perform cointegration 
> tests on the price and NAV time series of 51 ETFs.  I am able to run 
> Augmented Dickey-Fuller and Phillips-Perron tests in R.  However, I am 
> having a devil of a time getting ca.jo() to work.  If anyone can 
> provide guidance, I would be most appreciative.
>
> I suspect that the problem is related to how I have my data 
> organized.  I have one set of price data (called pt) for 51 ETFs with 
> one ETF daily price series per column and T+1 rows; the first row 
> contains headers.  The first column contains dates.  I have another 
> set of NAV data (called nt) organized in the same way.  The ETFs have 
> different incept dates, and the tables contain NA values.
>
> I want to test for price/NAV cointegration using the Johansen 
> procedures ? both eignevalue and trace test ? for each of the 51 
> pairs.  When I create a time series for, e.g., price[,49] and NAV[,49] 
> using the cbind() function I get:
>
>
>
> > johansen_pn <- ts(cbind(pt[,49],nt[,49]))
> > ca.jo(johansen_pn, type = c("trace"), ecdet = c("const"), K = 2, 
> spec=c("longrun"), season = NULL, dumvar = NULL)
>
> Error in dimnames(x) <- dn :
> length of 'dimnames' [2] not equal to array extent
>
>
>
> I have tried n different ways to construct the "x" (here, johansen_pn) 
> variable, and I get the same dimnames error every time.
>
> Can anyone suggest a remedy or a cookbook explanation of how to use 
> ca.jo() properly?
>
> Charles Evans
> cevans at chyden.net
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From spencer.graves at prodsyse.com  Sun May 17 23:01:30 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Sun, 17 May 2009 14:01:30 -0700
Subject: [R-SIG-Finance] extract parameters from fitCopula outputs !!
In-Reply-To: <eb0669d70905152307g5090b2deibd442bd044975af1@mail.gmail.com>
References: <eb0669d70905152307g5090b2deibd442bd044975af1@mail.gmail.com>
Message-ID: <4A107B2A.5000609@prodsyse.com>

      Your example is incomplete, because I do not have your "u". 

      However, the examples in the "fitCopula" help page should suffice 
to answer your question:  Consider in particular the multivariate example: 

    normal.cop <- normalCopula(c(0.6,0.36, 0.6),dim=3,dispstr="un")
     x <- rcopula(normal.cop, n)     ## true observations
     u <- apply(x, 2, rank) / (n + 1)  ## pseudo-observations
     ## inverting Kendall's tau
     fit.tau <- fitCopula(normal.cop, u, method="itau")

     
      Compare str(fit.tau) with print(fit.tau).  This suggests we can 
answer your question with "fit.tau at estimate". 


      Hope this helps. 
      Spencer Graves
   
tawfiq just wrote:
> hello
>
> i have the following output from fitting a normal copula (copula package)
>
>  normal.cop <- normalCopula(c(0,0,0,0,0,0,0,0,0,0),dim=5,dispstr="un")
> fit.tau <- fitCopula(normal.cop, u, method="itau")
>
> The estimation method is  Inversion of Kendall's Tau  based on  52
> observations.
>         Estimate Std. Error   z value Pr(>|z|)
> rho.1  0.7609442 0.08183654  9.298343        0
> rho.2  0.9123854 0.04060923 22.467440        0
> rho.3  0.8854560 0.04804054 18.431432        0
> rho.4  0.8730453 0.05525219 15.801100        0
> rho.5  0.8296553 0.05762548 14.397370        0
> rho.6  0.8022804 0.06063024 13.232347        0
> rho.7  0.8120697 0.06072519 13.372865        0
> rho.8  0.9470253 0.02154186 43.962098        0
> rho.9  0.9333255 0.02565925 36.373847        0
> rho.10 0.9591890 0.01818588 52.743616        0
>
> the question is how to extract the first column from the output, as a vector
> (0.7609442, 0.9123854, .....,0.9591890)
>
> thank you very much
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From just.tawfiq at gmail.com  Mon May 18 00:06:35 2009
From: just.tawfiq at gmail.com (tawfiq just)
Date: Sun, 17 May 2009 17:06:35 -0500
Subject: [R-SIG-Finance] extract parameters from fitCopula outputs !!
In-Reply-To: <4A107B2A.5000609@prodsyse.com>
References: <eb0669d70905152307g5090b2deibd442bd044975af1@mail.gmail.com>
	<4A107B2A.5000609@prodsyse.com>
Message-ID: <eb0669d70905171506s2b1d39l1a0f50dbb7301267@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090517/9acefcbe/attachment.pl>

From just.tawfiq at gmail.com  Mon May 18 00:16:09 2009
From: just.tawfiq at gmail.com (tawfiq just)
Date: Sun, 17 May 2009 17:16:09 -0500
Subject: [R-SIG-Finance] extract parameters from fitCopula outputs !!
In-Reply-To: <eb0669d70905171506s2b1d39l1a0f50dbb7301267@mail.gmail.com>
References: <eb0669d70905152307g5090b2deibd442bd044975af1@mail.gmail.com>
	<4A107B2A.5000609@prodsyse.com>
	<eb0669d70905171506s2b1d39l1a0f50dbb7301267@mail.gmail.com>
Message-ID: <eb0669d70905171516v43607660i57e9d6bce52d1447@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090517/eed315c5/attachment.pl>

From spencer.graves at prodsyse.com  Mon May 18 00:41:05 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Sun, 17 May 2009 15:41:05 -0700
Subject: [R-SIG-Finance] extract parameters from fitCopula outputs !!
In-Reply-To: <eb0669d70905171516v43607660i57e9d6bce52d1447@mail.gmail.com>
References: <eb0669d70905152307g5090b2deibd442bd044975af1@mail.gmail.com>	
	<4A107B2A.5000609@prodsyse.com>	
	<eb0669d70905171506s2b1d39l1a0f50dbb7301267@mail.gmail.com>
	<eb0669d70905171516v43607660i57e9d6bce52d1447@mail.gmail.com>
Message-ID: <4A109281.5030801@prodsyse.com>

      I have not used copulas, so I can't comment on whether what you've 
done is "correct". 


      In general, "all models are wrong, but some are useful." 
(http://en.wikiquote.org/wiki/George_Box)  It seems to me that most real 
data contain unmodeled components of variance, which generate the image 
of a lack of fit to one of the more traditional distributions like 
normal, binomial or Poisson.  This may be my own blindness, but I have 
yet to find a problem where I felt motivated to try copulas in place of 
more traditional methods provided, e.g., by qqnorm, nlme, lme4, dlm, 
sspir, and fda. 


      Hope this helps. 
      Spencer Graves


tawfiq just wrote:
> 2009/5/17 tawfiq just <just.tawfiq at gmail.com>
>
>   
>> 2009/5/17 spencerg <spencer.graves at prodsyse.com>
>>
>>     Your example is incomplete, because I do not have your "u".
>>     
>>>     However, the examples in the "fitCopula" help page should suffice to
>>> answer your question:  Consider in particular the multivariate example:
>>>   normal.cop <- normalCopula(c(0.6,0.36, 0.6),dim=3,dispstr="un")
>>>    x <- rcopula(normal.cop, n)     ## true observations
>>>    u <- apply(x, 2, rank) / (n + 1)  ## pseudo-observations
>>>    ## inverting Kendall's tau
>>>    fit.tau <- fitCopula(normal.cop, u, method="itau")
>>>
>>>         Compare str(fit.tau) with print(fit.tau).  This suggests we can
>>> answer your question with "fit.tau at estimate".
>>>
>>>     Hope this helps.     Spencer Graves
>>>  tawfiq just wrote:
>>>
>>>       
>>>> hello
>>>>
>>>> i have the following output from fitting a normal copula (copula package)
>>>>
>>>>  normal.cop <- normalCopula(c(0,0,0,0,0,0,0,0,0,0),dim=5,dispstr="un")
>>>> fit.tau <- fitCopula(normal.cop, u, method="itau")
>>>>
>>>> The estimation method is  Inversion of Kendall's Tau  based on  52
>>>> observations.
>>>>        Estimate Std. Error   z value Pr(>|z|)
>>>> rho.1  0.7609442 0.08183654  9.298343        0
>>>> rho.2  0.9123854 0.04060923 22.467440        0
>>>> rho.3  0.8854560 0.04804054 18.431432        0
>>>> rho.4  0.8730453 0.05525219 15.801100        0
>>>> rho.5  0.8296553 0.05762548 14.397370        0
>>>> rho.6  0.8022804 0.06063024 13.232347        0
>>>> rho.7  0.8120697 0.06072519 13.372865        0
>>>> rho.8  0.9470253 0.02154186 43.962098        0
>>>> rho.9  0.9333255 0.02565925 36.373847        0
>>>> rho.10 0.9591890 0.01818588 52.743616        0
>>>>
>>>> the question is how to extract the first column from the output, as a
>>>> vector
>>>> (0.7609442, 0.9123854, .....,0.9591890)
>>>>
>>>> thank you very much
>>>>
>>>>        [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>>
>>>>
>>>>         
>>> thak you spencer for answer, that's what i was looking for
>>>       
>> to be more clair, what i wanted to do is to simulate a multivariate normal
>> copula using suppose i have a matrix of returns 50 * 4 dimension
>> x <- matrix(rnorm(200),50,4)
>> u <- apply(x, 2, rank) / (n + 1)  ## empirical distribution of x
>>   ## inverting Kendall's tau
>> normal.cop1 <- normalCopula(c(0,0,0,0,0,0),dim=4,dispstr="un")
>>
>> fit.tau <- fitCopula(normal.cop1, G, method="itau")  # fit data to copula
>> to evaluate rho
>>
>> normal.cop <- normalCopula(  fit.tau at estimate,dim=4,dispstr="un")
>>
>> u <- rcopula(normal.cop, 10)     # simulate normal copula that have same
>> caracteristics than data
>>
>>
>> is it correct what i have done?
>>
>> thank you again
>>
>>     
>
> Error in the instruction
> fit.tau <- fitCopula(normal.cop1, G, method="itau")  # fit data to copula to
> evaluate rho
>
> u insteade of G
>
> fit.tau <- fitCopula(normal.cop1, u, method="itau")  # fit data to copula to
> evaluate rho
>
> sorry
>
>


From archstevej at gmail.com  Mon May 18 07:14:13 2009
From: archstevej at gmail.com (Steven Archambault)
Date: Sun, 17 May 2009 23:14:13 -0600
Subject: [R-SIG-Finance] Chi-sq Hausman test---R vs Stata
Message-ID: <a575b07e0905172214x5f0bbf84na73a331316697b3b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090517/64c886b7/attachment.pl>

From windspeedo99 at gmail.com  Mon May 18 12:47:29 2009
From: windspeedo99 at gmail.com (Wind)
Date: Mon, 18 May 2009 18:47:29 +0800
Subject: [R-SIG-Finance] Calculating SharpeRatio for several managers with
	PerformanceAnalytics
Message-ID: <d718c8210905180347u62f6c58fp24c3b744f302d7c0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090518/6e6f6a09/attachment.pl>

From singularitaet at gmx.net  Mon May 18 13:13:32 2009
From: singularitaet at gmx.net (Stefan Grosse)
Date: Mon, 18 May 2009 13:13:32 +0200
Subject: [R-SIG-Finance] Non-parametric tests in R
In-Reply-To: <de69a2b90905151222x11a518dcp6ab18ae092184cb3@mail.gmail.com>
References: <de69a2b90905151222x11a518dcp6ab18ae092184cb3@mail.gmail.com>
Message-ID: <20090518131332.2540814b@gmx.net>

On Fri, 15 May 2009 14:22:01 -0500 Subhrangshu Nandi
<nands31 at gmail.com> wrote:

SN> Lets say I have a population of 500 datapoints, which is clearly
SN> not normal. (Please see attached text file for an example
SN> population). These datapoints can be assumed to be IID. Now, I get
SN> 30 more datapoints and want to test if these have been drawn (with
SN> replacement) from the population. I'm doing the followin:

How about the Kolmogorov-Smirnov Test?
http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test
?ks.test


Stefan


From jeff.a.ryan at gmail.com  Mon May 18 16:23:38 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Mon, 18 May 2009 09:23:38 -0500
Subject: [R-SIG-Finance] Calculating SharpeRatio for several managers
	with PerformanceAnalytics
In-Reply-To: <d718c8210905180347u62f6c58fp24c3b744f302d7c0@mail.gmail.com>
References: <d718c8210905180347u62f6c58fp24c3b744f302d7c0@mail.gmail.com>
Message-ID: <e8e755250905180723p56c2799ao7838139aa8800b44@mail.gmail.com>

Hi Wind,

Try:

apply(managers[,c(1,3)],2, SharpeRatio)
  HAM1      HAM3
0.4481450 0.3289917

HTH
Jeff

On Mon, May 18, 2009 at 5:47 AM, Wind <windspeedo99 at gmail.com> wrote:
> I wonder whether there is any better method calculating ratios such as
> SharpRatio, SortinoRatio and so on, for several managers.
>
>>library(PerformanceAnalytics)
>>data(managers)
>
> It is ok if we just calculate SharpeRatio for one manager.
>> SharpeRatio(managers[,1,drop=FALSE])
> ? ?HAM1
> 0.448145
>
> But it seems that we could not calculate SharpeRatio for several managers at
> once.
>> SharpeRatio(managers[,c(1,3),drop=FALSE])
> ? ? HAM1 ? ? ?HAM3
> 0.4670831 0.3161724
>
> The answer for HAM1 is different now.
>
>> SharpeRatio(managers[,c(1,2,3),drop=FALSE])
> HAM1 HAM2 HAM3
> ?NA ? NA ? NA
>
> ?I guess maybe I have not use the function properly.
>
> If I change the mean function in SharpeRatio or SortinoRatio to colMeans
> function, it seems ok.
> Because PerformanceAnalytics provides many useful ratios I wonder whether
> there are some better methods so one could utilize the ratio calculating for
> several managers directly.
>
> Any suggestion would be appreciated.
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From spencer.graves at prodsyse.com  Mon May 18 16:27:37 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Mon, 18 May 2009 07:27:37 -0700
Subject: [R-SIG-Finance] Chi-sq Hausman test---R vs Stata
In-Reply-To: <a575b07e0905172214x5f0bbf84na73a331316697b3b@mail.gmail.com>
References: <a575b07e0905172214x5f0bbf84na73a331316697b3b@mail.gmail.com>
Message-ID: <4A117059.5050905@prodsyse.com>

      When different methods give different answers for ostensibly the 
same problem, it suggests that there is a difference in assumptions or a 
bug.  I suggest you write down the different models and try to 
understand what is different about them that might give different results. 


      If this procedure does not lead to enlightenment, please submit 
another post but include commented, minimal, self-contained, 
reproducible code, as suggested in the posting guide 
"http://www.R-project.org/posting-guide.html". 


      If you'd like to see what else is available in R, you might try 
"RSiteSearch.function", something like the following: 


library(RSiteSearch)
Hausman <- RSiteSearch.function('Hausman')
HTML(Hausman)

ptsr <- RSiteSearch.function('panel time series regression')
HTML(ptsr)


      Hope this helps. 
      Spencer Graves


Steven Archambault wrote:
> Hi all,
>
> I am running a panel time series regression testing Fixed Effects and Random
> Effects. I decided to calculate the chi-sq value for the Hausman test in
> both R (Phtest) and Stata. I get different results. Even within Stata,
> calculating the Chi-sq value with the canned procedure or by hand (using
> matrices) gives different results. So, the question should come up there as
> well.
>
> Does anybody have any insight on how to pick which results to use? I guess
> the one that gives the result I want? Having different programs give quite
> different values for the same tests is frustrating me.  I'd be interested in
> any feedback folks have!
>
> Thanks,
> Steve
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From peter at braverock.com  Mon May 18 16:29:12 2009
From: peter at braverock.com (Peter Carl)
Date: Mon, 18 May 2009 09:29:12 -0500 (CDT)
Subject: [R-SIG-Finance] Calculating SharpeRatio for several managers
 with PerformanceAnalytics
In-Reply-To: <d718c8210905180347u62f6c58fp24c3b744f302d7c0@mail.gmail.com>
References: <d718c8210905180347u62f6c58fp24c3b744f302d7c0@mail.gmail.com>
Message-ID: <26316.64.190.216.194.1242656952.squirrel@mail.braverock.com>

SharpeRatio was originally written to use on a single column, but because
of the behavior of sd, which provides multi-column support, the error
isn't obvious.  Thanks very much for bringing this to our attention, we'll
provide multi-column support for this function in the next release.

To ensure column-by-column results in the meantime, you might use:
sapply(managers[,c(1,3)], FUN=SharpeRatio)

> sapply(managers[,c(1,3)], FUN=SharpeRatio)
HAM1.Column HAM3.Column
  0.4481450   0.3289917
> SharpeRatio(managers[,1,drop=F])
    HAM1
0.448145

Sorry for the inconvenience, and thanks again for pointing this out.

pcc
-- 
Peter Carl
http://www.braverock.com/~peter

> I wonder whether there is any better method calculating ratios such as
> SharpRatio, SortinoRatio and so on, for several managers.
>
>>library(PerformanceAnalytics)
>>data(managers)
>
> It is ok if we just calculate SharpeRatio for one manager.
>> SharpeRatio(managers[,1,drop=FALSE])
>     HAM1
> 0.448145
>
> But it seems that we could not calculate SharpeRatio for several managers
> at
> once.
>> SharpeRatio(managers[,c(1,3),drop=FALSE])
>      HAM1      HAM3
> 0.4670831 0.3161724
>
> The answer for HAM1 is different now.
>
>> SharpeRatio(managers[,c(1,2,3),drop=FALSE])
> HAM1 HAM2 HAM3
>   NA   NA   NA
>
>  I guess maybe I have not use the function properly.
>
> If I change the mean function in SharpeRatio or SortinoRatio to colMeans
> function, it seems ok.
> Because PerformanceAnalytics provides many useful ratios I wonder whether
> there are some better methods so one could utilize the ratio calculating
> for
> several managers directly.
>
> Any suggestion would be appreciated.
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From frainj at tcd.ie  Mon May 18 17:23:37 2009
From: frainj at tcd.ie (John Frain)
Date: Mon, 18 May 2009 16:23:37 +0100
Subject: [R-SIG-Finance] Chi-sq Hausman test---R vs Stata
In-Reply-To: <4A117059.5050905@prodsyse.com>
References: <a575b07e0905172214x5f0bbf84na73a331316697b3b@mail.gmail.com>
	<4A117059.5050905@prodsyse.com>
Message-ID: <cfdde1650905180823n4bce4e42v6053b41a454adb8b@mail.gmail.com>

Steven needs to consult a Stata manual.  The default in Stata is to
calculate the test statistic based on the covariance matrix of both
the fixed effects an the random effects estimators.  This often leads
to a  non positive covariance matrix and there is an option to base
the statistic on either estimator.   Many persons do not notice this.
The statistic may also be based on auxiliary regressions.  You may
thus get some minor differences in your results.  Wooldridge's panel
data book or Cameron and Trivedi might provide some more guidance.  If
the differences are large you probably have a problem with a
covariance matrix


Best Regards

John

2009/5/18 spencerg <spencer.graves at prodsyse.com>:
> ? ? When different methods give different answers for ostensibly the sameproblem with your
> problem, it suggests that there is a difference in assumptions or a bug. ?I
> suggest you write down the different models and try to understand what is
> different about them that might give different results.
>
> ? ? If this procedure does not lead to enlighte.ment, please submit another
> post but include commented, minimal, self-contained, reproducible code, as
> suggested in the posting guide
> "http://www.R-project.org/posting-guide.html".
>
> ? ? If you'd like to see what else is available in R, you might try
> "RSiteSearch.function", something like the following:
>
> library(RSiteSearch)
> Hausman <- RSiteSearch.function('Hausman')
> HTML(Hausman)
>
> ptsr <- RSiteSearch.function('panel time series regression')
> HTML(ptsr)
>
>
> ? ? Hope this helps. ? ? Spencer Graves
>
>
> Steven Archambault wrote:
>>
>> Hi all,
>>
>> I am running a panel time series regression testing Fixed Effects and
>> Random
>> Effects. I decided to calculate the chi-sq value for the Hausman test in
>> both R (Phtest) and Stata. I get different results. Even within Stata,
>> calculating the Chi-sq value with the canned procedure or by hand (using
>> matrices) gives different results. So, the question should come up there
>> as
>> well.
>>
>> Does anybody have any insight on how to pick which results to use? I guess
>> the one that gives the result I want? Having different programs give quite
>> different values for the same tests is frustrating me. ?I'd be interested
>> in
>> any feedback folks have!
>>
>> Thanks,
>> Steve
>>
>> ? ? ? ?[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>



-- 
John C Frain
Trinity College Dublin
Dublin 2
Ireland
www.tcd.ie/Economics/staff/frainj/home.htm
mailto:frainj at tcd.ie
mailto:frainj at gmail.com


From windspeedo99 at gmail.com  Mon May 18 17:41:47 2009
From: windspeedo99 at gmail.com (Wind)
Date: Mon, 18 May 2009 23:41:47 +0800
Subject: [R-SIG-Finance] Calculating SharpeRatio for several managers
	with PerformanceAnalytics
In-Reply-To: <e8e755250905180723p56c2799ao7838139aa8800b44@mail.gmail.com>
References: <d718c8210905180347u62f6c58fp24c3b744f302d7c0@mail.gmail.com>
	<e8e755250905180723p56c2799ao7838139aa8800b44@mail.gmail.com>
Message-ID: <d718c8210905180841o754675e3xc44e789928d528a6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090518/1d995cb4/attachment.pl>

From archstevej at gmail.com  Mon May 18 20:26:47 2009
From: archstevej at gmail.com (Steven Archambault)
Date: Mon, 18 May 2009 12:26:47 -0600
Subject: [R-SIG-Finance] R: [Fwd: R-SIG-Finance Digest, Vol 60, Issue 18]
In-Reply-To: <28643F754DDB094D8A875617EC4398B202AE7AD1@BEMAILEXTV03.corp.generali.net>
References: <4A1138D0.9050004@unibas.ch>
	<28643F754DDB094D8A875617EC4398B202AE7AD1@BEMAILEXTV03.corp.generali.net>
Message-ID: <a575b07e0905181126m67cb8de2k6ee4c4153d69806@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090518/db92fb71/attachment.pl>

From Lmelovel at banrep.gov.co  Mon May 18 22:04:54 2009
From: Lmelovel at banrep.gov.co (Melo Velandia Luis Fernando)
Date: Mon, 18 May 2009 15:04:54 -0500
Subject: [R-SIG-Finance] R: [Fwd: R-SIG-Finance Digest, Vol 60, Issue 18]
In-Reply-To: <a575b07e0905181126m67cb8de2k6ee4c4153d69806@mail.gmail.com>
References: <4A1138D0.9050004@unibas.ch><28643F754DDB094D8A875617EC4398B202AE7AD1@BEMAILEXTV03.corp.generali.net>
	<a575b07e0905181126m67cb8de2k6ee4c4153d69806@mail.gmail.com>
Message-ID: <1736B063D13B064284FBC1697EFE0C230222338C@NMAIL1A.banrep.gov.co>

It seems that in STATA your are using a robust covariance-matrix
estimators meanwhile in R not. This might explain the difference in the
Hausman tests. I am not an expert in the PLM package, but there is a
function "vcovHC" which uses a robust covariance matrix estimator
(White). You might try this.

Sincerely,

Luis 
-----Mensaje original-----
De: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] En nombre de Steven
Archambault
Enviado el: Lunes, 18 de Mayo de 2009 01:27 p.m.
Para: Millo Giovanni
CC: r-sig-finance at stat.math.ethz.ch; Yves Croissant; Christian Kleiber
Asunto: Re: [R-SIG-Finance] R: [Fwd: R-SIG-Finance Digest, Vol 60, Issue
18]

Giovani,

Thank you so much for your comments. I am a bit new to R, and to these
mailing lists, so I apologize for being sparse on the details and
examples.
I am using Stata 9.2, which might be the answer to my problem, as you
described. I have done quite a bit of internet searching, and did not
read anywhere about the use of a different method for calculating the
chi-sq value, so thanks for that.

 One more issue I have been thinking about. I am assuming your Plm
package knows that the FE is the consistient model, as the same results
arrive if the code is phtest(femod, remod) or phtest(remod, femod). The
order does matter in Stata.

For complteness I am going to post my results using the same Grumfeld
dataset for both stata 9.2 (by hand calculation and canned procedure)
and R.  I am using the Plm package version 1 1-2.

Regards,
Steve



 ## begin Stata9.2 output##
xtreg inv value capital, robust re;

Random-effects GLS regression                   Number of obs      =
200
Group variable (i): firmid                      Number of groups   =
10

R-sq:  within  = 0.7668                         Obs per group: min =
20
       between = 0.8196                                        avg =
20.0
       overall = 0.8061                                        max =
20

Random effects u_i ~ Gaussian                   Wald chi2(3)       =
77.70
corr(u_i, X)       = 0 (assumed)                Prob > chi2        =
0.0000

------------------------------------------------------------------------
------
             |               Robust
      invest |      Coef.   Std. Err.      z    P>|z|     [95% Conf.
Interval]
-------------+----------------------------------------------------------
-------------+------
       value |   .1097811   .0197587     5.56   0.000     .0710547
.1485076
     capital |    .308113   .0418387     7.36   0.000     .2261107
.3901153
       _cons |  -57.83441   24.67795    -2.34   0.019    -106.2023
-9.466507
-------------+----------------------------------------------------------
-------------+------
     sigma_u |   84.20095
     sigma_e |  52.767964
         rho |  .71800838   (fraction of variance due to u_i)
------------------------------------------------------------------------
------

. matrix bfe=e(b);

. matrix vfe=e(V);

. estimates store remod;

. xtreg inv value capital, robust fe;

Fixed-effects (within) regression               Number of obs      =
200
Group variable (i): firmid                      Number of groups   =
10

R-sq:  within  = 0.7668                         Obs per group: min =
20
       between = 0.8194                                        avg =
20.0
       overall = 0.8060                                        max =
20

                                                F(2,188)           =
40.23
corr(u_i, Xb)  = -0.1517                        Prob > F           =
0.0000
------------------------------------------------------------------------
------
             |               Robust
      invest |      Coef.   Std. Err.      t    P>|t|     [95% Conf.
Interval]
-------------+----------------------------------------------------------
-------------+------
       value |   .1101238    .019378     5.68   0.000     .0718975
.1483501
     capital |   .3100653    .042795     7.25   0.000     .2256452
.3944854
       _cons |  -58.74393   23.37422    -2.51   0.013    -104.8534
-12.63449
-------------+----------------------------------------------------------
-------------+------
     sigma_u |  85.732501
     sigma_e |  52.767964
         rho |  .72525012   (fraction of variance due to u_i)
------------------------------------------------------------------------
------

 ###Hausman by hand###

. estimates store femod;

. matrix vre=e(V);

. matrix bre=e(b);

. matrix bdif=bfe-bre;

. matrix list bdif;

bdif[1,3]
         value     capital       _cons
y1  -.00034265  -.00195236   .90952273

. matrix bdifp=bdif';

. matrix dv=vfe-vre;

. matrix dvi=inv(dv);

. matrix list bdif;

bdif[1,3]
         value     capital       _cons
y1  -.00034265  -.00195236   .90952273

. matrix list bdifp;

bdifp[3,1]
                 y1
  value  -.00034265
capital  -.00195236
  _cons   .90952273

. matrix list dvi;

symmetric dvi[3,3]
              value     capital       _cons
  value  -7739.3615
capital   5808.2905   -5305.811
  _cons   3.6641311   .98569198  -.00051157

. matrix chisq=bdif*dvi*bdifp;

. matrix list chisq;

symmetric chisq[1,1]
            y1
y1  -.01956929
###Hausman canned###
.  hausman femod remod;

                 ---- Coefficients ----
             |      (b)          (B)            (b-B)
sqrt(diag(V_b-V_B))
             |     femod        remod        Difference          S.E.
-------------+----------------------------------------------------------
-------------+------
       value |    .1101238     .1097811        .0003427               .
     capital |    .3100653      .308113        .0019524        .0089965
------------------------------------------------------------------------
------
                           b = consistent under Ho and Ha; obtained from
xtreg
            B = inconsistent under Ha, efficient under Ho; obtained from
xtreg

    Test:  Ho:  difference in coefficients not systematic

                  chi2(2) = (b-B)'[(V_b-V_B)^(-1)](b-B)
                          =    -0.01    chi2<0 ==> model fitted on these
                                        data fails to meet the
asymptotic
                                        assumptions of the Hausman test;
                                        see suest for a generalized test
## end Stata9.2 output ##

##begin Output R, using PLM 1.1-2###

> test<-data(Grunfeld, package="Ecdat")
>
> fm <- inv~value+capital
> femod <- plm(fm, Grunfeld, model="within")
> summary(femod)
Oneway (individual) effect Within Model

Call:
plm(formula = fm, data = Grunfeld, model = "within")

Balanced Panel: n=10, T=20, N=200

Residuals :
    Min.  1st Qu.   Median  3rd Qu.     Max.
-184.000  -17.600    0.563   19.200  251.000

Coefficients :
        Estimate Std. Error t-value  Pr(>|t|)
value   0.110124   0.011857  9.2879 < 2.2e-16 ***
capital 0.310065   0.017355 17.8666 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Total Sum of Squares:    2244400
Residual Sum of Squares: 523480
F-statistic: 309.014 on 2 and 188 DF, p-value: < 2.22e-16
> remod <- plm(fm, Grunfeld, model="random")
> summary(remod)
Oneway (individual) effect Random Effect Model
   (Swamy-Arora's transformation)

Call:
plm(formula = fm, data = Grunfeld, model = "random")

Balanced Panel: n=10, T=20, N=200

Effects:
                   var  std.dev share
idiosyncratic 2784.458   52.768 0.282
individual    7089.800   84.201 0.718
theta:  0.86122

Residuals :
   Min. 1st Qu.  Median 3rd Qu.    Max.
-178.00  -19.70    4.69   19.50  253.00

Coefficients :
              Estimate Std. Error t-value Pr(>|t|)
(Intercept) -57.834415  28.898935 -2.0013  0.04536 *
value         0.109781   0.010493 10.4627  < 2e-16 ***
capital       0.308113   0.017180 17.9339  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Total Sum of Squares:    2381400
Residual Sum of Squares: 548900
F-statistic: 328.837 on 2 and 197 DF, p-value: < 2.22e-16
> phtest(femod, remod)

        Hausman Test

data:  fm
chisq = 2.3304, df = 2, p-value = 0.3119 alternative hypothesis: one
model is inconsistent

###end Plm###





On Mon, May 18, 2009 at 6:01 AM, Millo Giovanni
<Giovanni_Millo at generali.com
> wrote:

> Dear Steve,
>
> I got your inquiry courtesy of Christian Kleiber, who brought it to 
> our
> attention: please next time you post anything re a given package, 
> include the maintainer's address. We cannot guarantee to parse all the

> daily digests of the R system!
>
> Your problem: can you please provide a reproducible example? Else it 
> is difficult to help, not knowing your data, your results and even the

> Stata version you're using.
>
> In the following I replicate what you might have done on a well-known 
> dataset.
>
> From Stata10, on the usual Grunfeld data taken from package "Ecdat":
>
> ## begin Stata10 output ##
> . xtreg inv value capital
>
> Random-effects GLS regression                   Number of obs      =
> 200
> Group variable: firm                            Number of groups   =
> 10
>
> R-sq:  within  = 0.7668                         Obs per group: min =
> 20
>       between = 0.8196                                        avg =
> 20.0
>       overall = 0.8061                                        max =
> 20
>
> Random effects u_i ~ Gaussian                   Wald chi2(2)       =
> 657.67
> corr(u_i, X)       = 0 (assumed)                Prob > chi2        =
> 0.0000
>
> ----------------------------------------------------------------------
> --
> ------
>         inv |      Coef.   Std. Err.      z    P>|z|     [95% Conf.
> Interval]
> -------------+--------------------------------------------------------
> -------------+--
> ------
>       value |   .1097811   .0104927    10.46   0.000     .0892159
> .1303464
>     capital |    .308113   .0171805    17.93   0.000     .2744399
> .3417861
>       _cons |  -57.83441   28.89893    -2.00   0.045    -114.4753
> -1.193537
> -------------+--------------------------------------------------------
> -------------+--
> ------
>     sigma_u |   84.20095
>     sigma_e |  52.767964
>         rho |  .71800838   (fraction of variance due to u_i)
> ----------------------------------------------------------------------
> --
> ------
>
> . estimates store remod
>
> . xtreg inv value capital, fe
>
> Fixed-effects (within) regression               Number of obs      =
> 200
> Group variable: firm                            Number of groups   =
> 10
>
> R-sq:  within  = 0.7668                         Obs per group: min =
> 20
>       between = 0.8194                                        avg =
> 20.0
>       overall = 0.8060                                        max =
> 20
>
>                                                F(2,188)           =
> 309.01
> corr(u_i, Xb)  = -0.1517                        Prob > F           =
> 0.0000
>
> ----------------------------------------------------------------------
> --
> ------
>         inv |      Coef.   Std. Err.      t    P>|t|     [95% Conf.
> Interval]
> -------------+--------------------------------------------------------
> -------------+--
> ------
>       value |   .1101238   .0118567     9.29   0.000     .0867345
> .1335131
>     capital |   .3100653   .0173545    17.87   0.000     .2758308
> .3442999
>       _cons |  -58.74393   12.45369    -4.72   0.000    -83.31086
> -34.177
> -------------+--------------------------------------------------------
> -------------+--
> ------
>     sigma_u |  85.732501
>     sigma_e |  52.767964
>         rho |  .72525012   (fraction of variance due to u_i)
> ----------------------------------------------------------------------
> --
> ------
> F test that all u_i=0:     F(9, 188) =    49.18              Prob > F
=
> 0.0000
>
> . estimates store femod
>
> . hausman femod remod
>
>                 ---- Coefficients ----
>             |      (b)          (B)            (b-B)
> sqrt(diag(V_b-V_B))
>             |     femod        remod        Difference          S.E.
> -------------+--------------------------------------------------------
> -------------+--
> ------
>       value |    .1101238     .1097811        .0003427        .0055213
>     capital |    .3100653      .308113        .0019524        .0024516
> ----------------------------------------------------------------------
> --
> ------
>                           b = consistent under Ho and Ha; obtained 
> from xtreg
>            B = inconsistent under Ha, efficient under Ho; obtained 
> from xtreg
>
>    Test:  Ho:  difference in coefficients not systematic
>
>                  chi2(2) = (b-B)'[(V_b-V_B)^(-1)](b-B)
>                          =        2.33
>                Prob>chi2 =      0.3119
>
> .
> ## end Stata10 output ##
>
> while from plm I get
>
> ## begin R putput ##
> > data(Grunfeld, package="Ecdat")
> > fm <- inv~value+capital
> >
> > femod <- plm(fm, Grunfeld)
> > remod <- plm(fm, Grunfeld, model="random")
> >
> > phtest(femod, remod)
>
>        Hausman Test
>
> data:  fm
> chisq = 2.3304, df = 2, p-value = 0.3119 alternative hypothesis: one 
> model is inconsistent
>
> ## end R output ##
>
> which, besides testifying to the goodness and parsimony of an 
> object-oriented approach as far as screen output is concerned, looks 
> rather consistent to me.
>
> I cannot but guess that the problem might stem from different RE
> estimates: previous versions of Stata used the Wallace-Hussein method 
> by default for computing the variance of random effects. Now Stata 
> uses Swamy-Arora, which has been the default of 'plm' since the
beginning.
> Yet as plm() allows to choose, you can experiment with different 
> values for the 'random.method' argument in order to see if you get the

> Stata result. I suggest you start by comparing the coefficient 
> estimates you get from Stata and R: FE should be unambiguous, RE might

> vary as said above, and for good reason.
>
> You also didn't tell us whether your by-hand calculation agrees with
> phtest() output? (I guess it does not)
>
> Please let us know, possibly with a reproducible example and providing

> all the above info Giovanni
>
> PS please also make sure you're not using any VEEEEERY old version of 
> 'plm' (prior to, say, 0.3): these had a bug in the p-value calculation

> which made it depend on the order of models compared (so that in the 
> wrong case you got p.value=1).
>
> Giovanni Millo
> Research Dept.,
> Assicurazioni Generali SpA
> Via Machiavelli 4,
> 34132 Trieste (Italy)
> tel. +39 040 671184
> fax  +39 040 671160
>
> > --------------------------------------------------------------------
> > --
> > --
> >
> > Subject:
> > [R-SIG-Finance] Chi-sq Hausman test---R vs Stata
> > From:
> > Steven Archambault <archstevej at gmail.com>
> > Date:
> > Sun, 17 May 2009 23:14:13 -0600
> > To:
> > r-sig-finance at stat.math.ethz.ch
> >
> > To:
> > r-sig-finance at stat.math.ethz.ch
> >
> >
> > Hi all,
> >
> > I am running a panel time series regression testing Fixed Effects 
> > and Random Effects. I decided to calculate the chi-sq value for the 
> > Hausman test in both R (Phtest) and Stata. I get different results.
> > Even within Stata, calculating the Chi-sq value with the canned 
> > procedure or by hand (using
> > matrices) gives different results. So, the question should come up
> there as
> > well.
> >
> > Does anybody have any insight on how to pick which results to use? I

> > guess the one that gives the result I want? Having different 
> > programs give quite different values for the same tests is 
> > frustrating me.  I'd
>
> > be interested in any feedback folks have!
> >
> > Thanks,
> > Steve
> >
> >       [[alternative HTML version deleted]]
>

	[[alternative HTML version deleted]]

AVISO LEGAL: 
- Las opiniones que contenga este mensaje son exclusivas de su autor y no necesariamente representan la opini?n oficial del Banco de la Rep?blica o de sus autoridades.
- El receptor deber? verificar posibles virus inform?ticos que tenga el correo o cualquier anexo a ?l, raz?n por la cual el Banco de la Rep?blica no aceptar? responsabilidad alguna por da?os causados por cualquier virus transmitido en este correo.
- La informaci?n contenida en este mensaje y en los archivos electr?nicos adjuntos es confidencial y reservada, conforme a lo previsto en la Constituci?n y en la Ley del Banco de la Rep?blica, y est? dirigida exclusivamente a su destinatario, sin la intenci?n de que sea revelada o divulgada a 
otras personas. El acceso al contenido de esta comunicaci?n por cualquier otra persona diferente al destinatario no est? autorizado por el Banco de la Rep?blica y est? sancionado de acuerdo con las normas legales aplicables.
- El que il?citamente sustraiga, oculte, extrav?e, destruya, intercepte, controle o impida esta comunicaci?n, antes de que llegue a su destinatario, estar? sujeto a las sanciones penales correspondientes. Igualmente, incurrir? en sanciones penales el que, en provecho propio o ajeno o 
con perjuicio de otro, divulgue o emplee la informaci?n contenida en esta comunicaci?n. En particular, los servidores p?blicos que reciban este mensaje est?n obligados a asegurar y mantener la confidencialidad de la informaci?n en ?l contenida y, en general, a cumplir con los deberes de custodia, cuidado, manejo y dem?s previstos en el r?gimen disciplinario.
- Si por error recibe este mensaje, le solicitamos enviarlo de vuelta al Banco de la Rep?blica a la direcci?n de correo electr?nico que se lo envi? y borrarlo de sus archivos electr?nicos o destruirlo.
 
LEGAL NOTICE:
- Any opinions contained in this message are exclusive of its author and not necessarily represent the official position of Banco de la Rep?blica or of its authorities.
- The recipient must verify the presence of possible informatic viruses in the mail or in any annex thereto, and for this reason Banco de la Rep?blica shall not be made liable for any damages caused by viruses transmitted hereby.
- The information contained in this message and in any electronic files annexed thereto is
confidential and privileged, as per the Colombian Constitution and the Law that governs Banco de la Rep?blica, and is directed exclusively to its addressee, with no intention of it being disclosed or revealed to third parties. The access to the content of this communication by any person different from its addressee is not authorized by Banco de la Rep?blica and shall be penalized in accordance with the applicable legal dispositions.
- Any person who illicitly removes, hides, distracts, destroys, intercepts, controls, or otherwise prevents this communication from arriving to its addressee, shall be subject to the appropriate criminal penalties. Likewise, criminal penalties shall be incurred by any who, either for his/her own benefit or on behalf of third parties, or with prejudice of a third party, discloses or employs the information contained in this communication. In particular, public servants that may receive this message shall be obliged to ensure and keep the confidentiality of the information contained
therein and, in general, to comply with the duties of custody, care, handling and other provided under the disciplinary regime.
- If you should happen to receive this message by mistake, please send it back to Banco de la Rep?blica to the same e-mail address and either delete it from your electronic files or destroy it.

From archstevej at gmail.com  Mon May 18 22:06:31 2009
From: archstevej at gmail.com (Steven Archambault)
Date: Mon, 18 May 2009 14:06:31 -0600
Subject: [R-SIG-Finance] R: [Fwd: R-SIG-Finance Digest, Vol 60, Issue 18]
In-Reply-To: <a575b07e0905181126m67cb8de2k6ee4c4153d69806@mail.gmail.com>
References: <4A1138D0.9050004@unibas.ch>
	<28643F754DDB094D8A875617EC4398B202AE7AD1@BEMAILEXTV03.corp.generali.net>
	<a575b07e0905181126m67cb8de2k6ee4c4153d69806@mail.gmail.com>
Message-ID: <a575b07e0905181306v65067bd3s2313ccff9c55d4eb@mail.gmail.com>

I just realized I used Robust in my Stata 9.2 analysis. When I remove this,
the Chi-sq values are much closer to the values I get in R (but negative, as
the consistent model must be listed first in a chi-sq calculation). However,
with my own data I do get this positive definite error in Stata. Is this a
result of unbalanced data? R doesn't give an error, so I am inclined to
ignore it in Stata. I am posting my own results from R and Stata, and
attaching the data as a csv.

Thanks, hope I am not wasting too much of your time here.

-Steve

###R-Output###
> library("plm")
>
> fdi <- read.csv("C:/data/mydata.csv", na.strings=".")
> fdiplm<-plm.data(fdi, index = c("id_code_id", "year"))
series    are constants and have been removed
>
> fdi_test<-(lfdi_2000~ lagdlfdi+ laglnstock2000+ lagtradegdp +lagdlgdp)
>
> fdi_test_fe <- plm(fdi_test, data=fdiplm, model="within")
> fdi_test_re <- plm(fdi_test, data=fdiplm, model="random")
>
> summary (fdi_test_fe)
Oneway (individual) effect Within Model

Call:
plm(formula = fdi_test, data = fdiplm, model = "within")

Unbalanced Panel: n=149, T=3-27, N=2697

Residuals :
   Min. 1st Qu.  Median 3rd Qu.    Max.
-8.2100 -0.4760  0.0452  0.5670  4.8700

Coefficients :
                Estimate Std. Error t-value  Pr(>|t|)
lagdlfdi       0.1564759  0.0180645  8.6621 < 2.2e-16 ***
laglnstock2000 0.7621350  0.0246798 30.8809 < 2.2e-16 ***
lagtradegdp    0.0178568  0.0025859  6.9055 5.003e-12 ***
lagdlgdp       0.2601477  0.0427744  6.0818 1.188e-09 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Total Sum of Squares:    4606.7
Residual Sum of Squares: 2938
F-statistic: 361.237 on 4 and 2544 DF, p-value: < 2.22e-16
> summary (fdi_test_re)
Oneway (individual) effect Random Effect Model
   (Swamy-Arora's transformation)

Call:
plm(formula = fdi_test, data = fdiplm, model = "random")

Unbalanced Panel: n=149, T=3-27, N=2697

Effects:
                  var std.dev  share
idiosyncratic 1.15487 1.07465 0.6617
individual    0.59044 0.76840 0.3383
theta  :
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
 0.3718  0.6700  0.7081  0.6955  0.7355  0.7401

Residuals :
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
-9.15000 -0.47900  0.07270 -0.00713  0.59800  3.95000

Coefficients :
                 Estimate Std. Error  t-value  Pr(>|t|)
(Intercept)    16.7744214  0.1552868 108.0222 < 2.2e-16 ***
lagdlfdi        0.1632388  0.0181005   9.0185 < 2.2e-16 ***
laglnstock2000  0.8314432  0.0196444  42.3247 < 2.2e-16 ***
lagtradegdp     0.0119453  0.0020737   5.7605 8.386e-09 ***
lagdlgdp        0.2558009  0.0424599   6.0245 1.696e-09 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Total Sum of Squares:    9522.3
Residual Sum of Squares: 3140.8
F-statistic: 1367.42 on 4 and 2692 DF, p-value: < 2.22e-16
>
> phtest(fdi_test_re, fdi_test_fe)

        Hausman Test

data:  fdi_test
chisq = 23.7021, df = 4, p-value = 9.164e-05
alternative hypothesis: one model is inconsistent


###end R output###

###Stata 9.2 Output--canned###
xtreg lfdi_2000 lagdlfdi laglnstock2000 lagtradegdp lagdlgdp, fe;

Fixed-effects (within) regression               Number of obs      =
2697
Group variable (i): id_code_id                  Number of groups   =
149

R-sq:  within  = 0.3622                         Obs per group: min =
3
       between = 0.8234                                        avg =
18.1
       overall = 0.6998                                        max =
27

                                                F(4,2544)          =
361.24
corr(u_i, Xb)  = 0.3536                         Prob > F           =
0.0000

------------------------------------------------------------------------------
   lfdi_2000 |      Coef.   Std. Err.      t    P>|t|     [95% Conf.
Interval]
-------------+----------------------------------------------------------------
    lagdlfdi |   .1564758   .0180645     8.66   0.000     .1210532
.1918985
laglnst~2000 |    .762135   .0246798    30.88   0.000     .7137404
.8105295
 lagtradegdp |   .0178568   .0025859     6.91   0.000     .0127861
.0229274
    lagdlgdp |   .2601478   .0427744     6.08   0.000     .1762716
.3440241
       _cons |   17.01131   .1701713    99.97   0.000     16.67762
17.345
-------------+----------------------------------------------------------------
     sigma_u |  .93048942
     sigma_e |  1.0746505
         rho |  .42847396   (fraction of variance due to u_i)
------------------------------------------------------------------------------
F test that all u_i=0:     F(148, 2544) =    10.73           Prob > F =
0.0000

. estimates store FIX, title(The FE) ;

. xtreg lfdi_2000 lagdlfdi laglnstock2000 lagtradegdp lagdlgdp, re;

Random-effects GLS regression                   Number of obs      =
2697
Group variable (i): id_code_id                  Number of groups   =
149

R-sq:  within  = 0.3606                         Obs per group: min =
3
       between = 0.8402                                        avg =
18.1
       overall = 0.7128                                        max =
27

Random effects u_i ~ Gaussian                   Wald chi2(4)       =
2225.46
corr(u_i, X)       = 0 (assumed)                Prob > chi2        =
0.0000

------------------------------------------------------------------------------
   lfdi_2000 |      Coef.   Std. Err.      z    P>|z|     [95% Conf.
Interval]
-------------+----------------------------------------------------------------
    lagdlfdi |   .1631662   .0180937     9.02   0.000     .1277032
.1986291
laglnst~2000 |    .830845   .0196843    42.21   0.000     .7922645
.8694255
 lagtradegdp |    .011992   .0020779     5.77   0.000     .0079195
.0160645
    lagdlgdp |   .2558113   .0424486     6.03   0.000     .1726136
.3390091
       _cons |   16.77702   .1556693   107.77   0.000     16.47191
17.08212
-------------+----------------------------------------------------------------
     sigma_u |  .77431228
     sigma_e |  1.0746505
         rho |  .34173973   (fraction of variance due to u_i)
------------------------------------------------------------------------------

.  estimates store RAND, title(The RE) ;

. hausman FIX RAND;

                 ---- Coefficients ----
             |      (b)          (B)            (b-B)
sqrt(diag(V_b-V_B))
             |      FIX          RAND        Difference          S.E.
-------------+----------------------------------------------------------------
    lagdlfdi |    .1564758     .1631662       -.0066903               .
laglnst~2000 |     .762135      .830845         -.06871         .014887
 lagtradegdp |    .0178568      .011992        .0058648        .0015393
    lagdlgdp |    .2601478     .2558113        .0043365        .0052695
------------------------------------------------------------------------------
                           b = consistent under Ho and Ha; obtained from
xtreg
            B = inconsistent under Ha, efficient under Ho; obtained from
xtreg

    Test:  Ho:  difference in coefficients not systematic

                  chi2(4) = (b-B)'[(V_b-V_B)^(-1)](b-B)
                          =       22.94
                Prob>chi2 =      0.0001
                (V_b-V_B is not positive definite)
###End Stata 9.2####







On Mon, May 18, 2009 at 12:26 PM, Steven Archambault
<archstevej at gmail.com>wrote:

> Giovani,
>
> Thank you so much for your comments. I am a bit new to R, and to these
> mailing lists, so I apologize for being sparse on the details and examples.
> I am using Stata 9.2, which might be the answer to my problem, as you
> described. I have done quite a bit of internet searching, and did not read
> anywhere about the use of a different method for calculating the chi-sq
> value, so thanks for that.
>
>  One more issue I have been thinking about. I am assuming your Plm package
> knows that the FE is the consistient model, as the same results arrive if
> the code is phtest(femod, remod) or phtest(remod, femod). The order does
> matter in Stata.
>
> For complteness I am going to post my results using the same Grumfeld
> dataset for both stata 9.2 (by hand calculation and canned procedure) and
> R.  I am using the Plm package version 1 1-2.
>
> Regards,
> Steve
>
>
>
>  ## begin Stata9.2 output##
> xtreg inv value capital, robust re;
>
> Random-effects GLS regression                   Number of obs      =
> 200
> Group variable (i): firmid                      Number of groups   =
> 10
>
> R-sq:  within  = 0.7668                         Obs per group: min =
> 20
>        between = 0.8196                                        avg =
> 20.0
>        overall = 0.8061                                        max =
> 20
>
> Random effects u_i ~ Gaussian                   Wald chi2(3)       =
> 77.70
>
> corr(u_i, X)       = 0 (assumed)                Prob > chi2        =
> 0.0000
>
>
> ------------------------------------------------------------------------------
>              |               Robust
>       invest |      Coef.   Std. Err.      z    P>|z|     [95% Conf.
> Interval]
>
> -------------+----------------------------------------------------------------
>        value |   .1097811   .0197587     5.56   0.000     .0710547
> .1485076
>      capital |    .308113   .0418387     7.36   0.000     .2261107
> .3901153
>        _cons |  -57.83441   24.67795    -2.34   0.019    -106.2023
> -9.466507
>
> -------------+----------------------------------------------------------------
>
>      sigma_u |   84.20095
>      sigma_e |  52.767964
>          rho |  .71800838   (fraction of variance due to u_i)
>
> ------------------------------------------------------------------------------
>
> . matrix bfe=e(b);
>
> . matrix vfe=e(V);
>
> . estimates store remod;
>
> . xtreg inv value capital, robust fe;
>
> Fixed-effects (within) regression               Number of obs      =
> 200
> Group variable (i): firmid                      Number of groups   =
> 10
>
> R-sq:  within  = 0.7668                         Obs per group: min =
> 20
>        between = 0.8194                                        avg =
> 20.0
>        overall = 0.8060                                        max =
> 20
>
>                                                 F(2,188)           =
> 40.23
>
> corr(u_i, Xb)  = -0.1517                        Prob > F           =
> 0.0000
>
> ------------------------------------------------------------------------------
>              |               Robust
>       invest |      Coef.   Std. Err.      t    P>|t|     [95% Conf.
> Interval]
>
> -------------+----------------------------------------------------------------
>        value |   .1101238    .019378     5.68   0.000     .0718975
> .1483501
>      capital |   .3100653    .042795     7.25   0.000     .2256452
> .3944854
>        _cons |  -58.74393   23.37422    -2.51   0.013    -104.8534
> -12.63449
>
> -------------+----------------------------------------------------------------
>      sigma_u |  85.732501
>      sigma_e |  52.767964
>          rho |  .72525012   (fraction of variance due to u_i)
>
> ------------------------------------------------------------------------------
>
>  ###Hausman by hand###
>
> . estimates store femod;
>
> . matrix vre=e(V);
>
> . matrix bre=e(b);
>
> . matrix bdif=bfe-bre;
>
> . matrix list bdif;
>
> bdif[1,3]
>          value     capital       _cons
> y1  -.00034265  -.00195236   .90952273
>
> . matrix bdifp=bdif';
>
> . matrix dv=vfe-vre;
>
> . matrix dvi=inv(dv);
>
> . matrix list bdif;
>
> bdif[1,3]
>          value     capital       _cons
> y1  -.00034265  -.00195236   .90952273
>
> . matrix list bdifp;
>
> bdifp[3,1]
>                  y1
>   value  -.00034265
> capital  -.00195236
>   _cons   .90952273
>
> . matrix list dvi;
>
> symmetric dvi[3,3]
>               value     capital       _cons
>   value  -7739.3615
> capital   5808.2905   -5305.811
>   _cons   3.6641311   .98569198  -.00051157
>
> . matrix chisq=bdif*dvi*bdifp;
>
> . matrix list chisq;
>
> symmetric chisq[1,1]
>             y1
> y1  -.01956929
> ###Hausman canned###
> .  hausman femod remod;
>
>                  ---- Coefficients ----
>              |      (b)          (B)            (b-B)
> sqrt(diag(V_b-V_B))
>              |     femod        remod        Difference          S.E.
>
> -------------+----------------------------------------------------------------
>        value |    .1101238     .1097811        .0003427               .
>      capital |    .3100653      .308113        .0019524        .0089965
>
> ------------------------------------------------------------------------------
>                            b = consistent under Ho and Ha; obtained from
> xtreg
>             B = inconsistent under Ha, efficient under Ho; obtained from
> xtreg
>
>     Test:  Ho:  difference in coefficients not systematic
>
>                   chi2(2) = (b-B)'[(V_b-V_B)^(-1)](b-B)
>                           =    -0.01    chi2<0 ==> model fitted on these
>                                         data fails to meet the asymptotic
>                                         assumptions of the Hausman test;
>                                         see suest for a generalized test ##
> end Stata9.2 output ##
>
> ##begin Output R, using PLM 1.1-2###
>
> > test<-data(Grunfeld, package="Ecdat")
> >
> > fm <- inv~value+capital
> > femod <- plm(fm, Grunfeld, model="within")
> > summary(femod)
> Oneway (individual) effect Within Model
>
> Call:
> plm(formula = fm, data = Grunfeld, model = "within")
>
> Balanced Panel: n=10, T=20, N=200
>
> Residuals :
>     Min.  1st Qu.   Median  3rd Qu.     Max.
> -184.000  -17.600    0.563   19.200  251.000
>
> Coefficients :
>         Estimate Std. Error t-value  Pr(>|t|)
> value   0.110124   0.011857  9.2879 < 2.2e-16 ***
> capital 0.310065   0.017355 17.8666 < 2.2e-16 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Total Sum of Squares:    2244400
> Residual Sum of Squares: 523480
> F-statistic: 309.014 on 2 and 188 DF, p-value: < 2.22e-16
>
> > remod <- plm(fm, Grunfeld, model="random")
> > summary(remod)
> Oneway (individual) effect Random Effect Model
>    (Swamy-Arora's transformation)
>
> Call:
> plm(formula = fm, data = Grunfeld, model = "random")
>
> Balanced Panel: n=10, T=20, N=200
>
> Effects:
>                    var  std.dev share
> idiosyncratic 2784.458   52.768 0.282
> individual    7089.800   84.201 0.718
> theta:  0.86122
>
> Residuals :
>    Min. 1st Qu.  Median 3rd Qu.    Max.
> -178.00  -19.70    4.69   19.50  253.00
>
> Coefficients :
>               Estimate Std. Error t-value Pr(>|t|)
> (Intercept) -57.834415  28.898935 -2.0013  0.04536 *
> value         0.109781   0.010493 10.4627  < 2e-16 ***
> capital       0.308113   0.017180 17.9339  < 2e-16 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Total Sum of Squares:    2381400
> Residual Sum of Squares: 548900
> F-statistic: 328.837 on 2 and 197 DF, p-value: < 2.22e-16
> > phtest(femod, remod)
>
>         Hausman Test
>
> data:  fm
> chisq = 2.3304, df = 2, p-value = 0.3119
> alternative hypothesis: one model is inconsistent
>
> ###end Plm###
>
>
>
>
>
> On Mon, May 18, 2009 at 6:01 AM, Millo Giovanni <
> Giovanni_Millo at generali.com> wrote:
>
>> Dear Steve,
>>
>> I got your inquiry courtesy of Christian Kleiber, who brought it to our
>> attention: please next time you post anything re a given package,
>> include the maintainer's address. We cannot guarantee to parse all the
>> daily digests of the R system!
>>
>> Your problem: can you please provide a reproducible example? Else it is
>> difficult to help, not knowing your data, your results and even the
>> Stata version you're using.
>>
>> In the following I replicate what you might have done on a well-known
>> dataset.
>>
>> From Stata10, on the usual Grunfeld data taken from package "Ecdat":
>>
>> ## begin Stata10 output ##
>> . xtreg inv value capital
>>
>> Random-effects GLS regression                   Number of obs      =
>> 200
>> Group variable: firm                            Number of groups   =
>> 10
>>
>> R-sq:  within  = 0.7668                         Obs per group: min =
>> 20
>>       between = 0.8196                                        avg =
>> 20.0
>>       overall = 0.8061                                        max =
>> 20
>>
>> Random effects u_i ~ Gaussian                   Wald chi2(2)       =
>> 657.67
>> corr(u_i, X)       = 0 (assumed)                Prob > chi2        =
>> 0.0000
>>
>> ------------------------------------------------------------------------
>> ------
>>         inv |      Coef.   Std. Err.      z    P>|z|     [95% Conf.
>> Interval]
>> -------------+----------------------------------------------------------
>> ------
>>       value |   .1097811   .0104927    10.46   0.000     .0892159
>> .1303464
>>     capital |    .308113   .0171805    17.93   0.000     .2744399
>> .3417861
>>       _cons |  -57.83441   28.89893    -2.00   0.045    -114.4753
>> -1.193537
>> -------------+----------------------------------------------------------
>> ------
>>     sigma_u |   84.20095
>>     sigma_e |  52.767964
>>         rho |  .71800838   (fraction of variance due to u_i)
>> ------------------------------------------------------------------------
>> ------
>>
>> . estimates store remod
>>
>> . xtreg inv value capital, fe
>>
>> Fixed-effects (within) regression               Number of obs      =
>> 200
>> Group variable: firm                            Number of groups   =
>> 10
>>
>> R-sq:  within  = 0.7668                         Obs per group: min =
>> 20
>>       between = 0.8194                                        avg =
>> 20.0
>>       overall = 0.8060                                        max =
>> 20
>>
>>                                                F(2,188)           =
>> 309.01
>> corr(u_i, Xb)  = -0.1517                        Prob > F           =
>> 0.0000
>>
>> ------------------------------------------------------------------------
>> ------
>>         inv |      Coef.   Std. Err.      t    P>|t|     [95% Conf.
>> Interval]
>> -------------+----------------------------------------------------------
>> ------
>>       value |   .1101238   .0118567     9.29   0.000     .0867345
>> .1335131
>>     capital |   .3100653   .0173545    17.87   0.000     .2758308
>> .3442999
>>       _cons |  -58.74393   12.45369    -4.72   0.000    -83.31086
>> -34.177
>> -------------+----------------------------------------------------------
>> ------
>>     sigma_u |  85.732501
>>     sigma_e |  52.767964
>>         rho |  .72525012   (fraction of variance due to u_i)
>> ------------------------------------------------------------------------
>> ------
>> F test that all u_i=0:     F(9, 188) =    49.18              Prob > F =
>> 0.0000
>>
>> . estimates store femod
>>
>> . hausman femod remod
>>
>>                 ---- Coefficients ----
>>             |      (b)          (B)            (b-B)
>> sqrt(diag(V_b-V_B))
>>             |     femod        remod        Difference          S.E.
>> -------------+----------------------------------------------------------
>> ------
>>       value |    .1101238     .1097811        .0003427        .0055213
>>     capital |    .3100653      .308113        .0019524        .0024516
>> ------------------------------------------------------------------------
>> ------
>>                           b = consistent under Ho and Ha; obtained from
>> xtreg
>>            B = inconsistent under Ha, efficient under Ho; obtained from
>> xtreg
>>
>>    Test:  Ho:  difference in coefficients not systematic
>>
>>                  chi2(2) = (b-B)'[(V_b-V_B)^(-1)](b-B)
>>                          =        2.33
>>                Prob>chi2 =      0.3119
>>
>> .
>> ## end Stata10 output ##
>>
>> while from plm I get
>>
>> ## begin R putput ##
>> > data(Grunfeld, package="Ecdat")
>> > fm <- inv~value+capital
>> >
>> > femod <- plm(fm, Grunfeld)
>> > remod <- plm(fm, Grunfeld, model="random")
>> >
>> > phtest(femod, remod)
>>
>>        Hausman Test
>>
>> data:  fm
>> chisq = 2.3304, df = 2, p-value = 0.3119
>> alternative hypothesis: one model is inconsistent
>>
>> ## end R output ##
>>
>> which, besides testifying to the goodness and parsimony of an
>> object-oriented approach as far as screen output is concerned, looks
>> rather consistent to me.
>>
>> I cannot but guess that the problem might stem from different RE
>> estimates: previous versions of Stata used the Wallace-Hussein method by
>> default for computing the variance of random effects. Now Stata uses
>> Swamy-Arora, which has been the default of 'plm' since the beginning.
>> Yet as plm() allows to choose, you can experiment with different values
>> for the 'random.method' argument in order to see if you get the Stata
>> result. I suggest you start by comparing the coefficient estimates you
>> get from Stata and R: FE should be unambiguous, RE might vary as said
>> above, and for good reason.
>>
>> You also didn't tell us whether your by-hand calculation agrees with
>> phtest() output? (I guess it does not)
>>
>> Please let us know, possibly with a reproducible example and providing
>> all the above info
>> Giovanni
>>
>> PS please also make sure you're not using any VEEEEERY old version of
>> 'plm' (prior to, say, 0.3): these had a bug in the p-value calculation
>> which made it depend on the order of models compared (so that in the
>> wrong case you got p.value=1).
>>
>> Giovanni Millo
>> Research Dept.,
>> Assicurazioni Generali SpA
>> Via Machiavelli 4,
>> 34132 Trieste (Italy)
>> tel. +39 040 671184
>> fax  +39 040 671160
>>
>> > ----------------------------------------------------------------------
>> > --
>> >
>> > Subject:
>> > [R-SIG-Finance] Chi-sq Hausman test---R vs Stata
>> > From:
>> > Steven Archambault <archstevej at gmail.com>
>> > Date:
>> > Sun, 17 May 2009 23:14:13 -0600
>> > To:
>> > r-sig-finance at stat.math.ethz.ch
>> >
>> > To:
>> > r-sig-finance at stat.math.ethz.ch
>> >
>> >
>> > Hi all,
>> >
>> > I am running a panel time series regression testing Fixed Effects and
>> > Random Effects. I decided to calculate the chi-sq value for the
>> > Hausman test in both R (Phtest) and Stata. I get different results.
>> > Even within Stata, calculating the Chi-sq value with the canned
>> > procedure or by hand (using
>> > matrices) gives different results. So, the question should come up
>> there as
>> > well.
>> >
>> > Does anybody have any insight on how to pick which results to use? I
>> > guess the one that gives the result I want? Having different programs
>> > give quite different values for the same tests is frustrating me.  I'd
>>
>> > be interested in any feedback folks have!
>> >
>> > Thanks,
>> > Steve
>> >
>> >       [[alternative HTML version deleted]]
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090518/c6b65272/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: fdi_data.csv
Type: application/vnd.ms-excel
Size: 174320 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090518/c6b65272/attachment.xlb>

From arupsarkar at yahoo.com  Tue May 19 02:26:51 2009
From: arupsarkar at yahoo.com (Sarkar, Arup)
Date: Mon, 18 May 2009 20:26:51 -0400
Subject: [R-SIG-Finance] positions in timeSeries object
References: <00dc01c9cdfc$56eec0f0$04cc42d0$@com>
	<4A05E4D1.8010508@prodsyse.com> 
Message-ID: <030201c9d818$81747ec0$845d7c40$@com>

Spencer: If you could forward the solution to my problem I would be
grateful. I am still stuck at the same problem.

Regards
Arup

-----Original Message-----
From: Sarkar, Arup [mailto:arupsarkar at yahoo.com] 
Sent: Monday, May 11, 2009 11:05 PM
To: 'spencerg'
Cc: 'r-sig-finance at stat.math.ethz.ch'
Subject: RE: [R-SIG-Finance] positions in timeSeries object

Spencer: Thanks very much for the response, I did install RSiteSearch,
however when I am trying out 
tD <- RSiteSearch.function('timeDate') command I am getting the following
error.

> tD <- RSiteSearch.function('timeDate')
Error: could not find function "RSiteSearch.function"
 Do I have to install anything else?

Regards
Arup

-----Original Message-----
From: spencerg [mailto:spencer.graves at prodsyse.com] 
Sent: Saturday, May 09, 2009 4:17 PM
To: Sarkar, Arup
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] positions in timeSeries object

      First you need to examine the results of each step to isolate the 
problem. 


      When I tried to replicate your problem, I first replaced 
"as.data.frame(scan(...))" with the following: 


DF <- read.table('ibm1.txt', header=TRUE, sep='|')


      I also replaced "df" by "DF", because "df" is the function for the 
density of an F distribution, and I'd prefer not to mask that with a 
data.frame. 


      After that, I solved the problem by studying the examples in the 
help file for "timeDate". 


      To get there, however, I found that there were two functions 
called "timeDate", in packages "timeDate" and "fCalendar".  I found this 
as follows: 


library(RSiteSearch)
tD <- RSiteSearch.function('timeDate')
sum(tD$Function=='timeDate')
HTML(tD[tD$Function=='timeDate', ])


     These two functions are probably identical, but I don't know that. 
     
      Hope this helps. 
      Spencer Graves

Sarkar, Arup wrote:
> Hi: I am trying the following code to incorporate positions in the time
> series object. Can some please help me.
>
>  
>
> Data File Format:
>
>  
>
> SYMBOL|DATE|EX|TIME|PRICE|SIZE|COND|CORR|G127
>
> IBM|11/03/2008|N|9:30:07|93.0800|73600|@|0|0
>
> IBM|11/03/2008|N|9:30:07|92.9700|500|@|0|0
>
> IBM|11/03/2008|N|9:30:07|93.1100|100|@|0|0
>
> IBM|11/03/2008|N|9:30:07|92.9700|100|@|0|0
>
> IBM|11/03/2008|N|9:30:07|92.8500|200|@|0|0
>
> IBM|11/03/2008|N|9:30:07|92.8200|100|@|0|0
>
> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>
> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>
> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>
> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>
> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>
> IBM|11/03/2008|N|9:30:08|93.0000|100|@|0|0
>
>  
>
> R script:
>
>  
>
> ## Columns Names
>
> fields.list =
> list(Symbol="",Date="",Ex="",Time="",Price=0,Size=0,Cond="",Corr=0,G127=0)
>
> #create a data frame
>
> df =
>
as.data.frame(scan(file="c:/ibm_1.txt",what=fields.list,sep="|",skip=1,multi
> .line=TRUE,strip.white=TRUE),stringsAsFactors=F)
>
> dates.tmp = timeDate(charvec = df[, "Date"])
>
> times.tmp = df[, "Time"]
>
> td.tmp = paste(dates.tmp,times.tmp, sep=" ")
>
> ans = timeSeries(data = df[, setdiff(colIds(df), c("Date","Time"))],pos =
> td.tmp)
>
>  
>
> I am getting the following output when I am using the following command
> ans[1:5, ]
>
>  
>
>        Symbol Ex  Price   Size      Cond Corr G127
>
>      1 "IBM"  "N" "93.08" "  73600" "@"  "0"  "0" 
>
>      2 "IBM"  "N" "92.97" "    500" "@"  "0"  "0" 
>
>      3 "IBM"  "N" "93.11" "    100" "@"  "0"  "0" 
>
>      4 "IBM"  "N" "92.97" "    100" "@"  "0"  "0" 
>
>      5 "IBM"  "N" "92.85" "    200" "@"  "0"  "0"
>
>  
>
> I want to have the following output, how can I achieve it. Any help is
> highly appreciated.
>
>  
>
>      Positions  Symbol Ex  Price   Size      Cond Corr G127
>
>       "11/03/2008 9:30:07" "IBM"  "N" "93.08" "  73600" "@"  "0"  "0" 
>
>      "11/03/2008 9:30:07" "IBM"  "N" "92.97" "    500" "@"  "0"  "0" 
>
>      "11/03/2008 9:30:07" "IBM"  "N" "93.11" "    100" "@"  "0"  "0" 
>
>      "11/03/2008 9:30:07" "IBM"  "N" "92.97" "    100" "@"  "0"  "0" 
>
>      "11/03/2008 9:30:07" "IBM"  "N" "92.85" "    200" "@"  "0"  "0"
>
>  
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From spencer.graves at prodsyse.com  Tue May 19 02:43:56 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Mon, 18 May 2009 17:43:56 -0700
Subject: [R-SIG-Finance] positions in timeSeries object
In-Reply-To: <030201c9d818$81747ec0$845d7c40$@com>
References: <00dc01c9cdfc$56eec0f0$04cc42d0$@com>
	<4A05E4D1.8010508@prodsyse.com>
	<030201c9d818$81747ec0$845d7c40$@com>
Message-ID: <4A1200CC.1020808@prodsyse.com>

      You need first "install.packages('RSiteSearch')". 

      Then "library(RSiteSearch);  tD <- 
RSiteSearch.function('timeDate')" should work. 

      Spencer

Sarkar, Arup wrote:
> Spencer: If you could forward the solution to my problem I would be
> grateful. I am still stuck at the same problem.
>
> Regards
> Arup
>
> -----Original Message-----
> From: Sarkar, Arup [mailto:arupsarkar at yahoo.com] 
> Sent: Monday, May 11, 2009 11:05 PM
> To: 'spencerg'
> Cc: 'r-sig-finance at stat.math.ethz.ch'
> Subject: RE: [R-SIG-Finance] positions in timeSeries object
>
> Spencer: Thanks very much for the response, I did install RSiteSearch,
> however when I am trying out 
> tD <- RSiteSearch.function('timeDate') command I am getting the following
> error.
>
>   
>> tD <- RSiteSearch.function('timeDate')
>>     
> Error: could not find function "RSiteSearch.function"
>  Do I have to install anything else?
>
> Regards
> Arup
>
> -----Original Message-----
> From: spencerg [mailto:spencer.graves at prodsyse.com] 
> Sent: Saturday, May 09, 2009 4:17 PM
> To: Sarkar, Arup
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] positions in timeSeries object
>
>       First you need to examine the results of each step to isolate the 
> problem. 
>
>
>       When I tried to replicate your problem, I first replaced 
> "as.data.frame(scan(...))" with the following: 
>
>
> DF <- read.table('ibm1.txt', header=TRUE, sep='|')
>
>
>       I also replaced "df" by "DF", because "df" is the function for the 
> density of an F distribution, and I'd prefer not to mask that with a 
> data.frame. 
>
>
>       After that, I solved the problem by studying the examples in the 
> help file for "timeDate". 
>
>
>       To get there, however, I found that there were two functions 
> called "timeDate", in packages "timeDate" and "fCalendar".  I found this 
> as follows: 
>
>
> library(RSiteSearch)
> tD <- RSiteSearch.function('timeDate')
> sum(tD$Function=='timeDate')
> HTML(tD[tD$Function=='timeDate', ])
>
>
>      These two functions are probably identical, but I don't know that. 
>      
>       Hope this helps. 
>       Spencer Graves
>
> Sarkar, Arup wrote:
>   
>> Hi: I am trying the following code to incorporate positions in the time
>> series object. Can some please help me.
>>
>>  
>>
>> Data File Format:
>>
>>  
>>
>> SYMBOL|DATE|EX|TIME|PRICE|SIZE|COND|CORR|G127
>>
>> IBM|11/03/2008|N|9:30:07|93.0800|73600|@|0|0
>>
>> IBM|11/03/2008|N|9:30:07|92.9700|500|@|0|0
>>
>> IBM|11/03/2008|N|9:30:07|93.1100|100|@|0|0
>>
>> IBM|11/03/2008|N|9:30:07|92.9700|100|@|0|0
>>
>> IBM|11/03/2008|N|9:30:07|92.8500|200|@|0|0
>>
>> IBM|11/03/2008|N|9:30:07|92.8200|100|@|0|0
>>
>> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>>
>> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>>
>> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>>
>> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>>
>> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>>
>> IBM|11/03/2008|N|9:30:08|93.0000|100|@|0|0
>>
>>  
>>
>> R script:
>>
>>  
>>
>> ## Columns Names
>>
>> fields.list =
>> list(Symbol="",Date="",Ex="",Time="",Price=0,Size=0,Cond="",Corr=0,G127=0)
>>
>> #create a data frame
>>
>> df =
>>
>>     
> as.data.frame(scan(file="c:/ibm_1.txt",what=fields.list,sep="|",skip=1,multi
>   
>> .line=TRUE,strip.white=TRUE),stringsAsFactors=F)
>>
>> dates.tmp = timeDate(charvec = df[, "Date"])
>>
>> times.tmp = df[, "Time"]
>>
>> td.tmp = paste(dates.tmp,times.tmp, sep=" ")
>>
>> ans = timeSeries(data = df[, setdiff(colIds(df), c("Date","Time"))],pos =
>> td.tmp)
>>
>>  
>>
>> I am getting the following output when I am using the following command
>> ans[1:5, ]
>>
>>  
>>
>>        Symbol Ex  Price   Size      Cond Corr G127
>>
>>      1 "IBM"  "N" "93.08" "  73600" "@"  "0"  "0" 
>>
>>      2 "IBM"  "N" "92.97" "    500" "@"  "0"  "0" 
>>
>>      3 "IBM"  "N" "93.11" "    100" "@"  "0"  "0" 
>>
>>      4 "IBM"  "N" "92.97" "    100" "@"  "0"  "0" 
>>
>>      5 "IBM"  "N" "92.85" "    200" "@"  "0"  "0"
>>
>>  
>>
>> I want to have the following output, how can I achieve it. Any help is
>> highly appreciated.
>>
>>  
>>
>>      Positions  Symbol Ex  Price   Size      Cond Corr G127
>>
>>       "11/03/2008 9:30:07" "IBM"  "N" "93.08" "  73600" "@"  "0"  "0" 
>>
>>      "11/03/2008 9:30:07" "IBM"  "N" "92.97" "    500" "@"  "0"  "0" 
>>
>>      "11/03/2008 9:30:07" "IBM"  "N" "93.11" "    100" "@"  "0"  "0" 
>>
>>      "11/03/2008 9:30:07" "IBM"  "N" "92.97" "    100" "@"  "0"  "0" 
>>
>>      "11/03/2008 9:30:07" "IBM"  "N" "92.85" "    200" "@"  "0"  "0"
>>
>>  
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>   
>>     
>
>


From rlg at opendatagroup.com  Tue May 19 05:08:53 2009
From: rlg at opendatagroup.com (Robert Grossman)
Date: Mon, 18 May 2009 22:08:53 -0500
Subject: [R-SIG-Finance] some notes on using R in Amazon's EC2
Message-ID: <273c9b0b0905182008i1f2fda6dvde82b913158cc219@mail.gmail.com>

This is a follow up to a lecture I gave at the recent R/Finance 2009
Conference in Chicago
http://rinfinance.quantmod.com/.

I have prepared some informal notes for those familar with R but not
with Amazon's EC2 service about using R with EC2.  Please see:

http://blog.rgrossman.com/2009/05/17/running-r-on-amazons-ec2/

There is also a publically available Amazon AMI with R and several
time series packages installed.
The manifest is "opendatagroup/r-timeseries.manifest.xml" and the AMI
instance ?ami-ea846283?.  The notes contain more details.

I'll be revising the notes, so I'm interested in any feedback.

--Bob

Robert Grossman

Open Data Group &
University of Illinois at Chicago
blog.rgrossman.com


From j..... Tue May 19 11:10:50 2009
From: j. <j..@....org>
Date: Tue, 19 May 2009 10:10:50 +0100
Subject: [R-SIG-Finance] JOB: Permanent C++ Programmer in London, England, UK
Message-ID: <4A12779A.6060004@camalyn.org>

Hello ~

I'm recruiting for a hedge fund (type) business (based in London,
England, UK) that are looking for good C++ programmers with preferably
some experience in statistical modeling/programming/analysis.   Perhaps
you are a statistician, mathematician or physicist (for example) by
background but a programmer by experience (or a graduate)?


There is much more to the job than just scripting and programming,
potentially you could find yourself becoming actively involved in
statistical 	side too of the business too.

To learn more please contact me using james<at>....org

Kind regards,
JAMES

. . . .

James T...
......

....................


From shane.conway at gmail.com  Tue May 19 15:22:21 2009
From: shane.conway at gmail.com (Shane Conway)
Date: Tue, 19 May 2009 09:22:21 -0400
Subject: [R-SIG-Finance] positions in timeSeries object
In-Reply-To: <030201c9d818$81747ec0$845d7c40$@com>
References: <00dc01c9cdfc$56eec0f0$04cc42d0$@com>
	<4A05E4D1.8010508@prodsyse.com> 
	<030201c9d818$81747ec0$845d7c40$@com>
Message-ID: <dd3243090905190622m736652d8n53136b3b9a0a4638@mail.gmail.com>

You just need to format your input dates.

For instance:

> strptime('11/03/2008 9:30:07', "%m/%d/%Y %H:%M:%S")
[1] "2008-11-03 09:30:07 GMT"

Or using the Rmetrics function:

> strptimeDate('11/03/2008 9:30:07', "%m/%d/%Y %H:%M:%S")
GMT
[1] [2008-11-03 09:30:07]

The default format is usually yyyy-mm-dd hh:mm:ss.  Just make sure you
account for timezone as well.



On Mon, May 18, 2009 at 8:26 PM, Sarkar, Arup <arupsarkar at yahoo.com> wrote:
> Spencer: If you could forward the solution to my problem I would be
> grateful. I am still stuck at the same problem.
>
> Regards
> Arup
>
> -----Original Message-----
> From: Sarkar, Arup [mailto:arupsarkar at yahoo.com]
> Sent: Monday, May 11, 2009 11:05 PM
> To: 'spencerg'
> Cc: 'r-sig-finance at stat.math.ethz.ch'
> Subject: RE: [R-SIG-Finance] positions in timeSeries object
>
> Spencer: Thanks very much for the response, I did install RSiteSearch,
> however when I am trying out
> tD <- RSiteSearch.function('timeDate') command I am getting the following
> error.
>
>> tD <- RSiteSearch.function('timeDate')
> Error: could not find function "RSiteSearch.function"
> ?Do I have to install anything else?
>
> Regards
> Arup
>
> -----Original Message-----
> From: spencerg [mailto:spencer.graves at prodsyse.com]
> Sent: Saturday, May 09, 2009 4:17 PM
> To: Sarkar, Arup
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] positions in timeSeries object
>
> ? ? ?First you need to examine the results of each step to isolate the
> problem.
>
>
> ? ? ?When I tried to replicate your problem, I first replaced
> "as.data.frame(scan(...))" with the following:
>
>
> DF <- read.table('ibm1.txt', header=TRUE, sep='|')
>
>
> ? ? ?I also replaced "df" by "DF", because "df" is the function for the
> density of an F distribution, and I'd prefer not to mask that with a
> data.frame.
>
>
> ? ? ?After that, I solved the problem by studying the examples in the
> help file for "timeDate".
>
>
> ? ? ?To get there, however, I found that there were two functions
> called "timeDate", in packages "timeDate" and "fCalendar". ?I found this
> as follows:
>
>
> library(RSiteSearch)
> tD <- RSiteSearch.function('timeDate')
> sum(tD$Function=='timeDate')
> HTML(tD[tD$Function=='timeDate', ])
>
>
> ? ? These two functions are probably identical, but I don't know that.
>
> ? ? ?Hope this helps.
> ? ? ?Spencer Graves
>
> Sarkar, Arup wrote:
>> Hi: I am trying the following code to incorporate positions in the time
>> series object. Can some please help me.
>>
>>
>>
>> Data File Format:
>>
>>
>>
>> SYMBOL|DATE|EX|TIME|PRICE|SIZE|COND|CORR|G127
>>
>> IBM|11/03/2008|N|9:30:07|93.0800|73600|@|0|0
>>
>> IBM|11/03/2008|N|9:30:07|92.9700|500|@|0|0
>>
>> IBM|11/03/2008|N|9:30:07|93.1100|100|@|0|0
>>
>> IBM|11/03/2008|N|9:30:07|92.9700|100|@|0|0
>>
>> IBM|11/03/2008|N|9:30:07|92.8500|200|@|0|0
>>
>> IBM|11/03/2008|N|9:30:07|92.8200|100|@|0|0
>>
>> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>>
>> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>>
>> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>>
>> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>>
>> IBM|11/03/2008|N|9:30:08|92.7500|100|@|0|0
>>
>> IBM|11/03/2008|N|9:30:08|93.0000|100|@|0|0
>>
>>
>>
>> R script:
>>
>>
>>
>> ## Columns Names
>>
>> fields.list =
>> list(Symbol="",Date="",Ex="",Time="",Price=0,Size=0,Cond="",Corr=0,G127=0)
>>
>> #create a data frame
>>
>> df =
>>
> as.data.frame(scan(file="c:/ibm_1.txt",what=fields.list,sep="|",skip=1,multi
>> .line=TRUE,strip.white=TRUE),stringsAsFactors=F)
>>
>> dates.tmp = timeDate(charvec = df[, "Date"])
>>
>> times.tmp = df[, "Time"]
>>
>> td.tmp = paste(dates.tmp,times.tmp, sep=" ")
>>
>> ans = timeSeries(data = df[, setdiff(colIds(df), c("Date","Time"))],pos =
>> td.tmp)
>>
>>
>>
>> I am getting the following output when I am using the following command
>> ans[1:5, ]
>>
>>
>>
>> ? ? ? ?Symbol Ex ?Price ? Size ? ? ?Cond Corr G127
>>
>> ? ? ?1 "IBM" ?"N" "93.08" " ?73600" "@" ?"0" ?"0"
>>
>> ? ? ?2 "IBM" ?"N" "92.97" " ? ?500" "@" ?"0" ?"0"
>>
>> ? ? ?3 "IBM" ?"N" "93.11" " ? ?100" "@" ?"0" ?"0"
>>
>> ? ? ?4 "IBM" ?"N" "92.97" " ? ?100" "@" ?"0" ?"0"
>>
>> ? ? ?5 "IBM" ?"N" "92.85" " ? ?200" "@" ?"0" ?"0"
>>
>>
>>
>> I want to have the following output, how can I achieve it. Any help is
>> highly appreciated.
>>
>>
>>
>> ? ? ?Positions ?Symbol Ex ?Price ? Size ? ? ?Cond Corr G127
>>
>> ? ? ? "11/03/2008 9:30:07" "IBM" ?"N" "93.08" " ?73600" "@" ?"0" ?"0"
>>
>> ? ? ?"11/03/2008 9:30:07" "IBM" ?"N" "92.97" " ? ?500" "@" ?"0" ?"0"
>>
>> ? ? ?"11/03/2008 9:30:07" "IBM" ?"N" "93.11" " ? ?100" "@" ?"0" ?"0"
>>
>> ? ? ?"11/03/2008 9:30:07" "IBM" ?"N" "92.97" " ? ?100" "@" ?"0" ?"0"
>>
>> ? ? ?"11/03/2008 9:30:07" "IBM" ?"N" "92.85" " ? ?200" "@" ?"0" ?"0"
>>
>>
>>
>>
>> ? ? ? [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From Matthias.Koberstein at hsbctrinkaus.de  Tue May 19 17:21:15 2009
From: Matthias.Koberstein at hsbctrinkaus.de (Matthias.Koberstein at hsbctrinkaus.de)
Date: Tue, 19 May 2009 17:21:15 +0200
Subject: [R-SIG-Finance] Hamilton Filters
Message-ID: <OFA11C7CF9.9D47E76D-ONC12575BB.0053BF15-C12575BB.00545638@hsbctrinkaus.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090519/8b358335/attachment.pl>

From brian at braverock.com  Tue May 19 18:12:47 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 19 May 2009 11:12:47 -0500
Subject: [R-SIG-Finance] Hamilton Filters (and Kalman)
In-Reply-To: <OFA11C7CF9.9D47E76D-ONC12575BB.0053BF15-C12575BB.00545638@hsbctrinkaus.de>
References: <OFA11C7CF9.9D47E76D-ONC12575BB.0053BF15-C12575BB.00545638@hsbctrinkaus.de>
Message-ID: <4A12DA7F.4050406@braverock.com>

after some recent casting about on the several Kalman implementations in 
R, I suggest you take a close look at FKF:

http://cran.r-project.org/web/packages/FKF/index.html

I have no experience with the Hamilton filter.  Please share any code 
you come up with to implement it with the community.

Regards,

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock



Matthias.Koberstein at hsbctrinkaus.de wrote:
> Hello,
>
> just a quick question:
> Is there an example for application of Kalman filters in R? I found some 
> hints in the online search function on r-project.org but no real example. 
>
> Does anyone know where to find information on "Hamilton filters" as cited 
> in working paper No. 472 available under
>
> http://www.ecb.int/pub/scientific/wps/author/html/author598.en.html
>
> Thank you very much
>
> Matthias
>
>
> **** Ressourcen schonen, weniger drucken - Think before you print! ****
>
> ---------------------------------------------------------------------
> Diese E-Mail sowie eventuelle Anh??nge enthalten vertrauliche und / oder rechtlich gesch??tzte Informationen. Wenn Sie nicht der richtige Adressat sind oder diese E-Mail irrt??mlich erhalten haben, informieren Sie bitte sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren oder Speichern sowie die unbefugte Weitergabe dieser E-Mail sind nicht gestattet.
>
> This e-mail and any attachments may contain confidenti...{{dropped:13}}


From spencer.graves at prodsyse.com  Tue May 19 18:51:25 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Tue, 19 May 2009 09:51:25 -0700
Subject: [R-SIG-Finance] Hamilton Filters (and Kalman)
In-Reply-To: <4A12DA7F.4050406@braverock.com>
References: <OFA11C7CF9.9D47E76D-ONC12575BB.0053BF15-C12575BB.00545638@hsbctrinkaus.de>
	<4A12DA7F.4050406@braverock.com>
Message-ID: <4A12E38D.40600@prodsyse.com>

     If you have run "install.packages('RSiteSearch')" recently, I 
suggest you do so.  This will support the following: 


library(RSiteSearch)
k <- RSiteSearch.function('Kalman')
HTML(k)


      This identifies 60 help pages in 19 different packages that 
include the word "Kalman".  Another term for Kalman techniques is 
"dynamic linear models".  We can search for this and combine it with "k" 
as follows: 


dlm <- RSiteSearch.function('dynamic linear model')
dlms <- RSiteSearch.function('dynamic linear models')
# ".. model" and ".. models" may each find things the other misses. 
k. <- k | dlm | dlms
HTML(k.)


      By default, this sorts the 111 hits to display first the help 
pages in the package with the most hits.  In this case, "cts" appeared 
first with 12 hits, followed by "dlm" with 9.  I'm not familiar with 
"cts", but I've used "dlm".  It includes a vignette that helps someone 
get started.  A more complete description will be available in the 
soon-to-appear Petris, Petrone, and Campagnoli (2009) Dynamic Linear 
Models with R (Springer;  
http://www.amazon.com/Dynamic-Linear-Models-R-Use/dp/0387772375/ref=sr_1_1?ie=UTF8&s=books&qid=1242751460&sr=1-1).  



      "Hamilton Filter" is mentioned in 7 different help page identified 
by the following:      

hf <- RSiteSearch.function('Hamilton Filter')
HTML(hf)


      Hope this helps. 
      Spencer


Brian G. Peterson wrote:
> after some recent casting about on the several Kalman implementations 
> in R, I suggest you take a close look at FKF:
>
> http://cran.r-project.org/web/packages/FKF/index.html
>
> I have no experience with the Hamilton filter.  Please share any code 
> you come up with to implement it with the community.
>
> Regards,
>
>  - Brian
>


From ezivot at u.washington.edu  Tue May 19 19:12:35 2009
From: ezivot at u.washington.edu (Eric Zivot)
Date: Tue, 19 May 2009 10:12:35 -0700
Subject: [R-SIG-Finance] Hamilton Filters (and Kalman)
In-Reply-To: <4A12DA7F.4050406@braverock.com>
References: <OFA11C7CF9.9D47E76D-ONC12575BB.0053BF15-C12575BB.00545638@hsbctrinkaus.de>
	<4A12DA7F.4050406@braverock.com>
Message-ID: <003401c9d8a5$00d348e0$0279daa0$@washington.edu>

Actually, there are a lot of implementations of Kalman filters in R. For the
Hamilton filter, see the MSVAR package for an implementation. You can look
at some of the packages that implement hidden markov models (e.g.
hmm.discnp,  hsmm, HiddenMarkov) which implement MS type models where there
are only exogenous variables (no lagged dependent variables).

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Brian G.
Peterson
Sent: Tuesday, May 19, 2009 9:13 AM
To: Matthias.Koberstein at hsbctrinkaus.de
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Hamilton Filters (and Kalman)

after some recent casting about on the several Kalman implementations in 
R, I suggest you take a close look at FKF:

http://cran.r-project.org/web/packages/FKF/index.html

I have no experience with the Hamilton filter.  Please share any code 
you come up with to implement it with the community.

Regards,

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock



Matthias.Koberstein at hsbctrinkaus.de wrote:
> Hello,
>
> just a quick question:
> Is there an example for application of Kalman filters in R? I found some 
> hints in the online search function on r-project.org but no real example. 
>
> Does anyone know where to find information on "Hamilton filters" as cited 
> in working paper No. 472 available under
>
> http://www.ecb.int/pub/scientific/wps/author/html/author598.en.html
>
> Thank you very much
>
> Matthias
>
>
> **** Ressourcen schonen, weniger drucken - Think before you print! ****
>
> ---------------------------------------------------------------------
> Diese E-Mail sowie eventuelle Anh??nge enthalten vertrauliche und / oder
rechtlich gesch??tzte Informationen. Wenn Sie nicht der richtige Adressat
sind oder diese E-Mail irrt??mlich erhalten haben, informieren Sie bitte
sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren
oder Speichern sowie die unbefugte Weitergabe dieser E-Mail sind nicht
gestattet.
>
> This e-mail and any attachments may contain confidenti...{{dropped:13}}

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From archstevej at gmail.com  Tue May 19 21:16:30 2009
From: archstevej at gmail.com (Steven Archambault)
Date: Tue, 19 May 2009 13:16:30 -0600
Subject: [R-SIG-Finance] R: [Fwd: R-SIG-Finance Digest, Vol 60, Issue 18]
In-Reply-To: <74D924C9B7C148F587228E7D97044996@DellPC>
References: <4A1138D0.9050004@unibas.ch>
	<28643F754DDB094D8A875617EC4398B202AE7AD1@BEMAILEXTV03.corp.generali.net>
	<a575b07e0905181126m67cb8de2k6ee4c4153d69806@mail.gmail.com>
	<a575b07e0905181306v65067bd3s2313ccff9c55d4eb@mail.gmail.com>
	<74D924C9B7C148F587228E7D97044996@DellPC>
Message-ID: <a575b07e0905191216nf9272b2p835e69b7890f173@mail.gmail.com>

Oh, you are right. Here is the correct file. I sure have botched this query,
thanks for catching it Robert! Sorry for so many posts to the list.

Regards,
Steve



On Tue, May 19, 2009 at 12:19 PM, Robert Iquiapaza <rbali at ufmg.br> wrote:

>  Stev,
>
> The data you provided is not complete, lagdlfdi and laglnstock2000 are not
> in the csv file
>
> Robert
>
>  *From:* Steven Archambault <archstevej at gmail.com>
> *Sent:* Monday, May 18, 2009 5:06 PM
> *To:* Millo Giovanni <Giovanni_Millo at generali.com>
> *Cc:* r-sig-finance at stat.math.ethz.ch ; Yves Croissant<yves.croissant at let.ish-lyon.cnrs.fr>; Christian
> Kleiber <christian.kleiber at unibas.ch>
> *Subject:* Re: [R-SIG-Finance] R: [Fwd: R-SIG-Finance Digest, Vol 60,
> Issue 18]
>
> I just realized I used Robust in my Stata 9.2 analysis. When I remove this,
> the Chi-sq values are much closer to the values I get in R (but negative, as
> the consistent model must be listed first in a chi-sq calculation). However,
> with my own data I do get this positive definite error in Stata. Is this a
> result of unbalanced data? R doesn't give an error, so I am inclined to
> ignore it in Stata. I am posting my own results from R and Stata, and
> attaching the data as a csv.
>
> Thanks, hope I am not wasting too much of your time here.
>
> -Steve
>
> ###R-Output###
> > library("plm")
> >
> > fdi <- read.csv("C:/data/mydata.csv", na.strings=".")
> > fdiplm<-plm.data(fdi, index = c("id_code_id", "year"))
> series    are constants and have been removed
> >
> > fdi_test<-(lfdi_2000~ lagdlfdi+ laglnstock2000+ lagtradegdp +lagdlgdp)
> >
> > fdi_test_fe <- plm(fdi_test, data=fdiplm, model="within")
> > fdi_test_re <- plm(fdi_test, data=fdiplm, model="random")
> >
> > summary (fdi_test_fe)
> Oneway (individual) effect Within Model
>
> Call:
> plm(formula = fdi_test, data = fdiplm, model = "within")
>
> Unbalanced Panel: n=149, T=3-27, N=2697
>
> Residuals :
>    Min. 1st Qu.  Median 3rd Qu.    Max.
> -8.2100 -0.4760  0.0452  0.5670  4.8700
>
> Coefficients :
>                 Estimate Std. Error t-value  Pr(>|t|)
> lagdlfdi       0.1564759  0.0180645  8.6621 < 2.2e-16 ***
> laglnstock2000 0.7621350  0.0246798 30.8809 < 2.2e-16 ***
> lagtradegdp    0.0178568  0.0025859  6.9055 5.003e-12 ***
> lagdlgdp       0.2601477  0.0427744  6.0818 1.188e-09 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Total Sum of Squares:    4606.7
> Residual Sum of Squares: 2938
> F-statistic: 361.237 on 4 and 2544 DF, p-value: < 2.22e-16
> > summary (fdi_test_re)
> Oneway (individual) effect Random Effect Model
>    (Swamy-Arora's transformation)
>
> Call:
> plm(formula = fdi_test, data = fdiplm, model = "random")
>
> Unbalanced Panel: n=149, T=3-27, N=2697
>
> Effects:
>                   var std.dev  share
> idiosyncratic 1.15487 1.07465 0.6617
> individual    0.59044 0.76840 0.3383
> theta  :
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>  0.3718  0.6700  0.7081  0.6955  0.7355  0.7401
>
> Residuals :
>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
> -9.15000 -0.47900  0.07270 -0.00713  0.59800  3.95000
>
> Coefficients :
>                  Estimate Std. Error  t-value  Pr(>|t|)
> (Intercept)    16.7744214  0.1552868 108.0222 < 2.2e-16 ***
> lagdlfdi        0.1632388  0.0181005   9.0185 < 2.2e-16 ***
> laglnstock2000  0.8314432  0.0196444  42.3247 < 2.2e-16 ***
> lagtradegdp     0.0119453  0.0020737   5.7605 8.386e-09 ***
> lagdlgdp        0.2558009  0.0424599   6.0245 1.696e-09 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Total Sum of Squares:    9522.3
> Residual Sum of Squares: 3140.8
> F-statistic: 1367.42 on 4 and 2692 DF, p-value: < 2.22e-16
> >
> > phtest(fdi_test_re, fdi_test_fe)
>
>         Hausman Test
>
> data:  fdi_test
> chisq = 23.7021, df = 4, p-value = 9.164e-05
> alternative hypothesis: one model is inconsistent
>
>
> ###end R output###
>
> ###Stata 9.2 Output--canned###
> xtreg lfdi_2000 lagdlfdi laglnstock2000 lagtradegdp lagdlgdp, fe;
>
> Fixed-effects (within) regression               Number of obs      =
> 2697
> Group variable (i): id_code_id                  Number of groups   =
> 149
>
> R-sq:  within  = 0.3622                         Obs per group: min
> =         3
>        between = 0.8234                                        avg =
> 18.1
>        overall = 0.6998                                        max =
> 27
>
>                                                 F(4,2544)          =
> 361.24
> corr(u_i, Xb)  = 0.3536                         Prob > F           =
> 0.0000
>
>
> ------------------------------------------------------------------------------
>    lfdi_2000 |      Coef.   Std. Err.      t    P>|t|     [95% Conf.
> Interval]
>
> -------------+----------------------------------------------------------------
>     lagdlfdi |   .1564758   .0180645     8.66   0.000     .1210532
> .1918985
> laglnst~2000 |    .762135   .0246798    30.88   0.000     .7137404
> .8105295
>  lagtradegdp |   .0178568   .0025859     6.91   0.000     .0127861
> .0229274
>     lagdlgdp |   .2601478   .0427744     6.08   0.000     .1762716
> .3440241
>        _cons |   17.01131   .1701713    99.97   0.000     16.67762
> 17.345
>
> -------------+----------------------------------------------------------------
>      sigma_u |  .93048942
>      sigma_e |  1.0746505
>          rho |  .42847396   (fraction of variance due to u_i)
>
> ------------------------------------------------------------------------------
> F test that all u_i=0:     F(148, 2544) =    10.73           Prob > F =
> 0.0000
>
> . estimates store FIX, title(The FE) ;
>
> . xtreg lfdi_2000 lagdlfdi laglnstock2000 lagtradegdp lagdlgdp, re;
>
> Random-effects GLS regression                   Number of obs      =
> 2697
> Group variable (i): id_code_id                  Number of groups   =
> 149
>
> R-sq:  within  = 0.3606                         Obs per group: min
> =         3
>        between = 0.8402                                        avg =
> 18.1
>        overall = 0.7128                                        max =
> 27
>
> Random effects u_i ~ Gaussian                   Wald chi2(4)       =
> 2225.46
> corr(u_i, X)       = 0 (assumed)                Prob > chi2        =
> 0.0000
>
>
> ------------------------------------------------------------------------------
>    lfdi_2000 |      Coef.   Std. Err.      z    P>|z|     [95% Conf.
> Interval]
>
> -------------+----------------------------------------------------------------
>     lagdlfdi |   .1631662   .0180937     9.02   0.000     .1277032
> .1986291
> laglnst~2000 |    .830845   .0196843    42.21   0.000     .7922645
> .8694255
>  lagtradegdp |    .011992   .0020779     5.77   0.000     .0079195
> .0160645
>     lagdlgdp |   .2558113   .0424486     6.03   0.000     .1726136
> .3390091
>        _cons |   16.77702   .1556693   107.77   0.000     16.47191
> 17.08212
>
> -------------+----------------------------------------------------------------
>      sigma_u |  .77431228
>      sigma_e |  1.0746505
>          rho |  .34173973   (fraction of variance due to u_i)
>
> ------------------------------------------------------------------------------
>
> .  estimates store RAND, title(The RE) ;
>
> . hausman FIX RAND;
>
>                  ---- Coefficients ----
>              |      (b)          (B)            (b-B)
> sqrt(diag(V_b-V_B))
>              |      FIX          RAND        Difference          S.E.
>
> -------------+----------------------------------------------------------------
>     lagdlfdi |    .1564758     .1631662       -.0066903               .
> laglnst~2000 |     .762135      .830845         -.06871         .014887
>  lagtradegdp |    .0178568      .011992        .0058648        .0015393
>     lagdlgdp |    .2601478     .2558113        .0043365        .0052695
>
> ------------------------------------------------------------------------------
>                            b = consistent under Ho and Ha; obtained from
> xtreg
>             B = inconsistent under Ha, efficient under Ho; obtained from
> xtreg
>
>     Test:  Ho:  difference in coefficients not systematic
>
>                   chi2(4) = (b-B)'[(V_b-V_B)^(-1)](b-B)
>                           =       22.94
>                 Prob>chi2 =      0.0001
>                 (V_b-V_B is not positive definite)
> ###End Stata 9.2####
>
>
>
>
>
>
>
> On Mon, May 18, 2009 at 12:26 PM, Steven Archambault <archstevej at gmail.com
> > wrote:
>
>> Giovani,
>>
>> Thank you so much for your comments. I am a bit new to R, and to these
>> mailing lists, so I apologize for being sparse on the details and examples.
>> I am using Stata 9.2, which might be the answer to my problem, as you
>> described. I have done quite a bit of internet searching, and did not read
>> anywhere about the use of a different method for calculating the chi-sq
>> value, so thanks for that.
>>
>>  One more issue I have been thinking about. I am assuming your Plm
>> package knows that the FE is the consistient model, as the same results
>> arrive if the code is phtest(femod, remod) or phtest(remod, femod). The
>> order does matter in Stata.
>>
>> For complteness I am going to post my results using the same Grumfeld
>> dataset for both stata 9.2 (by hand calculation and canned procedure) and
>> R.  I am using the Plm package version 1 1-2.
>>
>> Regards,
>> Steve
>>
>>
>>
>>  ## begin Stata9.2 output##
>> xtreg inv value capital, robust re;
>>
>> Random-effects GLS regression                   Number of obs      =
>> 200
>> Group variable (i): firmid                      Number of groups
>> =        10
>>
>> R-sq:  within  = 0.7668                         Obs per group: min
>> =        20
>>        between = 0.8196                                        avg =
>> 20.0
>>        overall = 0.8061                                        max
>> =        20
>>
>> Random effects u_i ~ Gaussian                   Wald chi2(3)       =
>> 77.70
>>
>> corr(u_i, X)       = 0 (assumed)                Prob > chi2        =
>> 0.0000
>>
>>
>> ------------------------------------------------------------------------------
>>              |               Robust
>>       invest |      Coef.   Std. Err.      z    P>|z|     [95% Conf.
>> Interval]
>>
>> -------------+----------------------------------------------------------------
>>        value |   .1097811   .0197587     5.56   0.000     .0710547
>> .1485076
>>      capital |    .308113   .0418387     7.36   0.000     .2261107
>> .3901153
>>        _cons |  -57.83441   24.67795    -2.34   0.019    -106.2023
>> -9.466507
>>
>> -------------+----------------------------------------------------------------
>>
>>      sigma_u |   84.20095
>>      sigma_e |  52.767964
>>          rho |  .71800838   (fraction of variance due to u_i)
>>
>> ------------------------------------------------------------------------------
>>
>> . matrix bfe=e(b);
>>
>> . matrix vfe=e(V);
>>
>> . estimates store remod;
>>
>> . xtreg inv value capital, robust fe;
>>
>> Fixed-effects (within) regression               Number of obs      =
>> 200
>> Group variable (i): firmid                      Number of groups
>> =        10
>>
>> R-sq:  within  = 0.7668                         Obs per group: min
>> =        20
>>        between = 0.8194                                        avg =
>> 20.0
>>        overall = 0.8060                                        max
>> =        20
>>
>>                                                 F(2,188)           =
>> 40.23
>>
>> corr(u_i, Xb)  = -0.1517                        Prob > F           =
>> 0.0000
>>
>> ------------------------------------------------------------------------------
>>              |               Robust
>>       invest |      Coef.   Std. Err.      t    P>|t|     [95% Conf.
>> Interval]
>>
>> -------------+----------------------------------------------------------------
>>        value |   .1101238    .019378     5.68   0.000     .0718975
>> .1483501
>>      capital |   .3100653    .042795     7.25   0.000     .2256452
>> .3944854
>>        _cons |  -58.74393   23.37422    -2.51   0.013    -104.8534
>> -12.63449
>> -------------+----------------------------------------------------------------
>>
>>      sigma_u |  85.732501
>>      sigma_e |  52.767964
>>          rho |  .72525012   (fraction of variance due to u_i)
>>
>> ------------------------------------------------------------------------------
>>
>>  ###Hausman by hand###
>>
>> . estimates store femod;
>>
>> . matrix vre=e(V);
>>
>> . matrix bre=e(b);
>>
>> . matrix bdif=bfe-bre;
>>
>> . matrix list bdif;
>>
>> bdif[1,3]
>>          value     capital       _cons
>> y1  -.00034265  -.00195236   .90952273
>>
>> . matrix bdifp=bdif';
>>
>> . matrix dv=vfe-vre;
>>
>> . matrix dvi=inv(dv);
>>
>> . matrix list bdif;
>>
>> bdif[1,3]
>>          value     capital       _cons
>> y1  -.00034265  -.00195236   .90952273
>>
>> . matrix list bdifp;
>>
>> bdifp[3,1]
>>                  y1
>>   value  -.00034265
>> capital  -.00195236
>>   _cons   .90952273
>>
>> . matrix list dvi;
>>
>> symmetric dvi[3,3]
>>               value     capital       _cons
>>   value  -7739.3615
>> capital   5808.2905   -5305.811
>>   _cons   3.6641311   .98569198  -.00051157
>>
>> . matrix chisq=bdif*dvi*bdifp;
>>
>> . matrix list chisq;
>>
>> symmetric chisq[1,1]
>>             y1
>> y1  -.01956929
>> ###Hausman canned###
>> .  hausman femod remod;
>>
>>                  ---- Coefficients ----
>>              |      (b)          (B)            (b-B)
>> sqrt(diag(V_b-V_B))
>>              |     femod        remod        Difference          S.E.
>>
>> -------------+----------------------------------------------------------------
>>        value |    .1101238     .1097811        .0003427               .
>>      capital |    .3100653      .308113        .0019524        .0089965
>> ------------------------------------------------------------------------------
>>
>>                            b = consistent under Ho and Ha; obtained from
>> xtreg
>>             B = inconsistent under Ha, efficient under Ho; obtained from
>> xtreg
>>
>>     Test:  Ho:  difference in coefficients not systematic
>>
>>                   chi2(2) = (b-B)'[(V_b-V_B)^(-1)](b-B)
>>                           =    -0.01    chi2<0 ==> model fitted on these
>>                                         data fails to meet the asymptotic
>>                                         assumptions of the Hausman test;
>>                                         see suest for a generalized test ##
>> end Stata9.2 output ##
>>
>> ##begin Output R, using PLM 1.1-2###
>>
>> > test<-data(Grunfeld, package="Ecdat")
>> >
>> > fm <- inv~value+capital
>> > femod <- plm(fm, Grunfeld, model="within")
>> > summary(femod)
>> Oneway (individual) effect Within Model
>>
>> Call:
>> plm(formula = fm, data = Grunfeld, model = "within")
>>
>> Balanced Panel: n=10, T=20, N=200
>>
>> Residuals :
>>     Min.  1st Qu.   Median  3rd Qu.     Max.
>> -184.000  -17.600    0.563   19.200  251.000
>>
>> Coefficients :
>>         Estimate Std. Error t-value  Pr(>|t|)
>> value   0.110124   0.011857  9.2879 < 2.2e-16 ***
>> capital 0.310065   0.017355 17.8666 < 2.2e-16 ***
>> ---
>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>
>> Total Sum of Squares:    2244400
>> Residual Sum of Squares: 523480
>> F-statistic: 309.014 on 2 and 188 DF, p-value: < 2.22e-16
>>
>> > remod <- plm(fm, Grunfeld, model="random")
>> > summary(remod)
>> Oneway (individual) effect Random Effect Model
>>    (Swamy-Arora's transformation)
>>
>> Call:
>> plm(formula = fm, data = Grunfeld, model = "random")
>>
>> Balanced Panel: n=10, T=20, N=200
>>
>> Effects:
>>                    var  std.dev share
>> idiosyncratic 2784.458   52.768 0.282
>> individual    7089.800   84.201 0.718
>> theta:  0.86122
>>
>> Residuals :
>>    Min. 1st Qu.  Median 3rd Qu.    Max.
>> -178.00  -19.70    4.69   19.50  253.00
>>
>> Coefficients :
>>               Estimate Std. Error t-value Pr(>|t|)
>> (Intercept) -57.834415  28.898935 -2.0013  0.04536 *
>> value         0.109781   0.010493 10.4627  < 2e-16 ***
>> capital       0.308113   0.017180 17.9339  < 2e-16 ***
>> ---
>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>
>> Total Sum of Squares:    2381400
>> Residual Sum of Squares: 548900
>> F-statistic: 328.837 on 2 and 197 DF, p-value: < 2.22e-16
>> > phtest(femod, remod)
>>
>>         Hausman Test
>>
>> data:  fm
>> chisq = 2.3304, df = 2, p-value = 0.3119
>> alternative hypothesis: one model is inconsistent
>>
>> ###end Plm###
>>
>>
>>
>>
>>
>>   On Mon, May 18, 2009 at 6:01 AM, Millo Giovanni <
>> Giovanni_Millo at generali.com> wrote:
>>
>>> Dear Steve,
>>>
>>> I got your inquiry courtesy of Christian Kleiber, who brought it to our
>>> attention: please next time you post anything re a given package,
>>> include the maintainer's address. We cannot guarantee to parse all the
>>> daily digests of the R system!
>>>
>>> Your problem: can you please provide a reproducible example? Else it is
>>> difficult to help, not knowing your data, your results and even the
>>> Stata version you're using.
>>>
>>> In the following I replicate what you might have done on a well-known
>>> dataset.
>>>
>>> From Stata10, on the usual Grunfeld data taken from package "Ecdat":
>>>
>>> ## begin Stata10 output ##
>>> . xtreg inv value capital
>>>
>>> Random-effects GLS regression                   Number of obs      =
>>> 200
>>> Group variable: firm                            Number of groups   =
>>> 10
>>>
>>> R-sq:  within  = 0.7668                         Obs per group: min =
>>> 20
>>>       between = 0.8196                                        avg =
>>> 20.0
>>>       overall = 0.8061                                        max =
>>> 20
>>>
>>> Random effects u_i ~ Gaussian                   Wald chi2(2)       =
>>> 657.67
>>> corr(u_i, X)       = 0 (assumed)                Prob > chi2        =
>>> 0.0000
>>>
>>> ------------------------------------------------------------------------
>>> ------
>>>         inv |      Coef.   Std. Err.      z    P>|z|     [95% Conf.
>>> Interval]
>>> -------------+----------------------------------------------------------
>>> ------
>>>       value |   .1097811   .0104927    10.46   0.000     .0892159
>>> .1303464
>>>     capital |    .308113   .0171805    17.93   0.000     .2744399
>>> .3417861
>>>       _cons |  -57.83441   28.89893    -2.00   0.045    -114.4753
>>> -1.193537
>>> -------------+----------------------------------------------------------
>>> ------
>>>     sigma_u |   84.20095
>>>     sigma_e |  52.767964
>>>         rho |  .71800838   (fraction of variance due to u_i)
>>> ------------------------------------------------------------------------
>>> ------
>>>
>>> . estimates store remod
>>>
>>> . xtreg inv value capital, fe
>>>
>>> Fixed-effects (within) regression               Number of obs      =
>>> 200
>>> Group variable: firm                            Number of groups   =
>>> 10
>>>
>>> R-sq:  within  = 0.7668                         Obs per group: min =
>>> 20
>>>       between = 0.8194                                        avg =
>>> 20.0
>>>       overall = 0.8060                                        max =
>>> 20
>>>
>>>                                                F(2,188)           =
>>> 309.01
>>> corr(u_i, Xb)  = -0.1517                        Prob > F           =
>>> 0.0000
>>>
>>> ------------------------------------------------------------------------
>>> ------
>>>         inv |      Coef.   Std. Err.      t    P>|t|     [95% Conf.
>>> Interval]
>>> -------------+----------------------------------------------------------
>>> ------
>>>       value |   .1101238   .0118567     9.29   0.000     .0867345
>>> .1335131
>>>     capital |   .3100653   .0173545    17.87   0.000     .2758308
>>> .3442999
>>>       _cons |  -58.74393   12.45369    -4.72   0.000    -83.31086
>>> -34.177
>>> -------------+----------------------------------------------------------
>>> ------
>>>     sigma_u |  85.732501
>>>     sigma_e |  52.767964
>>>         rho |  .72525012   (fraction of variance due to u_i)
>>> ------------------------------------------------------------------------
>>> ------
>>> F test that all u_i=0:     F(9, 188) =    49.18              Prob > F =
>>> 0.0000
>>>
>>> . estimates store femod
>>>
>>> . hausman femod remod
>>>
>>>                 ---- Coefficients ----
>>>             |      (b)          (B)            (b-B)
>>> sqrt(diag(V_b-V_B))
>>>             |     femod        remod        Difference          S.E.
>>> -------------+----------------------------------------------------------
>>> ------
>>>       value |    .1101238     .1097811        .0003427        .0055213
>>>     capital |    .3100653      .308113        .0019524        .0024516
>>> ------------------------------------------------------------------------
>>> ------
>>>                           b = consistent under Ho and Ha; obtained from
>>> xtreg
>>>            B = inconsistent under Ha, efficient under Ho; obtained from
>>> xtreg
>>>
>>>    Test:  Ho:  difference in coefficients not systematic
>>>
>>>                  chi2(2) = (b-B)'[(V_b-V_B)^(-1)](b-B)
>>>                          =        2.33
>>>                Prob>chi2 =      0.3119
>>>
>>> .
>>> ## end Stata10 output ##
>>>
>>> while from plm I get
>>>
>>> ## begin R putput ##
>>> > data(Grunfeld, package="Ecdat")
>>> > fm <- inv~value+capital
>>> >
>>> > femod <- plm(fm, Grunfeld)
>>> > remod <- plm(fm, Grunfeld, model="random")
>>> >
>>> > phtest(femod, remod)
>>>
>>>        Hausman Test
>>>
>>> data:  fm
>>> chisq = 2.3304, df = 2, p-value = 0.3119
>>> alternative hypothesis: one model is inconsistent
>>>
>>> ## end R output ##
>>>
>>> which, besides testifying to the goodness and parsimony of an
>>> object-oriented approach as far as screen output is concerned, looks
>>> rather consistent to me.
>>>
>>> I cannot but guess that the problem might stem from different RE
>>> estimates: previous versions of Stata used the Wallace-Hussein method by
>>> default for computing the variance of random effects. Now Stata uses
>>> Swamy-Arora, which has been the default of 'plm' since the beginning.
>>> Yet as plm() allows to choose, you can experiment with different values
>>> for the 'random.method' argument in order to see if you get the Stata
>>> result. I suggest you start by comparing the coefficient estimates you
>>> get from Stata and R: FE should be unambiguous, RE might vary as said
>>> above, and for good reason.
>>>
>>> You also didn't tell us whether your by-hand calculation agrees with
>>> phtest() output? (I guess it does not)
>>>
>>> Please let us know, possibly with a reproducible example and providing
>>> all the above info
>>> Giovanni
>>>
>>> PS please also make sure you're not using any VEEEEERY old version of
>>> 'plm' (prior to, say, 0.3): these had a bug in the p-value calculation
>>> which made it depend on the order of models compared (so that in the
>>> wrong case you got p.value=1).
>>>
>>> Giovanni Millo
>>> Research Dept.,
>>> Assicurazioni Generali SpA
>>> Via Machiavelli 4,
>>> 34132 Trieste (Italy)
>>> tel. +39 040 671184
>>> fax  +39 040 671160
>>>
>>> > ----------------------------------------------------------------------
>>> > --
>>> >
>>> > Subject:
>>> > [R-SIG-Finance] Chi-sq Hausman test---R vs Stata
>>> > From:
>>> > Steven Archambault <archstevej at gmail.com>
>>> > Date:
>>> > Sun, 17 May 2009 23:14:13 -0600
>>> > To:
>>> > r-sig-finance at stat.math.ethz.ch
>>> >
>>> > To:
>>> > r-sig-finance at stat.math.ethz.ch
>>> >
>>> >
>>> > Hi all,
>>> >
>>> > I am running a panel time series regression testing Fixed Effects and
>>> > Random Effects. I decided to calculate the chi-sq value for the
>>> > Hausman test in both R (Phtest) and Stata. I get different results.
>>> > Even within Stata, calculating the Chi-sq value with the canned
>>> > procedure or by hand (using
>>> > matrices) gives different results. So, the question should come up
>>> there as
>>> > well.
>>> >
>>> > Does anybody have any insight on how to pick which results to use? I
>>> > guess the one that gives the result I want? Having different programs
>>> > give quite different values for the same tests is frustrating me.  I'd
>>>
>>> > be interested in any feedback folks have!
>>> >
>>> > Thanks,
>>> > Steve
>>> >
>>> >       [[alternative HTML version deleted]]
>>>
>>
>>
>  ------------------------------
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090519/79bfa6cf/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: fdi_data.csv
Type: application/vnd.ms-excel
Size: 284237 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090519/79bfa6cf/attachment.xlb>

From bearxu83 at gmail.com  Wed May 20 02:19:44 2009
From: bearxu83 at gmail.com (BearXu)
Date: Wed, 20 May 2009 01:19:44 +0100
Subject: [R-SIG-Finance] [R-sig-finance] Domestic risk free rate in FX
	option
In-Reply-To: <82527b5d0905120741t540ae439s22ebd1f8641d8279@mail.gmail.com>
References: <2255646.1242090275363.JavaMail.root@elwamui-hound.atl.sa.earthlink.net>
	<82527b5d0905120652w3e9ec3f3i785f13f255fc20ae@mail.gmail.com> 
	<23503701.post@talk.nabble.com>
	<82527b5d0905120741t540ae439s22ebd1f8641d8279@mail.gmail.com>
Message-ID: <82527b5d0905191719j4d49af05u3a0287ef6d97076a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090520/ba008fc7/attachment.pl>

From rbali at ufmg.br  Wed May 20 19:22:36 2009
From: rbali at ufmg.br (Robert Iquiapaza)
Date: Wed, 20 May 2009 14:22:36 -0300
Subject: [R-SIG-Finance] R: [Fwd: R-SIG-Finance Digest, Vol 60, Issue 18]
In-Reply-To: <a575b07e0905191216nf9272b2p835e69b7890f173@mail.gmail.com>
References: <4A1138D0.9050004@unibas.ch>
	<28643F754DDB094D8A875617EC4398B202AE7AD1@BEMAILEXTV03.corp.generali.net>
	<a575b07e0905181126m67cb8de2k6ee4c4153d69806@mail.gmail.com>
	<a575b07e0905181306v65067bd3s2313ccff9c55d4eb@mail.gmail.com>
	<74D924C9B7C148F587228E7D97044996@DellPC>
	<a575b07e0905191216nf9272b2p835e69b7890f173@mail.gmail.com>
Message-ID: <64325EAB34C44002AD2C1A1BCD197C3C@DellPC>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090520/a5d743c4/attachment.pl>

From babel at centrum.sk  Wed May 20 20:24:01 2009
From: babel at centrum.sk (babel at centrum.sk)
Date: Wed, 20 May 2009 20:24:01 +0200
Subject: [R-SIG-Finance] getSymbols in quantmod
Message-ID: <200905202024.28109@centrum.cz>

Dear friends
I would like to ask you, if there is an easy way,how to determined the correct symbol and the correct way to write it. Some symbols work fine and one can easily make up what the symbols are for example
getSymbols("GOOG",src="yahoo")
getSymbols("AAPL",src="yahoo") 

but how to find the other symbols for example currency pairs or commodities? Doesnt mean if they are from Yahoo or Google or FRED. Should I look in URL address in yahoo or google finance? And for which symbols I must specifie the start date, and other attributes? Some cheat sheet will surely help other people too. Many thanks you are doing a great job. 

Sincerely Jan


From archstevej at gmail.com  Wed May 20 22:18:18 2009
From: archstevej at gmail.com (Steven Archambault)
Date: Wed, 20 May 2009 14:18:18 -0600
Subject: [R-SIG-Finance] R: [Fwd: R-SIG-Finance Digest, Vol 60, Issue 18]
In-Reply-To: <64325EAB34C44002AD2C1A1BCD197C3C@DellPC>
References: <4A1138D0.9050004@unibas.ch>
	<28643F754DDB094D8A875617EC4398B202AE7AD1@BEMAILEXTV03.corp.generali.net>
	<a575b07e0905181126m67cb8de2k6ee4c4153d69806@mail.gmail.com>
	<a575b07e0905181306v65067bd3s2313ccff9c55d4eb@mail.gmail.com>
	<74D924C9B7C148F587228E7D97044996@DellPC>
	<a575b07e0905191216nf9272b2p835e69b7890f173@mail.gmail.com>
	<64325EAB34C44002AD2C1A1BCD197C3C@DellPC>
Message-ID: <a575b07e0905201318o6296b6d2ib60f2d1b336c654b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090520/fa2f553b/attachment.pl>

From cedrick at cedrickjohnson.com  Thu May 21 02:07:11 2009
From: cedrick at cedrickjohnson.com (Cedrick Johnson)
Date: Wed, 20 May 2009 20:07:11 -0400
Subject: [R-SIG-Finance] getSymbols in quantmod
In-Reply-To: <200905202024.28109@centrum.cz>
References: <200905202024.28109@centrum.cz>
Message-ID: <4A149B2F.4020404@cedrickjohnson.com>

to specify start and end dates:

getSymbols("AAPL",src="yahoo", from='2008-03-01', to='2008-03-10')

or
getSymbols("AAPL",src="yahoo", from='2008-03-01', to=Sys.Date())

yahoo dow:

getSymbols("^DJI", src="yahoo", from='2008-01-01', to=Sys.Date())

As for the other parts: commodities/dividends, etc... I haven't used the 
yhoo or goog feeds for those, perhaps someone can shed some light on 
those particular functions..

-cj

babel at centrum.sk wrote:
> Dear friends
> I would like to ask you, if there is an easy way,how to determined the correct symbol and the correct way to write it. Some symbols work fine and one can easily make up what the symbols are for example
> getSymbols("GOOG",src="yahoo")
> getSymbols("AAPL",src="yahoo") 
>
> but how to find the other symbols for example currency pairs or commodities? Doesnt mean if they are from Yahoo or Google or FRED. Should I look in URL address in yahoo or google finance? And for which symbols I must specifie the start date, and other attributes? Some cheat sheet will surely help other people too. Many thanks you are doing a great job. 
>
> Sincerely Jan
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From rbali at ufmg.br  Thu May 21 03:10:02 2009
From: rbali at ufmg.br (Robert Iquiapaza)
Date: Wed, 20 May 2009 22:10:02 -0300
Subject: [R-SIG-Finance] R: [Fwd: R-SIG-Finance Digest, Vol 60, Issue 18]
In-Reply-To: <a575b07e0905201318o6296b6d2ib60f2d1b336c654b@mail.gmail.com>
References: <4A1138D0.9050004@unibas.ch>
	<28643F754DDB094D8A875617EC4398B202AE7AD1@BEMAILEXTV03.corp.generali.net>
	<a575b07e0905181126m67cb8de2k6ee4c4153d69806@mail.gmail.com>
	<a575b07e0905181306v65067bd3s2313ccff9c55d4eb@mail.gmail.com>
	<74D924C9B7C148F587228E7D97044996@DellPC>
	<a575b07e0905191216nf9272b2p835e69b7890f173@mail.gmail.com>
	<64325EAB34C44002AD2C1A1BCD197C3C@DellPC>
	<a575b07e0905201318o6296b6d2ib60f2d1b336c654b@mail.gmail.com>
Message-ID: <EBC7043AC1A64DEA9EEDB0CC53A2DA6C@DellPC>

See "The Hausman test statistic can be negative even asymptotically" 
 Schreiber, S. 2008 Jahrbucher fur Nationalokonomie und Statistik 228 (4), pp. 394-405 

http://econ.schreiberlin.de/papers/schreiber_hausmantest_aug2008.pdf



From: Steven Archambault 
Sent: Wednesday, May 20, 2009 5:18 PM
To: Robert Iquiapaza ; r-sig-finance at stat.math.ethz.ch 
Subject: Re: [R-SIG-Finance] R: [Fwd: R-SIG-Finance Digest, Vol 60, Issue 18]


Thanks Robert. I have been playing around with sigmamore sigmaless. I cannot seem to duplicate the canned results when I do it by hand. Any ideas?

###STATA 9.2####

quietly xtreg lfdi_2000 lagdlfdi laglnstock2000 lagtradegdp lagdlgdp, fe;

. estimates store FIX, title(The FE);

. matrix bfe=e(b);

. matrix vfe=e(rmse);

. quietly xtreg lfdi_2000 lagdlfdi laglnstock2000 lagtradegdp lagdlgdp, re sa;

.  estimates store RAND, title(The RE);

. matrix bre=e(b);

. matrix vre=e(rmse);

. matrix bdif=bfe-bre;

. matrix bdifp=bdif';

. matrix dv=vre-vfe;

. matrix dvi=inv(dv);

. matrix chi1=bdif*dvi;

. matrix chisq=chi1*bdifp;

. matrix list chisq;

symmetric chisq[1,1]
           y1
y1  11.105892

. hausman FIX RAND, sigmamore;

                 ---- Coefficients ----
             |      (b)          (B)            (b-B)     sqrt(diag(V_b-V_B))
             |      FIX          RAND        Difference          S.E.
-------------+----------------------------------------------------------------
    lagdlfdi |    .1564758     .1632387       -.0067629        .0014297
laglnst~2000 |     .762135     .8314432       -.0693082        .0151471
 lagtradegdp |    .0178568     .0119453        .0059115        .0015669
    lagdlgdp |    .2601478      .255801        .0043468        .0067502
------------------------------------------------------------------------------
                           b = consistent under Ho and Ha; obtained from xtreg
            B = inconsistent under Ha, efficient under Ho; obtained from xtreg

    Test:  Ho:  difference in coefficients not systematic

                  chi2(4) = (b-B)'[(V_b-V_B)^(-1)](b-B)
                          =       31.32
                Prob>chi2 =      0.0000











On Wed, May 20, 2009 at 11:22 AM, Robert Iquiapaza <rbali at ufmg.br> wrote:

  Stev,

  You will get the same Chi-sq for Hausman test if you use Swamy-Arora's transformation for RE in stata. To avoid the variance not positive definite maybe you should use options sigmamore or sigmaless  in Stata (see http://www.stata.com/help.cgi?hausman), the results don't change. 
  I wonder why plm doesn't alert the variance not being positive definite.

  Robert

   # Stata 10.1
  xtreg lfdi_2000 lagdlfdi laglnstock2000 lagtradegdp lagdlgdp, re sa

  estimates store RANDsa, title(The REsa)

  hausman FIX RANDsa

   ---- Coefficients ----
   (b)          (B)            (b-B)     sqrt(diag(V_b-V_B))

   FIX         RANDsa       Difference          S.E.
   
  lagdlfdi .1564759     .1632388       -.0067629               .
  laglnst~2000 .762135     .8314432       -.0693082        .0149396
  lagtradegdp .0178568     .0119453        .0059115        .0015449
  lagdlgdp .2601477     .2558009        .0043468        .0051777 

   
   b = consistent under Ho and Ha; obtained from xtreg
  B = inconsistent under Ha, efficient under Ho; obtained from xtreg

  Test:  Ho: difference in coefficients not systematic

   chi2(4) = (b-B)'[(V_b-V_B)^(-1)](b-B)

   =       23.70 

   Prob>chi2 =      0.0001
   (V_b-V_B is not positive definite)



  From: Steven Archambault 
  Sent: Tuesday, May 19, 2009 4:16 PM
  To: Robert Iquiapaza 
  Cc: r-sig-finance at stat.math.ethz.ch 
  Subject: Re: [R-SIG-Finance] R: [Fwd: R-SIG-Finance Digest, Vol 60, Issue 18]


  Oh, you are right. Here is the correct file. I sure have botched this query, thanks for catching it Robert! Sorry for so many posts to the list.

  Regards,
  Steve




  On Tue, May 19, 2009 at 12:19 PM, Robert Iquiapaza <rbali at ufmg.br> wrote:

    Stev,

    The data you provided is not complete, lagdlfdi and laglnstock2000 are not in the csv file

    Robert


    From: Steven Archambault 
    Sent: Monday, May 18, 2009 5:06 PM
    To: Millo Giovanni 
    Cc: r-sig-finance at stat.math.ethz.ch ; Yves Croissant ; Christian Kleiber 
    Subject: Re: [R-SIG-Finance] R: [Fwd: R-SIG-Finance Digest, Vol 60, Issue 18]


    I just realized I used Robust in my Stata 9.2 analysis. When I remove this, the Chi-sq values are much closer to the values I get in R (but negative, as the consistent model must be listed first in a chi-sq calculation). However, with my own data I do get this positive definite error in Stata. Is this a result of unbalanced data? R doesn't give an error, so I am inclined to ignore it in Stata. I am posting my own results from R and Stata, and attaching the data as a csv.

    Thanks, hope I am not wasting too much of your time here.

    -Steve

    ###R-Output###
    > library("plm")
    > 
    > fdi <- read.csv("C:/data/mydata.csv", na.strings=".")
    > fdiplm<-plm.data(fdi, index = c("id_code_id", "year"))
    series    are constants and have been removed
    > 
    > fdi_test<-(lfdi_2000~ lagdlfdi+ laglnstock2000+ lagtradegdp +lagdlgdp)
    > 
    > fdi_test_fe <- plm(fdi_test, data=fdiplm, model="within")
    > fdi_test_re <- plm(fdi_test, data=fdiplm, model="random")
    > 
    > summary (fdi_test_fe)
    Oneway (individual) effect Within Model

    Call:
    plm(formula = fdi_test, data = fdiplm, model = "within")

    Unbalanced Panel: n=149, T=3-27, N=2697

    Residuals :
       Min. 1st Qu.  Median 3rd Qu.    Max. 
    -8.2100 -0.4760  0.0452  0.5670  4.8700 

    Coefficients :
                    Estimate Std. Error t-value  Pr(>|t|)    
    lagdlfdi       0.1564759  0.0180645  8.6621 < 2.2e-16 ***
    laglnstock2000 0.7621350  0.0246798 30.8809 < 2.2e-16 ***
    lagtradegdp    0.0178568  0.0025859  6.9055 5.003e-12 ***
    lagdlgdp       0.2601477  0.0427744  6.0818 1.188e-09 ***
    ---
    Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 

    Total Sum of Squares:    4606.7
    Residual Sum of Squares: 2938
    F-statistic: 361.237 on 4 and 2544 DF, p-value: < 2.22e-16
    > summary (fdi_test_re)
    Oneway (individual) effect Random Effect Model 
       (Swamy-Arora's transformation)

    Call:
    plm(formula = fdi_test, data = fdiplm, model = "random")

    Unbalanced Panel: n=149, T=3-27, N=2697

    Effects:
                      var std.dev  share
    idiosyncratic 1.15487 1.07465 0.6617
    individual    0.59044 0.76840 0.3383
    theta  : 
       Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
     0.3718  0.6700  0.7081  0.6955  0.7355  0.7401 

    Residuals :
        Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
    -9.15000 -0.47900  0.07270 -0.00713  0.59800  3.95000 

    Coefficients :
                     Estimate Std. Error  t-value  Pr(>|t|)    
    (Intercept)    16.7744214  0.1552868 108.0222 < 2.2e-16 ***
    lagdlfdi        0.1632388  0.0181005   9.0185 < 2.2e-16 ***
    laglnstock2000  0.8314432  0.0196444  42.3247 < 2.2e-16 ***
    lagtradegdp     0.0119453  0.0020737   5.7605 8.386e-09 ***
    lagdlgdp        0.2558009  0.0424599   6.0245 1.696e-09 ***
    ---
    Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 

    Total Sum of Squares:    9522.3
    Residual Sum of Squares: 3140.8
    F-statistic: 1367.42 on 4 and 2692 DF, p-value: < 2.22e-16
    > 
    > phtest(fdi_test_re, fdi_test_fe)

            Hausman Test

    data:  fdi_test 
    chisq = 23.7021, df = 4, p-value = 9.164e-05
    alternative hypothesis: one model is inconsistent 


    ###end R output###

    ###Stata 9.2 Output--canned###
    xtreg lfdi_2000 lagdlfdi laglnstock2000 lagtradegdp lagdlgdp, fe;

    Fixed-effects (within) regression               Number of obs      =      2697
    Group variable (i): id_code_id                  Number of groups   =       149

    R-sq:  within  = 0.3622                         Obs per group: min =         3
           between = 0.8234                                        avg =      18.1
           overall = 0.6998                                        max =        27

                                                    F(4,2544)          =    361.24
    corr(u_i, Xb)  = 0.3536                         Prob > F           =    0.0000

    ------------------------------------------------------------------------------
       lfdi_2000 |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
    -------------+----------------------------------------------------------------
        lagdlfdi |   .1564758   .0180645     8.66   0.000     .1210532    .1918985
    laglnst~2000 |    .762135   .0246798    30.88   0.000     .7137404    .8105295
     lagtradegdp |   .0178568   .0025859     6.91   0.000     .0127861    .0229274
        lagdlgdp |   .2601478   .0427744     6.08   0.000     .1762716    .3440241
           _cons |   17.01131   .1701713    99.97   0.000     16.67762      17.345
    -------------+----------------------------------------------------------------
         sigma_u |  .93048942
         sigma_e |  1.0746505
             rho |  .42847396   (fraction of variance due to u_i)
    ------------------------------------------------------------------------------
    F test that all u_i=0:     F(148, 2544) =    10.73           Prob > F = 0.0000

    . estimates store FIX, title(The FE) ;

    . xtreg lfdi_2000 lagdlfdi laglnstock2000 lagtradegdp lagdlgdp, re;

    Random-effects GLS regression                   Number of obs      =      2697
    Group variable (i): id_code_id                  Number of groups   =       149

    R-sq:  within  = 0.3606                         Obs per group: min =         3
           between = 0.8402                                        avg =      18.1
           overall = 0.7128                                        max =        27

    Random effects u_i ~ Gaussian                   Wald chi2(4)       =   2225.46
    corr(u_i, X)       = 0 (assumed)                Prob > chi2        =    0.0000

    ------------------------------------------------------------------------------
       lfdi_2000 |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
    -------------+----------------------------------------------------------------
        lagdlfdi |   .1631662   .0180937     9.02   0.000     .1277032    .1986291
    laglnst~2000 |    .830845   .0196843    42.21   0.000     .7922645    .8694255
     lagtradegdp |    .011992   .0020779     5.77   0.000     .0079195    .0160645
        lagdlgdp |   .2558113   .0424486     6.03   0.000     .1726136    .3390091
           _cons |   16.77702   .1556693   107.77   0.000     16.47191    17.08212
    -------------+----------------------------------------------------------------
         sigma_u |  .77431228
         sigma_e |  1.0746505
             rho |  .34173973   (fraction of variance due to u_i)
    ------------------------------------------------------------------------------

    .  estimates store RAND, title(The RE) ;

    . hausman FIX RAND;

                     ---- Coefficients ----
                 |      (b)          (B)            (b-B)     sqrt(diag(V_b-V_B))
                 |      FIX          RAND        Difference          S.E.
    -------------+----------------------------------------------------------------
        lagdlfdi |    .1564758     .1631662       -.0066903               .
    laglnst~2000 |     .762135      .830845         -.06871         .014887
     lagtradegdp |    .0178568      .011992        .0058648        .0015393
        lagdlgdp |    .2601478     .2558113        .0043365        .0052695
    ------------------------------------------------------------------------------
                               b = consistent under Ho and Ha; obtained from xtreg
                B = inconsistent under Ha, efficient under Ho; obtained from xtreg

        Test:  Ho:  difference in coefficients not systematic

                      chi2(4) = (b-B)'[(V_b-V_B)^(-1)](b-B)
                              =       22.94
                    Prob>chi2 =      0.0001
                    (V_b-V_B is not positive definite)
    ###End Stata 9.2####

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090520/15a64240/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/gif
Size: 43 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090520/15a64240/attachment.gif>

From comtech.usa at gmail.com  Thu May 21 17:04:14 2009
From: comtech.usa at gmail.com (Michael)
Date: Thu, 21 May 2009 08:04:14 -0700
Subject: [R-SIG-Finance] high frequency data analysis in R
Message-ID: <b1f16d9d0905210804s6fb9832crf935912bd7509a4c@mail.gmail.com>

Hi all,

I am wondering if there are some special toolboxes to handle high
frequency data in R?

I have some high frequency data and was wondering what meaningful
experiments can I run on these high frequency data.

Not sure if normal (low frequency) financial time series textbook data
analysis tools will work for high frequency data?

Let's say I run a correlation between two stocks using the high
frequency data, or run an ARMA model on one stock, will the results be
meaningful?

Could anybody point me some classroom types of treatment or lab
tutorial type of document which show me what meaningful
experiments/tests I can run on high frequency data?

Thanks a lot!


From jeff.a.ryan at gmail.com  Thu May 21 17:15:20 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 21 May 2009 10:15:20 -0500
Subject: [R-SIG-Finance] high frequency data analysis in R
In-Reply-To: <b1f16d9d0905210804s6fb9832crf935912bd7509a4c@mail.gmail.com>
References: <b1f16d9d0905210804s6fb9832crf935912bd7509a4c@mail.gmail.com>
Message-ID: <e8e755250905210815h742c04aekfd111ff7e4165c1d@mail.gmail.com>

Not my domain, but you will more than likely have to aggregate to some
sort of regular/homogenous type of series for most traditional tools
to work.

xts has to.period to aggregate up to a lower frequency from tick-level
data. Coupled with something like na.locf you can make yourself some
high frequency 'regular' data from 'irregular'

Regular and irregular of course depend on what you are looking at
(weekends missing in daily data can still be 'regular').

I'd be interested in hearing thoughts from those who actually tread in
the high-freq domain...

A wealth of information can be found here:

 http://www.olsen.ch/publications/working-papers/

Jeff

On Thu, May 21, 2009 at 10:04 AM, Michael <comtech.usa at gmail.com> wrote:
> Hi all,
>
> I am wondering if there are some special toolboxes to handle high
> frequency data in R?
>
> I have some high frequency data and was wondering what meaningful
> experiments can I run on these high frequency data.
>
> Not sure if normal (low frequency) financial time series textbook data
> analysis tools will work for high frequency data?
>
> Let's say I run a correlation between two stocks using the high
> frequency data, or run an ARMA model on one stock, will the results be
> meaningful?
>
> Could anybody point me some classroom types of treatment or lab
> tutorial type of document which show me what meaningful
> experiments/tests I can run on high frequency data?
>
> Thanks a lot!
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From comtech.usa at gmail.com  Thu May 21 17:21:38 2009
From: comtech.usa at gmail.com (Michael)
Date: Thu, 21 May 2009 08:21:38 -0700
Subject: [R-SIG-Finance] high frequency data analysis in R
In-Reply-To: <e8e755250905210815h742c04aekfd111ff7e4165c1d@mail.gmail.com>
References: <b1f16d9d0905210804s6fb9832crf935912bd7509a4c@mail.gmail.com>
	<e8e755250905210815h742c04aekfd111ff7e4165c1d@mail.gmail.com>
Message-ID: <b1f16d9d0905210821y6fa119f6jf4b15cc78ac64c8c@mail.gmail.com>

Thanks Jeff.

By high frequency I mean really the tick data. For example, during
peak time, the arrival of price events could be at about hundreds to
thousands within one second, irregularly spaced.

I've heard that forcing irregularly spaced data into regularly spaced
data(e.g. through interpolation) will lose information. How's that so?

Thanks!

On Thu, May 21, 2009 at 8:15 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> Not my domain, but you will more than likely have to aggregate to some
> sort of regular/homogenous type of series for most traditional tools
> to work.
>
> xts has to.period to aggregate up to a lower frequency from tick-level
> data. Coupled with something like na.locf you can make yourself some
> high frequency 'regular' data from 'irregular'
>
> Regular and irregular of course depend on what you are looking at
> (weekends missing in daily data can still be 'regular').
>
> I'd be interested in hearing thoughts from those who actually tread in
> the high-freq domain...
>
> A wealth of information can be found here:
>
> ?http://www.olsen.ch/publications/working-papers/
>
> Jeff
>
> On Thu, May 21, 2009 at 10:04 AM, Michael <comtech.usa at gmail.com> wrote:
>> Hi all,
>>
>> I am wondering if there are some special toolboxes to handle high
>> frequency data in R?
>>
>> I have some high frequency data and was wondering what meaningful
>> experiments can I run on these high frequency data.
>>
>> Not sure if normal (low frequency) financial time series textbook data
>> analysis tools will work for high frequency data?
>>
>> Let's say I run a correlation between two stocks using the high
>> frequency data, or run an ARMA model on one stock, will the results be
>> meaningful?
>>
>> Could anybody point me some classroom types of treatment or lab
>> tutorial type of document which show me what meaningful
>> experiments/tests I can run on high frequency data?
>>
>> Thanks a lot!
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> --
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com
>


From comtech.usa at gmail.com  Thu May 21 17:38:54 2009
From: comtech.usa at gmail.com (Michael)
Date: Thu, 21 May 2009 08:38:54 -0700
Subject: [R-SIG-Finance] high frequency data analysis in R
In-Reply-To: <b1f16d9d0905210821y6fa119f6jf4b15cc78ac64c8c@mail.gmail.com>
References: <b1f16d9d0905210804s6fb9832crf935912bd7509a4c@mail.gmail.com>
	<e8e755250905210815h742c04aekfd111ff7e4165c1d@mail.gmail.com>
	<b1f16d9d0905210821y6fa119f6jf4b15cc78ac64c8c@mail.gmail.com>
Message-ID: <b1f16d9d0905210838ud47dbe2n5555c50518794cca@mail.gmail.com>

My data are price change arrivals, irregularly spaced. But when there
is no price change, the price stays constant. Therefore, in fact, at
any time instant, you give me a time, I can give you the price at that
very instant of time. So irregularly spaced data can be easily sampled
to be regularly spaced data.
What do you think of this approach?

On Thu, May 21, 2009 at 8:21 AM, Michael <comtech.usa at gmail.com> wrote:
> Thanks Jeff.
>
> By high frequency I mean really the tick data. For example, during
> peak time, the arrival of price events could be at about hundreds to
> thousands within one second, irregularly spaced.
>
> I've heard that forcing irregularly spaced data into regularly spaced
> data(e.g. through interpolation) will lose information. How's that so?
>
> Thanks!
>
> On Thu, May 21, 2009 at 8:15 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
>> Not my domain, but you will more than likely have to aggregate to some
>> sort of regular/homogenous type of series for most traditional tools
>> to work.
>>
>> xts has to.period to aggregate up to a lower frequency from tick-level
>> data. Coupled with something like na.locf you can make yourself some
>> high frequency 'regular' data from 'irregular'
>>
>> Regular and irregular of course depend on what you are looking at
>> (weekends missing in daily data can still be 'regular').
>>
>> I'd be interested in hearing thoughts from those who actually tread in
>> the high-freq domain...
>>
>> A wealth of information can be found here:
>>
>> ?http://www.olsen.ch/publications/working-papers/
>>
>> Jeff
>>
>> On Thu, May 21, 2009 at 10:04 AM, Michael <comtech.usa at gmail.com> wrote:
>>> Hi all,
>>>
>>> I am wondering if there are some special toolboxes to handle high
>>> frequency data in R?
>>>
>>> I have some high frequency data and was wondering what meaningful
>>> experiments can I run on these high frequency data.
>>>
>>> Not sure if normal (low frequency) financial time series textbook data
>>> analysis tools will work for high frequency data?
>>>
>>> Let's say I run a correlation between two stocks using the high
>>> frequency data, or run an ARMA model on one stock, will the results be
>>> meaningful?
>>>
>>> Could anybody point me some classroom types of treatment or lab
>>> tutorial type of document which show me what meaningful
>>> experiments/tests I can run on high frequency data?
>>>
>>> Thanks a lot!
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>>
>>
>> --
>> Jeffrey Ryan
>> jeffrey.ryan at insightalgo.com
>>
>> ia: insight algorithmics
>> www.insightalgo.com
>>
>


From landronimirc at gmail.com  Thu May 21 18:08:42 2009
From: landronimirc at gmail.com (Liviu Andronic)
Date: Thu, 21 May 2009 18:08:42 +0200
Subject: [R-SIG-Finance] high frequency data analysis in R
In-Reply-To: <b1f16d9d0905210821y6fa119f6jf4b15cc78ac64c8c@mail.gmail.com>
References: <b1f16d9d0905210804s6fb9832crf935912bd7509a4c@mail.gmail.com> 
	<e8e755250905210815h742c04aekfd111ff7e4165c1d@mail.gmail.com> 
	<b1f16d9d0905210821y6fa119f6jf4b15cc78ac64c8c@mail.gmail.com>
Message-ID: <68b1e2610905210908k7f2f2d31ue5d3c1faae33a0e4@mail.gmail.com>

Hello Michael,

On Thu, May 21, 2009 at 5:21 PM, Michael <comtech.usa at gmail.com> wrote:
> By high frequency I mean really the tick data. For example, during
> peak time, the arrival of price events could be at about hundreds to
> thousands within one second, irregularly spaced.
>
If I understand correctly, you're dealing with an issue---that I'm
currently investigating---of nonsynchronous data. You may be
interested in library(realized), which implements at least the
Hayashi-Yoshida covariance estimator (2005). Be sure to check the
package's homepage for an extended user manual and a (possibly
obsolete) table of implemented methods. There is also a paper dealing
with synchronizing data using a "Refresh Time" methodology
("Multivariate realised kernels: consistent positive semi-definite
estimators of the covariation of equity prices with noise and
non-synchronous trading", BARNDORFF-NIELSEN, HANSEN, LUNDE and
SHEPHARD, 2008).

>From what I understood the HY estimator is appropriate for very
high-frequency data; unfortunately I am dealing with very
low-frequency non-synchronous data, and I'm still looking for a data
synchronization method/consistent covariance estimator. If anyone is
familiar with available methodology/R implementations, please share
your thoughts.

Best,
Liviu


From hakyim at gmail.com  Thu May 21 18:13:49 2009
From: hakyim at gmail.com (Hae Kyung Im)
Date: Thu, 21 May 2009 11:13:49 -0500
Subject: [R-SIG-Finance] high frequency data analysis in R
In-Reply-To: <e8e755250905210815h742c04aekfd111ff7e4165c1d@mail.gmail.com>
References: <b1f16d9d0905210804s6fb9832crf935912bd7509a4c@mail.gmail.com>
	<e8e755250905210815h742c04aekfd111ff7e4165c1d@mail.gmail.com>
Message-ID: <197a5bbc0905210913q421b2462tc2c9b5dcfa3ebb8e@mail.gmail.com>

I think in general you would need some sort of pre-processing before using R.

You can use periodic sampling of prices, but you may be throwing away
a lot of information. This is a method that used to be recommended
more than 5 years ago in order to mitigate the effect of market noise.
At least in the context of volatility estimation.

Here is my experience with tick data:

I used FX data to calculate estimated daily volatility using TSRV
(Zhang et al 2005
http://galton.uchicago.edu/~mykland/paperlinks/p1394.pdf). Using the
time series of estimated daily volatilities, I forecasted volatilities
for 1 day up to 1 year ahead. The tick data was in Quantitative
Analytics database. I used their C++ API to query daily data, computed
the TSRV estimator in C++ and saved the result in text file. Then I
used R to read the estimated volatilities and used FARIMA to forecast
volatility. An interesting thing about this type of series is that the
fractional coefficient is approximately 0.4 in many instances.
Bollerslev has a paper commenting on this fact.

In another project, I had treasury futures market depth data. The data
came in plain text format, with one file per day. Each day had more
than 1 million entries. I don't think I could handle this with R. To
get started I decided to use only actual trades. I used Python to
filter out the trades. So this came down to ~60K entries per day. This
I could handle with R. I used to.period from xts package to aggregate
the data.

In order to handle market depth data, we need some efficient way to
access (query) this huge database. I looked a little bit into kdb but
you have to pay ~25K to buy the software for one processor. I haven't
been able to look more into this for now.

Haky




On Thu, May 21, 2009 at 10:15 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> Not my domain, but you will more than likely have to aggregate to some
> sort of regular/homogenous type of series for most traditional tools
> to work.
>
> xts has to.period to aggregate up to a lower frequency from tick-level
> data. Coupled with something like na.locf you can make yourself some
> high frequency 'regular' data from 'irregular'
>
> Regular and irregular of course depend on what you are looking at
> (weekends missing in daily data can still be 'regular').
>
> I'd be interested in hearing thoughts from those who actually tread in
> the high-freq domain...
>
> A wealth of information can be found here:
>
> ?http://www.olsen.ch/publications/working-papers/
>
> Jeff
>
> On Thu, May 21, 2009 at 10:04 AM, Michael <comtech.usa at gmail.com> wrote:
>> Hi all,
>>
>> I am wondering if there are some special toolboxes to handle high
>> frequency data in R?
>>
>> I have some high frequency data and was wondering what meaningful
>> experiments can I run on these high frequency data.
>>
>> Not sure if normal (low frequency) financial time series textbook data
>> analysis tools will work for high frequency data?
>>
>> Let's say I run a correlation between two stocks using the high
>> frequency data, or run an ARMA model on one stock, will the results be
>> meaningful?
>>
>> Could anybody point me some classroom types of treatment or lab
>> tutorial type of document which show me what meaningful
>> experiments/tests I can run on high frequency data?
>>
>> Thanks a lot!
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> --
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From comtech.usa at gmail.com  Thu May 21 18:16:17 2009
From: comtech.usa at gmail.com (Michael)
Date: Thu, 21 May 2009 09:16:17 -0700
Subject: [R-SIG-Finance] high frequency data analysis in R
In-Reply-To: <197a5bbc0905210913q421b2462tc2c9b5dcfa3ebb8e@mail.gmail.com>
References: <b1f16d9d0905210804s6fb9832crf935912bd7509a4c@mail.gmail.com>
	<e8e755250905210815h742c04aekfd111ff7e4165c1d@mail.gmail.com>
	<197a5bbc0905210913q421b2462tc2c9b5dcfa3ebb8e@mail.gmail.com>
Message-ID: <b1f16d9d0905210916ve62c1a7o849d31a51a0a2f3b@mail.gmail.com>

If there is a way to call R functions within from C++, that should
solve the large-data-set problem, right?
On the other hand, you only need to truncate data into smaller trunks,
for example, using SAS?

On Thu, May 21, 2009 at 9:13 AM, Hae Kyung Im <hakyim at gmail.com> wrote:
> I think in general you would need some sort of pre-processing before using R.
>
> You can use periodic sampling of prices, but you may be throwing away
> a lot of information. This is a method that used to be recommended
> more than 5 years ago in order to mitigate the effect of market noise.
> At least in the context of volatility estimation.
>
> Here is my experience with tick data:
>
> I used FX data to calculate estimated daily volatility using TSRV
> (Zhang et al 2005
> http://galton.uchicago.edu/~mykland/paperlinks/p1394.pdf). Using the
> time series of estimated daily volatilities, I forecasted volatilities
> for 1 day up to 1 year ahead. The tick data was in Quantitative
> Analytics database. I used their C++ API to query daily data, computed
> the TSRV estimator in C++ and saved the result in text file. Then I
> used R to read the estimated volatilities and used FARIMA to forecast
> volatility. An interesting thing about this type of series is that the
> fractional coefficient is approximately 0.4 in many instances.
> Bollerslev has a paper commenting on this fact.
>
> In another project, I had treasury futures market depth data. The data
> came in plain text format, with one file per day. Each day had more
> than 1 million entries. I don't think I could handle this with R. To
> get started I decided to use only actual trades. I used Python to
> filter out the trades. So this came down to ~60K entries per day. This
> I could handle with R. I used to.period from xts package to aggregate
> the data.
>
> In order to handle market depth data, we need some efficient way to
> access (query) this huge database. I looked a little bit into kdb but
> you have to pay ~25K to buy the software for one processor. I haven't
> been able to look more into this for now.
>
> Haky
>
>
>
>
> On Thu, May 21, 2009 at 10:15 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
>> Not my domain, but you will more than likely have to aggregate to some
>> sort of regular/homogenous type of series for most traditional tools
>> to work.
>>
>> xts has to.period to aggregate up to a lower frequency from tick-level
>> data. Coupled with something like na.locf you can make yourself some
>> high frequency 'regular' data from 'irregular'
>>
>> Regular and irregular of course depend on what you are looking at
>> (weekends missing in daily data can still be 'regular').
>>
>> I'd be interested in hearing thoughts from those who actually tread in
>> the high-freq domain...
>>
>> A wealth of information can be found here:
>>
>> ?http://www.olsen.ch/publications/working-papers/
>>
>> Jeff
>>
>> On Thu, May 21, 2009 at 10:04 AM, Michael <comtech.usa at gmail.com> wrote:
>>> Hi all,
>>>
>>> I am wondering if there are some special toolboxes to handle high
>>> frequency data in R?
>>>
>>> I have some high frequency data and was wondering what meaningful
>>> experiments can I run on these high frequency data.
>>>
>>> Not sure if normal (low frequency) financial time series textbook data
>>> analysis tools will work for high frequency data?
>>>
>>> Let's say I run a correlation between two stocks using the high
>>> frequency data, or run an ARMA model on one stock, will the results be
>>> meaningful?
>>>
>>> Could anybody point me some classroom types of treatment or lab
>>> tutorial type of document which show me what meaningful
>>> experiments/tests I can run on high frequency data?
>>>
>>> Thanks a lot!
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>>
>>
>> --
>> Jeffrey Ryan
>> jeffrey.ryan at insightalgo.com
>>
>> ia: insight algorithmics
>> www.insightalgo.com
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>


From comtech.usa at gmail.com  Thu May 21 18:17:30 2009
From: comtech.usa at gmail.com (Michael)
Date: Thu, 21 May 2009 09:17:30 -0700
Subject: [R-SIG-Finance] high frequency data analysis in R
In-Reply-To: <b1f16d9d0905210838ud47dbe2n5555c50518794cca@mail.gmail.com>
References: <b1f16d9d0905210804s6fb9832crf935912bd7509a4c@mail.gmail.com>
	<e8e755250905210815h742c04aekfd111ff7e4165c1d@mail.gmail.com>
	<b1f16d9d0905210821y6fa119f6jf4b15cc78ac64c8c@mail.gmail.com>
	<b1f16d9d0905210838ud47dbe2n5555c50518794cca@mail.gmail.com>
Message-ID: <b1f16d9d0905210917l3854ae4dvaf3ac692c4336c5c@mail.gmail.com>

Could anybody comment on my approach of obtaining regularly spaced
data from irregularly spaced price changes, and then use R to process
them?

On Thu, May 21, 2009 at 8:38 AM, Michael <comtech.usa at gmail.com> wrote:
> My data are price change arrivals, irregularly spaced. But when there
> is no price change, the price stays constant. Therefore, in fact, at
> any time instant, you give me a time, I can give you the price at that
> very instant of time. So irregularly spaced data can be easily sampled
> to be regularly spaced data.
> What do you think of this approach?
>
> On Thu, May 21, 2009 at 8:21 AM, Michael <comtech.usa at gmail.com> wrote:
>> Thanks Jeff.
>>
>> By high frequency I mean really the tick data. For example, during
>> peak time, the arrival of price events could be at about hundreds to
>> thousands within one second, irregularly spaced.
>>
>> I've heard that forcing irregularly spaced data into regularly spaced
>> data(e.g. through interpolation) will lose information. How's that so?
>>
>> Thanks!
>>
>> On Thu, May 21, 2009 at 8:15 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
>>> Not my domain, but you will more than likely have to aggregate to some
>>> sort of regular/homogenous type of series for most traditional tools
>>> to work.
>>>
>>> xts has to.period to aggregate up to a lower frequency from tick-level
>>> data. Coupled with something like na.locf you can make yourself some
>>> high frequency 'regular' data from 'irregular'
>>>
>>> Regular and irregular of course depend on what you are looking at
>>> (weekends missing in daily data can still be 'regular').
>>>
>>> I'd be interested in hearing thoughts from those who actually tread in
>>> the high-freq domain...
>>>
>>> A wealth of information can be found here:
>>>
>>> ?http://www.olsen.ch/publications/working-papers/
>>>
>>> Jeff
>>>
>>> On Thu, May 21, 2009 at 10:04 AM, Michael <comtech.usa at gmail.com> wrote:
>>>> Hi all,
>>>>
>>>> I am wondering if there are some special toolboxes to handle high
>>>> frequency data in R?
>>>>
>>>> I have some high frequency data and was wondering what meaningful
>>>> experiments can I run on these high frequency data.
>>>>
>>>> Not sure if normal (low frequency) financial time series textbook data
>>>> analysis tools will work for high frequency data?
>>>>
>>>> Let's say I run a correlation between two stocks using the high
>>>> frequency data, or run an ARMA model on one stock, will the results be
>>>> meaningful?
>>>>
>>>> Could anybody point me some classroom types of treatment or lab
>>>> tutorial type of document which show me what meaningful
>>>> experiments/tests I can run on high frequency data?
>>>>
>>>> Thanks a lot!
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>
>>>
>>>
>>> --
>>> Jeffrey Ryan
>>> jeffrey.ryan at insightalgo.com
>>>
>>> ia: insight algorithmics
>>> www.insightalgo.com
>>>
>>
>


From jeff.a.ryan at gmail.com  Thu May 21 18:23:32 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 21 May 2009 11:23:32 -0500
Subject: [R-SIG-Finance] high frequency data analysis in R
In-Reply-To: <197a5bbc0905210913q421b2462tc2c9b5dcfa3ebb8e@mail.gmail.com>
References: <b1f16d9d0905210804s6fb9832crf935912bd7509a4c@mail.gmail.com>
	<e8e755250905210815h742c04aekfd111ff7e4165c1d@mail.gmail.com>
	<197a5bbc0905210913q421b2462tc2c9b5dcfa3ebb8e@mail.gmail.com>
Message-ID: <e8e755250905210923g43857fbds304b33c24dea849f@mail.gmail.com>

Not to distract from the underlying processing question, but to answer
the 'data' one:

The data in R should be too much of an issue, at least from a size perspective.

xts objects on the order of millions of observations are still fast
and memory friendly with respect to copying operations internal to
many xts calls (merge, subset, etc).

> x <- .xts(1:1e6, 1:1e6)
> system.time(merge(x,x))
   user  system elapsed
  0.037   0.015   0.053


7 million obs of a single column xts is ~54 Mb.  Certainly you can
handle quite a bit of data if you have anything more than trivial
amounts of RAM.

quantmod now has (devel) an attachSymbols function that makes
lazy-loading data very easy, so all your data can be stored as xts
objects and read in on-demand.

xts is also getting the ability to query subsets of data on disk, by
time.  This will have no practical limit.

For current data solutions xts, fts (C++), data.table, and some other
solutions should mitigate your problems, if not solve the 'data' side
all together.


HTH
Jeff



On Thu, May 21, 2009 at 11:13 AM, Hae Kyung Im <hakyim at gmail.com> wrote:
> I think in general you would need some sort of pre-processing before using R.
>
> You can use periodic sampling of prices, but you may be throwing away
> a lot of information. This is a method that used to be recommended
> more than 5 years ago in order to mitigate the effect of market noise.
> At least in the context of volatility estimation.
>
> Here is my experience with tick data:
>
> I used FX data to calculate estimated daily volatility using TSRV
> (Zhang et al 2005
> http://galton.uchicago.edu/~mykland/paperlinks/p1394.pdf). Using the
> time series of estimated daily volatilities, I forecasted volatilities
> for 1 day up to 1 year ahead. The tick data was in Quantitative
> Analytics database. I used their C++ API to query daily data, computed
> the TSRV estimator in C++ and saved the result in text file. Then I
> used R to read the estimated volatilities and used FARIMA to forecast
> volatility. An interesting thing about this type of series is that the
> fractional coefficient is approximately 0.4 in many instances.
> Bollerslev has a paper commenting on this fact.
>
> In another project, I had treasury futures market depth data. The data
> came in plain text format, with one file per day. Each day had more
> than 1 million entries. I don't think I could handle this with R. To
> get started I decided to use only actual trades. I used Python to
> filter out the trades. So this came down to ~60K entries per day. This
> I could handle with R. I used to.period from xts package to aggregate
> the data.
>
> In order to handle market depth data, we need some efficient way to
> access (query) this huge database. I looked a little bit into kdb but
> you have to pay ~25K to buy the software for one processor. I haven't
> been able to look more into this for now.
>
> Haky
>
>
>
>
> On Thu, May 21, 2009 at 10:15 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
>> Not my domain, but you will more than likely have to aggregate to some
>> sort of regular/homogenous type of series for most traditional tools
>> to work.
>>
>> xts has to.period to aggregate up to a lower frequency from tick-level
>> data. Coupled with something like na.locf you can make yourself some
>> high frequency 'regular' data from 'irregular'
>>
>> Regular and irregular of course depend on what you are looking at
>> (weekends missing in daily data can still be 'regular').
>>
>> I'd be interested in hearing thoughts from those who actually tread in
>> the high-freq domain...
>>
>> A wealth of information can be found here:
>>
>> ?http://www.olsen.ch/publications/working-papers/
>>
>> Jeff
>>
>> On Thu, May 21, 2009 at 10:04 AM, Michael <comtech.usa at gmail.com> wrote:
>>> Hi all,
>>>
>>> I am wondering if there are some special toolboxes to handle high
>>> frequency data in R?
>>>
>>> I have some high frequency data and was wondering what meaningful
>>> experiments can I run on these high frequency data.
>>>
>>> Not sure if normal (low frequency) financial time series textbook data
>>> analysis tools will work for high frequency data?
>>>
>>> Let's say I run a correlation between two stocks using the high
>>> frequency data, or run an ARMA model on one stock, will the results be
>>> meaningful?
>>>
>>> Could anybody point me some classroom types of treatment or lab
>>> tutorial type of document which show me what meaningful
>>> experiments/tests I can run on high frequency data?
>>>
>>> Thanks a lot!
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>>
>>
>> --
>> Jeffrey Ryan
>> jeffrey.ryan at insightalgo.com
>>
>> ia: insight algorithmics
>> www.insightalgo.com
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From hakyim at gmail.com  Thu May 21 18:51:23 2009
From: hakyim at gmail.com (Hae Kyung Im)
Date: Thu, 21 May 2009 11:51:23 -0500
Subject: [R-SIG-Finance] high frequency data analysis in R
In-Reply-To: <b1f16d9d0905210838ud47dbe2n5555c50518794cca@mail.gmail.com>
References: <b1f16d9d0905210804s6fb9832crf935912bd7509a4c@mail.gmail.com>
	<e8e755250905210815h742c04aekfd111ff7e4165c1d@mail.gmail.com>
	<b1f16d9d0905210821y6fa119f6jf4b15cc78ac64c8c@mail.gmail.com>
	<b1f16d9d0905210838ud47dbe2n5555c50518794cca@mail.gmail.com>
Message-ID: <197a5bbc0905210951oef8b273vda150c71a31ff38c@mail.gmail.com>

Relating the approach that turns irregular data into regular one,
I guess it's a complex question and how you approach it will depend on
the specific problem.

With your method, you would assume that the price is equal to the last
traded price or something like that. If there is no trade for some
time, would it make sense to say that the price is the last traded
price? If you wanted to actually buy/sell at that price, it's not
obvious that you'll be able to do so.

Also, if you only look at the time series of instantaneous prices, you
would be losing a lot of information about what happened in between
the time points. It makes more sense to aggregate and keep, for
example, open, high, low and close. Or some statistics on the
distribution of the prices between the endpoints.

If what you need to calculate is correlations, then I would look at
the papers that Liviu suggested. It seems that synchronicity is
critical. I heard there is an extension of TSRV to correlations.

If you only need to look at univariate time series, you may be able to
get away with your method more easily. It may not be statistically
efficient but may give you a good enough answer in some cases.


HTH
Haky



On Thu, May 21, 2009 at 10:38 AM, Michael <comtech.usa at gmail.com> wrote:
> My data are price change arrivals, irregularly spaced. But when there
> is no price change, the price stays constant. Therefore, in fact, at
> any time instant, you give me a time, I can give you the price at that
> very instant of time. So irregularly spaced data can be easily sampled
> to be regularly spaced data.
> What do you think of this approach?
>
> On Thu, May 21, 2009 at 8:21 AM, Michael <comtech.usa at gmail.com> wrote:
>> Thanks Jeff.
>>
>> By high frequency I mean really the tick data. For example, during
>> peak time, the arrival of price events could be at about hundreds to
>> thousands within one second, irregularly spaced.
>>
>> I've heard that forcing irregularly spaced data into regularly spaced
>> data(e.g. through interpolation) will lose information. How's that so?
>>
>> Thanks!
>>
>> On Thu, May 21, 2009 at 8:15 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
>>> Not my domain, but you will more than likely have to aggregate to some
>>> sort of regular/homogenous type of series for most traditional tools
>>> to work.
>>>
>>> xts has to.period to aggregate up to a lower frequency from tick-level
>>> data. Coupled with something like na.locf you can make yourself some
>>> high frequency 'regular' data from 'irregular'
>>>
>>> Regular and irregular of course depend on what you are looking at
>>> (weekends missing in daily data can still be 'regular').
>>>
>>> I'd be interested in hearing thoughts from those who actually tread in
>>> the high-freq domain...
>>>
>>> A wealth of information can be found here:
>>>
>>> ?http://www.olsen.ch/publications/working-papers/
>>>
>>> Jeff
>>>
>>> On Thu, May 21, 2009 at 10:04 AM, Michael <comtech.usa at gmail.com> wrote:
>>>> Hi all,
>>>>
>>>> I am wondering if there are some special toolboxes to handle high
>>>> frequency data in R?
>>>>
>>>> I have some high frequency data and was wondering what meaningful
>>>> experiments can I run on these high frequency data.
>>>>
>>>> Not sure if normal (low frequency) financial time series textbook data
>>>> analysis tools will work for high frequency data?
>>>>
>>>> Let's say I run a correlation between two stocks using the high
>>>> frequency data, or run an ARMA model on one stock, will the results be
>>>> meaningful?
>>>>
>>>> Could anybody point me some classroom types of treatment or lab
>>>> tutorial type of document which show me what meaningful
>>>> experiments/tests I can run on high frequency data?
>>>>
>>>> Thanks a lot!
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>
>>>
>>>
>>> --
>>> Jeffrey Ryan
>>> jeffrey.ryan at insightalgo.com
>>>
>>> ia: insight algorithmics
>>> www.insightalgo.com
>>>
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From hakyim at gmail.com  Thu May 21 19:02:20 2009
From: hakyim at gmail.com (Hae Kyung Im)
Date: Thu, 21 May 2009 12:02:20 -0500
Subject: [R-SIG-Finance] high frequency data analysis in R
In-Reply-To: <e8e755250905210923g43857fbds304b33c24dea849f@mail.gmail.com>
References: <b1f16d9d0905210804s6fb9832crf935912bd7509a4c@mail.gmail.com>
	<e8e755250905210815h742c04aekfd111ff7e4165c1d@mail.gmail.com>
	<197a5bbc0905210913q421b2462tc2c9b5dcfa3ebb8e@mail.gmail.com>
	<e8e755250905210923g43857fbds304b33c24dea849f@mail.gmail.com>
Message-ID: <197a5bbc0905211002x3a774a6eke672c3560e057d23@mail.gmail.com>

Jeff,

This is very impressive. Even on my Macbook Air it takes less than 0.2
seconds total.

> x <- .xts(1:1e6, 1:1e6)
> system.time(merge(x,x))
   user  system elapsed
  0.093   0.021   0.198


> quantmod now has (devel) an attachSymbols function that makes
> lazy-loading data very easy, so all your data can be stored as xts
> objects and read in on-demand.

When you say stored, does it mean on disk or memory?

> xts is also getting the ability to query subsets of data on disk, by
> time.  This will have no practical limit.

This would be great! Will we be able to append data to xts stored on disk?


Thanks
Haky



On Thu, May 21, 2009 at 11:23 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> Not to distract from the underlying processing question, but to answer
> the 'data' one:
>
> The data in R should be too much of an issue, at least from a size perspective.
>
> xts objects on the order of millions of observations are still fast
> and memory friendly with respect to copying operations internal to
> many xts calls (merge, subset, etc).
>
>> x <- .xts(1:1e6, 1:1e6)
>> system.time(merge(x,x))
> ? user ?system elapsed
> ?0.037 ? 0.015 ? 0.053
>
>
> 7 million obs of a single column xts is ~54 Mb. ?Certainly you can
> handle quite a bit of data if you have anything more than trivial
> amounts of RAM.
>
> quantmod now has (devel) an attachSymbols function that makes
> lazy-loading data very easy, so all your data can be stored as xts
> objects and read in on-demand.
>
> xts is also getting the ability to query subsets of data on disk, by
> time. ?This will have no practical limit.
>
> For current data solutions xts, fts (C++), data.table, and some other
> solutions should mitigate your problems, if not solve the 'data' side
> all together.
>
>
> HTH
> Jeff
>
>
>
> On Thu, May 21, 2009 at 11:13 AM, Hae Kyung Im <hakyim at gmail.com> wrote:
>> I think in general you would need some sort of pre-processing before using R.
>>
>> You can use periodic sampling of prices, but you may be throwing away
>> a lot of information. This is a method that used to be recommended
>> more than 5 years ago in order to mitigate the effect of market noise.
>> At least in the context of volatility estimation.
>>
>> Here is my experience with tick data:
>>
>> I used FX data to calculate estimated daily volatility using TSRV
>> (Zhang et al 2005
>> http://galton.uchicago.edu/~mykland/paperlinks/p1394.pdf). Using the
>> time series of estimated daily volatilities, I forecasted volatilities
>> for 1 day up to 1 year ahead. The tick data was in Quantitative
>> Analytics database. I used their C++ API to query daily data, computed
>> the TSRV estimator in C++ and saved the result in text file. Then I
>> used R to read the estimated volatilities and used FARIMA to forecast
>> volatility. An interesting thing about this type of series is that the
>> fractional coefficient is approximately 0.4 in many instances.
>> Bollerslev has a paper commenting on this fact.
>>
>> In another project, I had treasury futures market depth data. The data
>> came in plain text format, with one file per day. Each day had more
>> than 1 million entries. I don't think I could handle this with R. To
>> get started I decided to use only actual trades. I used Python to
>> filter out the trades. So this came down to ~60K entries per day. This
>> I could handle with R. I used to.period from xts package to aggregate
>> the data.
>>
>> In order to handle market depth data, we need some efficient way to
>> access (query) this huge database. I looked a little bit into kdb but
>> you have to pay ~25K to buy the software for one processor. I haven't
>> been able to look more into this for now.
>>
>> Haky
>>
>>
>>
>>
>> On Thu, May 21, 2009 at 10:15 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
>>> Not my domain, but you will more than likely have to aggregate to some
>>> sort of regular/homogenous type of series for most traditional tools
>>> to work.
>>>
>>> xts has to.period to aggregate up to a lower frequency from tick-level
>>> data. Coupled with something like na.locf you can make yourself some
>>> high frequency 'regular' data from 'irregular'
>>>
>>> Regular and irregular of course depend on what you are looking at
>>> (weekends missing in daily data can still be 'regular').
>>>
>>> I'd be interested in hearing thoughts from those who actually tread in
>>> the high-freq domain...
>>>
>>> A wealth of information can be found here:
>>>
>>> ?http://www.olsen.ch/publications/working-papers/
>>>
>>> Jeff
>>>
>>> On Thu, May 21, 2009 at 10:04 AM, Michael <comtech.usa at gmail.com> wrote:
>>>> Hi all,
>>>>
>>>> I am wondering if there are some special toolboxes to handle high
>>>> frequency data in R?
>>>>
>>>> I have some high frequency data and was wondering what meaningful
>>>> experiments can I run on these high frequency data.
>>>>
>>>> Not sure if normal (low frequency) financial time series textbook data
>>>> analysis tools will work for high frequency data?
>>>>
>>>> Let's say I run a correlation between two stocks using the high
>>>> frequency data, or run an ARMA model on one stock, will the results be
>>>> meaningful?
>>>>
>>>> Could anybody point me some classroom types of treatment or lab
>>>> tutorial type of document which show me what meaningful
>>>> experiments/tests I can run on high frequency data?
>>>>
>>>> Thanks a lot!
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>
>>>
>>>
>>> --
>>> Jeffrey Ryan
>>> jeffrey.ryan at insightalgo.com
>>>
>>> ia: insight algorithmics
>>> www.insightalgo.com
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>
>
>
> --
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com
>


From jeff.a.ryan at gmail.com  Thu May 21 19:25:11 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 21 May 2009 12:25:11 -0500
Subject: [R-SIG-Finance] high frequency data analysis in R
In-Reply-To: <197a5bbc0905211002x3a774a6eke672c3560e057d23@mail.gmail.com>
References: <b1f16d9d0905210804s6fb9832crf935912bd7509a4c@mail.gmail.com>
	<e8e755250905210815h742c04aekfd111ff7e4165c1d@mail.gmail.com>
	<197a5bbc0905210913q421b2462tc2c9b5dcfa3ebb8e@mail.gmail.com>
	<e8e755250905210923g43857fbds304b33c24dea849f@mail.gmail.com>
	<197a5bbc0905211002x3a774a6eke672c3560e057d23@mail.gmail.com>
Message-ID: <e8e755250905211025r3e3a064cg4ac827c4b148a10c@mail.gmail.com>

Haky,

My times are from a new R session on a MacBook 2.16, so yes it is fast.

On Thu, May 21, 2009 at 12:02 PM, Hae Kyung Im <hakyim at gmail.com> wrote:
> Jeff,
>
> This is very impressive. Even on my Macbook Air it takes less than 0.2
> seconds total.
>
>> x <- .xts(1:1e6, 1:1e6)
>> system.time(merge(x,x))
> ? user ?system elapsed
> ?0.093 ? 0.021 ? 0.198
>


>
>> quantmod now has (devel) an attachSymbols function that makes
>> lazy-loading data very easy, so all your data can be stored as xts
>> objects and read in on-demand.
>
> When you say stored, does it mean on disk or memory?
>

attachSymbols can use disk or memory for caching, but the files are
read with getSymbols, so they can realistically be stored anywhere.
The docs provide at least a small introduction.

The tutorial I gave at R/Finance 2009 gives a small example as well.

http://www.RinFinance.com/presentations

>> xts is also getting the ability to query subsets of data on disk, by
>> time. ?This will have no practical limit.
>
> This would be great! Will we be able to append data to xts stored on disk?
>

The core issue is read or write optimized.  I lean toward read
optimization, so something akin to a column-based structure.  This
will make writes more costly, but that would be acceptable to me at
the moment.  Probably keep some sort of write structure => read
structure tool in the mix as well.

I will of course keep the list updated on progress here once it is
ready for release.

>
> Thanks
> Haky
>

Thanks,
Jeff
>
>
> On Thu, May 21, 2009 at 11:23 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
>> Not to distract from the underlying processing question, but to answer
>> the 'data' one:
>>
>> The data in R should be too much of an issue, at least from a size perspective.
>>
>> xts objects on the order of millions of observations are still fast
>> and memory friendly with respect to copying operations internal to
>> many xts calls (merge, subset, etc).
>>
>>> x <- .xts(1:1e6, 1:1e6)
>>> system.time(merge(x,x))
>> ? user ?system elapsed
>> ?0.037 ? 0.015 ? 0.053
>>
>>
>> 7 million obs of a single column xts is ~54 Mb. ?Certainly you can
>> handle quite a bit of data if you have anything more than trivial
>> amounts of RAM.
>>
>> quantmod now has (devel) an attachSymbols function that makes
>> lazy-loading data very easy, so all your data can be stored as xts
>> objects and read in on-demand.
>>
>> xts is also getting the ability to query subsets of data on disk, by
>> time. ?This will have no practical limit.
>>
>> For current data solutions xts, fts (C++), data.table, and some other
>> solutions should mitigate your problems, if not solve the 'data' side
>> all together.
>>
>>
>> HTH
>> Jeff
>>
>>
>>
>> On Thu, May 21, 2009 at 11:13 AM, Hae Kyung Im <hakyim at gmail.com> wrote:
>>> I think in general you would need some sort of pre-processing before using R.
>>>
>>> You can use periodic sampling of prices, but you may be throwing away
>>> a lot of information. This is a method that used to be recommended
>>> more than 5 years ago in order to mitigate the effect of market noise.
>>> At least in the context of volatility estimation.
>>>
>>> Here is my experience with tick data:
>>>
>>> I used FX data to calculate estimated daily volatility using TSRV
>>> (Zhang et al 2005
>>> http://galton.uchicago.edu/~mykland/paperlinks/p1394.pdf). Using the
>>> time series of estimated daily volatilities, I forecasted volatilities
>>> for 1 day up to 1 year ahead. The tick data was in Quantitative
>>> Analytics database. I used their C++ API to query daily data, computed
>>> the TSRV estimator in C++ and saved the result in text file. Then I
>>> used R to read the estimated volatilities and used FARIMA to forecast
>>> volatility. An interesting thing about this type of series is that the
>>> fractional coefficient is approximately 0.4 in many instances.
>>> Bollerslev has a paper commenting on this fact.
>>>
>>> In another project, I had treasury futures market depth data. The data
>>> came in plain text format, with one file per day. Each day had more
>>> than 1 million entries. I don't think I could handle this with R. To
>>> get started I decided to use only actual trades. I used Python to
>>> filter out the trades. So this came down to ~60K entries per day. This
>>> I could handle with R. I used to.period from xts package to aggregate
>>> the data.
>>>
>>> In order to handle market depth data, we need some efficient way to
>>> access (query) this huge database. I looked a little bit into kdb but
>>> you have to pay ~25K to buy the software for one processor. I haven't
>>> been able to look more into this for now.
>>>
>>> Haky
>>>
>>>
>>>
>>>
>>> On Thu, May 21, 2009 at 10:15 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
>>>> Not my domain, but you will more than likely have to aggregate to some
>>>> sort of regular/homogenous type of series for most traditional tools
>>>> to work.
>>>>
>>>> xts has to.period to aggregate up to a lower frequency from tick-level
>>>> data. Coupled with something like na.locf you can make yourself some
>>>> high frequency 'regular' data from 'irregular'
>>>>
>>>> Regular and irregular of course depend on what you are looking at
>>>> (weekends missing in daily data can still be 'regular').
>>>>
>>>> I'd be interested in hearing thoughts from those who actually tread in
>>>> the high-freq domain...
>>>>
>>>> A wealth of information can be found here:
>>>>
>>>> ?http://www.olsen.ch/publications/working-papers/
>>>>
>>>> Jeff
>>>>
>>>> On Thu, May 21, 2009 at 10:04 AM, Michael <comtech.usa at gmail.com> wrote:
>>>>> Hi all,
>>>>>
>>>>> I am wondering if there are some special toolboxes to handle high
>>>>> frequency data in R?
>>>>>
>>>>> I have some high frequency data and was wondering what meaningful
>>>>> experiments can I run on these high frequency data.
>>>>>
>>>>> Not sure if normal (low frequency) financial time series textbook data
>>>>> analysis tools will work for high frequency data?
>>>>>
>>>>> Let's say I run a correlation between two stocks using the high
>>>>> frequency data, or run an ARMA model on one stock, will the results be
>>>>> meaningful?
>>>>>
>>>>> Could anybody point me some classroom types of treatment or lab
>>>>> tutorial type of document which show me what meaningful
>>>>> experiments/tests I can run on high frequency data?
>>>>>
>>>>> Thanks a lot!
>>>>>
>>>>> _______________________________________________
>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>> -- Subscriber-posting only.
>>>>> -- If you want to post, subscribe first.
>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> Jeffrey Ryan
>>>> jeffrey.ryan at insightalgo.com
>>>>
>>>> ia: insight algorithmics
>>>> www.insightalgo.com
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>
>>
>>
>>
>> --
>> Jeffrey Ryan
>> jeffrey.ryan at insightalgo.com
>>
>> ia: insight algorithmics
>> www.insightalgo.com
>>
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From edd at debian.org  Thu May 21 19:42:13 2009
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 21 May 2009 12:42:13 -0500
Subject: [R-SIG-Finance] Kdb (Was:  high frequency data analysis in R)
In-Reply-To: <197a5bbc0905210913q421b2462tc2c9b5dcfa3ebb8e@mail.gmail.com>
References: <b1f16d9d0905210804s6fb9832crf935912bd7509a4c@mail.gmail.com>
	<e8e755250905210815h742c04aekfd111ff7e4165c1d@mail.gmail.com>
	<197a5bbc0905210913q421b2462tc2c9b5dcfa3ebb8e@mail.gmail.com>
Message-ID: <18965.37493.165830.47114@ron.nulle.part>


On 21 May 2009 at 11:13, Hae Kyung Im wrote:
| access (query) this huge database. I looked a little bit into kdb but
| you have to pay ~25K to buy the software for one processor. I haven't

True, but you can have "free" (as in beer) 32bit version that times out after
two hours. That's not a bad compromise.    

I looked at it for a bit, and I has an R interface. (My blog has a patch to
fix their then-broken interface to R's Datetime; I think they may have
integrated that by now).  Then again you can also pre-process into RData
files, or use hdf5, or use a gazillion other methods.   But the free trial
version may just help for the odd research project like the one Haky
described. 

Dirk

-- 
Three out of two people have difficulties with fractions.


From etyurin at skipstonellc.com  Thu May 21 19:48:45 2009
From: etyurin at skipstonellc.com (Eugene Tyurin)
Date: Thu, 21 May 2009 13:48:45 -0400
Subject: [R-SIG-Finance] high frequency data analysis in R
In-Reply-To: <b1f16d9d0905210838ud47dbe2n5555c50518794cca@mail.gmail.com>
References: <b1f16d9d0905210804s6fb9832crf935912bd7509a4c@mail.gmail.com>
	<e8e755250905210815h742c04aekfd111ff7e4165c1d@mail.gmail.com>
	<b1f16d9d0905210821y6fa119f6jf4b15cc78ac64c8c@mail.gmail.com>
	<b1f16d9d0905210838ud47dbe2n5555c50518794cca@mail.gmail.com>
Message-ID: <e4adf3900905211048m3f40eb65x5d375801ec6f3374@mail.gmail.com>

High-frequency is not my specialty either, but a quote caught my attention:

On Thu, May 21, 2009 at 11:38 AM, Michael <comtech.usa at gmail.com> wrote:
> My data are price change arrivals, irregularly spaced. But when there
> is no price change, the price stays constant. Therefore, in fact, at
> any time instant, you give me a time, I can give you the price at that
> very instant of time. So irregularly spaced data can be easily sampled
> to be regularly spaced data.

>From a trader's perspective, you do not have "the price" at any time
outside of the instant a trade took place - you have NBBO (and market
depth). Last trade's price may or may not be transactable again on
either long or short side.

You can alternatively say that you have an instanteneous "mid-market
price" and a bid/ask spread to work with.

Correct me if I'm wrong - I'd like to know how people in HF really
look at their data.

-- ET.


From B_Rowe at ml.com  Thu May 21 19:52:51 2009
From: B_Rowe at ml.com (Rowe, Brian Lee Yung (Portfolio Analytics))
Date: Thu, 21 May 2009 13:52:51 -0400
Subject: [R-SIG-Finance] Preprocessing RData file (Was: Kdb (Was: high
	frequency data analysis in R))
In-Reply-To: <18965.37493.165830.47114@ron.nulle.part>
Message-ID: <3BAD818D9407B043817CC6D89ABA14EC032B85AE@MLNYC20MB051.amrs.win.ml.com>

Is there any literature on the relative performance gain of
preprocessing data into RData and then reading into R? Does it breakdown
anywhere? I have 4 GB of data that I'm reading in and I/O is a large
bottleneck.

Brian


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Dirk
Eddelbuettel
Sent: Thursday, May 21, 2009 1:42 PM
To: Hae Kyung Im
Cc: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Kdb (Was: high frequency data analysis in R)



On 21 May 2009 at 11:13, Hae Kyung Im wrote:
| access (query) this huge database. I looked a little bit into kdb but
| you have to pay ~25K to buy the software for one processor. I haven't

True, but you can have "free" (as in beer) 32bit version that times out
after
two hours. That's not a bad compromise.    

I looked at it for a bit, and I has an R interface. (My blog has a patch
to
fix their then-broken interface to R's Datetime; I think they may have
integrated that by now).  Then again you can also pre-process into RData
files, or use hdf5, or use a gazillion other methods.   But the free
trial
version may just help for the odd research project like the one Haky
described. 

Dirk

-- 
Three out of two people have difficulties with fractions.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

--------------------------------------------------------------------------
This message w/attachments (message) may be privileged, confidential or proprietary, and if you are not an intended recipient, please notify the sender, do not use or share it and delete it. Unless specifically indicated, this message is not an offer to sell or a solicitation of any investment products or other financial product or service, an official confirmation of any transaction, or an official statement of Merrill Lynch. Subject to applicable law, Merrill Lynch may monitor, review and retain e-communications (EC) traveling through its networks/systems. The laws of the country of each sender/recipient may impact the handling of EC, and EC may be archived, supervised and produced in countries other than the country in which you are located. This message cannot be guaranteed to be secure or error-free. References to "Merrill Lynch" are references to any company in the Merrill Lynch & Co., Inc. group of companies, which are wholly-owned by Bank of America Corporation. Securities and Insurance Products: * Are Not FDIC Insured * Are Not Bank Guaranteed * May Lose Value * Are Not a Bank Deposit * Are Not a Condition to Any Banking Service or Activity * Are Not Insured by Any Federal Government Agency. Attachments that are part of this E-communication may have additional important disclosures and disclaimers, which you should read. This message is subject to terms available at the following link: http://www.ml.com/e-communications_terms/. By messaging with Merrill Lynch you consent to the foregoing.
--------------------------------------------------------------------------


From jeff.a.ryan at gmail.com  Thu May 21 19:57:56 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 21 May 2009 12:57:56 -0500
Subject: [R-SIG-Finance] Preprocessing RData file (Was: Kdb (Was: high
	frequency data analysis in R))
In-Reply-To: <3BAD818D9407B043817CC6D89ABA14EC032B85AE@MLNYC20MB051.amrs.win.ml.com>
References: <18965.37493.165830.47114@ron.nulle.part>
	<3BAD818D9407B043817CC6D89ABA14EC032B85AE@MLNYC20MB051.amrs.win.ml.com>
Message-ID: <e8e755250905211057v6611568ev5f74f4cb5170bc5c@mail.gmail.com>

I feel like I should change the title again... :)

The RData files are compressed first off. If you don't want the gzip
overhead, get rid of it.

The xts format 'on-disk' is nothing more that the structure from
memory written to disk.  This manages to be both faster and takes up
less space.  It isn't a huge gain, but it allows for binary searching
of the index to get to the data you want.

I will put together a performance comparison at some point, and pass along.

Jeff

On Thu, May 21, 2009 at 12:52 PM, Rowe, Brian Lee Yung (Portfolio
Analytics) <B_Rowe at ml.com> wrote:
> Is there any literature on the relative performance gain of
> preprocessing data into RData and then reading into R? Does it breakdown
> anywhere? I have 4 GB of data that I'm reading in and I/O is a large
> bottleneck.
>
> Brian
>
>
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Dirk
> Eddelbuettel
> Sent: Thursday, May 21, 2009 1:42 PM
> To: Hae Kyung Im
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] Kdb (Was: high frequency data analysis in R)
>
>
>
> On 21 May 2009 at 11:13, Hae Kyung Im wrote:
> | access (query) this huge database. I looked a little bit into kdb but
> | you have to pay ~25K to buy the software for one processor. I haven't
>
> True, but you can have "free" (as in beer) 32bit version that times out
> after
> two hours. That's not a bad compromise.
>
> I looked at it for a bit, and I has an R interface. (My blog has a patch
> to
> fix their then-broken interface to R's Datetime; I think they may have
> integrated that by now). ?Then again you can also pre-process into RData
> files, or use hdf5, or use a gazillion other methods. ? But the free
> trial
> version may just help for the odd research project like the one Haky
> described.
>
> Dirk
>
> --
> Three out of two people have difficulties with fractions.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
> --------------------------------------------------------------------------
> This message w/attachments (message) may be privileged, confidential or proprietary, and if you are not an intended recipient, please notify the sender, do not use or share it and delete it. Unless specifically indicated, this message is not an offer to sell or a solicitation of any investment products or other financial product or service, an official confirmation of any transaction, or an official statement of Merrill Lynch. Subject to applicable law, Merrill Lynch may monitor, review and retain e-communications (EC) traveling through its networks/systems. The laws of the country of each sender/recipient may impact the handling of EC, and EC may be archived, supervised and produced in countries other than the country in which you are located. This message cannot be guaranteed to be secure or error-free. References to "Merrill Lynch" are references to any company in the Merrill Lynch & Co., Inc. group of companies, which are wholly-owned by Bank of America Corporation. Secu!
> ?rities and Insurance Products: * Are Not FDIC Insured * Are Not Bank Guaranteed * May Lose Value * Are Not a Bank Deposit * Are Not a Condition to Any Banking Service or Activity * Are Not Insured by Any Federal Government Agency. Attachments that are part of this E-communication may have additional important disclosures and disclaimers, which you should read. This message is subject to terms available at the following link: http://www.ml.com/e-communications_terms/. By messaging with Merrill Lynch you consent to the foregoing.
> --------------------------------------------------------------------------
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From B_Rowe at ml.com  Thu May 21 20:10:47 2009
From: B_Rowe at ml.com (Rowe, Brian Lee Yung (Portfolio Analytics))
Date: Thu, 21 May 2009 14:10:47 -0400
Subject: [R-SIG-Finance] Preprocessing RData file (Was: Kdb (Was: high
	frequency data analysis in R))
In-Reply-To: <e8e755250905211057v6611568ev5f74f4cb5170bc5c@mail.gmail.com>
Message-ID: <3BAD818D9407B043817CC6D89ABA14EC032B85AF@MLNYC20MB051.amrs.win.ml.com>

... and possibly even a list change.

Do you plan on making this compatible with ff or bigmemory? Seems like this theme is making its rounds.

Brian 

-----Original Message-----
From: Jeff Ryan [mailto:jeff.a.ryan at gmail.com] 
Sent: Thursday, May 21, 2009 1:58 PM
To: Rowe, Brian Lee Yung (Portfolio Analytics)
Cc: Dirk Eddelbuettel; Hae Kyung Im; r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Preprocessing RData file (Was: Kdb (Was: high frequency data analysis in R))


I feel like I should change the title again... :)

The RData files are compressed first off. If you don't want the gzip
overhead, get rid of it.

The xts format 'on-disk' is nothing more that the structure from
memory written to disk.  This manages to be both faster and takes up
less space.  It isn't a huge gain, but it allows for binary searching
of the index to get to the data you want.

I will put together a performance comparison at some point, and pass along.

Jeff

On Thu, May 21, 2009 at 12:52 PM, Rowe, Brian Lee Yung (Portfolio
Analytics) <B_Rowe at ml.com> wrote:
> Is there any literature on the relative performance gain of
> preprocessing data into RData and then reading into R? Does it breakdown
> anywhere? I have 4 GB of data that I'm reading in and I/O is a large
> bottleneck.
>
> Brian
>
>
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Dirk
> Eddelbuettel
> Sent: Thursday, May 21, 2009 1:42 PM
> To: Hae Kyung Im
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] Kdb (Was: high frequency data analysis in R)
>
>
>
> On 21 May 2009 at 11:13, Hae Kyung Im wrote:
> | access (query) this huge database. I looked a little bit into kdb but
> | you have to pay ~25K to buy the software for one processor. I haven't
>
> True, but you can have "free" (as in beer) 32bit version that times out
> after
> two hours. That's not a bad compromise.
>
> I looked at it for a bit, and I has an R interface. (My blog has a patch
> to
> fix their then-broken interface to R's Datetime; I think they may have
> integrated that by now). ?Then again you can also pre-process into RData
> files, or use hdf5, or use a gazillion other methods. ? But the free
> trial
> version may just help for the odd research project like the one Haky
> described.
>
> Dirk
>
> --
> Three out of two people have difficulties with fractions.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
> --------------------------------------------------------------------------
> This message w/attachments (message) may be privileged, confidential or proprietary, and if you are not an intended recipient, please notify the sender, do not use or share it and delete it. Unless specifically indicated, this message is not an offer to sell or a solicitation of any investment products or other financial product or service, an official confirmation of any transaction, or an official statement of Merrill Lynch. Subject to applicable law, Merrill Lynch may monitor, review and retain e-communications (EC) traveling through its networks/systems. The laws of the country of each sender/recipient may impact the handling of EC, and EC may be archived, supervised and produced in countries other than the country in which you are located. This message cannot be guaranteed to be secure or error-free. References to "Merrill Lynch" are references to any company in the Merrill Lynch & Co., Inc. group of companies, which are wholly-owned by Bank of America Corporation. Secu!
> ?rities and Insurance Products: * Are Not FDIC Insured * Are Not Bank Guaranteed * May Lose Value * Are Not a Bank Deposit * Are Not a Condition to Any Banking Service or Activity * Are Not Insured by Any Federal Government Agency. Attachments that are part of this E-communication may have additional important disclosures and disclaimers, which you should read. This message is subject to terms available at the following link: http://www.ml.com/e-communications_terms/. By messaging with Merrill Lynch you consent to the foregoing.
> --------------------------------------------------------------------------
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From shane.conway at gmail.com  Thu May 21 20:15:06 2009
From: shane.conway at gmail.com (Shane Conway)
Date: Thu, 21 May 2009 14:15:06 -0400
Subject: [R-SIG-Finance] high frequency data analysis in R
In-Reply-To: <197a5bbc0905210951oef8b273vda150c71a31ff38c@mail.gmail.com>
References: <b1f16d9d0905210804s6fb9832crf935912bd7509a4c@mail.gmail.com> 
	<e8e755250905210815h742c04aekfd111ff7e4165c1d@mail.gmail.com> 
	<b1f16d9d0905210821y6fa119f6jf4b15cc78ac64c8c@mail.gmail.com> 
	<b1f16d9d0905210838ud47dbe2n5555c50518794cca@mail.gmail.com> 
	<197a5bbc0905210951oef8b273vda150c71a31ff38c@mail.gmail.com>
Message-ID: <dd3243090905211115j62f3bbabkf824c60bd00c6b13@mail.gmail.com>

Some resources:

If you want to deal with irregular data, Eric Zivot's book on
financial time series mentions some operators that are available in
S-Plus based on this Zumback/Muller paper:
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=208278.  These
could be easily adapted to R.  Ruey Tsay's book also has a chapter
that touches on it.  In general terms, Dacorogna et. al. is a good
overview.

And as already mentioned, definitely look at the realized package:
http://students.washington.edu/spayseur/realized/.


On Thu, May 21, 2009 at 12:51 PM, Hae Kyung Im <hakyim at gmail.com> wrote:
> Relating the approach that turns irregular data into regular one,
> I guess it's a complex question and how you approach it will depend on
> the specific problem.
>
> With your method, you would assume that the price is equal to the last
> traded price or something like that. If there is no trade for some
> time, would it make sense to say that the price is the last traded
> price? If you wanted to actually buy/sell at that price, it's not
> obvious that you'll be able to do so.
>
> Also, if you only look at the time series of instantaneous prices, you
> would be losing a lot of information about what happened in between
> the time points. It makes more sense to aggregate and keep, for
> example, open, high, low and close. Or some statistics on the
> distribution of the prices between the endpoints.
>
> If what you need to calculate is correlations, then I would look at
> the papers that Liviu suggested. It seems that synchronicity is
> critical. I heard there is an extension of TSRV to correlations.
>
> If you only need to look at univariate time series, you may be able to
> get away with your method more easily. It may not be statistically
> efficient but may give you a good enough answer in some cases.
>
>
> HTH
> Haky
>
>
>
> On Thu, May 21, 2009 at 10:38 AM, Michael <comtech.usa at gmail.com> wrote:
>> My data are price change arrivals, irregularly spaced. But when there
>> is no price change, the price stays constant. Therefore, in fact, at
>> any time instant, you give me a time, I can give you the price at that
>> very instant of time. So irregularly spaced data can be easily sampled
>> to be regularly spaced data.
>> What do you think of this approach?
>>
>> On Thu, May 21, 2009 at 8:21 AM, Michael <comtech.usa at gmail.com> wrote:
>>> Thanks Jeff.
>>>
>>> By high frequency I mean really the tick data. For example, during
>>> peak time, the arrival of price events could be at about hundreds to
>>> thousands within one second, irregularly spaced.
>>>
>>> I've heard that forcing irregularly spaced data into regularly spaced
>>> data(e.g. through interpolation) will lose information. How's that so?
>>>
>>> Thanks!
>>>
>>> On Thu, May 21, 2009 at 8:15 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
>>>> Not my domain, but you will more than likely have to aggregate to some
>>>> sort of regular/homogenous type of series for most traditional tools
>>>> to work.
>>>>
>>>> xts has to.period to aggregate up to a lower frequency from tick-level
>>>> data. Coupled with something like na.locf you can make yourself some
>>>> high frequency 'regular' data from 'irregular'
>>>>
>>>> Regular and irregular of course depend on what you are looking at
>>>> (weekends missing in daily data can still be 'regular').
>>>>
>>>> I'd be interested in hearing thoughts from those who actually tread in
>>>> the high-freq domain...
>>>>
>>>> A wealth of information can be found here:
>>>>
>>>> ?http://www.olsen.ch/publications/working-papers/
>>>>
>>>> Jeff
>>>>
>>>> On Thu, May 21, 2009 at 10:04 AM, Michael <comtech.usa at gmail.com> wrote:
>>>>> Hi all,
>>>>>
>>>>> I am wondering if there are some special toolboxes to handle high
>>>>> frequency data in R?
>>>>>
>>>>> I have some high frequency data and was wondering what meaningful
>>>>> experiments can I run on these high frequency data.
>>>>>
>>>>> Not sure if normal (low frequency) financial time series textbook data
>>>>> analysis tools will work for high frequency data?
>>>>>
>>>>> Let's say I run a correlation between two stocks using the high
>>>>> frequency data, or run an ARMA model on one stock, will the results be
>>>>> meaningful?
>>>>>
>>>>> Could anybody point me some classroom types of treatment or lab
>>>>> tutorial type of document which show me what meaningful
>>>>> experiments/tests I can run on high frequency data?
>>>>>
>>>>> Thanks a lot!
>>>>>
>>>>> _______________________________________________
>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>> -- Subscriber-posting only.
>>>>> -- If you want to post, subscribe first.
>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> Jeffrey Ryan
>>>> jeffrey.ryan at insightalgo.com
>>>>
>>>> ia: insight algorithmics
>>>> www.insightalgo.com
>>>>
>>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From Reena.Bansal at moorecap.com  Thu May 21 20:21:01 2009
From: Reena.Bansal at moorecap.com (Reena Bansal)
Date: Thu, 21 May 2009 14:21:01 -0400
Subject: [R-SIG-Finance] Financial time series data mining in R
Message-ID: <4AAD56F399C8564C9EB6817C17618CDD0251EA79@NYC-XCH3.win.moorecap.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090521/717eaa61/attachment.pl>

From jeff.a.ryan at gmail.com  Thu May 21 20:22:38 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 21 May 2009 13:22:38 -0500
Subject: [R-SIG-Finance] Preprocessing RData file (Was: Kdb (Was: high
	frequency data analysis in R))
In-Reply-To: <3BAD818D9407B043817CC6D89ABA14EC032B85AF@MLNYC20MB051.amrs.win.ml.com>
References: <e8e755250905211057v6611568ev5f74f4cb5170bc5c@mail.gmail.com>
	<3BAD818D9407B043817CC6D89ABA14EC032B85AF@MLNYC20MB051.amrs.win.ml.com>
Message-ID: <e8e755250905211122p47446673td142a0bae175d008@mail.gmail.com>

I have given some thought to both ff and bigmemory.  I am not a huge
fan of the "ff" license.

http://cran.r-project.org/web/packages/ff/LICENSE

bigmemory is interesting in that you can bypass the R memory issues on
Windows, but I haven't had incredible luck with it.  Me and C++ don't
like each other, so maybe it is something related to that :).  I can
get around the Windows issues by using something non-windows...

Supposedly the changes to the most recent bigmemory are quite good,
but trying the shared memory route (one of the biggest reasons I would
like to use) has caused me catastrophic failure.

At the end of the day there is no good way to make xts rely on
bigmemory. As so much code in in C for xts, you can't readily operate
on the external pointers from there.  You need to read in the data
(via `[` ) and at that point it is resident to the R process, so you
are only getting the penalty of the memory allocation, and none of the
gain.

Of course this is my 2c.  Maybe we need another time-series library :)

Jeff




On Thu, May 21, 2009 at 1:10 PM, Rowe, Brian Lee Yung (Portfolio
Analytics) <B_Rowe at ml.com> wrote:
> ... and possibly even a list change.
>
> Do you plan on making this compatible with ff or bigmemory? Seems like this theme is making its rounds.
>
> Brian
>
> -----Original Message-----
> From: Jeff Ryan [mailto:jeff.a.ryan at gmail.com]
> Sent: Thursday, May 21, 2009 1:58 PM
> To: Rowe, Brian Lee Yung (Portfolio Analytics)
> Cc: Dirk Eddelbuettel; Hae Kyung Im; r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] Preprocessing RData file (Was: Kdb (Was: high frequency data analysis in R))
>
>
> I feel like I should change the title again... :)
>
> The RData files are compressed first off. If you don't want the gzip
> overhead, get rid of it.
>
> The xts format 'on-disk' is nothing more that the structure from
> memory written to disk. ?This manages to be both faster and takes up
> less space. ?It isn't a huge gain, but it allows for binary searching
> of the index to get to the data you want.
>
> I will put together a performance comparison at some point, and pass along.
>
> Jeff
>
> On Thu, May 21, 2009 at 12:52 PM, Rowe, Brian Lee Yung (Portfolio
> Analytics) <B_Rowe at ml.com> wrote:
>> Is there any literature on the relative performance gain of
>> preprocessing data into RData and then reading into R? Does it breakdown
>> anywhere? I have 4 GB of data that I'm reading in and I/O is a large
>> bottleneck.
>>
>> Brian
>>
>>
>> -----Original Message-----
>> From: r-sig-finance-bounces at stat.math.ethz.ch
>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Dirk
>> Eddelbuettel
>> Sent: Thursday, May 21, 2009 1:42 PM
>> To: Hae Kyung Im
>> Cc: r-sig-finance at stat.math.ethz.ch
>> Subject: [R-SIG-Finance] Kdb (Was: high frequency data analysis in R)
>>
>>
>>
>> On 21 May 2009 at 11:13, Hae Kyung Im wrote:
>> | access (query) this huge database. I looked a little bit into kdb but
>> | you have to pay ~25K to buy the software for one processor. I haven't
>>
>> True, but you can have "free" (as in beer) 32bit version that times out
>> after
>> two hours. That's not a bad compromise.
>>
>> I looked at it for a bit, and I has an R interface. (My blog has a patch
>> to
>> fix their then-broken interface to R's Datetime; I think they may have
>> integrated that by now). ?Then again you can also pre-process into RData
>> files, or use hdf5, or use a gazillion other methods. ? But the free
>> trial
>> version may just help for the odd research project like the one Haky
>> described.
>>
>> Dirk
>>
>> --
>> Three out of two people have difficulties with fractions.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>> --------------------------------------------------------------------------
>> This message w/attachments (message) may be privileged, confidential or proprietary, and if you are not an intended recipient, please notify the sender, do not use or share it and delete it. Unless specifically indicated, this message is not an offer to sell or a solicitation of any investment products or other financial product or service, an official confirmation of any transaction, or an official statement of Merrill Lynch. Subject to applicable law, Merrill Lynch may monitor, review and retain e-communications (EC) traveling through its networks/systems. The laws of the country of each sender/recipient may impact the handling of EC, and EC may be archived, supervised and produced in countries other than the country in which you are located. This message cannot be guaranteed to be secure or error-free. References to "Merrill Lynch" are references to any company in the Merrill Lynch & Co., Inc. group of companies, which are wholly-owned by Bank of America Corporation. Secu!
>> ?rities and Insurance Products: * Are Not FDIC Insured * Are Not Bank Guaranteed * May Lose Value * Are Not a Bank Deposit * Are Not a Condition to Any Banking Service or Activity * Are Not Insured by Any Federal Government Agency. Attachments that are part of this E-communication may have additional important disclosures and disclaimers, which you should read. This message is subject to terms available at the following link: http://www.ml.com/e-communications_terms/. By messaging with Merrill Lynch you consent to the foregoing.
>> --------------------------------------------------------------------------
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> --
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From jeff.a.ryan at gmail.com  Thu May 21 20:24:21 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 21 May 2009 13:24:21 -0500
Subject: [R-SIG-Finance] Preprocessing RData file (Was: Kdb (Was: high
	frequency data analysis in R))
In-Reply-To: <e8e755250905211122p47446673td142a0bae175d008@mail.gmail.com>
References: <e8e755250905211057v6611568ev5f74f4cb5170bc5c@mail.gmail.com>
	<3BAD818D9407B043817CC6D89ABA14EC032B85AF@MLNYC20MB051.amrs.win.ml.com>
	<e8e755250905211122p47446673td142a0bae175d008@mail.gmail.com>
Message-ID: <e8e755250905211124j6610428bw6626e7adb4e3c13d@mail.gmail.com>

I failed to point out that data.table can make use of both (?) those packages.

It isn't a time-series library per se, but it make one very cool
in-memory database.  Similar in spirit to some of the not-so-free ones
out there...

Jeff

On Thu, May 21, 2009 at 1:22 PM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> I have given some thought to both ff and bigmemory. ?I am not a huge
> fan of the "ff" license.
>
> http://cran.r-project.org/web/packages/ff/LICENSE
>
> bigmemory is interesting in that you can bypass the R memory issues on
> Windows, but I haven't had incredible luck with it. ?Me and C++ don't
> like each other, so maybe it is something related to that :). ?I can
> get around the Windows issues by using something non-windows...
>
> Supposedly the changes to the most recent bigmemory are quite good,
> but trying the shared memory route (one of the biggest reasons I would
> like to use) has caused me catastrophic failure.
>
> At the end of the day there is no good way to make xts rely on
> bigmemory. As so much code in in C for xts, you can't readily operate
> on the external pointers from there. ?You need to read in the data
> (via `[` ) and at that point it is resident to the R process, so you
> are only getting the penalty of the memory allocation, and none of the
> gain.
>
> Of course this is my 2c. ?Maybe we need another time-series library :)
>
> Jeff
>
>
>
>
> On Thu, May 21, 2009 at 1:10 PM, Rowe, Brian Lee Yung (Portfolio
> Analytics) <B_Rowe at ml.com> wrote:
>> ... and possibly even a list change.
>>
>> Do you plan on making this compatible with ff or bigmemory? Seems like this theme is making its rounds.
>>
>> Brian
>>
>> -----Original Message-----
>> From: Jeff Ryan [mailto:jeff.a.ryan at gmail.com]
>> Sent: Thursday, May 21, 2009 1:58 PM
>> To: Rowe, Brian Lee Yung (Portfolio Analytics)
>> Cc: Dirk Eddelbuettel; Hae Kyung Im; r-sig-finance at stat.math.ethz.ch
>> Subject: Re: [R-SIG-Finance] Preprocessing RData file (Was: Kdb (Was: high frequency data analysis in R))
>>
>>
>> I feel like I should change the title again... :)
>>
>> The RData files are compressed first off. If you don't want the gzip
>> overhead, get rid of it.
>>
>> The xts format 'on-disk' is nothing more that the structure from
>> memory written to disk. ?This manages to be both faster and takes up
>> less space. ?It isn't a huge gain, but it allows for binary searching
>> of the index to get to the data you want.
>>
>> I will put together a performance comparison at some point, and pass along.
>>
>> Jeff
>>
>> On Thu, May 21, 2009 at 12:52 PM, Rowe, Brian Lee Yung (Portfolio
>> Analytics) <B_Rowe at ml.com> wrote:
>>> Is there any literature on the relative performance gain of
>>> preprocessing data into RData and then reading into R? Does it breakdown
>>> anywhere? I have 4 GB of data that I'm reading in and I/O is a large
>>> bottleneck.
>>>
>>> Brian
>>>
>>>
>>> -----Original Message-----
>>> From: r-sig-finance-bounces at stat.math.ethz.ch
>>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Dirk
>>> Eddelbuettel
>>> Sent: Thursday, May 21, 2009 1:42 PM
>>> To: Hae Kyung Im
>>> Cc: r-sig-finance at stat.math.ethz.ch
>>> Subject: [R-SIG-Finance] Kdb (Was: high frequency data analysis in R)
>>>
>>>
>>>
>>> On 21 May 2009 at 11:13, Hae Kyung Im wrote:
>>> | access (query) this huge database. I looked a little bit into kdb but
>>> | you have to pay ~25K to buy the software for one processor. I haven't
>>>
>>> True, but you can have "free" (as in beer) 32bit version that times out
>>> after
>>> two hours. That's not a bad compromise.
>>>
>>> I looked at it for a bit, and I has an R interface. (My blog has a patch
>>> to
>>> fix their then-broken interface to R's Datetime; I think they may have
>>> integrated that by now). ?Then again you can also pre-process into RData
>>> files, or use hdf5, or use a gazillion other methods. ? But the free
>>> trial
>>> version may just help for the odd research project like the one Haky
>>> described.
>>>
>>> Dirk
>>>
>>> --
>>> Three out of two people have difficulties with fractions.
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>> --------------------------------------------------------------------------
>>> This message w/attachments (message) may be privileged, confidential or proprietary, and if you are not an intended recipient, please notify the sender, do not use or share it and delete it. Unless specifically indicated, this message is not an offer to sell or a solicitation of any investment products or other financial product or service, an official confirmation of any transaction, or an official statement of Merrill Lynch. Subject to applicable law, Merrill Lynch may monitor, review and retain e-communications (EC) traveling through its networks/systems. The laws of the country of each sender/recipient may impact the handling of EC, and EC may be archived, supervised and produced in countries other than the country in which you are located. This message cannot be guaranteed to be secure or error-free. References to "Merrill Lynch" are references to any company in the Merrill Lynch & Co., Inc. group of companies, which are wholly-owned by Bank of America Corporation. Secu!
>>> ?rities and Insurance Products: * Are Not FDIC Insured * Are Not Bank Guaranteed * May Lose Value * Are Not a Bank Deposit * Are Not a Condition to Any Banking Service or Activity * Are Not Insured by Any Federal Government Agency. Attachments that are part of this E-communication may have additional important disclosures and disclaimers, which you should read. This message is subject to terms available at the following link: http://www.ml.com/e-communications_terms/. By messaging with Merrill Lynch you consent to the foregoing.
>>> --------------------------------------------------------------------------
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>>
>>
>> --
>> Jeffrey Ryan
>> jeffrey.ryan at insightalgo.com
>>
>> ia: insight algorithmics
>> www.insightalgo.com
>>
>
>
>
> --
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From patrick at burns-stat.com  Thu May 21 20:29:44 2009
From: patrick at burns-stat.com (Patrick Burns)
Date: Thu, 21 May 2009 19:29:44 +0100
Subject: [R-SIG-Finance] Financial time series data mining in R
In-Reply-To: <4AAD56F399C8564C9EB6817C17618CDD0251EA79@NYC-XCH3.win.moorecap.com>
References: <4AAD56F399C8564C9EB6817C17618CDD0251EA79@NYC-XCH3.win.moorecap.com>
Message-ID: <4A159D98.6050508@burns-stat.com>

I think you are looking for something
like:

lapply(z, rle)

assuming 'z' is a data frame.



Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of "The R Inferno" and "A Guide for the Unwilling S User")

Reena Bansal wrote:
> Hi All,
> 
> This is a financial time series data mining question. Suppose I have a
> time series as below, a typical fetch with NA's and prices.
> 
>> z
>         y
> 1      NA
> 2      NA
> 3  884.96
> 4  894.07
> 5  902.33
> 6  810.36
> 7  810.52
> 8  811.94
> 9  812.12
> 10 826.85
> 11 826.45
> 12 808.03
> 13 800.28
> 14 800.70
> 15 800.68
> 16     NA
> 17     NA
> 18 800.49
> 19 800.65
> 20 801.82
> 
> I want to create summary of the data, which should give me the length of
> each contiguous block of NA and NUMBER(prices here), and start and end
> index, so a sample output might be. 
> 
> Type		Length	StartIndex	EndIndex	
> NA		2	1		2
> NUMBER 	13	3		15	
> NA		2	16		17
> NUMBER 	3	18		20
> 
> I looked into arules package but didn't find anything that did the same.
> Any ideas?
> 
> Thanks everybody,
> Reena
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
>


From jeff.a.ryan at gmail.com  Thu May 21 20:32:33 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 21 May 2009 13:32:33 -0500
Subject: [R-SIG-Finance] Financial time series data mining in R
In-Reply-To: <4AAD56F399C8564C9EB6817C17618CDD0251EA79@NYC-XCH3.win.moorecap.com>
References: <4AAD56F399C8564C9EB6817C17618CDD0251EA79@NYC-XCH3.win.moorecap.com>
Message-ID: <e8e755250905211132j4240ecfdm226c9d6e9d847013@mail.gmail.com>

Reena,

> r <- rnorm(20)
> r[c(1,2,16,17)] <- NA

> r
 [1]          NA          NA  1.01972607 -0.26526079 -0.48193672  1.26074421
 [7]  1.09832958 -0.57868771  0.64506617  0.37299142  0.63672673 -0.02503177
[13]  1.44064294  0.23808208  1.33212831          NA          NA  0.34509281
[19] -0.29853578  0.49943889

> rle(is.na(r))
Run Length Encoding
  lengths: int [1:4] 2 13 2 3
  values : logi [1:4] TRUE FALSE TRUE FALSE

You can then find the starting and stopping points from there.


HTH,
Jeff

On Thu, May 21, 2009 at 1:21 PM, Reena Bansal <Reena.Bansal at moorecap.com> wrote:
> Hi All,
>
> This is a financial time series data mining question. Suppose I have a
> time series as below, a typical fetch with NA's and prices.
>
>> z
> ? ? ? ?y
> 1 ? ? ?NA
> 2 ? ? ?NA
> 3 ?884.96
> 4 ?894.07
> 5 ?902.33
> 6 ?810.36
> 7 ?810.52
> 8 ?811.94
> 9 ?812.12
> 10 826.85
> 11 826.45
> 12 808.03
> 13 800.28
> 14 800.70
> 15 800.68
> 16 ? ? NA
> 17 ? ? NA
> 18 800.49
> 19 800.65
> 20 801.82
>
> I want to create summary of the data, which should give me the length of
> each contiguous block of NA and NUMBER(prices here), and start and end
> index, so a sample output might be.
>
> Type ? ? ? ? ? ?Length ?StartIndex ? ? ?EndIndex
> NA ? ? ? ? ? ? ?2 ? ? ? 1 ? ? ? ? ? ? ? 2
> NUMBER ?13 ? ? ?3 ? ? ? ? ? ? ? 15
> NA ? ? ? ? ? ? ?2 ? ? ? 16 ? ? ? ? ? ? ?17
> NUMBER ?3 ? ? ? 18 ? ? ? ? ? ? ?20
>
> I looked into arules package but didn't find anything that did the same.
> Any ideas?
>
> Thanks everybody,
> Reena
>
>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From Reena.Bansal at moorecap.com  Thu May 21 20:36:08 2009
From: Reena.Bansal at moorecap.com (Reena Bansal)
Date: Thu, 21 May 2009 14:36:08 -0400
Subject: [R-SIG-Finance] Financial time series data mining in R
In-Reply-To: <e8e755250905211132j4240ecfdm226c9d6e9d847013@mail.gmail.com>
Message-ID: <4AAD56F399C8564C9EB6817C17618CDD0251EA7C@NYC-XCH3.win.moorecap.com>

Thanks Jeff and Patrick, rle works perfect! 

-----Original Message-----
From: Jeff Ryan [mailto:jeff.a.ryan at gmail.com] 
Sent: Thursday, May 21, 2009 02:33 PM
To: Reena Bansal
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Financial time series data mining in R

Reena,

> r <- rnorm(20)
> r[c(1,2,16,17)] <- NA

> r
 [1]          NA          NA  1.01972607 -0.26526079 -0.48193672  1.26074421
 [7]  1.09832958 -0.57868771  0.64506617  0.37299142  0.63672673 -0.02503177
[13]  1.44064294  0.23808208  1.33212831          NA          NA  0.34509281
[19] -0.29853578  0.49943889

> rle(is.na(r))
Run Length Encoding
  lengths: int [1:4] 2 13 2 3
  values : logi [1:4] TRUE FALSE TRUE FALSE

You can then find the starting and stopping points from there.


HTH,
Jeff

On Thu, May 21, 2009 at 1:21 PM, Reena Bansal <Reena.Bansal at moorecap.com> wrote:
> Hi All,
>
> This is a financial time series data mining question. Suppose I have a 
> time series as below, a typical fetch with NA's and prices.
>
>> z
> ? ? ? ?y
> 1 ? ? ?NA
> 2 ? ? ?NA
> 3 ?884.96
> 4 ?894.07
> 5 ?902.33
> 6 ?810.36
> 7 ?810.52
> 8 ?811.94
> 9 ?812.12
> 10 826.85
> 11 826.45
> 12 808.03
> 13 800.28
> 14 800.70
> 15 800.68
> 16 ? ? NA
> 17 ? ? NA
> 18 800.49
> 19 800.65
> 20 801.82
>
> I want to create summary of the data, which should give me the length 
> of each contiguous block of NA and NUMBER(prices here), and start and 
> end index, so a sample output might be.
>
> Type ? ? ? ? ? ?Length ?StartIndex ? ? ?EndIndex NA ? ? ? ? ? ? ?2 ? ? ? 
> 1 ? ? ? ? ? ? ? 2 NUMBER ?13 ? ? ?3 ? ? ? ? ? ? ? 15 NA ? ? ? ? ? ? ?2 ? ? ? 
> 16 ? ? ? ? ? ? ?17 NUMBER ?3 ? ? ? 18 ? ? ? ? ? ? ?20
>
> I looked into arules package but didn't find anything that did the same.
> Any ideas?
>
> Thanks everybody,
> Reena
>
>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



--
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From markleeds at verizon.net  Thu May 21 20:44:08 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Thu, 21 May 2009 13:44:08 -0500 (CDT)
Subject: [R-SIG-Finance] high frequency data analysis in R
Message-ID: <2145379628.900181.1242931448232.JavaMail.root@vms227.mailsrvcs.net>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090521/aa37d525/attachment.html>

From comtech.usa at gmail.com  Thu May 21 22:31:19 2009
From: comtech.usa at gmail.com (Michael)
Date: Thu, 21 May 2009 13:31:19 -0700
Subject: [R-SIG-Finance] high frequency data analysis in R
In-Reply-To: <197a5bbc0905210951oef8b273vda150c71a31ff38c@mail.gmail.com>
References: <b1f16d9d0905210804s6fb9832crf935912bd7509a4c@mail.gmail.com>
	<e8e755250905210815h742c04aekfd111ff7e4165c1d@mail.gmail.com>
	<b1f16d9d0905210821y6fa119f6jf4b15cc78ac64c8c@mail.gmail.com>
	<b1f16d9d0905210838ud47dbe2n5555c50518794cca@mail.gmail.com>
	<197a5bbc0905210951oef8b273vda150c71a31ff38c@mail.gmail.com>
Message-ID: <b1f16d9d0905211331q39a9031fyc1ecb77eaf68f494@mail.gmail.com>

In fact, I have the whole jump processes of best bid, and best ask, at
a continuous level (in the sense of time-stamped arrival data), and
also the jump process of the last trade price, at a continuous level
(in the sense of time-stamped arrival data). Any more thoughts?


On Thu, May 21, 2009 at 9:51 AM, Hae Kyung Im <hakyim at gmail.com> wrote:
> Relating the approach that turns irregular data into regular one,
> I guess it's a complex question and how you approach it will depend on
> the specific problem.
>
> With your method, you would assume that the price is equal to the last
> traded price or something like that. If there is no trade for some
> time, would it make sense to say that the price is the last traded
> price? If you wanted to actually buy/sell at that price, it's not
> obvious that you'll be able to do so.
>
> Also, if you only look at the time series of instantaneous prices, you
> would be losing a lot of information about what happened in between
> the time points. It makes more sense to aggregate and keep, for
> example, open, high, low and close. Or some statistics on the
> distribution of the prices between the endpoints.
>
> If what you need to calculate is correlations, then I would look at
> the papers that Liviu suggested. It seems that synchronicity is
> critical. I heard there is an extension of TSRV to correlations.
>
> If you only need to look at univariate time series, you may be able to
> get away with your method more easily. It may not be statistically
> efficient but may give you a good enough answer in some cases.
>
>
> HTH
> Haky
>
>
>
> On Thu, May 21, 2009 at 10:38 AM, Michael <comtech.usa at gmail.com> wrote:
>> My data are price change arrivals, irregularly spaced. But when there
>> is no price change, the price stays constant. Therefore, in fact, at
>> any time instant, you give me a time, I can give you the price at that
>> very instant of time. So irregularly spaced data can be easily sampled
>> to be regularly spaced data.
>> What do you think of this approach?
>>
>> On Thu, May 21, 2009 at 8:21 AM, Michael <comtech.usa at gmail.com> wrote:
>>> Thanks Jeff.
>>>
>>> By high frequency I mean really the tick data. For example, during
>>> peak time, the arrival of price events could be at about hundreds to
>>> thousands within one second, irregularly spaced.
>>>
>>> I've heard that forcing irregularly spaced data into regularly spaced
>>> data(e.g. through interpolation) will lose information. How's that so?
>>>
>>> Thanks!
>>>
>>> On Thu, May 21, 2009 at 8:15 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
>>>> Not my domain, but you will more than likely have to aggregate to some
>>>> sort of regular/homogenous type of series for most traditional tools
>>>> to work.
>>>>
>>>> xts has to.period to aggregate up to a lower frequency from tick-level
>>>> data. Coupled with something like na.locf you can make yourself some
>>>> high frequency 'regular' data from 'irregular'
>>>>
>>>> Regular and irregular of course depend on what you are looking at
>>>> (weekends missing in daily data can still be 'regular').
>>>>
>>>> I'd be interested in hearing thoughts from those who actually tread in
>>>> the high-freq domain...
>>>>
>>>> A wealth of information can be found here:
>>>>
>>>> ?http://www.olsen.ch/publications/working-papers/
>>>>
>>>> Jeff
>>>>
>>>> On Thu, May 21, 2009 at 10:04 AM, Michael <comtech.usa at gmail.com> wrote:
>>>>> Hi all,
>>>>>
>>>>> I am wondering if there are some special toolboxes to handle high
>>>>> frequency data in R?
>>>>>
>>>>> I have some high frequency data and was wondering what meaningful
>>>>> experiments can I run on these high frequency data.
>>>>>
>>>>> Not sure if normal (low frequency) financial time series textbook data
>>>>> analysis tools will work for high frequency data?
>>>>>
>>>>> Let's say I run a correlation between two stocks using the high
>>>>> frequency data, or run an ARMA model on one stock, will the results be
>>>>> meaningful?
>>>>>
>>>>> Could anybody point me some classroom types of treatment or lab
>>>>> tutorial type of document which show me what meaningful
>>>>> experiments/tests I can run on high frequency data?
>>>>>
>>>>> Thanks a lot!
>>>>>
>>>>> _______________________________________________
>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>> -- Subscriber-posting only.
>>>>> -- If you want to post, subscribe first.
>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> Jeffrey Ryan
>>>> jeffrey.ryan at insightalgo.com
>>>>
>>>> ia: insight algorithmics
>>>> www.insightalgo.com
>>>>
>>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>


From markleeds at verizon.net  Thu May 21 22:37:31 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Thu, 21 May 2009 15:37:31 -0500 (CDT)
Subject: [R-SIG-Finance] high frequency data analysis in R
Message-ID: <322632563.910800.1242938252059.JavaMail.root@vms227.mailsrvcs.net>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090521/2a899aa6/attachment.html>

From comtech.usa at gmail.com  Thu May 21 22:43:14 2009
From: comtech.usa at gmail.com (Michael)
Date: Thu, 21 May 2009 13:43:14 -0700
Subject: [R-SIG-Finance] high frequency data analysis in R
In-Reply-To: <2145379628.900181.1242931448232.JavaMail.root@vms227.mailsrvcs.net>
References: <2145379628.900181.1242931448232.JavaMail.root@vms227.mailsrvcs.net>
Message-ID: <b1f16d9d0905211343n57c63391ve2338916a87ceb10@mail.gmail.com>

In fact, I have the whole jump processes of best bid, and best ask, at
a continuous level (in the sense of time-stamped arrival data), and
also the jump process of the last trade price, at a continuous level
(in the sense of time-stamped arrival data).

I don't understand why you say I lose information. Of course, I lose
the arrival information by "flattening" the arrivals. But it's the
regularly spaced data that's studied by the correlation, right?

Any more thoughts?


On Thu, May 21, 2009 at 11:44 AM,  <markleeds at verizon.net> wrote:
> hi: but if you make it regularly spaced, then you will be removing data and
> therefore possibly information.? i'm sure what you're doing? is what is
> usually done and it's probably fine ( i really don't know to be totally
> honest ) but i'm just saying that irregularly spaced data is not that simple
> if you don't want to make assumptions.
>
>
>
>
> On May 21, 2009, Michael <comtech.usa at gmail.com> wrote:
>
> My data are price change arrivals, irregularly spaced. But when there
> is no price change, the price stays constant. Therefore, in fact, at
> any time instant, you give me a time, I can give you the price at that
> very instant of time. So irregularly spaced data can be easily sampled
> to be regularly spaced data.
> What do you think of this approach?
>
> On Thu, May 21, 2009 at 8:21 AM, Michael <comtech.usa at gmail.com> wrote:
>> Thanks Jeff.
>>
>> By high frequency I mean really the tick data. For example, during
>> peak time, the arrival of price events could be at about hundreds to
>> thousands within one second, irregularly spaced.
>>
>> I've heard that forcing irregularly spaced data into regularly spaced
>> data(e.g. through interpolation) will lose information. How's that so?
>>
>> Thanks!
>>
>> On Thu, May 21, 2009 at 8:15 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
>>> Not my domain, but you will more than likely have to aggregate to some
>>> sort of regular/homogenous type of series for most traditional tools
>>> to work.
>>>
>>> xts has to.period to aggregate up to a lower frequency from tick-level
>>> data. Coupled with something like na.locf you can make yourself some
>>> high frequency 'regular' data from 'irregular'
>>>
>>> Regular and irregular of course depend on what you are looking at
>>> (weekends missing in daily data can still be 'regular').
>>>
>>> I'd be interested in hearing thoughts from those who actually tread in
>>> the high-freq domain...
>>>
>>> A wealth of information can be found here:
>>>
>>> ?http://www.olsen.ch/publications/working-papers/
>>>
>>> Jeff
>>>
>>> On Thu, May 21, 2009 at 10:04 AM, Michael <comtech.usa at gmail.com> wrote:
>>>> Hi all,
>>>>
>>>> I am wondering if there are some special toolboxes to handle high
>>>> frequency data in R?
>>>>
>>>> I have some high frequency data and was wondering what meaningful
>>>> experiments can I run on these high frequency data.
>>>>
>>>> Not sure if normal (low frequency) financial time series textbook data
>>>> analysis tools will work for high frequency data?
>>>>
>>>> Let's say I run a correlation between two stocks using the high
>>>> frequency data, or run an ARMA model on one stock, will the results be
>>>> meaningful?
>>>>
>>>> Could anybody point me some classroom types of treatment or lab
>>>> tutorial type of document which show me what meaningful
>>>> experiments/tests I can run on high frequency data?
>>>>
>>>> Thanks a lot!
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>
>>>
>>>
>>> --
>>> Jeffrey Ryan
>>> jeffrey.ryan at insightalgo.com
>>>
>>> ia: insight algorithmics
>>> www.insightalgo.com
>>>
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From comtech.usa at gmail.com  Thu May 21 22:45:34 2009
From: comtech.usa at gmail.com (Michael)
Date: Thu, 21 May 2009 13:45:34 -0700
Subject: [R-SIG-Finance] high frequency data analysis in R
In-Reply-To: <322632563.910800.1242938252059.JavaMail.root@vms227.mailsrvcs.net>
References: <322632563.910800.1242938252059.JavaMail.root@vms227.mailsrvcs.net>
Message-ID: <b1f16d9d0905211345m4bebe7a5n55d219e5b3d66bdb@mail.gmail.com>

I want to see what statistical experiments I can run on my data.
The very first thing came to my mind was the "correlation" ...
But I am not sure if the concept of usual "correlation" is directly
applicable after I resampled the data into regularly spaced data. But
then again another question is what's a good resampling period? Maybe
"correlation" is sensitive to the resampling period...

On Thu, May 21, 2009 at 1:37 PM,  <markleeds at verizon.net> wrote:
> in that case, it begs the question of why you want to regularly space your
> data ?
> all the info is there so why reduce the amount of it by regularly spacing ?
>
>
>
>
> On May 21, 2009, Michael <comtech.usa at gmail.com> wrote:
>
> In fact, I have the whole jump processes of best bid, and best ask, at
> a continuous level (in the sense of time-stamped arrival data), and
> also the jump process of the last trade price, at a continuous level
> (in the sense of time-stamped arrival data). Any more thoughts?
>
>
> On Thu, May 21, 2009 at 9:51 AM, Hae Kyung Im <hakyim at gmail.com> wrote:
>> Relating the approach that turns irregular data into regular one,
>> I guess it's a complex question and how you approach it will depend on
>> the specific problem.
>>
>> With your method, you would assume that the price is equal to the last
>> traded price or something like that. If there is no trade for some
>> time, would it make sense to say that the price is the last traded
>> price? If you wanted to actually buy/sell at that price, it's not
>> obvious that you'll be able to do so.
>>
>> Also, if you only look at the time series of instantaneous prices, you
>> would be losing a lot of information about what happened in between
>> the time points. It makes more sense to aggregate and keep, for
>> example, open, high, low and close. Or some statistics on the
>> distribution of the prices between the endpoints.
>>
>> If what you need to calculate is correlations, then I would look at
>> the papers that Liviu suggested. It seems that synchronicity is
>> critical. I heard there is an extension of TSRV to correlations.
>>
>> If you only need to look at univariate time series, you may be able to
>> get away with your method more easily. It may not be statistically
>> efficient but may give you a good enough answer in some cases.
>>
>>
>> HTH
>> Haky
>>
>>
>>
>> On Thu, May 21, 2009 at 10:38 AM, Michael <comtech.usa at gmail.com> wrote:
>>> My data are price change arrivals, irregularly spaced. But when there
>>> is no price change, the price stays constant. Therefore, in fact, at
>>> any time instant, you give me a time, I can give you the price at that
>>> very instant of time. So irregularly spaced data can be easily sampled
>>> to be regularly spaced data.
>>> What do you think of this approach?
>>>
>>> On Thu, May 21, 2009 at 8:21 AM, Michael <comtech.usa at gmail.com> wrote:
>>>> Thanks Jeff.
>>>>
>>>> By high frequency I mean really the tick data. For example, during
>>>> peak time, the arrival of price events could be at about hundreds to
>>>> thousands within one second, irregularly spaced.
>>>>
>>>> I've heard that forcing irregularly spaced data into regularly spaced
>>>> data(e.g. through interpolation) will lose information. How's that so?
>>>>
>>>> Thanks!
>>>>
>>>> On Thu, May 21, 2009 at 8:15 AM, Jeff Ryan <jeff.a.ryan at gmail.com>
>>>> wrote:
>>>>> Not my domain, but you will more than likely have to aggregate to some
>>>>> sort of regular/homogenous type of series for most traditional tools
>>>>> to work.
>>>>>
>>>>> xts has to.period to aggregate up to a lower frequency from tick-level
>>>>> data. Coupled with something like na.locf you can make yourself some
>>>>> high frequency 'regular' data from 'irregular'
>>>>>
>>>>> Regular and irregular of course depend on what you are looking at
>>>>> (weekends missing in daily data can still be 'regular').
>>>>>
>>>>> I'd be interested in hearing thoughts from those who actually tread in
>>>>> the high-freq domain...
>>>>>
>>>>> A wealth of information can be found here:
>>>>>
>>>>> ?http://www.olsen.ch/publications/working-papers/
>>>>>
>>>>> Jeff
>>>>>
>>>>> On Thu, May 21, 2009 at 10:04 AM, Michael <comtech.usa at gmail.com>
>>>>> wrote:
>>>>>> Hi all,
>>>>>>
>>>>>> I am wondering if there are some special toolboxes to handle high
>>>>>> frequency data in R?
>>>>>>
>>>>>> I have some high frequency data and was wondering what meaningful
>>>>>> experiments can I run on these high frequency data.
>>>>>>
>>>>>> Not sure if normal (low frequency) financial time series textbook data
>>>>>> analysis tools will work for high frequency data?
>>>>>>
>>>>>> Let's say I run a correlation between two stocks using the high
>>>>>> frequency data, or run an ARMA model on one stock, will the results be
>>>>>> meaningful?
>>>>>>
>>>>>> Could anybody point me some classroom types of treatment or lab
>>>>>> tutorial type of document which show me what meaningful
>>>>>> experiments/tests I can run on high frequency data?
>>>>>>
>>>>>> Thanks a lot!
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>>> -- Subscriber-posting only.
>>>>>> -- If you want to post, subscribe first.
>>>>>>
>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> Jeffrey Ryan
>>>>> jeffrey.ryan at insightalgo.com
>>>>>
>>>>> ia: insight algorithmics
>>>>> www.insightalgo.com
>>>>>
>>>>
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From bogaso.christofer at gmail.com  Thu May 21 23:39:45 2009
From: bogaso.christofer at gmail.com (bogaso.christofer)
Date: Fri, 22 May 2009 03:09:45 +0530
Subject: [R-SIG-Finance] Newbie question on risk free Interest Rate
In-Reply-To: <C57E6CE6.443%g1enn.roberts@btinternet.com>
Message-ID: <4a15c704.0aaa660a.7004.ffff84ce@mx.google.com>

Hi, I have come across one more question. I understood that for BS options
pricing, I should take short rate i.e. overnight rate because BS derive
option price through some replicating portfolio which is changed
instantaneously. However if I price an instrument using Black formula,
wherein only the distribution of underlying at maturity period is considered
i.e. in this case there is no replicating portfolio story, shouldn't I
consider risk free rate for longer horizon i.e. a rate whose maturity period
exactly matches with the life of the instrument?

I mean to say, under Black's framework, one only needs to calculate expected
value of the instrument like E[max(0, S[T] - K)] at maturity and then to
calculate the present value of that. In this case there is nothing abt
replicating portfolio. Therefore I feel that to calculate PV I should
consider LIBOR with maturity [o, T]. 

What you feel on that? If I am correct i.e. if I price same option using BS
and Black, there must be some fundamental difference in theoretical option
price.

-----Original Message-----
From: glenn [mailto:g1enn.roberts at btinternet.com] 
Sent: 29 December 2008 17:33
To: bogaso.christofer
Subject: Re: [R-SIG-Finance] Newbie question on risk free Interest Rate

Further to Mahesh's answer Christofer, think of it like this;

The rate in the BS calculation represents a rate that any portfolio
consisting of an option and the delta equivalent of the underlying (in your
example a swap maybe) MUST earn. Think about how long the portfolio will
remain delta neutral (risk free) for before a re-balence is needed. That's
the rate you want i.e the short rate.

Glenn


On 28/12/2008 21:53, "bogaso.christofer" <bogaso.christofer at gmail.com>
wrote:

> Hi,
> 
>  
> 
> I would like to ask one newbie question on risk free interest rate. This
is
> the essential part to price any financial derivatives, like options,
> Interest Rate only [IO] strip etc. My question is standing at time "t"
which
> risk free interest rate I should consider? 3 month, 6 month, 10 year
t-bill
> or t-bond ? for example suppose, I need to price a call option using BS
> formula, whose remaining life time is 2 years and another option whose
life
> time is 5 months. Which interest rate I need to take to value those 2
> options? After some goggling it is suggested to take 3 month t-bill as
risk
> free rate. What is the logic behind that?
> 
>  
> 
> Again suppose, an Investor is to purchase an IO strip for 7 years, on a 10
> years mortgage. In this case, I saw one book [by Cuthbertson], suggested
to
> take annual yield on 10-year t-bond to calculate NPV of all future
Interest
> payment against mortgage. However again it did not say why to take 10-year
> bond not, 3-month t-bill.
> 
>  
> 
> Can anyone here please clarify me on above doubts? Your help will be
highly
> appreciated.
> 
>  
> 
> Thanks and regards,
> 
> 
> [[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From neilt at neiltiffin.com  Fri May 22 00:06:25 2009
From: neilt at neiltiffin.com (Neil Tiffin)
Date: Thu, 21 May 2009 17:06:25 -0500
Subject: [R-SIG-Finance] high frequency data analysis in R
In-Reply-To: <b1f16d9d0905211345m4bebe7a5n55d219e5b3d66bdb@mail.gmail.com>
References: <322632563.910800.1242938252059.JavaMail.root@vms227.mailsrvcs.net>
	<b1f16d9d0905211345m4bebe7a5n55d219e5b3d66bdb@mail.gmail.com>
Message-ID: <21CAA23F-336A-4FD4-AFF6-156C6E577DA6@neiltiffin.com>

What I have been interested in, just started looking at, but not found  
is what predictive value is available from changes in the patterns in  
the tick data.  Now I am a complete neophyte so please do shoot me down.

Let me use an example, because I am not even sure of the right language.

If a stock is trading with roughly equal transactions at the bid and  
at the ask price* with some pattern of volume, lets say mostly small  
lots.  There are a number of things that can happen at this point.   
The transaction price can move up or down, the bid and ask price can  
move up or down, the absolute volume traded at either the bid or ask  
price* can go up or down, the bid ask spread can increase or decrease,  
the block size of the trades can go up or down, the frequency of  
trades can go up or down and probably something I missed.  This seems  
like a lot of information that goes away when you go to 1 minutes  
open, close, high, and low data.

I have been trading for some time without using any statistics so I  
understand some of the manipulations that occur in this whole scheme.   
So the answer that this level of analysis does not mean anything is  
not what  I am looking for.  I understand that any analysis of this  
type is likely to be specific to market conditions.  If not, then we  
would have the magic bullet.

My question is can any of these changes predict what will happen short  
term or even that changes are afloat?  There is probably some back  
room someplace where this is the secret formula never to be divulged.   
But I have to ask, where has this been researched and what is publicly  
available.

* keep in mind that the transactions can be between, over or under the  
bid and ask price.

On May 21, 2009, at 3:45 PM, Michael wrote:

> I want to see what statistical experiments I can run on my data.
> The very first thing came to my mind was the "correlation" ...
> But I am not sure if the concept of usual "correlation" is directly
> applicable after I resampled the data into regularly spaced data. But
> then again another question is what's a good resampling period? Maybe
> "correlation" is sensitive to the resampling period...
>
> On Thu, May 21, 2009 at 1:37 PM,  <markleeds at verizon.net> wrote:
>> in that case, it begs the question of why you want to regularly  
>> space your
>> data ?
>> all the info is there so why reduce the amount of it by regularly  
>> spacing ?
>>
>>
>>
>>
>> On May 21, 2009, Michael <comtech.usa at gmail.com> wrote:
>>
>> In fact, I have the whole jump processes of best bid, and best ask,  
>> at
>> a continuous level (in the sense of time-stamped arrival data), and
>> also the jump process of the last trade price, at a continuous level
>> (in the sense of time-stamped arrival data). Any more thoughts?
>>
>>
>> On Thu, May 21, 2009 at 9:51 AM, Hae Kyung Im <hakyim at gmail.com>  
>> wrote:
>>> Relating the approach that turns irregular data into regular one,
>>> I guess it's a complex question and how you approach it will  
>>> depend on
>>> the specific problem.
>>>
>>> With your method, you would assume that the price is equal to the  
>>> last
>>> traded price or something like that. If there is no trade for some
>>> time, would it make sense to say that the price is the last traded
>>> price? If you wanted to actually buy/sell at that price, it's not
>>> obvious that you'll be able to do so.
>>>
>>> Also, if you only look at the time series of instantaneous prices,  
>>> you
>>> would be losing a lot of information about what happened in between
>>> the time points. It makes more sense to aggregate and keep, for
>>> example, open, high, low and close. Or some statistics on the
>>> distribution of the prices between the endpoints.
>>>
>>> If what you need to calculate is correlations, then I would look at
>>> the papers that Liviu suggested. It seems that synchronicity is
>>> critical. I heard there is an extension of TSRV to correlations.
>>>
>>> If you only need to look at univariate time series, you may be  
>>> able to
>>> get away with your method more easily. It may not be statistically
>>> efficient but may give you a good enough answer in some cases.
>>>
>>>
>>> HTH
>>> Haky
>>>
>>>
>>>
>>> On Thu, May 21, 2009 at 10:38 AM, Michael <comtech.usa at gmail.com>  
>>> wrote:
>>>> My data are price change arrivals, irregularly spaced. But when  
>>>> there
>>>> is no price change, the price stays constant. Therefore, in fact,  
>>>> at
>>>> any time instant, you give me a time, I can give you the price at  
>>>> that
>>>> very instant of time. So irregularly spaced data can be easily  
>>>> sampled
>>>> to be regularly spaced data.
>>>> What do you think of this approach?
>>>>
>>>> On Thu, May 21, 2009 at 8:21 AM, Michael <comtech.usa at gmail.com>  
>>>> wrote:
>>>>> Thanks Jeff.
>>>>>
>>>>> By high frequency I mean really the tick data. For example, during
>>>>> peak time, the arrival of price events could be at about  
>>>>> hundreds to
>>>>> thousands within one second, irregularly spaced.
>>>>>
>>>>> I've heard that forcing irregularly spaced data into regularly  
>>>>> spaced
>>>>> data(e.g. through interpolation) will lose information. How's  
>>>>> that so?
>>>>>
>>>>> Thanks!
>>>>>
>>>>> On Thu, May 21, 2009 at 8:15 AM, Jeff Ryan <jeff.a.ryan at gmail.com>
>>>>> wrote:
>>>>>> Not my domain, but you will more than likely have to aggregate  
>>>>>> to some
>>>>>> sort of regular/homogenous type of series for most traditional  
>>>>>> tools
>>>>>> to work.
>>>>>>
>>>>>> xts has to.period to aggregate up to a lower frequency from  
>>>>>> tick-level
>>>>>> data. Coupled with something like na.locf you can make yourself  
>>>>>> some
>>>>>> high frequency 'regular' data from 'irregular'
>>>>>>
>>>>>> Regular and irregular of course depend on what you are looking at
>>>>>> (weekends missing in daily data can still be 'regular').
>>>>>>
>>>>>> I'd be interested in hearing thoughts from those who actually  
>>>>>> tread in
>>>>>> the high-freq domain...
>>>>>>
>>>>>> A wealth of information can be found here:
>>>>>>
>>>>>>  http://www.olsen.ch/publications/working-papers/
>>>>>>
>>>>>> Jeff
>>>>>>
>>>>>> On Thu, May 21, 2009 at 10:04 AM, Michael <comtech.usa at gmail.com>
>>>>>> wrote:
>>>>>>> Hi all,
>>>>>>>
>>>>>>> I am wondering if there are some special toolboxes to handle  
>>>>>>> high
>>>>>>> frequency data in R?
>>>>>>>
>>>>>>> I have some high frequency data and was wondering what  
>>>>>>> meaningful
>>>>>>> experiments can I run on these high frequency data.
>>>>>>>
>>>>>>> Not sure if normal (low frequency) financial time series  
>>>>>>> textbook data
>>>>>>> analysis tools will work for high frequency data?
>>>>>>>
>>>>>>> Let's say I run a correlation between two stocks using the high
>>>>>>> frequency data, or run an ARMA model on one stock, will the  
>>>>>>> results be
>>>>>>> meaningful?
>>>>>>>
>>>>>>> Could anybody point me some classroom types of treatment or lab
>>>>>>> tutorial type of document which show me what meaningful
>>>>>>> experiments/tests I can run on high frequency data?
>>>>>>>
>>>>>>> Thanks a lot!
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>>>> -- Subscriber-posting only.
>>>>>>> -- If you want to post, subscribe first.
>>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> --
>>>>>> Jeffrey Ryan
>>>>>> jeffrey.ryan at insightalgo.com
>>>>>>
>>>>>> ia: insight algorithmics
>>>>>> www.insightalgo.com
>>>>>>
>>>>>
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From armstrong.whit at gmail.com  Fri May 22 00:14:19 2009
From: armstrong.whit at gmail.com (Whit Armstrong)
Date: Thu, 21 May 2009 18:14:19 -0400
Subject: [R-SIG-Finance] Preprocessing RData file (Was: Kdb (Was: high
	frequency data analysis in R))
In-Reply-To: <e8e755250905211122p47446673td142a0bae175d008@mail.gmail.com>
References: <e8e755250905211057v6611568ev5f74f4cb5170bc5c@mail.gmail.com>
	<3BAD818D9407B043817CC6D89ABA14EC032B85AF@MLNYC20MB051.amrs.win.ml.com>
	<e8e755250905211122p47446673td142a0bae175d008@mail.gmail.com>
Message-ID: <8ec76080905211514g4b086868t173ccb62127859d1@mail.gmail.com>

fts underneath is a c++ policy template based class.

The underlying storage can be pretty much anything as long as you
implement the public api.

here is the backend for R:
http://github.com/armstrtw/r.tslib.backend/tree/master

the backend could just as easily be a python object allocator, a
dataframe, or a flat file.

-Whit

On Thu, May 21, 2009 at 2:22 PM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> I have given some thought to both ff and bigmemory. ?I am not a huge
> fan of the "ff" license.
>
> http://cran.r-project.org/web/packages/ff/LICENSE
>
> bigmemory is interesting in that you can bypass the R memory issues on
> Windows, but I haven't had incredible luck with it. ?Me and C++ don't
> like each other, so maybe it is something related to that :). ?I can
> get around the Windows issues by using something non-windows...
>
> Supposedly the changes to the most recent bigmemory are quite good,
> but trying the shared memory route (one of the biggest reasons I would
> like to use) has caused me catastrophic failure.
>
> At the end of the day there is no good way to make xts rely on
> bigmemory. As so much code in in C for xts, you can't readily operate
> on the external pointers from there. ?You need to read in the data
> (via `[` ) and at that point it is resident to the R process, so you
> are only getting the penalty of the memory allocation, and none of the
> gain.
>
> Of course this is my 2c. ?Maybe we need another time-series library :)
>
> Jeff
>
>
>
>
> On Thu, May 21, 2009 at 1:10 PM, Rowe, Brian Lee Yung (Portfolio
> Analytics) <B_Rowe at ml.com> wrote:
>> ... and possibly even a list change.
>>
>> Do you plan on making this compatible with ff or bigmemory? Seems like this theme is making its rounds.
>>
>> Brian
>>
>> -----Original Message-----
>> From: Jeff Ryan [mailto:jeff.a.ryan at gmail.com]
>> Sent: Thursday, May 21, 2009 1:58 PM
>> To: Rowe, Brian Lee Yung (Portfolio Analytics)
>> Cc: Dirk Eddelbuettel; Hae Kyung Im; r-sig-finance at stat.math.ethz.ch
>> Subject: Re: [R-SIG-Finance] Preprocessing RData file (Was: Kdb (Was: high frequency data analysis in R))
>>
>>
>> I feel like I should change the title again... :)
>>
>> The RData files are compressed first off. If you don't want the gzip
>> overhead, get rid of it.
>>
>> The xts format 'on-disk' is nothing more that the structure from
>> memory written to disk. ?This manages to be both faster and takes up
>> less space. ?It isn't a huge gain, but it allows for binary searching
>> of the index to get to the data you want.
>>
>> I will put together a performance comparison at some point, and pass along.
>>
>> Jeff
>>
>> On Thu, May 21, 2009 at 12:52 PM, Rowe, Brian Lee Yung (Portfolio
>> Analytics) <B_Rowe at ml.com> wrote:
>>> Is there any literature on the relative performance gain of
>>> preprocessing data into RData and then reading into R? Does it breakdown
>>> anywhere? I have 4 GB of data that I'm reading in and I/O is a large
>>> bottleneck.
>>>
>>> Brian
>>>
>>>
>>> -----Original Message-----
>>> From: r-sig-finance-bounces at stat.math.ethz.ch
>>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Dirk
>>> Eddelbuettel
>>> Sent: Thursday, May 21, 2009 1:42 PM
>>> To: Hae Kyung Im
>>> Cc: r-sig-finance at stat.math.ethz.ch
>>> Subject: [R-SIG-Finance] Kdb (Was: high frequency data analysis in R)
>>>
>>>
>>>
>>> On 21 May 2009 at 11:13, Hae Kyung Im wrote:
>>> | access (query) this huge database. I looked a little bit into kdb but
>>> | you have to pay ~25K to buy the software for one processor. I haven't
>>>
>>> True, but you can have "free" (as in beer) 32bit version that times out
>>> after
>>> two hours. That's not a bad compromise.
>>>
>>> I looked at it for a bit, and I has an R interface. (My blog has a patch
>>> to
>>> fix their then-broken interface to R's Datetime; I think they may have
>>> integrated that by now). ?Then again you can also pre-process into RData
>>> files, or use hdf5, or use a gazillion other methods. ? But the free
>>> trial
>>> version may just help for the odd research project like the one Haky
>>> described.
>>>
>>> Dirk
>>>
>>> --
>>> Three out of two people have difficulties with fractions.
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>> --------------------------------------------------------------------------
>>> This message w/attachments (message) may be privileged, confidential or proprietary, and if you are not an intended recipient, please notify the sender, do not use or share it and delete it. Unless specifically indicated, this message is not an offer to sell or a solicitation of any investment products or other financial product or service, an official confirmation of any transaction, or an official statement of Merrill Lynch. Subject to applicable law, Merrill Lynch may monitor, review and retain e-communications (EC) traveling through its networks/systems. The laws of the country of each sender/recipient may impact the handling of EC, and EC may be archived, supervised and produced in countries other than the country in which you are located. This message cannot be guaranteed to be secure or error-free. References to "Merrill Lynch" are references to any company in the Merrill Lynch & Co., Inc. group of companies, which are wholly-owned by Bank of America Corporation. Secu!
>>> ?rities and Insurance Products: * Are Not FDIC Insured * Are Not Bank Guaranteed * May Lose Value * Are Not a Bank Deposit * Are Not a Condition to Any Banking Service or Activity * Are Not Insured by Any Federal Government Agency. Attachments that are part of this E-communication may have additional important disclosures and disclaimers, which you should read. This message is subject to terms available at the following link: http://www.ml.com/e-communications_terms/. By messaging with Merrill Lynch you consent to the foregoing.
>>> --------------------------------------------------------------------------
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>>
>>
>> --
>> Jeffrey Ryan
>> jeffrey.ryan at insightalgo.com
>>
>> ia: insight algorithmics
>> www.insightalgo.com
>>
>
>
>
> --
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From comtech.usa at gmail.com  Fri May 22 01:20:56 2009
From: comtech.usa at gmail.com (Michael)
Date: Thu, 21 May 2009 16:20:56 -0700
Subject: [R-SIG-Finance] hands-on model selection and statistical data
	analysis books in R?
Message-ID: <b1f16d9d0905211620y3b6c78c8s4d0f9cbaf3ca6d59@mail.gmail.com>

Hi all,

I am fitting a model to time series of intra-day financial data.

Could anybody point me to some hands-on books about model selection,
model specification test, goodness-of-fit test, feature selection and
statistical time series data analysis? I am looking not for
theoretical or math books, but for guided lab type of books, so that I
could realistically know how to choose the best model, while without
the overfitting. Hopefully it's in R.

Thanks!


From irafuchs at gmail.com  Fri May 22 02:41:14 2009
From: irafuchs at gmail.com (Fuchs Ira)
Date: Thu, 21 May 2009 20:41:14 -0400
Subject: [R-SIG-Finance] portfolio rebalancing
Message-ID: <8DE4BC9F-238A-44D8-8E15-BD23CC498671@gmail.com>

I'm trying (so far unsuccessfully) to figure out how I might write a  
function (or more likely a set of functions) to backtest a few  
rebalancing algorithms. for example, starting with some amount of  
capital I want to equal weight purchase a set of equities and then  
rebalance in a variety of frequencies (daily, weekly, monthly, and  
perhaps based on % change from equal weights).  Getting the prices  
with getSymbols is easy enough but things kind of go downhill from  
there. I know this should be easy for anyone familiar with R but my  
background is APL and whenever I try to do something that I think  
should work, it doesn't.  I know this is just because I don't have  
enough R experience but if anyone can suggest any functions  that I  
could reuse for this purpose, that would be much appreciated.

As an example of my bad R intuition:

Let's say I want the prices of stock tickers MSFT, IBM, ORCL:

I say

getSymbols(c("MSFT","IBM","ORCL"))

Now I have 3 data frames which I think would be easier to deal with as  
one so I say

prices=merge(ORCL[,6],merge(MSFT[,6],IBM[,6]))

so far so good.

Now I have a dataframe with 3 columns each with the adjusted prices of  
a stock.

I can calculate the number of shares for the equal weight purchase for  
a specific date (2009-01-02] by saying:

shares=(capital/3)/prices["2009-01-02"] #determine shares

but now I'd like to do this calculation each day, or week, or month  
and after each calculation determine the new total value and redo the  
equal weighting (or for that matter a more complex weighting).  
functions like to.weekly won;t work because the dataframe is not OHLC.  
Are there other similar functions that will just give me the rows for  
some set of periodic dates?

Here's where my R intuition starts to break down...I try

sum(shares*prices["2009-02-02"]) #calculate total share value on  
2009-02-02

I get:

[1] 0

not quite what I expected..so guessing that multiplying data.frames is  
not good form, I try:

capital=sum(shares*as.numeric(prices["2009-02-02"]))

which works:

 > capital
[1] 945462

OK, so now I have a new starting capital value and I can repeat as  
above but with the following day's or week's or month's prices.

shares=(capital/3)/prices["2009-02-02"] # gives me the number of  
shares to own on 2009-02-02

but what if I want shares (and the capital values) to be a time series  
so I can see the share and capital amounts by date...I tried

shares[,2]=(capital/3)/prices["2009-02-02"] gives:

Error in NextMethod(.Generic) :
   number of items to replace is not a multiple of replacement length

No doubt this is simple to do...right?

Of course, the real goal is to have a set of functions that would  
permit easy changes to the frequencies, the threshold for rebalancing  
and so on but maybe some kind soul could put me on the right R way to  
think about this.

Thanks.


From daler at uic.edu  Fri May 22 15:59:40 2009
From: daler at uic.edu (Dale W.R. Rosenthal)
Date: Fri, 22 May 2009 08:59:40 -0500
Subject: [R-SIG-Finance] high frequency data analysis in R
In-Reply-To: <mailman.1.1242986401.30725.r-sig-finance@stat.math.ethz.ch>
References: <mailman.1.1242986401.30725.r-sig-finance@stat.math.ethz.ch>
Message-ID: <4A16AFCC.1010308@uic.edu>


Having looked a lot at high-frequency data, let me make a few 
corrections here.

1) True, you have no transaction prices outside of when trades happen.  
You could impute prices; but, that would also need to account for the 
downward bias when an instrument has not traded for a while.  Also, you 
cannot trust trade timestamps 100%; there is some publishing delay.  
(The usual reasoning being that market makers need time to hedge after 
getting hit/lifted.)

2) You do always have the NBBO and maybe even the book.  Better still, 
you can probably trust quote timestamps.

3) Using these two data streams together requires caution.  Trades are 
published with delay; and, while the delay might be small, it is not 
small relative to the number of quote changes.  Therefore, you have 
NBBOs and you have trade prices; but, matching them up is not 
straightforward.  Worse:  There is endogeneity that can creep in.  A 
trade may induce a change in quotes.  Comparing the trade price to 
quotes after the trade occurred will bias any comparison.

If you want to read up on a way to handle that delay, look at sections 3 
and 4 of my trade direction paper at 
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1032701.  The basic 
idea: you might want to average quotes before the trade using something 
like a gamma distribution for the delay -- so you have \hat{q}_t = 
\int_0^T GammaCDF(s) q_{t-s} ds.

You can also find code to do this at http://tigger.uic.edu/~daler/code.html

Just a few thoughts since we seem to be drifting toward mixing quotes 
and trades indiscriminately.

Dale

> Message: 14
> Date: Thu, 21 May 2009 13:48:45 -0400
> From: Eugene Tyurin <etyurin at skipstonellc.com>
> Subject: Re: [R-SIG-Finance] high frequency data analysis in R
> To: Michael <comtech.usa at gmail.com>, r-sig-finance at stat.math.ethz.ch
> High-frequency is not my specialty either, but a quote caught my 
> attention:
> On Thu, May 21, 2009 at 11:38 AM, Michael <comtech.usa at gmail.com> wrote:
>> > My data are price change arrivals, irregularly spaced. But when there
>> > is no price change, the price stays constant. Therefore, in fact, at
>> > any time instant, you give me a time, I can give you the price at that
>> > very instant of time. So irregularly spaced data can be easily sampled
>> > to be regularly spaced data.
>>     
> >From a trader's perspective, you do not have "the price" at any time 
> outside of the instant a trade took place - you have NBBO (and market 
> depth). Last trade's price may or may not be transactable again on 
> either long or short side. You can alternatively say that you have an 
> instanteneous "mid-market price" and a bid/ask spread to work with. 
> Correct me if I'm wrong - I'd like to know how people in HF really 
> look at their data. -- ET.
-- 
Dale W.R. Rosenthal
Assistant Professor, Department of Finance
University of Illinois at Chicago
http://tigger.uic.edu/~daler
SSRN: http://ssrn.com/author=906862


From sjaffe at riskspan.com  Fri May 22 16:18:38 2009
From: sjaffe at riskspan.com (Steve Jaffe)
Date: Fri, 22 May 2009 07:18:38 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Preprocessing RData file
 (data.table and ff, bigmemory)
In-Reply-To: <e8e755250905211124j6610428bw6626e7adb4e3c13d@mail.gmail.com>
References: <b1f16d9d0905210804s6fb9832crf935912bd7509a4c@mail.gmail.com>
	<e8e755250905210815h742c04aekfd111ff7e4165c1d@mail.gmail.com>
	<197a5bbc0905210913q421b2462tc2c9b5dcfa3ebb8e@mail.gmail.com>
	<18965.37493.165830.47114@ron.nulle.part>
	<3BAD818D9407B043817CC6D89ABA14EC032B85AE@MLNYC20MB051.amrs.win.ml.com>
	<e8e755250905211057v6611568ev5f74f4cb5170bc5c@mail.gmail.com>
	<3BAD818D9407B043817CC6D89ABA14EC032B85AF@MLNYC20MB051.amrs.win.ml.com>
	<e8e755250905211122p47446673td142a0bae175d008@mail.gmail.com>
	<e8e755250905211124j6610428bw6626e7adb4e3c13d@mail.gmail.com>
Message-ID: <23671313.post@talk.nabble.com>


I'm new to R and interested in working with large amounts of data
(timeseries, but regularly spaced.) Can you point me to a good reference for
using data.table with bigmemory or ff? 

(I'm a bit puzzled about what exactly these packages provide. As I
understand it, on 32-bit platforms files are subject to the same 2GB limit
as in-process memory, so I assume that dealing with a larger dataset still
requires breaking it up into multiple files...)

Thanks for your help.



I failed to point out that data.table can make use of both (?) those
packages. [ff, bigmemory]

It isn't a time-series library per se, but it make one very cool
in-memory database.  Similar in spirit to some of the not-so-free ones
out there...

Jeff


-- 
View this message in context: http://www.nabble.com/high-frequency-data-analysis-in-R-tp23654793p23671313.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From Jose at erini.ac.uk  Fri May 22 16:38:01 2009
From: Jose at erini.ac.uk (Jose Iparraguirre D'Elia)
Date: Fri, 22 May 2009 15:38:01 +0100
Subject: [R-SIG-Finance] [R-sig-finance] Preprocessing RData file
	(data.table and ff, bigmemory)
Message-ID: <C9328F0EEDC3BC439FDABD12060E91098E1FEE@erini1.ERINI.local>

You could also have a look at filehash, with which I've been playing around for a while. 

Furthermore, I recently found a package still in Beta version, colbycol, written by Carlos J. Gil Bellosta (http://www.datanalytics.com) which seems to do what's on the tin: reading and managing large datasets well beyond the in-process memory limits...

Jos?

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Steve Jaffe
Sent: 22 May 2009 15:19
To: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] [R-sig-finance] Preprocessing RData file (data.table and ff, bigmemory)


I'm new to R and interested in working with large amounts of data
(timeseries, but regularly spaced.) Can you point me to a good reference for
using data.table with bigmemory or ff? 

(I'm a bit puzzled about what exactly these packages provide. As I
understand it, on 32-bit platforms files are subject to the same 2GB limit
as in-process memory, so I assume that dealing with a larger dataset still
requires breaking it up into multiple files...)

Thanks for your help.



I failed to point out that data.table can make use of both (?) those
packages. [ff, bigmemory]

It isn't a time-series library per se, but it make one very cool
in-memory database.  Similar in spirit to some of the not-so-free ones
out there...

Jeff


-- 
View this message in context: http://www.nabble.com/high-frequency-data-analysis-in-R-tp23654793p23671313.html
Sent from the Rmetrics mailing list archive at Nabble.com.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From jeff.a.ryan at gmail.com  Fri May 22 16:33:10 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 22 May 2009 09:33:10 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Preprocessing RData file
	(data.table and ff, bigmemory)
In-Reply-To: <23671313.post@talk.nabble.com>
References: <b1f16d9d0905210804s6fb9832crf935912bd7509a4c@mail.gmail.com>
	<e8e755250905210815h742c04aekfd111ff7e4165c1d@mail.gmail.com>
	<197a5bbc0905210913q421b2462tc2c9b5dcfa3ebb8e@mail.gmail.com>
	<18965.37493.165830.47114@ron.nulle.part>
	<3BAD818D9407B043817CC6D89ABA14EC032B85AE@MLNYC20MB051.amrs.win.ml.com>
	<e8e755250905211057v6611568ev5f74f4cb5170bc5c@mail.gmail.com>
	<3BAD818D9407B043817CC6D89ABA14EC032B85AF@MLNYC20MB051.amrs.win.ml.com>
	<e8e755250905211122p47446673td142a0bae175d008@mail.gmail.com>
	<e8e755250905211124j6610428bw6626e7adb4e3c13d@mail.gmail.com>
	<23671313.post@talk.nabble.com>
Message-ID: <e8e755250905220733q2dfce52elece3eb0aad30b507@mail.gmail.com>

>From a recent post by the author:

http://finzi.psych.upenn.edu/R/Rhelp08/2009-March/193490.html

Further information on 'ff' and 'bigmemory' is covered in those
respective packages.

As far as combining the two/three, I would wait to hear back from Matt
on exactly how to do that.  I thought there was an example somewhere
if I recall...

The main advantage to using large datasets in RAM is simply
efficiency.  'ff' makes that process manageable without a lot of RAM,
bigmemory can bypass single process limits of R (and do some cool
memory sharing).  The advantage to both is really confined to 32bit
processing, if I am thinking straight.

This is probably more of a question for R-help at this point, or even
R-Sig-db though, as the 'finance' part is only tangential.

If you can break the data up with something like a db scheme, then xts
will be faster than all (?) the other solutions for in-memory
manipulation -- as it is time-series oriented.  And if you've got
64bits and lots of RAM it should do most of what you need.

HTH
Jeff


On Fri, May 22, 2009 at 9:18 AM, Steve Jaffe <sjaffe at riskspan.com> wrote:
>
> I'm new to R and interested in working with large amounts of data
> (timeseries, but regularly spaced.) Can you point me to a good reference for
> using data.table with bigmemory or ff?
>
> (I'm a bit puzzled about what exactly these packages provide. As I
> understand it, on 32-bit platforms files are subject to the same 2GB limit
> as in-process memory, so I assume that dealing with a larger dataset still
> requires breaking it up into multiple files...)
>
> Thanks for your help.
>
>
>
> I failed to point out that data.table can make use of both (?) those
> packages. [ff, bigmemory]
>
> It isn't a time-series library per se, but it make one very cool
> in-memory database. ?Similar in spirit to some of the not-so-free ones
> out there...
>
> Jeff
>
>
> --
> View this message in context: http://www.nabble.com/high-frequency-data-analysis-in-R-tp23654793p23671313.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From cgb at datanalytics.com  Fri May 22 17:17:56 2009
From: cgb at datanalytics.com (Carlos J. Gil Bellosta )
Date: Fri, 22 May 2009 17:17:56 +0200
Subject: [R-SIG-Finance] [R-sig-finance] Preprocessing RData file
	(data.table and ff, bigmemory)
In-Reply-To: <C9328F0EEDC3BC439FDABD12060E91098E1FEE@erini1.ERINI.local>
References: <C9328F0EEDC3BC439FDABD12060E91098E1FEE@erini1.ERINI.local>
Message-ID: <b028350f0905220817m172650afhb7c72e54dde4e04@mail.gmail.com>

Incidentally, my package is already available at

http://r-forge.r-project.org/projects/colbycol/

You can install it by

install.packages("colbycol",repos="http://R-Forge.R-project.org")

There may be some issues as it depends on Python (and it does not work
with Python 3.0). If you work on Windows, you will have to make Python
available to R setting the path properly.

I am working in fixing these nuances. And I do welcome willing beta-testers!

Best regards,

Carlos J. Gil Bellosta
http://www.datanalytics.com


2009/5/22 Jose Iparraguirre D'Elia <Jose at erini.ac.uk>:
> You could also have a look at filehash, with which I've been playing around for a while.
>
> Furthermore, I recently found a package still in Beta version, colbycol, written by Carlos J. Gil Bellosta (http://www.datanalytics.com) which seems to do what's on the tin: reading and managing large datasets well beyond the in-process memory limits...
>
> Jos?


From swisdom at gmail.com  Fri May 22 18:51:04 2009
From: swisdom at gmail.com (Steve Wisdom)
Date: Fri, 22 May 2009 12:51:04 -0400
Subject: [R-SIG-Finance] Thoughts for "Michael" (was "high frequency")
Message-ID: <8890ce50905220951t5f6ac4e1q51b5114ad366b784@mail.gmail.com>

Michael <comtech.usa at gmail.com>

> In fact, I have the whole jump processes of best bid, and best ask, at
> a continuous level (in the sense of time-stamped arrival data), and
> also the jump process of the last trade price, at a continuous level
> (in the sense of time-stamped arrival data). Any more thoughts?

Thoughts:

1) Confine R-Sig-Fin posts to... R-Sig-Fin questions

2) Supply a real name (first, last)... this isn't elitetrader.com

3) Read a book or books on the topic

4) Have a beer with a quant from one of the brand-name shops

HTH, Steve


From comtech.usa at gmail.com  Sat May 23 02:24:10 2009
From: comtech.usa at gmail.com (Michael)
Date: Fri, 22 May 2009 17:24:10 -0700
Subject: [R-SIG-Finance] why does interpolation in high frequency time
	series create spurious correlation?
Message-ID: <b1f16d9d0905221724q3950bda8s9010e8d845e7d926@mail.gmail.com>

Hi all,

I am reading some papers on high frequency financial data analysis, by Engle.

Could anybody point me to some more indepth/tutorial treatment (such
as books), where it talks about why interpolation in high frequency
time series (resampling irregularly spaced transaction data into
regularly spaced usual time series) creates spurious correlation? (My
goal is to study the correlation of two high frequency time series,
and see if there could be pairs trading opportunities or other trading
opportunities.)

Thank you!


From markleeds at verizon.net  Sat May 23 03:13:25 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Fri, 22 May 2009 20:13:25 -0500 (CDT)
Subject: [R-SIG-Finance] time series question
Message-ID: <497350999.915258.1243041205704.JavaMail.root@vms229.mailsrvcs.net>

Hi everyone: Normally, if one has a single realization of a time series and one wants to estimate 
say an ARMA(p,q) , where p and q are known ( for simplicity )  then one estimates it and that's that. 

But, suppose that one has more than one realization  of the time series ( assuming each series is the same length) and yet still wants to estimate the "best" arma(p,q) , over all the realizations,  again where p and q are known. 

I'm somewhat familiar with the literature but I don't know how to do this nor do I know of a book
that talks about this problem.  The only thing I could think of was casting the arma(p,q) in its equivalent state space form and then possibly using the dlm package ?. Is this the only way to do this ? I was hoping that there was a simpler way ? or if anyone knows of a relevant paper or book, that would
be appreciated.

also, if assuming that p=1 and q=1 makes this question simpler, i'm willing to make that assumption also. thanks.


From spencer.graves at prodsyse.com  Sat May 23 04:31:50 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Fri, 22 May 2009 19:31:50 -0700
Subject: [R-SIG-Finance] time series question
In-Reply-To: <497350999.915258.1243041205704.JavaMail.root@vms229.mailsrvcs.net>
References: <497350999.915258.1243041205704.JavaMail.root@vms229.mailsrvcs.net>
Message-ID: <4A176016.80601@prodsyse.com>

Hi, Mark: 


      Have you considered using 'lme' in the 'nlme' package with a 
'corARMA' correlation structure, as described in sec. 5.3.3 of Pinheiro 
and Bates (2000) Mixed-Effects Models in S and S-PLUS (Springer)?  This 
package includes in a director " system.file('scripts', package='nlme')" 
files with names like "ch05.R" code to work essentially all the examples 
in the indicated chapters.  After you understand the contents of this 
book, and especially how to use this code for the type of problem you 
just described, you may wish to use the "dlm" package in conjunction 
with the "nlme" function in the "nlme" package.


      Hope this helps. 
      Spencer
p.s.  You mentioned "dlm".  In addition to having a vignette that helped 
me learn how to use it, it will soon have a companion book:  Petris, 
Petrone, and Campagnoli (2009) Dynamic Linear Models with R (Springer;  
scheduled to be available after June 26, according to Amazon).  I have 
not seen the book, but I like the "dlm" package, including the vignette. 


markleeds at verizon.net wrote:
> Hi everyone: Normally, if one has a single realization of a time series and one wants to estimate 
> say an ARMA(p,q) , where p and q are known ( for simplicity )  then one estimates it and that's that. 
>
> But, suppose that one has more than one realization  of the time series ( assuming each series is the same length) and yet still wants to estimate the "best" arma(p,q) , over all the realizations,  again where p and q are known. 
>
> I'm somewhat familiar with the literature but I don't know how to do this nor do I know of a book
> that talks about this problem.  The only thing I could think of was casting the arma(p,q) in its equivalent state space form and then possibly using the dlm package ?. Is this the only way to do this ? I was hoping that there was a simpler way ? or if anyone knows of a relevant paper or book, that would
> be appreciated.
>
> also, if assuming that p=1 and q=1 makes this question simpler, i'm willing to make that assumption also. thanks.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From comtech.usa at gmail.com  Sat May 23 05:24:54 2009
From: comtech.usa at gmail.com (Michael)
Date: Fri, 22 May 2009 20:24:54 -0700
Subject: [R-SIG-Finance] intraday data for VIX?
Message-ID: <b1f16d9d0905222024h518ce642h76ab4c49b1588f76@mail.gmail.com>

Hi all,

Is there a way to get recent (2009) intraday data for VIX ?

I know a database called TAQ, but that's mostly for historical NBBO
prices up to Oct. 2008.

Even 5-min data for VIX is good for me.

Thank you!


From cedrick at cedrickjohnson.com  Sat May 23 17:40:54 2009
From: cedrick at cedrickjohnson.com (Cedrick Johnson)
Date: Sat, 23 May 2009 11:40:54 -0400
Subject: [R-SIG-Finance] intraday data for VIX?
In-Reply-To: <b1f16d9d0905222024h518ce642h76ab4c49b1588f76@mail.gmail.com>
References: <b1f16d9d0905222024h518ce642h76ab4c49b1588f76@mail.gmail.com>
Message-ID: <4A181906.5000202@cedrickjohnson.com>

You could obtain the datasets via Bloomberg (RBloomberg).......


Michael wrote:
> Hi all,
>
> Is there a way to get recent (2009) intraday data for VIX ?
>
> I know a database called TAQ, but that's mostly for historical NBBO
> prices up to Oct. 2008.
>
> Even 5-min data for VIX is good for me.
>
> Thank you!
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From cevans at chyden.net  Sat May 23 17:50:19 2009
From: cevans at chyden.net (Charles Evans)
Date: Sat, 23 May 2009 11:50:19 -0400
Subject: [R-SIG-Finance] RBloomberg (was Re:  intraday data for VIX?)
In-Reply-To: <4A181906.5000202@cedrickjohnson.com>
References: <b1f16d9d0905222024h518ce642h76ab4c49b1588f76@mail.gmail.com>
	<4A181906.5000202@cedrickjohnson.com>
Message-ID: <4A4292D3-8BCD-4CD6-BA25-760F145D2378@chyden.net>

OK, *this* is cool!  Can anyone download data from Bloomberg with  
this, or does it require a subscription?

C.Evans


On 23 May 2009, at 11:40, Cedrick Johnson wrote:

> You could obtain the datasets via Bloomberg (RBloomberg).......
>
>
> Michael wrote:
>> Hi all,
>>
>> Is there a way to get recent (2009) intraday data for VIX ?
>>
>> I know a database called TAQ, but that's mostly for historical NBBO
>> prices up to Oct. 2008.
>>
>> Even 5-min data for VIX is good for me.
>>
>> Thank you!


From cedrick at cedrickjohnson.com  Sat May 23 17:53:33 2009
From: cedrick at cedrickjohnson.com (Cedrick Johnson)
Date: Sat, 23 May 2009 11:53:33 -0400
Subject: [R-SIG-Finance] RBloomberg (was Re:  intraday data for VIX?)
In-Reply-To: <4A4292D3-8BCD-4CD6-BA25-760F145D2378@chyden.net>
References: <b1f16d9d0905222024h518ce642h76ab4c49b1588f76@mail.gmail.com>	<4A181906.5000202@cedrickjohnson.com>
	<4A4292D3-8BCD-4CD6-BA25-760F145D2378@chyden.net>
Message-ID: <4A181BFD.8060202@cedrickjohnson.com>

Unfortunately, it requires a subscription to a BBG terminal.

Charles Evans wrote:
> OK, *this* is cool!  Can anyone download data from Bloomberg with 
> this, or does it require a subscription?
>
> C.Evans
>
>
> On 23 May 2009, at 11:40, Cedrick Johnson wrote:
>
>> You could obtain the datasets via Bloomberg (RBloomberg).......
>>
>>
>> Michael wrote:
>>> Hi all,
>>>
>>> Is there a way to get recent (2009) intraday data for VIX ?
>>>
>>> I know a database called TAQ, but that's mostly for historical NBBO
>>> prices up to Oct. 2008.
>>>
>>> Even 5-min data for VIX is good for me.
>>>
>>> Thank you!
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From comtech.usa at gmail.com  Sat May 23 19:15:08 2009
From: comtech.usa at gmail.com (Michael)
Date: Sat, 23 May 2009 10:15:08 -0700
Subject: [R-SIG-Finance] intraday data for VIX?
In-Reply-To: <4A181906.5000202@cedrickjohnson.com>
References: <b1f16d9d0905222024h518ce642h76ab4c49b1588f76@mail.gmail.com>
	<4A181906.5000202@cedrickjohnson.com>
Message-ID: <b1f16d9d0905231015j7fa29e5eh428d9660e7d5b96e@mail.gmail.com>

Thanks! How high frequency could it be?

I guess RBloomberg needs a subscription?

On Sat, May 23, 2009 at 8:40 AM, Cedrick Johnson
<cedrick at cedrickjohnson.com> wrote:
> You could obtain the datasets via Bloomberg (RBloomberg).......
>
>
> Michael wrote:
>>
>> Hi all,
>>
>> Is there a way to get recent (2009) intraday data for VIX ?
>>
>> I know a database called TAQ, but that's mostly for historical NBBO
>> prices up to Oct. 2008.
>>
>> Even 5-min data for VIX is good for me.
>>
>> Thank you!
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>


From ajayshah at mayin.org  Sat May 23 20:20:42 2009
From: ajayshah at mayin.org (Ajay Shah)
Date: Sat, 23 May 2009 23:50:42 +0530
Subject: [R-SIG-Finance] time series question
In-Reply-To: <497350999.915258.1243041205704.JavaMail.root@vms229.mailsrvcs.net>
References: <497350999.915258.1243041205704.JavaMail.root@vms229.mailsrvcs.net>
Message-ID: <20090523182042.GF89968@ajay-shahs-macbook-pro.local>

On Fri, May 22, 2009 at 08:13:25PM -0500, markleeds at verizon.net wrote:
> Hi everyone: Normally, if one has a single realization of a time series and one wants to estimate 
> say an ARMA(p,q) , where p and q are known ( for simplicity )  then one estimates it and that's that. 
> 
> But, suppose that one has more than one realization  of the time series ( assuming each series is the same length) and yet still wants to estimate the "best" arma(p,q) , over all the realizations,  again where p and q are known. 

Could we perhaps think of this as follows.

We are holding two realisations from the same process:
   x1, x2, ... xN
   y1, y2, ... yN

and let's suppose these two realisations are completely
independent. Think of two parallel experiments running with the
identical data generating process but a different set of random
shocks.

Then you could construct the overall log likelihood of what you have
observed as logl(theta; x) + logl(theta; y) and maximise that.

Is there an existing R function off the shelf which yields the ARMA
log likelihood? If so then it should be easy to put together an
overall logl() function for this problem which can be then given to
optim() to do estimation.

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From babel at centrum.sk  Sat May 23 20:25:11 2009
From: babel at centrum.sk (babel at centrum.sk)
Date: Sat, 23 May 2009 20:25:11 +0200
Subject: [R-SIG-Finance] legend in quantmod
Message-ID: <200905232025.17896@centrum.cz>

Hi guys
Is it possible to draw a legend in chartSeries in quantmod package? I know I can write som text in "name" parameter, but I am looking for something similar to legend or text in plot function? 
Thank you

Jan


From spencer.graves at prodsyse.com  Sat May 23 20:57:16 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Sat, 23 May 2009 11:57:16 -0700
Subject: [R-SIG-Finance] time series question
In-Reply-To: <20090523182042.GF89968@ajay-shahs-macbook-pro.local>
References: <497350999.915258.1243041205704.JavaMail.root@vms229.mailsrvcs.net>
	<20090523182042.GF89968@ajay-shahs-macbook-pro.local>
Message-ID: <4A18470C.9070908@prodsyse.com>

      Have you tried the corARMA capabilities in the lme function, nlme 
package, as suggested in my earlier reply to this thread 
(https://stat.ethz.ch/pipermail/r-sig-finance/2009q2/004152.html)?  I'm 
not sure, but I believe you can either get one set of estimates or one 
for each series, estimating between-series variances, etc.  If so, you 
can also test hypotheses such as whether the ARMA models for the 
different series are different.  Moreover, the different series can be 
of different lengths. 


      Hope this helps. 
      Spencer Graves

Ajay Shah wrote:
> On Fri, May 22, 2009 at 08:13:25PM -0500, markleeds at verizon.net wrote:
>   
>> Hi everyone: Normally, if one has a single realization of a time series and one wants to estimate 
>> say an ARMA(p,q) , where p and q are known ( for simplicity )  then one estimates it and that's that. 
>>
>> But, suppose that one has more than one realization  of the time series ( assuming each series is the same length) and yet still wants to estimate the "best" arma(p,q) , over all the realizations,  again where p and q are known. 
>>     
>
> Could we perhaps think of this as follows.
>
> We are holding two realisations from the same process:
>    x1, x2, ... xN
>    y1, y2, ... yN
>
> and let's suppose these two realisations are completely
> independent. Think of two parallel experiments running with the
> identical data generating process but a different set of random
> shocks.
>
> Then you could construct the overall log likelihood of what you have
> observed as logl(theta; x) + logl(theta; y) and maximise that.
>
> Is there an existing R function off the shelf which yields the ARMA
> log likelihood? If so then it should be easy to put together an
> overall logl() function for this problem which can be then given to
> optim() to do estimation.
>
>


From markleeds at verizon.net  Sat May 23 22:06:05 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Sat, 23 May 2009 15:06:05 -0500 (CDT)
Subject: [R-SIG-Finance] time series question
Message-ID: <124247309.381229.1243109165524.JavaMail.root@vms125.mailsrvcs.net>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090523/d7a48e20/attachment.html>

From spencer.graves at prodsyse.com  Sat May 23 22:31:45 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Sat, 23 May 2009 13:31:45 -0700
Subject: [R-SIG-Finance] time series question
In-Reply-To: <124247309.381229.1243109165524.JavaMail.root@vms125.mailsrvcs.net>
References: <124247309.381229.1243109165524.JavaMail.root@vms125.mailsrvcs.net>
Message-ID: <4A185D31.8020602@prodsyse.com>

      The "lme" function maximizes a likelihood specified by the model 
implicit in the formula. 


      If you are worried about a lack of normality, I suggest that 
before you do a lot of work to invent your own thing, you try "lme", and 
plot residuals, estimated coefficients using "coef.lme", etc.  If you 
see evidence that the normal likelihood is not adequate, you would then 
have justification for doing something more complicated. 


      Spencer Graves    

markleeds at verizon.net wrote:
> Hi Ajay:  That's a good point. It's really a maximization of the sum of the  
> likelihoods of the individual series if you assume independent shocks.   I'd 
> have to look inside arima ( when I got the courage ),
> extract the likelihood piece and then put ithe sum inside say optim That's why I 
> was kind of hoping there might be something out there , even if independence 
> needed to be assumed.
>
> But,  I don't think your idea is quite equivalent to the DLM approach because 
> there you are able to
> specify correlation structure on the multiple series rather than assuming 
> independence of each series. For my problem, I have no idea whether relaxing the 
> assumption as your idea would do, would matter ? All these things are 
> approximations to reality anyway so who ever knows ?
>
> I'll I either go the DLM route ( spencer mentioned that I should also look at 
> Pinheiro and Bates ) or your route but I'm not there yet anyway. I was just 
> thinking about this for the down the road if and
> when I need it and I hope that I do because that would indicate progress.
>
>
>
>
>
>
>
> On May 23, 2009, *Ajay Shah* <ajayshah at mayin.org> wrote:
>
>     On Fri, May 22, 2009 at 08:13:25PM -0500, markleeds at verizon.net
>     <mailto:markleeds at verizon.net> wrote:
>      > Hi everyone: Normally, if one has a single realization of a time series
>     and one wants to estimate
>      > say an ARMA(p,q) , where p and q are known ( for simplicity ) then one
>     estimates it and that's that.
>      >
>      > But, suppose that one has more than one realization of the time series (
>     assuming each series is the same length) and yet still wants to estimate the
>     "best" arma(p,q) , over all the realizations, again where p and q are known.
>
>     Could we perhaps think of this as follows.
>
>     We are holding two realisations from the same process:
>     x1, x2, ... xN
>     y1, y2, ... yN
>
>     and let's suppose these two realisations are completely
>     independent. Think of two parallel experiments running with the
>     identical data generating process but a different set of random
>     shocks.
>
>     Then you could construct the overall log likelihood of what you have
>     observed as logl(theta; x) + logl(theta; y) and maximise that.
>
>     Is there an existing R function off the shelf which yields the ARMA
>     log likelihood? If so then it should be easy to put together an
>     overall logl() function for this problem which can be then given to
>     optim() to do estimation.
>
>     -- 
>     Ajay Shah http://www.mayin.org/ajayshah
>     ajayshah at mayin.org <mailto:ajayshah at mayin.org> http://ajayshahblog.blogspot.com
>     <*(:-? - wizard who doesn't know the answer.
>
>   
> ------------------------------------------------------------------------
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From markleeds at verizon.net  Sat May 23 22:36:54 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Sat, 23 May 2009 15:36:54 -0500 (CDT)
Subject: [R-SIG-Finance] time series question
Message-ID: <511450446.381778.1243111014571.JavaMail.root@vms125.mailsrvcs.net>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090523/165a385a/attachment.html>

From spencer.graves at prodsyse.com  Sun May 24 04:20:51 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Sat, 23 May 2009 19:20:51 -0700
Subject: [R-SIG-Finance] portfolio rebalancing
In-Reply-To: <8DE4BC9F-238A-44D8-8E15-BD23CC498671@gmail.com>
References: <8DE4BC9F-238A-44D8-8E15-BD23CC498671@gmail.com>
Message-ID: <4A18AF03.6010209@prodsyse.com>

Hello: 


      The error "number of items to replace is not a multiple of 
replacement length" occurs because shares[, 2] is an xts object with one 
row and one column, and you are trying to replace that single element by 
(capital/3)/prices["2009-02-02"], which is an xts object with one row 
and three columns.  Three numbers won't fit into one storage location. 


      Beyond this, my favorite tool for finding capabilities in 
contributed R packages is "RSiteSearch.function": 


library(RSiteSearch)
pr <- RSiteSearch.function('portfolio rebalancing')
nrow(pr)  # 0
pb <- RSiteSearch.function('portfolio balancing')
nrow(pb)  # 1
HTML(pb)  # data sets;  not what you want. 


      These search terms produced nothing, and trying these terms in 
RSiteSearch including the email archives failed to produce more.  I 
seemed to have better results Googling for "R-SIG-Finance portfolio 
balancing".  Of course, you would need to decide if you found anything 
relevant there. 


      Hope this helps. 
      Spencer
     

Fuchs Ira wrote:
> I'm trying (so far unsuccessfully) to figure out how I might write a 
> function (or more likely a set of functions) to backtest a few 
> rebalancing algorithms. for example, starting with some amount of 
> capital I want to equal weight purchase a set of equities and then 
> rebalance in a variety of frequencies (daily, weekly, monthly, and 
> perhaps based on % change from equal weights).  Getting the prices 
> with getSymbols is easy enough but things kind of go downhill from 
> there. I know this should be easy for anyone familiar with R but my 
> background is APL and whenever I try to do something that I think 
> should work, it doesn't.  I know this is just because I don't have 
> enough R experience but if anyone can suggest any functions  that I 
> could reuse for this purpose, that would be much appreciated.
>
> As an example of my bad R intuition:
>
> Let's say I want the prices of stock tickers MSFT, IBM, ORCL:
>
> I say
>
> getSymbols(c("MSFT","IBM","ORCL"))
>
> Now I have 3 data frames which I think would be easier to deal with as 
> one so I say
>
> prices=merge(ORCL[,6],merge(MSFT[,6],IBM[,6]))
>
> so far so good.
>
> Now I have a dataframe with 3 columns each with the adjusted prices of 
> a stock.
>
> I can calculate the number of shares for the equal weight purchase for 
> a specific date (2009-01-02] by saying:
>
> shares=(capital/3)/prices["2009-01-02"] #determine shares
>
> but now I'd like to do this calculation each day, or week, or month 
> and after each calculation determine the new total value and redo the 
> equal weighting (or for that matter a more complex weighting). 
> functions like to.weekly won;t work because the dataframe is not OHLC. 
> Are there other similar functions that will just give me the rows for 
> some set of periodic dates?
>
> Here's where my R intuition starts to break down...I try
>
> sum(shares*prices["2009-02-02"]) #calculate total share value on 
> 2009-02-02
>
> I get:
>
> [1] 0
>
> not quite what I expected..so guessing that multiplying data.frames is 
> not good form, I try:
>
> capital=sum(shares*as.numeric(prices["2009-02-02"]))
>
> which works:
>
> > capital
> [1] 945462
>
> OK, so now I have a new starting capital value and I can repeat as 
> above but with the following day's or week's or month's prices.
>
> shares=(capital/3)/prices["2009-02-02"] # gives me the number of 
> shares to own on 2009-02-02
>
> but what if I want shares (and the capital values) to be a time series 
> so I can see the share and capital amounts by date...I tried
>
> shares[,2]=(capital/3)/prices["2009-02-02"] gives:
>
> Error in NextMethod(.Generic) :
>   number of items to replace is not a multiple of replacement length
>
> No doubt this is simple to do...right?
>
> Of course, the real goal is to have a set of functions that would 
> permit easy changes to the frequencies, the threshold for rebalancing 
> and so on but maybe some kind soul could put me on the right R way to 
> think about this.
>
> Thanks.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From spencer.graves at prodsyse.com  Sun May 24 21:22:48 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Sun, 24 May 2009 12:22:48 -0700
Subject: [R-SIG-Finance] intraday data for VIX?
In-Reply-To: <b1f16d9d0905231015j7fa29e5eh428d9660e7d5b96e@mail.gmail.com>
References: <b1f16d9d0905222024h518ce642h76ab4c49b1588f76@mail.gmail.com>	<4A181906.5000202@cedrickjohnson.com>
	<b1f16d9d0905231015j7fa29e5eh428d9660e7d5b96e@mail.gmail.com>
Message-ID: <4A199E88.5070503@prodsyse.com>

      I have no direct experience with this myself, but you can get some 
data from Bloomberg for free.  However, professional traders routinely 
pay for essentially instantaneous Bloomberg data.  I could not find 
anything mentioning rates, but I remember hearing a number like $1,700 
per month for a single Bloomberg station.  I presume that you could get 
"tick" data, which is a record of every trade milliseconds after the 
transaction completes. 


      Hope this helps. 
      Spencer

Michael wrote:
> Thanks! How high frequency could it be?
>
> I guess RBloomberg needs a subscription?
>
> On Sat, May 23, 2009 at 8:40 AM, Cedrick Johnson
> <cedrick at cedrickjohnson.com> wrote:
>   
>> You could obtain the datasets via Bloomberg (RBloomberg).......
>>
>>
>> Michael wrote:
>>     
>>> Hi all,
>>>
>>> Is there a way to get recent (2009) intraday data for VIX ?
>>>
>>> I know a database called TAQ, but that's mostly for historical NBBO
>>> prices up to Oct. 2008.
>>>
>>> Even 5-min data for VIX is good for me.
>>>
>>> Thank you!
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>>       
>>     
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From cedrick at cedrickjohnson.com  Sun May 24 21:25:28 2009
From: cedrick at cedrickjohnson.com (Cedrick Johnson)
Date: Sun, 24 May 2009 15:25:28 -0400
Subject: [R-SIG-Finance] intraday data for VIX?
In-Reply-To: <4A199E88.5070503@prodsyse.com>
References: <b1f16d9d0905222024h518ce642h76ab4c49b1588f76@mail.gmail.com>	<4A181906.5000202@cedrickjohnson.com>
	<b1f16d9d0905231015j7fa29e5eh428d9660e7d5b96e@mail.gmail.com>
	<4A199E88.5070503@prodsyse.com>
Message-ID: <4A199F28.4000102@cedrickjohnson.com>

The fee for my terminal jumped in Dec 08.. $1900/mo + exchanges and 
other feeds (in realtime as opposed to delayed)..

-c

spencerg wrote:
>      I have no direct experience with this myself, but you can get 
> some data from Bloomberg for free.  However, professional traders 
> routinely pay for essentially instantaneous Bloomberg data.  I could 
> not find anything mentioning rates, but I remember hearing a number 
> like $1,700 per month for a single Bloomberg station.  I presume that 
> you could get "tick" data, which is a record of every trade 
> milliseconds after the transaction completes.
>
>      Hope this helps.      Spencer
>
> Michael wrote:
>> Thanks! How high frequency could it be?
>>
>> I guess RBloomberg needs a subscription?
>>
>> On Sat, May 23, 2009 at 8:40 AM, Cedrick Johnson
>> <cedrick at cedrickjohnson.com> wrote:
>>  
>>> You could obtain the datasets via Bloomberg (RBloomberg).......
>>>
>>>
>>> Michael wrote:
>>>    
>>>> Hi all,
>>>>
>>>> Is there a way to get recent (2009) intraday data for VIX ?
>>>>
>>>> I know a database called TAQ, but that's mostly for historical NBBO
>>>> prices up to Oct. 2008.
>>>>
>>>> Even 5-min data for VIX is good for me.
>>>>
>>>> Thank you!
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>>       
>>>     
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>   
>


From stefano.iacus at unimi.it  Sun May 24 22:08:30 2009
From: stefano.iacus at unimi.it (stefano iacus)
Date: Sun, 24 May 2009 22:08:30 +0200
Subject: [R-SIG-Finance] why does interpolation in high frequency
	time	series create spurious correlation?
In-Reply-To: <b1f16d9d0905221724q3950bda8s9010e8d845e7d926@mail.gmail.com>
References: <b1f16d9d0905221724q3950bda8s9010e8d845e7d926@mail.gmail.com>
Message-ID: <925D7CDA-64B6-4F54-8337-93D1AC1140F6@unimi.it>

Have a look at Hayashi-Yoshida papers or Malliavin-Mancino paper, or  
google for EPPS effect.
stefano

On 23/mag/09, at 02:24, Michael wrote:

> Hi all,
>
> I am reading some papers on high frequency financial data analysis,  
> by Engle.
>
> Could anybody point me to some more indepth/tutorial treatment (such
> as books), where it talks about why interpolation in high frequency
> time series (resampling irregularly spaced transaction data into
> regularly spaced usual time series) creates spurious correlation? (My
> goal is to study the correlation of two high frequency time series,
> and see if there could be pairs trading opportunities or other trading
> opportunities.)
>
> Thank you!
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From spencer.graves at prodsyse.com  Mon May 25 00:09:25 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Sun, 24 May 2009 15:09:25 -0700
Subject: [R-SIG-Finance] intraday data for VIX?
In-Reply-To: <4A199F28.4000102@cedrickjohnson.com>
References: <b1f16d9d0905222024h518ce642h76ab4c49b1588f76@mail.gmail.com>	<4A181906.5000202@cedrickjohnson.com>
	<b1f16d9d0905231015j7fa29e5eh428d9660e7d5b96e@mail.gmail.com>
	<4A199E88.5070503@prodsyse.com>
	<4A199F28.4000102@cedrickjohnson.com>
Message-ID: <4A19C595.9070605@prodsyse.com>

Hi, Cedrick: 


      Thanks for the info. 


      What do you mean by "exchanges and other feeds"? 


      Spencer Graves


Cedrick Johnson wrote:
> The fee for my terminal jumped in Dec 08.. $1900/mo + exchanges and 
> other feeds (in realtime as opposed to delayed)..
>
> -c
>
> spencerg wrote:
>>      I have no direct experience with this myself, but you can get 
>> some data from Bloomberg for free.  However, professional traders 
>> routinely pay for essentially instantaneous Bloomberg data.  I could 
>> not find anything mentioning rates, but I remember hearing a number 
>> like $1,700 per month for a single Bloomberg station.  I presume that 
>> you could get "tick" data, which is a record of every trade 
>> milliseconds after the transaction completes.
>>
>>      Hope this helps.      Spencer
>>
>> Michael wrote:
>>> Thanks! How high frequency could it be?
>>>
>>> I guess RBloomberg needs a subscription?
>>>
>>> On Sat, May 23, 2009 at 8:40 AM, Cedrick Johnson
>>> <cedrick at cedrickjohnson.com> wrote:
>>>  
>>>> You could obtain the datasets via Bloomberg (RBloomberg).......
>>>>
>>>>
>>>> Michael wrote:
>>>>   
>>>>> Hi all,
>>>>>
>>>>> Is there a way to get recent (2009) intraday data for VIX ?
>>>>>
>>>>> I know a database called TAQ, but that's mostly for historical NBBO
>>>>> prices up to Oct. 2008.
>>>>>
>>>>> Even 5-min data for VIX is good for me.
>>>>>
>>>>> Thank you!
>>>>>
>>>>> _______________________________________________
>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>> -- Subscriber-posting only.
>>>>> -- If you want to post, subscribe first.
>>>>>
>>>>>       
>>>>     
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>>   
>>
>
>


From guygreen at netvigator.com  Mon May 25 02:00:16 2009
From: guygreen at netvigator.com (gug)
Date: Sun, 24 May 2009 17:00:16 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] intraday data for VIX?
In-Reply-To: <4A19C595.9070605@prodsyse.com>
References: <b1f16d9d0905222024h518ce642h76ab4c49b1588f76@mail.gmail.com>
	<4A181906.5000202@cedrickjohnson.com>
	<b1f16d9d0905231015j7fa29e5eh428d9660e7d5b96e@mail.gmail.com>
	<4A199E88.5070503@prodsyse.com>
	<4A199F28.4000102@cedrickjohnson.com>
	<4A19C595.9070605@prodsyse.com>
Message-ID: <23699509.post@talk.nabble.com>


Data providers (Bloomberg, Reuters, etc - even Yahoo) generally give you
historical and delayed (e.g. 15 minutes late) data as part of your basic
package.  However if you want real-time prices, you have to pay for them. 
Those fees go to the exchange - they are exchange by exchange.

Guy



spencerg wrote:
> 
> Hi, Cedrick: 
> 
> 
>       Thanks for the info. 
> 
> 
>       What do you mean by "exchanges and other feeds"? 
> 
> 
>       Spencer Graves
> 
> 

-- 
View this message in context: http://www.nabble.com/intraday-data-for-VIX--tp23681090p23699509.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From spencer.graves at prodsyse.com  Mon May 25 03:03:49 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Sun, 24 May 2009 18:03:49 -0700
Subject: [R-SIG-Finance] [R-sig-finance] intraday data for VIX?
In-Reply-To: <23699509.post@talk.nabble.com>
References: <b1f16d9d0905222024h518ce642h76ab4c49b1588f76@mail.gmail.com>	<4A181906.5000202@cedrickjohnson.com>	<b1f16d9d0905231015j7fa29e5eh428d9660e7d5b96e@mail.gmail.com>	<4A199E88.5070503@prodsyse.com>	<4A199F28.4000102@cedrickjohnson.com>	<4A19C595.9070605@prodsyse.com>
	<23699509.post@talk.nabble.com>
Message-ID: <4A19EE75.6000504@prodsyse.com>

Hi, Guy: 


      Thanks very much.  That's helpful. 


      The Wikipedia article on the "Bloomberg Terminal" 
(http://en.wikipedia.org/wiki/Bloomberg_Terminal) helped me understand 
why people pay for that service, but I still have another question: 


      Do you get the same data from Bloomberg as, e.g., Yahoo?  Or does 
Bloomberg generally provide better quality or more comprehensive data? 


      Thanks again. 
      Spencer Graves


gug wrote:
> Data providers (Bloomberg, Reuters, etc - even Yahoo) generally give you
> historical and delayed (e.g. 15 minutes late) data as part of your basic
> package.  However if you want real-time prices, you have to pay for them. 
> Those fees go to the exchange - they are exchange by exchange.
>
> Guy
>
>
>
> spencerg wrote:
>   
>> Hi, Cedrick: 
>>
>>
>>       Thanks for the info. 
>>
>>
>>       What do you mean by "exchanges and other feeds"? 
>>
>>
>>       Spencer Graves
>>
>>
>>     
>
>


From cevans at chyden.net  Mon May 25 03:24:25 2009
From: cevans at chyden.net (Charles Evans)
Date: Sun, 24 May 2009 21:24:25 -0400
Subject: [R-SIG-Finance] intraday data for VIX?
In-Reply-To: <4A199E88.5070503@prodsyse.com>
References: <b1f16d9d0905222024h518ce642h76ab4c49b1588f76@mail.gmail.com>	<4A181906.5000202@cedrickjohnson.com>
	<b1f16d9d0905231015j7fa29e5eh428d9660e7d5b96e@mail.gmail.com>
	<4A199E88.5070503@prodsyse.com>
Message-ID: <F05E3E10-43A6-4C2C-8EDA-BE31DF646628@chyden.net>

Just tossing in my 2? here...

I work in an academic environment.  The interfaces that data vendors  
sell us are less intuitive and feature-rich than the ones that  
professional traders get.  On the other hand, academic packages are  
also much less expensive than professional packages, and our time is  
not worth as much on a currency-unit/hour basis.

One of my great frustrations is getting access to data that my  
university does not subscribe to.

If you have *any* active academic affiliation - e.g., you teach as an  
adjunct, you are a part-time student, etc. - *sometimes* you can sweet- 
talk your way into discounted or even pro-bono access to a data feed.   
I have not approached Bloomberg, per se, and your mileage may vary.

If you are trying to get free access to data, do *not* be in a hurry.   
Plan ahead and make contact with the data vendor as early as possible.

Dunno if this helps.  Ignore it, if it doesn't.  ;-)

C.Evans



On 24 May 2009, at 15:22, spencerg wrote:

>     I have no direct experience with this myself, but you can get  
> some data from Bloomberg for free.  However, professional traders  
> routinely pay for essentially instantaneous Bloomberg data.  I could  
> not find anything mentioning rates, but I remember hearing a number  
> like $1,700 per month for a single Bloomberg station.  I presume  
> that you could get "tick" data, which is a record of every trade  
> milliseconds after the transaction completes.
>
>     Hope this helps.      Spencer
>
> Michael wrote:
>> Thanks! How high frequency could it be?
>>
>> I guess RBloomberg needs a subscription?
>>
>> On Sat, May 23, 2009 at 8:40 AM, Cedrick Johnson
>> <cedrick at cedrickjohnson.com> wrote:
>>
>>> You could obtain the datasets via Bloomberg (RBloomberg).......
>>>
>>>
>>> Michael wrote:
>>>
>>>> Hi all,
>>>>
>>>> Is there a way to get recent (2009) intraday data for VIX ?
>>>>
>>>> I know a database called TAQ, but that's mostly for historical NBBO
>>>> prices up to Oct. 2008.
>>>>
>>>> Even 5-min data for VIX is good for me.
>>>>
>>>> Thank you!
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>>
>>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.

Charles Evans
cevans at chyden.net

The cornerstone of democracy is
the fallacy that everyone's opinion
is equally well informed.
Sue Donna Moss


From guygreen at netvigator.com  Mon May 25 03:55:16 2009
From: guygreen at netvigator.com (gug)
Date: Sun, 24 May 2009 18:55:16 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] intraday data for VIX?
In-Reply-To: <4A19EE75.6000504@prodsyse.com>
References: <b1f16d9d0905222024h518ce642h76ab4c49b1588f76@mail.gmail.com>
	<4A181906.5000202@cedrickjohnson.com>
	<b1f16d9d0905231015j7fa29e5eh428d9660e7d5b96e@mail.gmail.com>
	<4A199E88.5070503@prodsyse.com>
	<4A199F28.4000102@cedrickjohnson.com>
	<4A19C595.9070605@prodsyse.com> <23699509.post@talk.nabble.com>
	<4A19EE75.6000504@prodsyse.com>
Message-ID: <23700148.post@talk.nabble.com>


Hi Spencer,

That's a hard question to answer - it is pretty open-ended.  It depends what
you are trying to get out of it, and there are big differences between the
different professional packages.  Bloomberg (and other professional
platforms) generally gives you either more depth of data, or more ability to
manipulate data, or more consistently organised data.

If you just want historical closes, Yahoo will give you reasonable data: not
perfect, but even a professional data package will have some data integrity
issues.  If you want to be able to manipulate that data within the package
(e.g. graph a spread between two instruments) then clearly Yahoo cannot do
that.

If you want to see intraday charts, but do nothing else, Yahoo will enable
you to see the chart, but not get the data in a table form, or manipulate. 
A professional package will often (depending on which one) allow you to do
that.

And at the low end of the professional packages, you may well find that
Yahoo (and the internet in general) is at least as good.  I have seen one
paid but cheap package that was inferior to Yahoo in all but one or two
respects.

Guy


spencerg wrote:
> 
> Hi, Guy: 
> 
>       Thanks very much.  That's helpful. 
> 
>       The Wikipedia article on the "Bloomberg Terminal" 
> (http://en.wikipedia.org/wiki/Bloomberg_Terminal) helped me understand 
> why people pay for that service, but I still have another question: 
> 
>       Do you get the same data from Bloomberg as, e.g., Yahoo?  Or does 
> Bloomberg generally provide better quality or more comprehensive data? 
> 
>       Thanks again. 
>       Spencer Graves
> 
> gug wrote:
>> Data providers (Bloomberg, Reuters, etc - even Yahoo) generally give you
>> historical and delayed (e.g. 15 minutes late) data as part of your basic
>> package.  However if you want real-time prices, you have to pay for them. 
>> Those fees go to the exchange - they are exchange by exchange.
>>
>> Guy
>>
>>
>> spencerg wrote:
>>   
>>> Hi, Cedrick: 
>>>
>>>       Thanks for the info. 
>>>
>>>       What do you mean by "exchanges and other feeds"? 
>>>
>>>       Spencer Graves
> 
> 

-- 
View this message in context: http://www.nabble.com/intraday-data-for-VIX--tp23681090p23700148.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From cedrick at cedrickjohnson.com  Mon May 25 13:48:41 2009
From: cedrick at cedrickjohnson.com (Cedrick Johnson)
Date: Mon, 25 May 2009 07:48:41 -0400
Subject: [R-SIG-Finance] RBloomberg WAS: [R-sig-finance] intraday data
 for VIX?
In-Reply-To: <23700148.post@talk.nabble.com>
References: <b1f16d9d0905222024h518ce642h76ab4c49b1588f76@mail.gmail.com>	<4A181906.5000202@cedrickjohnson.com>	<b1f16d9d0905231015j7fa29e5eh428d9660e7d5b96e@mail.gmail.com>	<4A199E88.5070503@prodsyse.com>	<4A199F28.4000102@cedrickjohnson.com>	<4A19C595.9070605@prodsyse.com>
	<23699509.post@talk.nabble.com>	<4A19EE75.6000504@prodsyse.com>
	<23700148.post@talk.nabble.com>
Message-ID: <4A1A8599.5000206@cedrickjohnson.com>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090525/b377726f/attachment.html>

From Martin.Prins at ingim.com  Mon May 25 17:07:24 2009
From: Martin.Prins at ingim.com (Martin.Prins at ingim.com)
Date: Mon, 25 May 2009 17:07:24 +0200
Subject: [R-SIG-Finance] Rquantlib discount curve
Message-ID: <BC679B03BACABB4C9045E3C5E0CB2AC303518E26@ing.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090525/66ec0e01/attachment.pl>

From brian at braverock.com  Mon May 25 17:20:38 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 25 May 2009 10:20:38 -0500
Subject: [R-SIG-Finance] Rquantlib discount curve
In-Reply-To: <BC679B03BACABB4C9045E3C5E0CB2AC303518E26@ing.com>
References: <BC679B03BACABB4C9045E3C5E0CB2AC303518E26@ing.com>
Message-ID: <4A1AB746.9090907@braverock.com>

Martin.Prins at ingim.com wrote:
> Is it somehow possible to have the function "DiscountCurve" accept the
> 50-year and 100-year point as well?
>   
As there are no 50 and 100 year bonds, doesn't this seem like worthless 
and even perhaps dangerous extrapolation on not enough data?

Regards,

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From cedrick at cedrickjohnson.com  Mon May 25 17:38:30 2009
From: cedrick at cedrickjohnson.com (Cedrick Johnson)
Date: Mon, 25 May 2009 11:38:30 -0400
Subject: [R-SIG-Finance] Rquantlib discount curve
In-Reply-To: <4A1AB746.9090907@braverock.com>
References: <BC679B03BACABB4C9045E3C5E0CB2AC303518E26@ing.com>
	<4A1AB746.9090907@braverock.com>
Message-ID: <4A1ABB76.3000003@cedrickjohnson.com>

That's not *entirely* true, there are some corporates out there...

Take a look at GS Cap II (5.793 cpn), maturing 12/31/2099
KO (7 cpn) 5/15/2098
Citi (6 7/8 cpn) 2/15/2098
just to name a few...

As to the original poster's question, that I do not have the answer to..

-c



Brian G. Peterson wrote:
> Martin.Prins at ingim.com wrote:
>> Is it somehow possible to have the function "DiscountCurve" accept the
>> 50-year and 100-year point as well?
>>   
> As there are no 50 and 100 year bonds, doesn't this seem like 
> worthless and even perhaps dangerous extrapolation on not enough data?
>
> Regards,
>
>  - Brian
>


From knguyen at cs.umb.edu  Mon May 25 17:54:33 2009
From: knguyen at cs.umb.edu (Khanh Nguyen)
Date: Mon, 25 May 2009 11:54:33 -0400
Subject: [R-SIG-Finance] Rquantlib discount curve
In-Reply-To: <4A1ABB76.3000003@cedrickjohnson.com>
References: <BC679B03BACABB4C9045E3C5E0CB2AC303518E26@ing.com>
	<4A1AB746.9090907@braverock.com> <4A1ABB76.3000003@cedrickjohnson.com>
Message-ID: <2871c9e10905250854n5b8638abm71cc22cb9abf64b5@mail.gmail.com>

What is the error that you got? Could it be that QuantLib doesn't like
50-year and 100-year points?

-k
On Mon, May 25, 2009 at 11:38 AM, Cedrick Johnson
<cedrick at cedrickjohnson.com> wrote:
> That's not *entirely* true, there are some corporates out there...
>
> Take a look at GS Cap II (5.793 cpn), maturing 12/31/2099
> KO (7 cpn) 5/15/2098
> Citi (6 7/8 cpn) 2/15/2098
> just to name a few...
>
> As to the original poster's question, that I do not have the answer to..
>
> -c
>
>
>
> Brian G. Peterson wrote:
>>
>> Martin.Prins at ingim.com wrote:
>>>
>>> Is it somehow possible to have the function "DiscountCurve" accept the
>>> 50-year and 100-year point as well?
>>>
>>
>> As there are no 50 and 100 year bonds, doesn't this seem like worthless
>> and even perhaps dangerous extrapolation on not enough data?
>>
>> Regards,
>>
>> ?- Brian
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From heshriti at gmail.com  Tue May 26 05:33:02 2009
From: heshriti at gmail.com (Mahesh Krishnan)
Date: Tue, 26 May 2009 03:33:02 +0000
Subject: [R-SIG-Finance] quantmod chart parameters
Message-ID: <33f6aa6f0905252033s1cbdf76fs1ed8d8de7825e181@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090526/434246e3/attachment.pl>

From nands31 at gmail.com  Tue May 26 21:16:36 2009
From: nands31 at gmail.com (Subhrangshu Nandi)
Date: Tue, 26 May 2009 14:16:36 -0500
Subject: [R-SIG-Finance] Rglpk_solve_LP
Message-ID: <de69a2b90905261216g5249d6cfn4fa025d7ac95a769@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090526/dce22119/attachment.pl>

From ymoser at gmail.com  Wed May 27 07:54:10 2009
From: ymoser at gmail.com (Yaakov Moser)
Date: Wed, 27 May 2009 08:54:10 +0300
Subject: [R-SIG-Finance] fPortfolio - Maximum Return Portfolio
Message-ID: <4A1CD582.40802@gmail.com>

Can anyone suggest a simple way to find the maximum return portfolio on 
an efficient frontier with fPortfolio?

Without constraints, this is simply the asset with the highest return.
However, with constraints, it needs to be solved.

The only option I have come up with so far is to use the 
portfolioFrontier function (ideally with a large number of points), and 
then take the end one.
However, this point varies depending on how many points were selected in 
the Spec...

As far as I can tell, there is no built in functionality equivalent to 
the minriskPortfolio.

Thanks

Yaakov


From wuertz at itp.phys.ethz.ch  Wed May 27 08:31:46 2009
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Wed, 27 May 2009 08:31:46 +0200
Subject: [R-SIG-Finance] fPortfolio - Maximum Return Portfolio
In-Reply-To: <4A1CD582.40802@gmail.com>
References: <4A1CD582.40802@gmail.com>
Message-ID: <4A1CDE52.60106@itp.phys.ethz.ch>

Yaakov Moser wrote:
> Can anyone suggest a simple way to find the maximum return portfolio 
> on an efficient frontier with fPortfolio?
>
> Without constraints, this is simply the asset with the highest return.
> However, with constraints, it needs to be solved.
>
> The only option I have come up with so far is to use the 
> portfolioFrontier function (ideally with a large number of points), 
> and then take the end one.
That's true.

It is quite difficult to find the end of the frontier with constraints. 
To find the endpoint of the frontier, one has to go along the frontier 
with the function portfolioFrontier which can even fail if the constraints
are to restrictive, since then no solution can be found by the solver.

Maybe a nested interval solver can help:  take the return of the minimum 
global risk portfolio, the return of the endpoint of the (unconstrained) 
frontier, and the point of the return in between. When the intermediate 
point exists go up the frontier, otherwise go down the frontier, repeat 
this until you have the desired precision. Use the R function try() to 
find out if the intermediate point fails or not.
After a few steps one should have reached the endpoint of the frontier.

Has anybody a better and/or faster (more efficient) algorithmic (maybe 
analytic) solution to find the frontier endpoint under constraints?

There may be another problem which I often observed, that the solver may 
become unstable close to the endpoint of the constrained frontier.
Has anybody a good argument why this may happen and how to circumvent this?


Diethelm


> However, this point varies depending on how many points were selected 
> in the Spec...
>
> As far as I can tell, there is no built in functionality equivalent to 
> the minriskPortfolio.
That is true, a first implemetation can be done easily along the recipe 
given above.
>
> Thanks
>
> Yaakov
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From wuertz at itp.phys.ethz.ch  Wed May 27 09:05:42 2009
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Wed, 27 May 2009 09:05:42 +0200
Subject: [R-SIG-Finance] fPortfolio - Maximum Return Portfolio
In-Reply-To: <4A1CD582.40802@gmail.com>
References: <4A1CD582.40802@gmail.com>
Message-ID: <4A1CE646.2020900@itp.phys.ethz.ch>

Yaakov Moser wrote:

There may be a faster solution (compared to my previous email), just 
look for the portfolio with the highest risk, i.e. the lowest negative 
risk. That can be easily implemented
by reversion of the sign of the objective risk function in the portfolio 
optimization.
> Can anyone suggest a simple way to find the maximum return portfolio 
> on an efficient frontier with fPortfolio?
>
> Without constraints, this is simply the asset with the highest return.
> However, with constraints, it needs to be solved.
>
> The only option I have come up with so far is to use the 
> portfolioFrontier function (ideally with a large number of points), 
> and then take the end one.
> However, this point varies depending on how many points were selected 
> in the Spec...
>
> As far as I can tell, there is no built in functionality equivalent to 
> the minriskPortfolio.
>
> Thanks
>
> Yaakov
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ymoser at gmail.com  Wed May 27 11:28:02 2009
From: ymoser at gmail.com (Yaakov Moser)
Date: Wed, 27 May 2009 12:28:02 +0300
Subject: [R-SIG-Finance] fPortfolio - Maximum Return Portfolio
In-Reply-To: <4A1CE646.2020900@itp.phys.ethz.ch>
References: <4A1CD582.40802@gmail.com> <4A1CE646.2020900@itp.phys.ethz.ch>
Message-ID: <4A1D07A2.7000608@gmail.com>

I tried reversing the sign by redefining a maxriskPortfolio function 
based on the minriskPortfolio as you suggested.

I changed the one line to be:

targetRisk = -ans$objective


The function ran - and found something close to the maxriskPortfolio, 
but it is not the end of the efficient frontier...

My constraints were long-only, so it should have been 100% in one asset, 
but it turned out to be 99.54% only, with the rest elsewhere.


Any suggestions?


See sample program below.


Thanks


Yaakov


library(fPortfolio)
Data=SMALLCAP.RET
Data=Data[,c(1:3)]
Spec=portfolioSpec()
constraints="long-only"
maxriskPortfolio <- function (data, spec = portfolioSpec(), constraints 
= "LongOnly")
{
    Data = portfolioData(data, spec)
    data <- getSeries(Data)
    targetRiskFun <- function(x, data, spec, constraints) {
        setTargetReturn(spec) = x[1]
        Solver = match.fun(getSolver(spec))
        ans = Solver(data, spec, constraints)
        targetRisk = -ans$objective
        attr(targetRisk, "weights") <- ans$weights
        attr(targetRisk, "status") <- ans$status
        return(targetRisk)
    }
    portfolio <- optimize(targetRiskFun, interval = range(getMu(Data)),
        data = Data, spec = spec, constraints = constraints)
    STATUS = attr(portfolio$objective, "status")
    if (STATUS != 0) {
        cat("\nExecution stopped:")
        cat("\n  The minimum risk portfolio could not be computed.")
        cat("\nPossible Reason:")
        cat("\n  Your portfolio constraints may be too restrictive.")
        cat("\nStatus Information:")
        cat("\n  status=", STATUS, " from solver ", getSolver(spec),
            ".", sep = "")
        cat("\n")
        stop(call. = FALSE, show.error.messages = "\n  returned from 
Rmetrics")
    }
    setWeights(spec) <- attr(portfolio$objective, "weights")
    setStatus(spec) <- attr(portfolio$objective, "status")
    portfolio = feasiblePortfolio(data, spec, constraints)
    portfolio at call = match.call()
    portfolio at title = "Maximum Risk Portfolio"
    portfolio
}
minriskPortfolio(Data,Spec,constraints)
maxriskPortfolio(Data,Spec,constraints)
portfolioFrontier(Data,Spec,constraints)





-------- Original Message --------
Subject: Re: [R-SIG-Finance] fPortfolio - Maximum Return Portfolio
From: Diethelm Wuertz <wuertz at itp.phys.ethz.ch>
To: Yaakov Moser <ymoser at gmail.com>
CC: r-sig-finance at stat.math.ethz.ch
Date: 27 May 2009 10:05:42

> Yaakov Moser wrote:
>
> There may be a faster solution (compared to my previous email), just 
> look for the portfolio with the highest risk, i.e. the lowest negative 
> risk. That can be easily implemented
> by reversion of the sign of the objective risk function in the 
> portfolio optimization.
>> Can anyone suggest a simple way to find the maximum return portfolio 
>> on an efficient frontier with fPortfolio?
>>
>> Without constraints, this is simply the asset with the highest return.
>> However, with constraints, it needs to be solved.
>>
>> The only option I have come up with so far is to use the 
>> portfolioFrontier function (ideally with a large number of points), 
>> and then take the end one.
>> However, this point varies depending on how many points were selected 
>> in the Spec...
>>
>> As far as I can tell, there is no built in functionality equivalent 
>> to the minriskPortfolio.
>>
>> Thanks
>>
>> Yaakov
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>


From guy.yollin at rotellacapital.com  Wed May 27 16:47:48 2009
From: guy.yollin at rotellacapital.com (Guy Yollin)
Date: Wed, 27 May 2009 09:47:48 -0500
Subject: [R-SIG-Finance] Rglpk_solve_LP
In-Reply-To: <de69a2b90905261216g5249d6cfn4fa025d7ac95a769@mail.gmail.com>
References: <de69a2b90905261216g5249d6cfn4fa025d7ac95a769@mail.gmail.com>
Message-ID: <E4259A82356E7F46B4F911FB27B0D725170A4116D2@AUSP01VMBX02.collaborationhost.net>

Nandi,

Rglpk_solve_LP is simply a general-purpose linear program solver.

Some portfolio optimization problems like conditional value-at-risk (CVaR) optimization or mean absolute deviation (MAD) optimization can be formulated as linear programs and solved using an LP solver like Rglpk_solve_LP.

To understand how this is done I would highly recommend texts like:
  Introduction to Modern Portfolio Optimization with NuOPT, Bernd Scherer, Doug Martin, 2007
  Portfolio Construction and Risk Budgeting, Bernd Scherer, 2007

In these types of optimization problems, the entire returns matrix (which intrinsically includes all of the information regarding covariance) is typically passed to linear programming solver.

Best,

Guy


  


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Subhrangshu Nandi
Sent: Tuesday, May 26, 2009 12:17 PM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Rglpk_solve_LP

When you are trying to solve a portfolio optimization problem using the
package Rglpk_solve_LP, how do you put in the covariance matrix of the
assets? From some of the examples that are available online, it looks like
it can only be used in portfolios where the assets are uncorrelated. I may
be incorrectly interpreting the features of the package. Any thoughts?

Thanks,
-Nandi.

-- 
I'm a great believer in luck, and I find the harder I work the more I have
of it.  ~Thomas Jefferson

Subhrangshu Nandi
High Frequency Trading
Greater Chicago Area
Office:(312) 601-8096
EFax: (703) 852-7405

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From briankim19 at yahoo.com  Wed May 27 17:09:53 2009
From: briankim19 at yahoo.com (B Kim)
Date: Wed, 27 May 2009 08:09:53 -0700 (PDT)
Subject: [R-SIG-Finance] xts and TTR problems
Message-ID: <523985.98760.qm@web62505.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090527/a201f7c5/attachment.pl>

From wuertz at itp.phys.ethz.ch  Thu May 28 00:20:37 2009
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Thu, 28 May 2009 00:20:37 +0200
Subject: [R-SIG-Finance] fPortfolio - Maximum Return Portfolio
In-Reply-To: <4A1D07A2.7000608@gmail.com>
References: <4A1CD582.40802@gmail.com> <4A1CE646.2020900@itp.phys.ethz.ch>
	<4A1D07A2.7000608@gmail.com>
Message-ID: <4A1DBCB5.5000004@itp.phys.ethz.ch>

Yaakov Moser wrote:

This works


maxriskPortfolio <-
function (data, spec = portfolioSpec(), constraints = "LongOnly")
{
   Data = portfolioData(data, spec)
   data <- getSeries(Data)
   targetRiskFun <- function(x, data, spec, constraints) {
       setTargetReturn(spec) = x[1]
       Solver = match.fun(getSolver(spec))
       ans = Solver(data, spec, constraints)
       # DW:
       # Take care that the status ans$status is always 0
       #  If the solver fails set the value of the risk to the global
       #    min risk portfolio!
       # Use the function try() that the calculation does not break!
       targetRisk = -ans$objective
       attr(targetRisk, "weights") <- ans$weights
       attr(targetRisk, "status") <- ans$status
       return(targetRisk)
   }
  
   # DW:
   # Take care that the interval range may be large enough if short selling
   # is allowed, that requires an adaption of the range!
   # DW:
   # Increase the tolerance to be sure that optimize has converged!
   portfolio <- optimize(targetRiskFun, interval = range(getMu(Data)),
       data = Data, spec = spec, constraints = constraints,
       tol = .Machine$double.eps^0.5)
     
   setWeights(spec) <- attr(portfolio$objective, "weights")
   setStatus(spec) <- attr(portfolio$objective, "status")
   portfolio = feasiblePortfolio(data, spec, constraints)
   portfolio at call = match.call()
   portfolio at title = "Maximum Risk Portfolio"
   portfolio
}

maxriskPortfolio(SMALLCAP.RET[, 1:3])


-d
> I tried reversing the sign by redefining a maxriskPortfolio function 
> based on the minriskPortfolio as you suggested.
>
> I changed the one line to be:
>
> targetRisk = -ans$objective
>
>
> The function ran - and found something close to the maxriskPortfolio, 
> but it is not the end of the efficient frontier...
>
> My constraints were long-only, so it should have been 100% in one 
> asset, but it turned out to be 99.54% only, with the rest elsewhere.
>
>
> Any suggestions?
>
>
> See sample program below.
>
>
> Thanks
>
>
> Yaakov
>
>
> library(fPortfolio)
> Data=SMALLCAP.RET
> Data=Data[,c(1:3)]
> Spec=portfolioSpec()
> constraints="long-only"
> maxriskPortfolio <- function (data, spec = portfolioSpec(), 
> constraints = "LongOnly")
> {
>    Data = portfolioData(data, spec)
>    data <- getSeries(Data)
>    targetRiskFun <- function(x, data, spec, constraints) {
>        setTargetReturn(spec) = x[1]
>        Solver = match.fun(getSolver(spec))
>        ans = Solver(data, spec, constraints)
>        targetRisk = -ans$objective
>        attr(targetRisk, "weights") <- ans$weights
>        attr(targetRisk, "status") <- ans$status
>        return(targetRisk)
>    }
>    portfolio <- optimize(targetRiskFun, interval = range(getMu(Data)),
>        data = Data, spec = spec, constraints = constraints)
>    STATUS = attr(portfolio$objective, "status")
>    if (STATUS != 0) {
>        cat("\nExecution stopped:")
>        cat("\n  The minimum risk portfolio could not be computed.")
>        cat("\nPossible Reason:")
>        cat("\n  Your portfolio constraints may be too restrictive.")
>        cat("\nStatus Information:")
>        cat("\n  status=", STATUS, " from solver ", getSolver(spec),
>            ".", sep = "")
>        cat("\n")
>        stop(call. = FALSE, show.error.messages = "\n  returned from 
> Rmetrics")
>    }
>    setWeights(spec) <- attr(portfolio$objective, "weights")
>    setStatus(spec) <- attr(portfolio$objective, "status")
>    portfolio = feasiblePortfolio(data, spec, constraints)
>    portfolio at call = match.call()
>    portfolio at title = "Maximum Risk Portfolio"
>    portfolio
> }
> minriskPortfolio(Data,Spec,constraints)
> maxriskPortfolio(Data,Spec,constraints)
> portfolioFrontier(Data,Spec,constraints)
>
>
>
>
>
> -------- Original Message --------
> Subject: Re: [R-SIG-Finance] fPortfolio - Maximum Return Portfolio
> From: Diethelm Wuertz <wuertz at itp.phys.ethz.ch>
> To: Yaakov Moser <ymoser at gmail.com>
> CC: r-sig-finance at stat.math.ethz.ch
> Date: 27 May 2009 10:05:42
>
>> Yaakov Moser wrote:
>>
>> There may be a faster solution (compared to my previous email), just 
>> look for the portfolio with the highest risk, i.e. the lowest 
>> negative risk. That can be easily implemented
>> by reversion of the sign of the objective risk function in the 
>> portfolio optimization.
>>> Can anyone suggest a simple way to find the maximum return portfolio 
>>> on an efficient frontier with fPortfolio?
>>>
>>> Without constraints, this is simply the asset with the highest return.
>>> However, with constraints, it needs to be solved.
>>>
>>> The only option I have come up with so far is to use the 
>>> portfolioFrontier function (ideally with a large number of points), 
>>> and then take the end one.
>>> However, this point varies depending on how many points were 
>>> selected in the Spec...
>>>
>>> As far as I can tell, there is no built in functionality equivalent 
>>> to the minriskPortfolio.
>>>
>>> Thanks
>>>
>>> Yaakov
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>>
>


From vsdimitrov at yahoo.com  Thu May 28 15:52:31 2009
From: vsdimitrov at yahoo.com (Valentin Dimitrov)
Date: Thu, 28 May 2009 13:52:31 +0000 (GMT)
Subject: [R-SIG-Finance] fArma- Prediction for ARFIMA not yet implemented?
Message-ID: <119260.94684.qm@web23703.mail.ird.yahoo.com>


Dear all,

does anyone know when will prediction be implemented in R for Arfima Models? For instance in the Ox software it is already implemented (package ARFIMA 1.04), unfortunately the old function in R arfimaOxFit is not supported from fArma any more. I have older versions of R too (2.6.1) with the arfimaOxFit supported, unfortunately I do not have the files arfimaOxFit.ox and ArfimaOxPredict.ox, which are needed for arfimaOxfit. 

Thus, can anyone send to me the arfimaOxFit.ox and ArfimaOxPredict.ox files, or, alternatively give me some clues as of how to make predictions in R with an ARFIMA model?

Best regards,
Valentin





From patrick at burns-stat.com  Thu May 28 17:03:03 2009
From: patrick at burns-stat.com (Patrick Burns)
Date: Thu, 28 May 2009 16:03:03 +0100
Subject: [R-SIG-Finance] R training in London (and a talk)
Message-ID: <4A1EA7A7.8060405@burns-stat.com>


On 2009 July 13-14 in London
Patrick Burns will present a training course

"Statistical Programming in Finance with R"
See http://www.burns-stat.com/pages/training.html
for more details.


On 2009 June 03 in London
Patrick Burns will give a talk and demonstration

"Using Random Portfolios with R"
See http://www.burns-stat.com/pages/finance.html#events
for more details.



Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com


From josh.m.ulrich at gmail.com  Thu May 28 17:18:39 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Thu, 28 May 2009 10:18:39 -0500
Subject: [R-SIG-Finance] xts and TTR problems
In-Reply-To: <523985.98760.qm@web62505.mail.re1.yahoo.com>
References: <523985.98760.qm@web62505.mail.re1.yahoo.com>
Message-ID: <8cca69990905280818y1fa73d42ha33566eb0fd5b1c0@mail.gmail.com>

Hi Brian,

To summarize our off-list discussion for the benefit of the list:

Your first issue had been reported about a week ago and has been fixed
on the r-forge version of xts.  Your second issue has been corrected
in xts_0.6-5 on CRAN.

Please let us know if you still have any issues.

Best,
Josh
--
http://www.fosstrading.com




On Wed, May 27, 2009 at 10:09 AM, B Kim <briankim19 at yahoo.com> wrote:
> Hi,
>
> I'm having some problems using the xts and TTR packages. ?After running the code below (contents of TY1.txt are at the end of the email), I have two questions:
>
> 1) daily.xts has no timestamps but when I pass it in to RSI(), the return object is timestamped. ?why does this occur?
> 2) why does the object returned from ATR() contain multiple rows with the same date where duplicate date rows contain NA's?
>
> The ultimate goal of this code snippet is to run various indicators, cbind them, and write them to a file for further processing.
>
> thanks,
> Brian
>
> code:
> library(quantmod);
> library(TTR);
>
> TY1.mat = read.table(paste("TY1.txt", sep = ""), header = T);
> TY1.mat[,"date"] = as.Date(TY1.mat[,"date"], format = "%Y/%m/%d");
> TY1.mat[,"datetime"] = paste(TY1.mat[,"date"], TY1.mat[,"time"], sep = " ");
>
> TY1.xts = xts(TY1.mat[,"TY1"], as.POSIXct(TY1.mat[,"datetime"]));
>
> daily.xts = to.daily(TY1.xts);
>
> myrsi = RSI(daily.xts[,"TY1.xts.Close"]);
> myatr = ATR(daily.xts[,c("TY1.xts.High", "TY1.xts.Low", "TY1.xts.Close")]);
>
> file.data = as.matrix(cbind(my.xts, myrsi, myatr));
> write.table(file = "TY1.data", append = F, sep = "\t", file.data);
>
>
> TY1.txt contains the following data:
> date ? ?time ? ?TY1
> 2009/01/02 ? ? ? ? ? ? ? ? ? ? ?08:00:00 ? ? ? ? ? ? ? ? ? ? ? ?126.328125
> 2009/01/02 ? ? ? ? ? ? ? ? ? ? ?12:00:00 ? ? ? ? ? ? ? ? ? ? ? ?126.765625
> 2009/01/02 ? ? ? ? ? ? ? ? ? ? ?13:00:00 ? ? ? ? ? ? ? ? ? ? ? ?124.25
> 2009/01/05 ? ? ? ? ? ? ? ? ? ? ?08:00:00 ? ? ? ? ? ? ? ? ? ? ? ?124.5
> 2009/01/05 ? ? ? ? ? ? ? ? ? ? ?12:00:00 ? ? ? ? ? ? ? ? ? ? ? ?124.78125
> 2009/01/05 ? ? ? ? ? ? ? ? ? ? ?13:00:00 ? ? ? ? ? ? ? ? ? ? ? ?123.625
> 2009/01/06 ? ? ? ? ? ? ? ? ? ? ?08:00:00 ? ? ? ? ? ? ? ? ? ? ? ?124.359375
> 2009/01/06 ? ? ? ? ? ? ? ? ? ? ?12:00:00 ? ? ? ? ? ? ? ? ? ? ? ?124.90625
> 2009/01/06 ? ? ? ? ? ? ? ? ? ? ?13:00:00 ? ? ? ? ? ? ? ? ? ? ? ?123.28125
> 2009/01/07 ? ? ? ? ? ? ? ? ? ? ?08:00:00 ? ? ? ? ? ? ? ? ? ? ? ?124.78125
> 2009/01/07 ? ? ? ? ? ? ? ? ? ? ?12:00:00 ? ? ? ? ? ? ? ? ? ? ? ?124.875
> 2009/01/07 ? ? ? ? ? ? ? ? ? ? ?13:00:00 ? ? ? ? ? ? ? ? ? ? ? ?124.015625
> 2009/01/08 ? ? ? ? ? ? ? ? ? ? ?08:00:00 ? ? ? ? ? ? ? ? ? ? ? ?124.328125
> 2009/01/08 ? ? ? ? ? ? ? ? ? ? ?12:00:00 ? ? ? ? ? ? ? ? ? ? ? ?125.6875
> 2009/01/08 ? ? ? ? ? ? ? ? ? ? ?13:00:00 ? ? ? ? ? ? ? ? ? ? ? ?124.265625
> 2009/01/09 ? ? ? ? ? ? ? ? ? ? ?08:00:00 ? ? ? ? ? ? ? ? ? ? ? ?125.203125
> 2009/01/09 ? ? ? ? ? ? ? ? ? ? ?12:00:00 ? ? ? ? ? ? ? ? ? ? ? ?126.34375
> 2009/01/09 ? ? ? ? ? ? ? ? ? ? ?13:00:00 ? ? ? ? ? ? ? ? ? ? ? ?124.875
> 2009/01/12 ? ? ? ? ? ? ? ? ? ? ?08:00:00 ? ? ? ? ? ? ? ? ? ? ? ?125.625
> 2009/01/12 ? ? ? ? ? ? ? ? ? ? ?12:00:00 ? ? ? ? ? ? ? ? ? ? ? ?126.359375
> 2009/01/12 ? ? ? ? ? ? ? ? ? ? ?13:00:00 ? ? ? ? ? ? ? ? ? ? ? ?125.109375
> 2009/01/13 ? ? ? ? ? ? ? ? ? ? ?08:00:00 ? ? ? ? ? ? ? ? ? ? ? ?126.3125
> 2009/01/13 ? ? ? ? ? ? ? ? ? ? ?12:00:00 ? ? ? ? ? ? ? ? ? ? ? ?126.359375
> 2009/01/13 ? ? ? ? ? ? ? ? ? ? ?13:00:00 ? ? ? ? ? ? ? ? ? ? ? ?125.75
> 2009/01/14 ? ? ? ? ? ? ? ? ? ? ?08:00:00 ? ? ? ? ? ? ? ? ? ? ? ?126.28125
> 2009/01/14 ? ? ? ? ? ? ? ? ? ? ?12:00:00 ? ? ? ? ? ? ? ? ? ? ? ?127.484375
> 2009/01/14 ? ? ? ? ? ? ? ? ? ? ?13:00:00 ? ? ? ? ? ? ? ? ? ? ? ?125.8125
> 2009/01/15 ? ? ? ? ? ? ? ? ? ? ?08:00:00 ? ? ? ? ? ? ? ? ? ? ? ?127.125
> 2009/01/15 ? ? ? ? ? ? ? ? ? ? ?12:00:00 ? ? ? ? ? ? ? ? ? ? ? ?127.515625
> 2009/01/15 ? ? ? ? ? ? ? ? ? ? ?13:00:00 ? ? ? ? ? ? ? ? ? ? ? ?126.71875
> 2009/01/16 ? ? ? ? ? ? ? ? ? ? ?08:00:00 ? ? ? ? ? ? ? ? ? ? ? ?126.859375
> 2009/01/16 ? ? ? ? ? ? ? ? ? ? ?12:00:00 ? ? ? ? ? ? ? ? ? ? ? ?126.875
> 2009/01/16 ? ? ? ? ? ? ? ? ? ? ?13:00:00 ? ? ? ? ? ? ? ? ? ? ? ?125.40625
> 2009/01/20 ? ? ? ? ? ? ? ? ? ? ?08:00:00 ? ? ? ? ? ? ? ? ? ? ? ?125.828125
> 2009/01/20 ? ? ? ? ? ? ? ? ? ? ?12:00:00 ? ? ? ? ? ? ? ? ? ? ? ?126.421875
> 2009/01/20 ? ? ? ? ? ? ? ? ? ? ?13:00:00 ? ? ? ? ? ? ? ? ? ? ? ?124.5
> 2009/01/21 ? ? ? ? ? ? ? ? ? ? ?08:00:00 ? ? ? ? ? ? ? ? ? ? ? ?125.96875
> 2009/01/21 ? ? ? ? ? ? ? ? ? ? ?12:00:00 ? ? ? ? ? ? ? ? ? ? ? ?125.96875
> 2009/01/21 ? ? ? ? ? ? ? ? ? ? ?13:00:00 ? ? ? ? ? ? ? ? ? ? ? ?124.578125
> 2009/01/22 ? ? ? ? ? ? ? ? ? ? ?08:00:00 ? ? ? ? ? ? ? ? ? ? ? ?124.734375
> 2009/01/22 ? ? ? ? ? ? ? ? ? ? ?12:00:00 ? ? ? ? ? ? ? ? ? ? ? ?125.0625
> 2009/01/22 ? ? ? ? ? ? ? ? ? ? ?13:00:00 ? ? ? ? ? ? ? ? ? ? ? ?123.703125
> 2009/01/23 ? ? ? ? ? ? ? ? ? ? ?08:00:00 ? ? ? ? ? ? ? ? ? ? ? ?124.25
> 2009/01/23 ? ? ? ? ? ? ? ? ? ? ?12:00:00 ? ? ? ? ? ? ? ? ? ? ? ?124.765625
> 2009/01/23 ? ? ? ? ? ? ? ? ? ? ?13:00:00 ? ? ? ? ? ? ? ? ? ? ? ?123.328125
> 2009/01/26 ? ? ? ? ? ? ? ? ? ? ?08:00:00 ? ? ? ? ? ? ? ? ? ? ? ?123.890625
> 2009/01/26 ? ? ? ? ? ? ? ? ? ? ?12:00:00 ? ? ? ? ? ? ? ? ? ? ? ?124.109375
> 2009/01/26 ? ? ? ? ? ? ? ? ? ? ?13:00:00 ? ? ? ? ? ? ? ? ? ? ? ?123.171875
>
>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

From edd at debian.org  Thu May 28 17:31:29 2009
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 28 May 2009 10:31:29 -0500
Subject: [R-SIG-Finance] fArma- Prediction for ARFIMA not yet
	implemented?
In-Reply-To: <119260.94684.qm@web23703.mail.ird.yahoo.com>
References: <119260.94684.qm@web23703.mail.ird.yahoo.com>
Message-ID: <18974.44625.272853.719573@ron.nulle.part>


On 28 May 2009 at 13:52, Valentin Dimitrov wrote:
| Dear all,
| 
| does anyone know when will prediction be implemented in R for Arfima Models? 

Pick either a), b) or c):   

     a) Whenever you get it finished.

     b) Whenever the person you pay to implement it for you gets it finished.

     c) Whenever someone else follows a) or b) before you do.

and your selection may depend on your sense of urgency.

| For instance in the Ox software it is already implemented (package ARFIMA
| 1.04), unfortunately the old function in R arfimaOxFit is not supported
| from fArma any more. I have older versions of R too (2.6.1) with the
| arfimaOxFit supported, unfortunately I do not have the files arfimaOxFit.ox
| and ArfimaOxPredict.ox, which are needed for arfimaOxfit.  
| 
| Thus, can anyone send to me the arfimaOxFit.ox and ArfimaOxPredict.ox

Did you discover the per-package archive section on CRAN ?

| files, or, alternatively give me some clues as of how to make predictions
| in R with an ARFIMA model? 

Maybe a starting point studying the predict methods for standard ARIMA models.

Dirk

-- 
Three out of two people have difficulties with fractions.


From knguyen at cs.umb.edu  Fri May 29 22:21:00 2009
From: knguyen at cs.umb.edu (Khanh Nguyen)
Date: Fri, 29 May 2009 16:21:00 -0400
Subject: [R-SIG-Finance] quantmod getSymbols
Message-ID: <2871c9e10905291321y5b7b2dcat1be1f2e14f274525@mail.gmail.com>

Hi,

I downloaded a yahoo.csv file for Yahoo from Yahoo Finance. It has this format

Date,Open,High,Low,Close,Volume,Adj Close
2009-05-28,141.65,145.29,139.29,144.65,14803100,144.65
2009-05-27,142.22,145.49,140.01,140.01,16696900,140.01
2009-05-26,134.61,142.07,134.61,142.05,13768700,142.05
2009-05-22,137.78,138.99,136.05,136.35,9058400,136.35
2009-05-21,134.05,139.45,133.92,137.16,16135500,137.16
...

when I try to read it to quantmod, I got

> getSymbols("yahoo", src="csv")
Error in `[.data.frame`(x, indx) : undefined columns selected

What am I missing? Thanks.

-k


From spencer.graves at prodsyse.com  Sat May 30 05:10:06 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Fri, 29 May 2009 20:10:06 -0700
Subject: [R-SIG-Finance] quantmod getSymbols
In-Reply-To: <2871c9e10905291321y5b7b2dcat1be1f2e14f274525@mail.gmail.com>
References: <2871c9e10905291321y5b7b2dcat1be1f2e14f274525@mail.gmail.com>
Message-ID: <4A20A38E.6030300@prodsyse.com>

      From, e.g., "http://www.google.com/finance?q=yahoo", I found that 
the NASDAQ stock symbol for Yahoo is "YHOO".  With this info, the 
following read and stored an "xts" object "YHOO": 


getSymbols("YHOO")
str(YHOO)


      Hope this helps. 
      Spencer Graves


Khanh Nguyen wrote:
> Hi,
>
> I downloaded a yahoo.csv file for Yahoo from Yahoo Finance. It has this format
>
> Date,Open,High,Low,Close,Volume,Adj Close
> 2009-05-28,141.65,145.29,139.29,144.65,14803100,144.65
> 2009-05-27,142.22,145.49,140.01,140.01,16696900,140.01
> 2009-05-26,134.61,142.07,134.61,142.05,13768700,142.05
> 2009-05-22,137.78,138.99,136.05,136.35,9058400,136.35
> 2009-05-21,134.05,139.45,133.92,137.16,16135500,137.16
> ...
>
> when I try to read it to quantmod, I got
>
>   
>> getSymbols("yahoo", src="csv")
>>     
> Error in `[.data.frame`(x, indx) : undefined columns selected
>
> What am I missing? Thanks.
>
> -k
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From dkattimu at gmail.com  Sun May 31 02:37:45 2009
From: dkattimu at gmail.com (Dodzi Attimu)
Date: Sat, 30 May 2009 20:37:45 -0400
Subject: [R-SIG-Finance] periodReturn() does not work anymore except for
	period="daily".
Message-ID: <e84a26080905301737s1d3cfe0em40c14ba375686c5f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090530/2a982ada/attachment.pl>

From spencer.graves at prodsyse.com  Sun May 31 03:08:05 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Sat, 30 May 2009 18:08:05 -0700
Subject: [R-SIG-Finance] periodReturn() does not work anymore except for
 period="daily".
In-Reply-To: <e84a26080905301737s1d3cfe0em40c14ba375686c5f@mail.gmail.com>
References: <e84a26080905301737s1d3cfe0em40c14ba375686c5f@mail.gmail.com>
Message-ID: <4A21D875.1090201@prodsyse.com>

      Please restore the previous version from CRAN, and in the future 
refer bug reports like this to the R-Forge developers.  Code on R-Forge 
is "under development", which suggests that it will contain bugs. 

      Spencer Graves

Dodzi Attimu wrote:
> Hello,
>
> periodReturn does not seem to work anymore. In fact, this example in the
> documentation below which used to work before I updated my packages from
> R-forge today now produces the result below:
>
>   
>> getSymbols('QQQQ',src='yahoo')
>>     
> [1] "QQQQ"
>   
>> allReturns(QQQQ)  # returns all periods
>>     
> Error in `colnames<-`(`*tmp*`, value = "Delt.0.arithmetic") :
>   attempt to set colnames on object with less than two dimensions
>
> Thanks
>
>


From jeff.a.ryan at gmail.com  Sun May 31 04:31:44 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Sat, 30 May 2009 21:31:44 -0500
Subject: [R-SIG-Finance] periodReturn() does not work anymore except for
	period="daily".
In-Reply-To: <4A21D875.1090201@prodsyse.com>
References: <e84a26080905301737s1d3cfe0em40c14ba375686c5f@mail.gmail.com>
	<4A21D875.1090201@prodsyse.com>
Message-ID: <e8e755250905301931m49f96f0n94ef001b03aeccbd@mail.gmail.com>

A quick follow-up.  This has been fixed on R-forge now.

Thanks for the report, though as Spencer mentioned the R-forge repos
is always under development, so your mileage may vary.

Thanks,
Jeff

On Sat, May 30, 2009 at 8:08 PM, spencerg <spencer.graves at prodsyse.com> wrote:
> ? ? Please restore the previous version from CRAN, and in the future refer
> bug reports like this to the R-Forge developers. ?Code on R-Forge is "under
> development", which suggests that it will contain bugs.
> ? ? Spencer Graves
>
> Dodzi Attimu wrote:
>>
>> Hello,
>>
>> periodReturn does not seem to work anymore. In fact, this example in the
>> documentation below which used to work before I updated my packages from
>> R-forge today now produces the result below:
>>
>>
>>>
>>> getSymbols('QQQQ',src='yahoo')
>>>
>>
>> [1] "QQQQ"
>>
>>>
>>> allReturns(QQQQ) ?# returns all periods
>>>
>>
>> Error in `colnames<-`(`*tmp*`, value = "Delt.0.arithmetic") :
>> ?attempt to set colnames on object with less than two dimensions
>>
>> Thanks
>>
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From jeff.a.ryan at gmail.com  Sun May 31 04:35:10 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Sat, 30 May 2009 21:35:10 -0500
Subject: [R-SIG-Finance] quantmod getSymbols
In-Reply-To: <2871c9e10905291321y5b7b2dcat1be1f2e14f274525@mail.gmail.com>
References: <2871c9e10905291321y5b7b2dcat1be1f2e14f274525@mail.gmail.com>
Message-ID: <e8e755250905301935k171f4e42h9d294acbd5623103@mail.gmail.com>

Khanh,

To follow up to the list with the solution.

This is a (bad) result of ordering an xts object that is read in in
non-ascending time order.

xts has been patched on R-forge now to properly handle this.

Thanks for the report!

Jeff

On Fri, May 29, 2009 at 3:21 PM, Khanh Nguyen <knguyen at cs.umb.edu> wrote:
> Hi,
>
> I downloaded a yahoo.csv file for Yahoo from Yahoo Finance. It has this format
>
> Date,Open,High,Low,Close,Volume,Adj Close
> 2009-05-28,141.65,145.29,139.29,144.65,14803100,144.65
> 2009-05-27,142.22,145.49,140.01,140.01,16696900,140.01
> 2009-05-26,134.61,142.07,134.61,142.05,13768700,142.05
> 2009-05-22,137.78,138.99,136.05,136.35,9058400,136.35
> 2009-05-21,134.05,139.45,133.92,137.16,16135500,137.16
> ...
>
> when I try to read it to quantmod, I got
>
>> getSymbols("yahoo", src="csv")
> Error in `[.data.frame`(x, indx) : undefined columns selected
>
> What am I missing? Thanks.
>
> -k
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From stanley.neo at gmail.com  Sun May 31 18:35:14 2009
From: stanley.neo at gmail.com (Stanley Neo)
Date: Mon, 1 Jun 2009 00:35:14 +0800
Subject: [R-SIG-Finance]  TTR Stochastics function - internal smoothing
Message-ID: <e4d27dad0905310935y2c89d250k80a5c0085ca67c05@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090601/d0522f1f/attachment.pl>

From josh.m.ulrich at gmail.com  Mon Jun  1 02:29:49 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Sun, 31 May 2009 19:29:49 -0500
Subject: [R-SIG-Finance] TTR Stochastics function - internal smoothing
In-Reply-To: <e4d27dad0905310935y2c89d250k80a5c0085ca67c05@mail.gmail.com>
References: <e4d27dad0905310935y2c89d250k80a5c0085ca67c05@mail.gmail.com>
Message-ID: <8cca69990905311729x3a5d937ek204ee5e8bd6472fd@mail.gmail.com>

Stanley,

Thanks for pointing this out.

On Sun, May 31, 2009 at 11:35 AM, Stanley Neo <stanley.neo at gmail.com> wrote:
> just want to highlight,
>
> For Stochastics implementation on most trading platforms, besides apply
> moving on K to get a smooth K (aka Slow K) there is an option for internal
> smoothing of %K. Ths works by take a moving average of the differences (i.e.
> C-LowestL and HighestH-LowestL) before computing %K.
>
> TTR stoch function, smoothing is applied on final K values to obtain slow K
> (or fast D) and slow D. Implicitly, this means internal smoothing period =
> 1. not too sure if there will be an update to include the internal smoothing
> option for internal smoothing period > 1.
>
I wasn't planning on providing an update, but I will now that you've
brought it to my attention. :-)
How would you suggest it be implemented?  Should I do more than add a
slowK=1 argument and add slowK output?

Just to make sure I understand, slowK and fastD should be equal when
the slowK periods = 1, correct?

> this might answer some doubts on why stochastics on TTR may differ from some
> trading platforms where the default internal smoothing period is 3.
>
> rgds
> Stanley
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

Best,
Josh
--
http://www.fosstrading.com


From stanley.neo at gmail.com  Mon Jun  1 11:01:09 2009
From: stanley.neo at gmail.com (Stanley Neo)
Date: Mon, 1 Jun 2009 17:01:09 +0800
Subject: [R-SIG-Finance] TTR Stochastics function - internal smoothing
In-Reply-To: <8cca69990905311729x3a5d937ek204ee5e8bd6472fd@mail.gmail.com>
References: <e4d27dad0905310935y2c89d250k80a5c0085ca67c05@mail.gmail.com>
	<8cca69990905311729x3a5d937ek204ee5e8bd6472fd@mail.gmail.com>
Message-ID: <e4d27dad0906010201i4ba66fcarc577b4884b0e3077@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090601/93c88218/attachment.pl>

From bogaso.christofer at gmail.com  Mon Jun  1 15:32:27 2009
From: bogaso.christofer at gmail.com (Bogaso)
Date: Mon, 1 Jun 2009 06:32:27 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Newbie question on risk free
	Interest Rate
In-Reply-To: <4a15c704.0aaa660a.7004.ffff84ce@mx.google.com>
References: <4a15c704.0aaa660a.7004.ffff84ce@mx.google.com>
Message-ID: <23815043.post@talk.nabble.com>


any view pls?


Bogaso wrote:
> 
> Hi, I have come across one more question. I understood that for BS options
> pricing, I should take short rate i.e. overnight rate because BS derive
> option price through some replicating portfolio which is changed
> instantaneously. However if I price an instrument using Black formula,
> wherein only the distribution of underlying at maturity period is
> considered
> i.e. in this case there is no replicating portfolio story, shouldn't I
> consider risk free rate for longer horizon i.e. a rate whose maturity
> period
> exactly matches with the life of the instrument?
> 
> I mean to say, under Black's framework, one only needs to calculate
> expected
> value of the instrument like E[max(0, S[T] - K)] at maturity and then to
> calculate the present value of that. In this case there is nothing abt
> replicating portfolio. Therefore I feel that to calculate PV I should
> consider LIBOR with maturity [o, T]. 
> 
> What you feel on that? If I am correct i.e. if I price same option using
> BS
> and Black, there must be some fundamental difference in theoretical option
> price.
> 
> -----Original Message-----
> From: glenn [mailto:g1enn.roberts at btinternet.com] 
> Sent: 29 December 2008 17:33
> To: bogaso.christofer
> Subject: Re: [R-SIG-Finance] Newbie question on risk free Interest Rate
> 
> Further to Mahesh's answer Christofer, think of it like this;
> 
> The rate in the BS calculation represents a rate that any portfolio
> consisting of an option and the delta equivalent of the underlying (in
> your
> example a swap maybe) MUST earn. Think about how long the portfolio will
> remain delta neutral (risk free) for before a re-balence is needed. That's
> the rate you want i.e the short rate.
> 
> Glenn
> 
> 
> On 28/12/2008 21:53, "bogaso.christofer" <bogaso.christofer at gmail.com>
> wrote:
> 
>> Hi,
>> 
>>  
>> 
>> I would like to ask one newbie question on risk free interest rate. This
> is
>> the essential part to price any financial derivatives, like options,
>> Interest Rate only [IO] strip etc. My question is standing at time "t"
> which
>> risk free interest rate I should consider? 3 month, 6 month, 10 year
> t-bill
>> or t-bond ? for example suppose, I need to price a call option using BS
>> formula, whose remaining life time is 2 years and another option whose
> life
>> time is 5 months. Which interest rate I need to take to value those 2
>> options? After some goggling it is suggested to take 3 month t-bill as
> risk
>> free rate. What is the logic behind that?
>> 
>>  
>> 
>> Again suppose, an Investor is to purchase an IO strip for 7 years, on a
>> 10
>> years mortgage. In this case, I saw one book [by Cuthbertson], suggested
> to
>> take annual yield on 10-year t-bond to calculate NPV of all future
> Interest
>> payment against mortgage. However again it did not say why to take
>> 10-year
>> bond not, 3-month t-bill.
>> 
>>  
>> 
>> Can anyone here please clarify me on above doubts? Your help will be
> highly
>> appreciated.
>> 
>>  
>> 
>> Thanks and regards,
>> 
>> 
>> [[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/Re%3A-Newbie-question-on-risk-free-Interest-Rate-tp23660753p23815043.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From markleeds at verizon.net  Mon Jun  1 19:58:08 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Mon, 01 Jun 2009 12:58:08 -0500 (CDT)
Subject: [R-SIG-Finance] [R-sig-finance] Newbie question on risk
	free	Interest Rate
Message-ID: <2082074325.1256965.1243879088938.JavaMail.root@vms229.mailsrvcs.net>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090601/6ca013f9/attachment.html>

From josh.m.ulrich at gmail.com  Tue Jun  2 04:00:33 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Mon, 1 Jun 2009 21:00:33 -0500
Subject: [R-SIG-Finance] TTR Stochastics function - internal smoothing
In-Reply-To: <e4d27dad0906010201i4ba66fcarc577b4884b0e3077@mail.gmail.com>
References: <e4d27dad0905310935y2c89d250k80a5c0085ca67c05@mail.gmail.com>
	<8cca69990905311729x3a5d937ek204ee5e8bd6472fd@mail.gmail.com>
	<e4d27dad0906010201i4ba66fcarc577b4884b0e3077@mail.gmail.com>
Message-ID: <8cca69990906011900n58cb98f8ybd72fd69775f7b36@mail.gmail.com>

Stanley,

Thank you very much for this detailed explanation.  I will let you
know when TTR has been updated on r-forge.

Best,
Josh
--
http://www.fosstrading.com



On Mon, Jun 1, 2009 at 4:01 AM, Stanley Neo <stanley.neo at gmail.com> wrote:
> Hi Joshua,
>
> I would suggest the following:
> 1. an additional argument "IntSmooth" for internal smoothing period
> (probably with default as 1 so that the current users need not change any
> inputs).
> 2. applying simple moving average on the differences (i.e. C-LL and HH-LL)
> with IntSmooth value as the SMA period.
>
> The SMA of C-LL and HH-LL can then be used to compute the usual Fast K, Fast
> D, Slow D. We will not need to have an another output as in practice, the
> raw K is the Fast K, the SMA on Fast K is the Slow K (which is also known as
> the Fast D) and the SMA on Slow K is the Slow D.
>
> Therefore, if IntSmooth = 1, the SMA on differences will result in the usual
> values that the stoch currently generates. If the users desire to set a
> different value, then the stoch will have the raw K smoothed internally.
>
> In short:
> 1. SMA on Differences => internal smoothing, upon which, we compute Raw K
> 2. Raw K aka Fast K
> 3. Slow K aka Fast D = Moving Average on Raw K
> 4. Slow D = Moving Average on Slow K
>
> rgds
> Stanley
>
> 2009/6/1 Joshua Ulrich <josh.m.ulrich at gmail.com>
>>
>> Stanley,
>>
>> Thanks for pointing this out.
>>
>> On Sun, May 31, 2009 at 11:35 AM, Stanley Neo <stanley.neo at gmail.com>
>> wrote:
>> > just want to highlight,
>> >
>> > For Stochastics implementation on most trading platforms, besides apply
>> > moving on K to get a smooth K (aka Slow K) there is an option for
>> > internal
>> > smoothing of %K. Ths works by take a moving average of the differences
>> > (i.e.
>> > C-LowestL and HighestH-LowestL) before computing %K.
>> >
>> > TTR stoch function, smoothing is applied on final K values to obtain
>> > slow K
>> > (or fast D) and slow D. Implicitly, this means internal smoothing period
>> > =
>> > 1. not too sure if there will be an update to include the internal
>> > smoothing
>> > option for internal smoothing period > 1.
>> >
>> I wasn't planning on providing an update, but I will now that you've
>> brought it to my attention. :-)
>> How would you suggest it be implemented? ?Should I do more than add a
>> slowK=1 argument and add slowK output?
>>
>> Just to make sure I understand, slowK and fastD should be equal when
>> the slowK periods = 1, correct?
>>
>> > this might answer some doubts on why stochastics on TTR may differ from
>> > some
>> > trading platforms where the default internal smoothing period is 3.
>> >
>> > rgds
>> > Stanley
>> >
>> > ? ? ? ?[[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> > -- Subscriber-posting only.
>> > -- If you want to post, subscribe first.
>> >
>>
>> Best,
>> Josh
>> --
>> http://www.fosstrading.com
>


From megh700004 at yahoo.com  Tue Jun  2 08:44:03 2009
From: megh700004 at yahoo.com (megh)
Date: Mon, 1 Jun 2009 23:44:03 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Surface plot of multivariate time
	series
Message-ID: <23827284.post@talk.nabble.com>


Hi, I have following multivariate time series :
start = as.Date("01/01/05", format="%m/%d/%y")
end = as.Date("12/31/05", format="%m/%d/%y")
dates = seq(start, end, by=1)

dat = matrix(rnorm(length(dates)*4), length(dates))
dat1 = t(apply(dat, 1, function(x) x+t(c(100, 110, 120, 130))))

library(zoo)
dat2 = zoo(dat1, dates); head(dat2)

Now I want to get "surface plot" wherein x-axis is time, y-axis represents
1,2,3,4, and z-axis is for values of "dat2". Is there any R drawing device
to do that?

Regards,
-- 
View this message in context: http://www.nabble.com/Surface-plot-of-multivariate-time-series-tp23827284p23827284.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From Matthias.Koberstein at hsbctrinkaus.de  Tue Jun  2 09:03:19 2009
From: Matthias.Koberstein at hsbctrinkaus.de (Matthias.Koberstein at hsbctrinkaus.de)
Date: Tue, 2 Jun 2009 09:03:19 +0200
Subject: [R-SIG-Finance] Antwort: Re: [R-sig-finance] Newbie question on
	risk	free	Interest Rate
In-Reply-To: <2082074325.1256965.1243879088938.JavaMail.root@vms229.mailsrvcs.net>
References: <2082074325.1256965.1243879088938.JavaMail.root@vms229.mailsrvcs.net>
Message-ID: <OF32506143.7F571FE6-ONC12575C9.0024E551-C12575C9.0026C1EA@hsbctrinkaus.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090602/96bdf94d/attachment.pl>

From vsdimitrov at yahoo.com  Tue Jun  2 14:07:12 2009
From: vsdimitrov at yahoo.com (Valentin Dimitrov)
Date: Tue, 2 Jun 2009 12:07:12 +0000 (GMT)
Subject: [R-SIG-Finance] skew normal cond.dist in fGarch::garchFit
Message-ID: <82956.62760.qm@web23704.mail.ird.yahoo.com>


Dear all,

I am trying to use the skew normal distribution as conditional distribution under the garchFit function. I estimate the parameters using garchFit and the parameter governing the skewness is "skew". 

My question is: how to interpret that parameter skew? is this the shape parameter of a skew normal distribution (0 for standard normal) or is this something else (if so, what's the value of skew for the standard normal?)

Thank you in advance for your help.

Best regards,

Valentin


   

From stanley.neo at gmail.com  Tue Jun  2 15:43:35 2009
From: stanley.neo at gmail.com (Stanley Neo)
Date: Tue, 2 Jun 2009 21:43:35 +0800
Subject: [R-SIG-Finance] R-SIG-Finance Digest, Vol 61, Issue 2
In-Reply-To: <mailman.1.1243936801.12567.r-sig-finance@stat.math.ethz.ch>
References: <mailman.1.1243936801.12567.r-sig-finance@stat.math.ethz.ch>
Message-ID: <e4d27dad0906020643j703837fakaa52d0091d09cb76@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090602/366d6795/attachment.pl>

From pgilbert at bank-banque-canada.ca  Tue Jun  2 16:39:07 2009
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Tue, 02 Jun 2009 10:39:07 -0400
Subject: [R-SIG-Finance] [R-sig-finance] Surface plot of multivariate
 time	series
In-Reply-To: <23827284.post@talk.nabble.com>
References: <23827284.post@talk.nabble.com>
Message-ID: <4A25398B.70308@bank-banque-canada.ca>



megh wrote:
> Hi, I have following multivariate time series :
> start = as.Date("01/01/05", format="%m/%d/%y")
> end = as.Date("12/31/05", format="%m/%d/%y")
> dates = seq(start, end, by=1)
> 
> dat = matrix(rnorm(length(dates)*4), length(dates))
> dat1 = t(apply(dat, 1, function(x) x+t(c(100, 110, 120, 130))))
> 
> library(zoo)
> dat2 = zoo(dat1, dates); head(dat2)
> 
> Now I want to get "surface plot" wherein x-axis is time, y-axis represents
> 1,2,3,4, and z-axis is for values of "dat2". Is there any R drawing device
> to do that?

tfpersp in package tramePlus does this.

> 
> Regards,
====================================================================================

La version fran?aise suit le texte anglais.

------------------------------------------------------------------------------------

This email may contain privileged and/or confidential information, and the Bank of
Canada does not waive any related rights. Any distribution, use, or copying of this
email or the information it contains by other than the intended recipient is
unauthorized. If you received this email in error please delete it immediately from
your system and notify the sender promptly by email that you have done so. 

------------------------------------------------------------------------------------

Le pr?sent courriel peut contenir de l'information privil?gi?e ou confidentielle.
La Banque du Canada ne renonce pas aux droits qui s'y rapportent. Toute diffusion,
utilisation ou copie de ce courriel ou des renseignements qu'il contient par une
personne autre que le ou les destinataires d?sign?s est interdite. Si vous recevez
ce courriel par erreur, veuillez le supprimer imm?diatement et envoyer sans d?lai ?
l'exp?diteur un message ?lectronique pour l'aviser que vous avez ?limin? de votre
ordinateur toute copie du courriel re?u.

From megh700004 at yahoo.com  Tue Jun  2 19:25:32 2009
From: megh700004 at yahoo.com (megh)
Date: Tue, 2 Jun 2009 10:25:32 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Surface plot of multivariate
	time series
In-Reply-To: <23827284.post@talk.nabble.com>
References: <23827284.post@talk.nabble.com>
Message-ID: <23836769.post@talk.nabble.com>


It is saying that, this package is not available :

> install.packages("tramePlus")
Warning in install.packages("tramePlus") :
  argument 'lib' is missing: using
'C:\Users\Arrun's\Documents/R/win-library/2.9'
--- Please select a CRAN mirror for use in this session ---
Warning message:
In getDependencies(pkgs, dependencies, available, lib) :
  package ?tramePlus? is not available

Any better idea please?




megh wrote:
> 
> Hi, I have following multivariate time series :
> start = as.Date("01/01/05", format="%m/%d/%y")
> end = as.Date("12/31/05", format="%m/%d/%y")
> dates = seq(start, end, by=1)
> 
> dat = matrix(rnorm(length(dates)*4), length(dates))
> dat1 = t(apply(dat, 1, function(x) x+t(c(100, 110, 120, 130))))
> 
> library(zoo)
> dat2 = zoo(dat1, dates); head(dat2)
> 
> Now I want to get "surface plot" wherein x-axis is time, y-axis represents
> 1,2,3,4, and z-axis is for values of "dat2". Is there any R drawing device
> to do that?
> 
> Regards,
> 

-- 
View this message in context: http://www.nabble.com/Surface-plot-of-multivariate-time-series-tp23827284p23836769.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From shane.conway at gmail.com  Tue Jun  2 19:31:24 2009
From: shane.conway at gmail.com (Shane Conway)
Date: Tue, 2 Jun 2009 13:31:24 -0400
Subject: [R-SIG-Finance] [R-sig-finance] Surface plot of multivariate
	time series
In-Reply-To: <23836769.post@talk.nabble.com>
References: <23827284.post@talk.nabble.com> <23836769.post@talk.nabble.com>
Message-ID: <dd3243090906021031g65fff62h88c22e0e7ccea851@mail.gmail.com>

It's actually tframePlus: http://cran.r-project.org/web/packages/tframePlus/.

On Tue, Jun 2, 2009 at 1:25 PM, megh <megh700004 at yahoo.com> wrote:
>
> It is saying that, this package is not available :
>
>> install.packages("tramePlus")
> Warning in install.packages("tramePlus") :
> ?argument 'lib' is missing: using
> 'C:\Users\Arrun's\Documents/R/win-library/2.9'
> --- Please select a CRAN mirror for use in this session ---
> Warning message:
> In getDependencies(pkgs, dependencies, available, lib) :
> ?package ?tramePlus? is not available
>
> Any better idea please?
>
>
>
>
> megh wrote:
>>
>> Hi, I have following multivariate time series :
>> start = as.Date("01/01/05", format="%m/%d/%y")
>> end = as.Date("12/31/05", format="%m/%d/%y")
>> dates = seq(start, end, by=1)
>>
>> dat = matrix(rnorm(length(dates)*4), length(dates))
>> dat1 = t(apply(dat, 1, function(x) x+t(c(100, 110, 120, 130))))
>>
>> library(zoo)
>> dat2 = zoo(dat1, dates); head(dat2)
>>
>> Now I want to get "surface plot" wherein x-axis is time, y-axis represents
>> 1,2,3,4, and z-axis is for values of "dat2". Is there any R drawing device
>> to do that?
>>
>> Regards,
>>
>
> --
> View this message in context: http://www.nabble.com/Surface-plot-of-multivariate-time-series-tp23827284p23836769.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From jeff.a.ryan at gmail.com  Tue Jun  2 19:34:46 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Tue, 2 Jun 2009 12:34:46 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Surface plot of multivariate
	time series
In-Reply-To: <dd3243090906021031g65fff62h88c22e0e7ccea851@mail.gmail.com>
References: <23827284.post@talk.nabble.com> <23836769.post@talk.nabble.com>
	<dd3243090906021031g65fff62h88c22e0e7ccea851@mail.gmail.com>
Message-ID: <e8e755250906021034h7ae40157pf9b9b4189f58137b@mail.gmail.com>

Shane beat me to it w.r.t the link...

That aside, I posted this a while back as an example for doing what
you want.  quantmod will be getting this natively at some point, but
for now the code can serve as an example:

http://www.quantmod.com/examples/chartSeries3d/

HTH
Jeff

On Tue, Jun 2, 2009 at 12:31 PM, Shane Conway <shane.conway at gmail.com> wrote:
> It's actually tframePlus: http://cran.r-project.org/web/packages/tframePlus/.
>
> On Tue, Jun 2, 2009 at 1:25 PM, megh <megh700004 at yahoo.com> wrote:
>>
>> It is saying that, this package is not available :
>>
>>> install.packages("tramePlus")
>> Warning in install.packages("tramePlus") :
>> ?argument 'lib' is missing: using
>> 'C:\Users\Arrun's\Documents/R/win-library/2.9'
>> --- Please select a CRAN mirror for use in this session ---
>> Warning message:
>> In getDependencies(pkgs, dependencies, available, lib) :
>> ?package ?tramePlus? is not available
>>
>> Any better idea please?
>>
>>
>>
>>
>> megh wrote:
>>>
>>> Hi, I have following multivariate time series :
>>> start = as.Date("01/01/05", format="%m/%d/%y")
>>> end = as.Date("12/31/05", format="%m/%d/%y")
>>> dates = seq(start, end, by=1)
>>>
>>> dat = matrix(rnorm(length(dates)*4), length(dates))
>>> dat1 = t(apply(dat, 1, function(x) x+t(c(100, 110, 120, 130))))
>>>
>>> library(zoo)
>>> dat2 = zoo(dat1, dates); head(dat2)
>>>
>>> Now I want to get "surface plot" wherein x-axis is time, y-axis represents
>>> 1,2,3,4, and z-axis is for values of "dat2". Is there any R drawing device
>>> to do that?
>>>
>>> Regards,
>>>
>>
>> --
>> View this message in context: http://www.nabble.com/Surface-plot-of-multivariate-time-series-tp23827284p23836769.html
>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From megh700004 at yahoo.com  Tue Jun  2 19:35:52 2009
From: megh700004 at yahoo.com (megh)
Date: Tue, 2 Jun 2009 10:35:52 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Surface plot of multivariate
	time series
In-Reply-To: <23836769.post@talk.nabble.com>
References: <23827284.post@talk.nabble.com> <23836769.post@talk.nabble.com>
Message-ID: <23837058.post@talk.nabble.com>


I have installed that, but getting following error while loading that :
> library(tframePlus)
Loading required package: tframe
Loading required package: setRNG
Error : object 'tfSet' not found whilst loading namespace 'tframePlus'
Error: package/namespace load failed for 'tframePlus'

Any way out please?


megh wrote:
> 
> It is saying that, this package is not available :
> 
>> install.packages("tramePlus")
> Warning in install.packages("tramePlus") :
>   argument 'lib' is missing: using
> 'C:\Users\Arrun's\Documents/R/win-library/2.9'
> --- Please select a CRAN mirror for use in this session ---
> Warning message:
> In getDependencies(pkgs, dependencies, available, lib) :
>   package ?tramePlus? is not available
> 
> Any better idea please?
> 
> 
> 
> 
> megh wrote:
>> 
>> Hi, I have following multivariate time series :
>> start = as.Date("01/01/05", format="%m/%d/%y")
>> end = as.Date("12/31/05", format="%m/%d/%y")
>> dates = seq(start, end, by=1)
>> 
>> dat = matrix(rnorm(length(dates)*4), length(dates))
>> dat1 = t(apply(dat, 1, function(x) x+t(c(100, 110, 120, 130))))
>> 
>> library(zoo)
>> dat2 = zoo(dat1, dates); head(dat2)
>> 
>> Now I want to get "surface plot" wherein x-axis is time, y-axis
>> represents 1,2,3,4, and z-axis is for values of "dat2". Is there any R
>> drawing device to do that?
>> 
>> Regards,
>> 
> 
> 

-- 
View this message in context: http://www.nabble.com/Surface-plot-of-multivariate-time-series-tp23827284p23837058.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From etyurin at skipstonellc.com  Tue Jun  2 20:18:46 2009
From: etyurin at skipstonellc.com (Eugene Tyurin)
Date: Tue, 2 Jun 2009 14:18:46 -0400
Subject: [R-SIG-Finance] iterations inside odfWeave
Message-ID: <e4adf3900906021118x14be8d18k544425d50e0deb56@mail.gmail.com>

I am writing a report using odfWeave, and I need to run a function
several times to produce several pages of charts.

Here's my best attempt - unfortunately it only produces one chart
instead of multiple:

<<fig3,echo=FALSE,fig=TRUE>>=
for( q in tickers_) {
	multi_graph(q,asofDate)
}

Is there a "right" way to do it?

Thanks!
-- Eugene.


From megh700004 at yahoo.com  Tue Jun  2 21:15:44 2009
From: megh700004 at yahoo.com (Megh Dal)
Date: Tue, 2 Jun 2009 12:15:44 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Surface plot of multivariate
	time series
Message-ID: <558542.74912.qm@web58102.mail.re3.yahoo.com>


It seems package "tframe" installed properly.

> library(tframe)
Loading required package: setRNG
> library(tframePlus)
Error : object 'tfSet' not found whilst loading namespace 'tframePlus'
Error: package/namespace load failed for 'tframePlus'


--- On Tue, 6/2/09, Paul Gilbert <pgilbert at bank-banque-canada.ca> wrote:

> From: Paul Gilbert <pgilbert at bank-banque-canada.ca>
> Subject: Re: [R-SIG-Finance] [R-sig-finance] Surface plot of multivariate time series
> To: "megh" <megh700004 at yahoo.com>
> Date: Tuesday, June 2, 2009, 11:48 PM
> tframePlus requires package tframe,
> so it needs to be installed too. I'm 
> a bit confused by the error message, because it looks like
> the loading 
> of tframe worked ok,? but I cannot see any other
> reason for this error. 
> ???Do you have package tframe installed
> too?
> 
> Paul
> 
> megh wrote:
> > I have installed that, but getting following error
> while loading that :
> > 
> >>library(tframePlus)
> > 
> > Loading required package: tframe
> > Loading required package: setRNG
> > Error : object 'tfSet' not found whilst loading
> namespace 'tframePlus'
> > Error: package/namespace load failed for 'tframePlus'
> > 
> > Any way out please?
> > 
> > 
> > megh wrote:
> > 
> >>It is saying that, this package is not available :
> >>
> >>
> >>>install.packages("tramePlus")
> >>
> >>Warning in install.packages("tramePlus") :
> >>? argument 'lib' is missing: using
> >>'C:\Users\Arrun's\Documents/R/win-library/2.9'
> >>--- Please select a CRAN mirror for use in this
> session ---
> >>Warning message:
> >>In getDependencies(pkgs, dependencies, available,
> lib) :
> >>? package ?tramePlus? is not available
> >>
> >>Any better idea please?
> >>
> >>
> >>
> >>
> >>megh wrote:
> >>
> >>>Hi, I have following multivariate time series
> :
> >>>start = as.Date("01/01/05", format="%m/%d/%y")
> >>>end = as.Date("12/31/05", format="%m/%d/%y")
> >>>dates = seq(start, end, by=1)
> >>>
> >>>dat = matrix(rnorm(length(dates)*4),
> length(dates))
> >>>dat1 = t(apply(dat, 1, function(x) x+t(c(100,
> 110, 120, 130))))
> >>>
> >>>library(zoo)
> >>>dat2 = zoo(dat1, dates); head(dat2)
> >>>
> >>>Now I want to get "surface plot" wherein x-axis
> is time, y-axis
> >>>represents 1,2,3,4, and z-axis is for values of
> "dat2". Is there any R
> >>>drawing device to do that?
> >>>
> >>>Regards,
> >>>
> >>
> >>
> > 
> ====================================================================================
> 
> La version fran?aise suit le texte anglais.
> 
> ------------------------------------------------------------------------------------
> 
> This email may contain privileged and/or confidential
> information, and the Bank of
> Canada does not waive any related rights. Any distribution,
> use, or copying of this
> email or the information it contains by other than the
> intended recipient is
> unauthorized. If you received this email in error please
> delete it immediately from
> your system and notify the sender promptly by email that
> you have done so. 
> 
> ------------------------------------------------------------------------------------
> 
> Le pr?sent courriel peut contenir de l'information
> privil?gi?e ou confidentielle.
> La Banque du Canada ne renonce pas aux droits qui s'y
> rapportent. Toute diffusion,
> utilisation ou copie de ce courriel ou des renseignements
> qu'il contient par une
> personne autre que le ou les destinataires d?sign?s est
> interdite. Si vous recevez
> ce courriel par erreur, veuillez le supprimer
> imm?diatement et envoyer sans d?lai ?
> l'exp?diteur un message ?lectronique pour l'aviser que
> vous avez ?limin? de votre
> ordinateur toute copie du courriel re?u.
>





From brian at braverock.com  Wed Jun  3 01:43:27 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 02 Jun 2009 18:43:27 -0500
Subject: [R-SIG-Finance] iterations inside odfWeave
In-Reply-To: <e4adf3900906021118x14be8d18k544425d50e0deb56@mail.gmail.com>
References: <e4adf3900906021118x14be8d18k544425d50e0deb56@mail.gmail.com>
Message-ID: <4A25B91F.4010802@braverock.com>

Eugene Tyurin wrote:
> I am writing a report using odfWeave, and I need to run a function
> several times to produce several pages of charts.
>
> Here's my best attempt - unfortunately it only produces one chart
> instead of multiple:
>
> <<fig3,echo=FALSE,fig=TRUE>>=
> for( q in tickers_) {
> 	multi_graph(q,asofDate)
> }
>
> Is there a "right" way to do it?
>   
I'd suggest emailing the maintainer of odfWeave, and perhaps sending 
this query to r-help.  It has nothing directly to do with finance.

Regards,

    - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From stanley.neo at gmail.com  Wed Jun  3 02:31:09 2009
From: stanley.neo at gmail.com (Stanley Neo)
Date: Wed, 3 Jun 2009 08:31:09 +0800
Subject: [R-SIG-Finance] Newbie question on risk free Interest Rate
Message-ID: <e4d27dad0906021731l177b4c60l1497488a25aaafe7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090603/3e7e562b/attachment.pl>

From etyurin at skipstonellc.com  Wed Jun  3 03:18:06 2009
From: etyurin at skipstonellc.com (Eugene Tyurin)
Date: Tue, 02 Jun 2009 21:18:06 -0400
Subject: [R-SIG-Finance] iterations inside odfWeave
In-Reply-To: <4A25B91F.4010802@braverock.com>
References: <e4adf3900906021118x14be8d18k544425d50e0deb56@mail.gmail.com>
	<4A25B91F.4010802@braverock.com>
Message-ID: <4A25CF4E.9000305@skipstonellc.com>

Brian,

Your point is taken.

However, odfWeave does not have a dedicated mailing list, and, in my 
experience, similar problems seem to arise in pursuit of similar 
objectives.

How often does a statistician write a paper where he does not know the 
structure of his data beforehand? But a trader never knows how many 
securities are going to satisfy his pre-screening criteria.


On 6/2/2009 7:43 PM, Brian G. Peterson wrote:
> Eugene Tyurin wrote:
>> I am writing a report using odfWeave, and I need to run a function
>> several times to produce several pages of charts.
>>
>> Here's my best attempt - unfortunately it only produces one chart
>> instead of multiple:
>>
>> <<fig3,echo=FALSE,fig=TRUE>>=
>> for( q in tickers_) {
>>     multi_graph(q,asofDate)
>> }
>>
>> Is there a "right" way to do it?
> I'd suggest emailing the maintainer of odfWeave, and perhaps sending 
> this query to r-help.  It has nothing directly to do with finance.
>
> Regards,
>
>    - Brian
>


From josh.m.ulrich at gmail.com  Wed Jun  3 03:38:57 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Tue, 2 Jun 2009 20:38:57 -0500
Subject: [R-SIG-Finance] iterations inside odfWeave
In-Reply-To: <4A25CF4E.9000305@skipstonellc.com>
References: <e4adf3900906021118x14be8d18k544425d50e0deb56@mail.gmail.com>
	<4A25B91F.4010802@braverock.com> <4A25CF4E.9000305@skipstonellc.com>
Message-ID: <8cca69990906021838k4be240cehb6066af8452a9299@mail.gmail.com>

On Tue, Jun 2, 2009 at 8:18 PM, Eugene Tyurin <etyurin at skipstonellc.com> wrote:
> Brian,
>
> Your point is taken.
>
> However, odfWeave does not have a dedicated mailing list, and, in my
> experience, similar problems seem to arise in pursuit of similar objectives.
>
Yes, but similar problems do not necessarily require similar
solutions.  And the lack of a dedicated mailing list is why Brian
suggested contacting the package maintainer and/or R-help.

> How often does a statistician write a paper where he does not know the
> structure of his data beforehand? But a trader never knows how many
> securities are going to satisfy his pre-screening criteria.
>
Determining the structure of your data does not require a report built
with odfWeave.  This may be a more appropriate mailing list if you
were searching for a more general solution, but your problem is really
with odfWeave.  Brian and I don't intend to be rude; your question
will simply be more likely answered better/faster on a different list.

>
> On 6/2/2009 7:43 PM, Brian G. Peterson wrote:
>>
>> Eugene Tyurin wrote:
>>>
>>> I am writing a report using odfWeave, and I need to run a function
>>> several times to produce several pages of charts.
>>>
>>> Here's my best attempt - unfortunately it only produces one chart
>>> instead of multiple:
>>>
>>> <<fig3,echo=FALSE,fig=TRUE>>=
>>> for( q in tickers_) {
>>> ? ?multi_graph(q,asofDate)
>>> }
>>>
>>> Is there a "right" way to do it?
>>
>> I'd suggest emailing the maintainer of odfWeave, and perhaps sending this
>> query to r-help. ?It has nothing directly to do with finance.
>>
>> Regards,
>>
>> ? - Brian
>>
>
>

Best,
Josh
--
http://www.fosstrading.com


From Matthias.Kohl at stamats.de  Wed Jun  3 05:15:57 2009
From: Matthias.Kohl at stamats.de (Matthias Kohl)
Date: Wed, 03 Jun 2009 05:15:57 +0200
Subject: [R-SIG-Finance] iterations inside odfWeave
In-Reply-To: <8cca69990906021838k4be240cehb6066af8452a9299@mail.gmail.com>
References: <e4adf3900906021118x14be8d18k544425d50e0deb56@mail.gmail.com>	<4A25B91F.4010802@braverock.com>
	<4A25CF4E.9000305@skipstonellc.com>
	<8cca69990906021838k4be240cehb6066af8452a9299@mail.gmail.com>
Message-ID: <4A25EAED.2090108@stamats.de>

take a look at

http://www.statistik.lmu.de/~leisch/Sweave/FAQ.html

in particular A.9

hth,
Matthias

Joshua Ulrich schrieb:
> On Tue, Jun 2, 2009 at 8:18 PM, Eugene Tyurin <etyurin at skipstonellc.com> wrote:
>   
>> Brian,
>>
>> Your point is taken.
>>
>> However, odfWeave does not have a dedicated mailing list, and, in my
>> experience, similar problems seem to arise in pursuit of similar objectives.
>>
>>     
> Yes, but similar problems do not necessarily require similar
> solutions.  And the lack of a dedicated mailing list is why Brian
> suggested contacting the package maintainer and/or R-help.
>
>   
>> How often does a statistician write a paper where he does not know the
>> structure of his data beforehand? But a trader never knows how many
>> securities are going to satisfy his pre-screening criteria.
>>
>>     
> Determining the structure of your data does not require a report built
> with odfWeave.  This may be a more appropriate mailing list if you
> were searching for a more general solution, but your problem is really
> with odfWeave.  Brian and I don't intend to be rude; your question
> will simply be more likely answered better/faster on a different list.
>
>   
>> On 6/2/2009 7:43 PM, Brian G. Peterson wrote:
>>     
>>> Eugene Tyurin wrote:
>>>       
>>>> I am writing a report using odfWeave, and I need to run a function
>>>> several times to produce several pages of charts.
>>>>
>>>> Here's my best attempt - unfortunately it only produces one chart
>>>> instead of multiple:
>>>>
>>>> <<fig3,echo=FALSE,fig=TRUE>>=
>>>> for( q in tickers_) {
>>>>    multi_graph(q,asofDate)
>>>> }
>>>>
>>>> Is there a "right" way to do it?
>>>>         
>>> I'd suggest emailing the maintainer of odfWeave, and perhaps sending this
>>> query to r-help.  It has nothing directly to do with finance.
>>>
>>> Regards,
>>>
>>>   - Brian
>>>
>>>       
>>     
>
> Best,
> Josh
> --
> http://www.fosstrading.com
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>   

-- 
Dr. Matthias Kohl
www.stamats.de


From icos.atropa at gmail.com  Wed Jun  3 06:17:12 2009
From: icos.atropa at gmail.com (Christian Gunning)
Date: Tue, 2 Jun 2009 21:17:12 -0700
Subject: [R-SIG-Finance] working levelplot with zoo - surface plot of
	multivariate time series
Message-ID: <681d07c20906022117s1ad6698fy71a028cfdbb24f01@mail.gmail.com>

On Tue, Jun 2, 2009 at 3:00 AM,
<r-sig-finance-request at stat.math.ethz.ch> wrote:
>
> Now I want to get "surface plot" wherein x-axis is time, y-axis represents
> 1,2,3,4, and z-axis is for values of "dat2". Is there any R drawing device
> to do that?
>

I just ran into this independently.  Is a method of levelplot for zoo
objects on anyone's horizon?  Nontrivial?  I got the following to
work:

 start = as.Date("01/01/05", format="%m/%d/%y")
 end = as.Date("12/31/05", format="%m/%d/%y")
 dates = seq(start, end, by=1)

 dat = matrix(rnorm(length(dates)*4), length(dates))
 dat1 = t(apply(dat, 1, function(x) x+t(c(100, 110, 120, 130))))

 library(zoo)
 dat2 = zoo(dat1, dates); head(dat2)


### With Date index This works, but it isn't pretty:

mylab=lattice:::formattedTicksAndLabels.Date(index(dat2))
levelplot(coredata(dat2),
scales=list(x=list(rot=90,at=mylab$at-mylab$num.limit[1],
labels=mylab$labels)), aspect='fill')

This only works for dates, where the index vector increment and the
numeric representation of the Date increment are both 1. You can
subtract out "mylab$num.limit" to get them to align.  This doesn't
work for POSIXct, where the numeric representation of the POSIXct
increment is in seconds (unless the index vector is by second). Here,
I extract the numeric representation of dt from the index vector, and
divide the "at" vector by dt to align it with the labels.  It works,
just really messy.

### With POSIXct index
start =as.Date("01/01/05", format="%m/%d/%y")
end = as.Date("12/31/05", format="%m/%d/%y")
dates = as.POSIXct(seq(start, end, by=1)) ####changed

dat = matrix(rnorm(length(dates)*4), length(dates))
 dat1 = t(apply(dat, 1, function(x) x+t(c(100, 110, 120, 130))))

mylab=lattice:::formattedTicksAndLabels.POSIXct(index(dat2))
mylab.at = (mylab$at-mylab$num.limit[1])/(as.numeric(dates[2])-as.numeric(dates[1]))

levelplot(coredata(dat2), scales=list(x=list(rot=90,at=mylab.at,
labels=mylab$labels)), aspect='fill')
###################


hope this helps,
christian


-- 
Far better an approximate answer to the right question, which is often
vague, than the exact answer to the wrong question, which can always
be made precise -- j.w. tukey


From ron_michael70 at yahoo.com  Wed Jun  3 10:19:14 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Wed, 3 Jun 2009 01:19:14 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] A question on VECM
Message-ID: <23847406.post@talk.nabble.com>


In my textbook, I found that for a vector error correction model, the "beta"
matrix i.e. which represents the co-integrating vectors can be represented
in a speacial matrix wherein first rxr partition is Identity matrix like :

beta[rxn] = (I(r), beta[rx(n-r)])

Is there any R function to do that representation?

Regards,
-- 
View this message in context: http://www.nabble.com/A-question-on-VECM-tp23847406p23847406.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From Bernhard_Pfaff at fra.invesco.com  Wed Jun  3 14:11:47 2009
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Wed, 3 Jun 2009 13:11:47 +0100
Subject: [R-SIG-Finance] [R-sig-finance] A question on VECM
In-Reply-To: <23847406.post@talk.nabble.com>
References: <23847406.post@talk.nabble.com>
Message-ID: <B89F0CE41D45644A97CCC93DF548C1C31ADBDFC9@GBHENXMB02.corp.amvescap.net>

>-----Urspr?ngliche Nachricht-----
>Von: r-sig-finance-bounces at stat.math.ethz.ch 
>[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von RON70
>Gesendet: Mittwoch, 3. Juni 2009 10:19
>An: r-sig-finance at stat.math.ethz.ch
>Betreff: [R-SIG-Finance] [R-sig-finance] A question on VECM
>
>
>In my textbook, I found that for a vector error correction 
>model, the "beta"
>matrix i.e. which represents the co-integrating vectors can be 
>represented
>in a speacial matrix wherein first rxr partition is Identity 
>matrix like :
>
>beta[rxn] = (I(r), beta[rx(n-r)])
>
>Is there any R function to do that representation?
>

Dear Ron?

have you considered the CRAN package 'urca' and there the function cajorls()?

library(urca)
example(cajorls)

Best,
Bernhard


>Regards,
>-- 
>View this message in context: 
>http://www.nabble.com/A-question-on-VECM-tp23847406p23847406.html
>Sent from the Rmetrics mailing list archive at Nabble.com.
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only.
>-- If you want to post, subscribe first.
>
*****************************************************************
Confidentiality Note: The information contained in this ...{{dropped:10}}


From m.breman at yahoo.com  Wed Jun  3 21:15:17 2009
From: m.breman at yahoo.com (Mark Breman)
Date: Wed, 3 Jun 2009 12:15:17 -0700 (PDT)
Subject: [R-SIG-Finance] determine non-linear correlation
Message-ID: <90598.5865.qm@web38605.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090603/6e029438/attachment.pl>

From landronimirc at gmail.com  Wed Jun  3 21:51:16 2009
From: landronimirc at gmail.com (Liviu Andronic)
Date: Wed, 3 Jun 2009 21:51:16 +0200
Subject: [R-SIG-Finance] determine non-linear correlation
In-Reply-To: <90598.5865.qm@web38605.mail.mud.yahoo.com>
References: <90598.5865.qm@web38605.mail.mud.yahoo.com>
Message-ID: <68b1e2610906031251m440fcdc2kfd4d15f0dcba33e0@mail.gmail.com>

On Wed, Jun 3, 2009 at 9:15 PM, Mark Breman <m.breman at yahoo.com> wrote:
> I would like to know if two financial time-series are nonlinear correlated, and if so, what that correlation function is.
> Is there an easy way to do this with R?
>
What about non-parametric correlation coefficients? They are based on
ranks, and should detect any monotonic relationship between the time
series. Check ?cor and ?cor.test methods "spearman" and "kendall", and
also the Wikipedia articles for further references.

Best,
Liviu


> I have read a thesis about the "high order correlation coefficient to solve the nonlinear correlation problem" but I'm not able to translate this into a solution for my problem. All these statistics are very interesting but also challenging for me...
>
> Kind regards,
>
> -Mark-
>
>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Do you know how to read?
http://www.alienetworks.com/srtest.cfm
Do you know how to write?
http://garbl.home.comcast.net/~garbl/stylemanual/e.htm#e-mail


From windspeedo99 at gmail.com  Thu Jun  4 10:59:26 2009
From: windspeedo99 at gmail.com (Wind)
Date: Thu, 4 Jun 2009 16:59:26 +0800
Subject: [R-SIG-Finance] client id issue with IBrokers
Message-ID: <d718c8210906040159o70ebe22aw9f58d999bfbe6124@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090604/b3e485ad/attachment.pl>

From m.breman at yahoo.com  Thu Jun  4 11:20:17 2009
From: m.breman at yahoo.com (Mark Breman)
Date: Thu, 4 Jun 2009 02:20:17 -0700 (PDT)
Subject: [R-SIG-Finance] determine non-linear correlation
In-Reply-To: <68b1e2610906031251m440fcdc2kfd4d15f0dcba33e0@mail.gmail.com>
References: <90598.5865.qm@web38605.mail.mud.yahoo.com>
	<68b1e2610906031251m440fcdc2kfd4d15f0dcba33e0@mail.gmail.com>
Message-ID: <907255.1639.qm@web38602.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090604/37137451/attachment.pl>

From ron_michael70 at yahoo.com  Thu Jun  4 11:30:03 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Thu, 4 Jun 2009 02:30:03 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] A question on VECM
In-Reply-To: <B89F0CE41D45644A97CCC93DF548C1C31ADBDFC9@GBHENXMB02.corp.amvescap.net>
References: <23847406.post@talk.nabble.com>
	<B89F0CE41D45644A97CCC93DF548C1C31ADBDFC9@GBHENXMB02.corp.amvescap.net>
Message-ID: <23866615.post@talk.nabble.com>


Thanks Bernhard for this reply.

However actually I was thinking there might be some matrix property for any
rxn (rank "r") matrix to equivalently explain in a combination of Identity
and rx(n-r) matrices. Is it so? Actually I got this feeling from a statement
saying that, "normalization is always possible if variables arranged
properly". Therefore suppose I have some economic theory to express C.I.
vectors in original term i.e. arbitrary C.I. matrix, based on some
economics. Then I arrange them i.e. do matrix manipulation to make C.I.
matrix Normalized i.e. let say, I have following original C.I. matrix (based
on some economics) on 10 variables :

n = 10
r = 4
C.I.matrix = matrix(rnorm(10*4), 4)

Now I want to make it (I[4], C.I.matrix.modified[4x6] )

Here I am rather interested is there any R function to do this kind of
"matrix-normalization", not so interested to get a "already normalized" C.I.
matrix.

Is there any?

Thanks



Pfaff, Bernhard Dr. wrote:
> 
>>-----Urspr?ngliche Nachricht-----
>>Von: r-sig-finance-bounces at stat.math.ethz.ch 
>>[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von RON70
>>Gesendet: Mittwoch, 3. Juni 2009 10:19
>>An: r-sig-finance at stat.math.ethz.ch
>>Betreff: [R-SIG-Finance] [R-sig-finance] A question on VECM
>>
>>
>>In my textbook, I found that for a vector error correction 
>>model, the "beta"
>>matrix i.e. which represents the co-integrating vectors can be 
>>represented
>>in a speacial matrix wherein first rxr partition is Identity 
>>matrix like :
>>
>>beta[rxn] = (I(r), beta[rx(n-r)])
>>
>>Is there any R function to do that representation?
>>
> 
> Dear Ron?
> 
> have you considered the CRAN package 'urca' and there the function
> cajorls()?
> 
> library(urca)
> example(cajorls)
> 
> Best,
> Bernhard
> 
> 
>>Regards,
>>-- 
>>View this message in context: 
>>http://www.nabble.com/A-question-on-VECM-tp23847406p23847406.html
>>Sent from the Rmetrics mailing list archive at Nabble.com.
>>
>>_______________________________________________
>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>-- Subscriber-posting only.
>>-- If you want to post, subscribe first.
>>
> *****************************************************************
> Confidentiality Note: The information contained in this ...{{dropped:10}}
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/A-question-on-VECM-tp23847406p23866615.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From Bernhard_Pfaff at fra.invesco.com  Thu Jun  4 11:43:10 2009
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Thu, 4 Jun 2009 10:43:10 +0100
Subject: [R-SIG-Finance] [R-sig-finance] A question on VECM
In-Reply-To: <23866615.post@talk.nabble.com>
References: <23847406.post@talk.nabble.com><B89F0CE41D45644A97CCC93DF548C1C31ADBDFC9@GBHENXMB02.corp.amvescap.net>
	<23866615.post@talk.nabble.com>
Message-ID: <B89F0CE41D45644A97CCC93DF548C1C31B02DF6C@GBHENXMB02.corp.amvescap.net>

Dear Ron, 

if I understand you correctly, you have a-priori knowledge about some of the CI-relations? If so, why don't you compute them in advance and then work further? This would also reduce the dimension of your VECM.

Best,
Bernhard

ps: Incidentally, the returned list element 'beta' of cajorls is computed pretty much in sync what you have quoted, i.e., "normalization is always possible if variables arranged properly".  



>Von: r-sig-finance-bounces at stat.math.ethz.ch 
>[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von RON70
>Gesendet: Donnerstag, 4. Juni 2009 11:30
>An: r-sig-finance at stat.math.ethz.ch
>Betreff: Re: [R-SIG-Finance] [R-sig-finance] A question on VECM
>
>
>Thanks Bernhard for this reply.
>
>However actually I was thinking there might be some matrix 
>property for any
>rxn (rank "r") matrix to equivalently explain in a combination 
>of Identity
>and rx(n-r) matrices. Is it so? Actually I got this feeling 
>from a statement
>saying that, "normalization is always possible if variables arranged
>properly". Therefore suppose I have some economic theory to 
>express C.I.
>vectors in original term i.e. arbitrary C.I. matrix, based on some
>economics. Then I arrange them i.e. do matrix manipulation to make C.I.
>matrix Normalized i.e. let say, I have following original C.I. 
>matrix (based
>on some economics) on 10 variables :
>
>n = 10
>r = 4
>C.I.matrix = matrix(rnorm(10*4), 4)
>
>Now I want to make it (I[4], C.I.matrix.modified[4x6] )
>
>Here I am rather interested is there any R function to do this kind of
>"matrix-normalization", not so interested to get a "already 
>normalized" C.I.
>matrix.
>
>Is there any?
>
>Thanks
>
>
>
>Pfaff, Bernhard Dr. wrote:
>> 
>>>-----Urspr?ngliche Nachricht-----
>>>Von: r-sig-finance-bounces at stat.math.ethz.ch 
>>>[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von RON70
>>>Gesendet: Mittwoch, 3. Juni 2009 10:19
>>>An: r-sig-finance at stat.math.ethz.ch
>>>Betreff: [R-SIG-Finance] [R-sig-finance] A question on VECM
>>>
>>>
>>>In my textbook, I found that for a vector error correction 
>>>model, the "beta"
>>>matrix i.e. which represents the co-integrating vectors can be 
>>>represented
>>>in a speacial matrix wherein first rxr partition is Identity 
>>>matrix like :
>>>
>>>beta[rxn] = (I(r), beta[rx(n-r)])
>>>
>>>Is there any R function to do that representation?
>>>
>> 
>> Dear Ron?
>> 
>> have you considered the CRAN package 'urca' and there the function
>> cajorls()?
>> 
>> library(urca)
>> example(cajorls)
>> 
>> Best,
>> Bernhard
>> 
>> 
>>>Regards,
>>>-- 
>>>View this message in context: 
>>>http://www.nabble.com/A-question-on-VECM-tp23847406p23847406.html
>>>Sent from the Rmetrics mailing list archive at Nabble.com.
>>>
>>>_______________________________________________
>>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>-- Subscriber-posting only.
>>>-- If you want to post, subscribe first.
>>>
>> *****************************************************************
>> Confidentiality Note: The information contained in this 
>...{{dropped:10}}
>> 
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>> 
>> 
>
>-- 
>View this message in context: 
>http://www.nabble.com/A-question-on-VECM-tp23847406p23866615.html
>Sent from the Rmetrics mailing list archive at Nabble.com.
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only.
>-- If you want to post, subscribe first.
>


From ron_michael70 at yahoo.com  Thu Jun  4 12:40:50 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Thu, 4 Jun 2009 03:40:50 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] A question on VECM
In-Reply-To: <B89F0CE41D45644A97CCC93DF548C1C31B02DF6C@GBHENXMB02.corp.amvescap.net>
References: <23847406.post@talk.nabble.com>
	<B89F0CE41D45644A97CCC93DF548C1C31ADBDFC9@GBHENXMB02.corp.amvescap.net>
	<23866615.post@talk.nabble.com>
	<B89F0CE41D45644A97CCC93DF548C1C31B02DF6C@GBHENXMB02.corp.amvescap.net>
Message-ID: <23867519.post@talk.nabble.com>


Right now I am not interested on estimation however trying to convince myself
to justify this statement "normalization is always possible if variables
arranged properly". I am trying to answer "why and how it is always
possible?"

PS: we know that having information on C.I. matrix does not improve coef
estimation as rate of convergence for C.I. coef are much faster than rest.



Pfaff, Bernhard Dr. wrote:
> 
> Dear Ron, 
> 
> if I understand you correctly, you have a-priori knowledge about some of
> the CI-relations? If so, why don't you compute them in advance and then
> work further? This would also reduce the dimension of your VECM.
> 
> Best,
> Bernhard
> 
> ps: Incidentally, the returned list element 'beta' of cajorls is computed
> pretty much in sync what you have quoted, i.e., "normalization is always
> possible if variables arranged properly".  
> 
> 
> 
>>Von: r-sig-finance-bounces at stat.math.ethz.ch 
>>[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von RON70
>>Gesendet: Donnerstag, 4. Juni 2009 11:30
>>An: r-sig-finance at stat.math.ethz.ch
>>Betreff: Re: [R-SIG-Finance] [R-sig-finance] A question on VECM
>>
>>
>>Thanks Bernhard for this reply.
>>
>>However actually I was thinking there might be some matrix 
>>property for any
>>rxn (rank "r") matrix to equivalently explain in a combination 
>>of Identity
>>and rx(n-r) matrices. Is it so? Actually I got this feeling 
>>from a statement
>>saying that, "normalization is always possible if variables arranged
>>properly". Therefore suppose I have some economic theory to 
>>express C.I.
>>vectors in original term i.e. arbitrary C.I. matrix, based on some
>>economics. Then I arrange them i.e. do matrix manipulation to make C.I.
>>matrix Normalized i.e. let say, I have following original C.I. 
>>matrix (based
>>on some economics) on 10 variables :
>>
>>n = 10
>>r = 4
>>C.I.matrix = matrix(rnorm(10*4), 4)
>>
>>Now I want to make it (I[4], C.I.matrix.modified[4x6] )
>>
>>Here I am rather interested is there any R function to do this kind of
>>"matrix-normalization", not so interested to get a "already 
>>normalized" C.I.
>>matrix.
>>
>>Is there any?
>>
>>Thanks
>>
>>
>>
>>Pfaff, Bernhard Dr. wrote:
>>> 
>>>>-----Urspr?ngliche Nachricht-----
>>>>Von: r-sig-finance-bounces at stat.math.ethz.ch 
>>>>[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von RON70
>>>>Gesendet: Mittwoch, 3. Juni 2009 10:19
>>>>An: r-sig-finance at stat.math.ethz.ch
>>>>Betreff: [R-SIG-Finance] [R-sig-finance] A question on VECM
>>>>
>>>>
>>>>In my textbook, I found that for a vector error correction 
>>>>model, the "beta"
>>>>matrix i.e. which represents the co-integrating vectors can be 
>>>>represented
>>>>in a speacial matrix wherein first rxr partition is Identity 
>>>>matrix like :
>>>>
>>>>beta[rxn] = (I(r), beta[rx(n-r)])
>>>>
>>>>Is there any R function to do that representation?
>>>>
>>> 
>>> Dear Ron?
>>> 
>>> have you considered the CRAN package 'urca' and there the function
>>> cajorls()?
>>> 
>>> library(urca)
>>> example(cajorls)
>>> 
>>> Best,
>>> Bernhard
>>> 
>>> 
>>>>Regards,
>>>>-- 
>>>>View this message in context: 
>>>>http://www.nabble.com/A-question-on-VECM-tp23847406p23847406.html
>>>>Sent from the Rmetrics mailing list archive at Nabble.com.
>>>>
>>>>_______________________________________________
>>>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>-- Subscriber-posting only.
>>>>-- If you want to post, subscribe first.
>>>>
>>> *****************************************************************
>>> Confidentiality Note: The information contained in this 
>>...{{dropped:10}}
>>> 
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>> 
>>> 
>>
>>-- 
>>View this message in context: 
>>http://www.nabble.com/A-question-on-VECM-tp23847406p23866615.html
>>Sent from the Rmetrics mailing list archive at Nabble.com.
>>
>>_______________________________________________
>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>-- Subscriber-posting only.
>>-- If you want to post, subscribe first.
>>
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/A-question-on-VECM-tp23847406p23867519.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From frainj at tcd.ie  Thu Jun  4 13:39:21 2009
From: frainj at tcd.ie (John Frain)
Date: Thu, 4 Jun 2009 12:39:21 +0100
Subject: [R-SIG-Finance] [R-sig-finance] A question on VECM
In-Reply-To: <23867519.post@talk.nabble.com>
References: <23847406.post@talk.nabble.com>
	<B89F0CE41D45644A97CCC93DF548C1C31ADBDFC9@GBHENXMB02.corp.amvescap.net>
	<23866615.post@talk.nabble.com>
	<B89F0CE41D45644A97CCC93DF548C1C31B02DF6C@GBHENXMB02.corp.amvescap.net>
	<23867519.post@talk.nabble.com>
Message-ID: <cfdde1650906040439r16cc6389w89c4d71041d25faa@mail.gmail.com>

Let beta be (r by n) of rank r. (r<n).  Let Q (r by r) be the first r
columns of this matrix. Let P = inv(Q).  Then P * pi is of the form
you require.  (I presume that Q is always invertible - see
Johansen(1995), Likelihood based inference in Cointegrated Vector
Autoregressive Models, Oxford.).  In the early days this method was
seen as one way of achieving identification of the model. Regrettably,
in most cases, it does not have any economic content

Best regards

John

2009/6/4 RON70 <ron_michael70 at yahoo.com>:
>
> Right now I am not interested on estimation however trying to convince myself
> to justify this statement "normalization is always possible if variables
> arranged properly". I am trying to answer "why and how it is always
> possible?"
>
> PS: we know that having information on C.I. matrix does not improve coef
> estimation as rate of convergence for C.I. coef are much faster than rest.
>
>
>
> Pfaff, Bernhard Dr. wrote:
>>
>> Dear Ron,
>>
>> if I understand you correctly, you have a-priori knowledge about some of
>> the CI-relations? If so, why don't you compute them in advance and then
>> work further? This would also reduce the dimension of your VECM.
>>
>> Best,
>> Bernhard
>>
>> ps: Incidentally, the returned list element 'beta' of cajorls is computed
>> pretty much in sync what you have quoted, i.e., "normalization is always
>> possible if variables arranged properly".
>>
>>
>>
>>>Von: r-sig-finance-bounces at stat.math.ethz.ch
>>>[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von RON70
>>>Gesendet: Donnerstag, 4. Juni 2009 11:30
>>>An: r-sig-finance at stat.math.ethz.ch
>>>Betreff: Re: [R-SIG-Finance] [R-sig-finance] A question on VECM
>>>
>>>
>>>Thanks Bernhard for this reply.
>>>
>>>However actually I was thinking there might be some matrix
>>>property for any
>>>rxn (rank "r") matrix to equivalently explain in a combination
>>>of Identity
>>>and rx(n-r) matrices. Is it so? Actually I got this feeling
>>>from a statement
>>>saying that, "normalization is always possible if variables arranged
>>>properly". Therefore suppose I have some economic theory to
>>>express C.I.
>>>vectors in original term i.e. arbitrary C.I. matrix, based on some
>>>economics. Then I arrange them i.e. do matrix manipulation to make C.I.
>>>matrix Normalized i.e. let say, I have following original C.I.
>>>matrix (based
>>>on some economics) on 10 variables :
>>>
>>>n = 10
>>>r = 4
>>>C.I.matrix = matrix(rnorm(10*4), 4)
>>>
>>>Now I want to make it (I[4], C.I.matrix.modified[4x6] )
>>>
>>>Here I am rather interested is there any R function to do this kind of
>>>"matrix-normalization", not so interested to get a "already
>>>normalized" C.I.
>>>matrix.
>>>
>>>Is there any?
>>>
>>>Thanks
>>>
>>>
>>>
>>>Pfaff, Bernhard Dr. wrote:
>>>>
>>>>>-----Urspr?ngliche Nachricht-----
>>>>>Von: r-sig-finance-bounces at stat.math.ethz.ch
>>>>>[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von RON70
>>>>>Gesendet: Mittwoch, 3. Juni 2009 10:19
>>>>>An: r-sig-finance at stat.math.ethz.ch
>>>>>Betreff: [R-SIG-Finance] [R-sig-finance] A question on VECM
>>>>>
>>>>>
>>>>>In my textbook, I found that for a vector error correction
>>>>>model, the "beta"
>>>>>matrix i.e. which represents the co-integrating vectors can be
>>>>>represented
>>>>>in a speacial matrix wherein first rxr partition is Identity
>>>>>matrix like :
>>>>>
>>>>>beta[rxn] = (I(r), beta[rx(n-r)])
>>>>>
>>>>>Is there any R function to do that representation?
>>>>>
>>>>
>>>> Dear Ron?
>>>>
>>>> have you considered the CRAN package 'urca' and there the function
>>>> cajorls()?
>>>>
>>>> library(urca)
>>>> example(cajorls)
>>>>
>>>> Best,
>>>> Bernhard
>>>>
>>>>
>>>>>Regards,
>>>>>--
>>>>>View this message in context:
>>>>>http://www.nabble.com/A-question-on-VECM-tp23847406p23847406.html
>>>>>Sent from the Rmetrics mailing list archive at Nabble.com.
>>>>>
>>>>>_______________________________________________
>>>>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>>-- Subscriber-posting only.
>>>>>-- If you want to post, subscribe first.
>>>>>
>>>> *****************************************************************
>>>> Confidentiality Note: The information contained in this
>>>...{{dropped:10}}
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>>
>>>
>>>--
>>>View this message in context:
>>>http://www.nabble.com/A-question-on-VECM-tp23847406p23866615.html
>>>Sent from the Rmetrics mailing list archive at Nabble.com.
>>>
>>>_______________________________________________
>>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>-- Subscriber-posting only.
>>>-- If you want to post, subscribe first.
>>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>
>
> --
> View this message in context: http://www.nabble.com/A-question-on-VECM-tp23847406p23867519.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.



-- 
John C Frain, Ph.D.
Trinity College Dublin
Dublin 2
Ireland
www.tcd.ie/Economics/staff/frainj/home.htm
mailto:frainj at tcd.ie
mailto:frainj at gmail.com


From ron_michael70 at yahoo.com  Thu Jun  4 14:01:29 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Thu, 4 Jun 2009 05:01:29 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] A question on VECM
In-Reply-To: <cfdde1650906040439r16cc6389w89c4d71041d25faa@mail.gmail.com>
References: <23847406.post@talk.nabble.com>
	<B89F0CE41D45644A97CCC93DF548C1C31ADBDFC9@GBHENXMB02.corp.amvescap.net>
	<23866615.post@talk.nabble.com>
	<B89F0CE41D45644A97CCC93DF548C1C31B02DF6C@GBHENXMB02.corp.amvescap.net>
	<23867519.post@talk.nabble.com>
	<cfdde1650906040439r16cc6389w89c4d71041d25faa@mail.gmail.com>
Message-ID: <23868560.post@talk.nabble.com>


Thanks, I did following :

n = 10
r = 4
beta = matrix(rnorm(10*4), 4)
Q = beta[1:r, 1:r]
P = solve(Q)
beta.norm = P %*% beta  # This is the normalized, according to you.

Now how can I say "beta" and "beta.norm" are indeed equivalent?

Regards,




John C. Frain wrote:
> 
> Let beta be (r by n) of rank r. (r<n).  Let Q (r by r) be the first r
> columns of this matrix. Let P = inv(Q).  Then P * pi is of the form
> you require.  (I presume that Q is always invertible - see
> Johansen(1995), Likelihood based inference in Cointegrated Vector
> Autoregressive Models, Oxford.).  In the early days this method was
> seen as one way of achieving identification of the model. Regrettably,
> in most cases, it does not have any economic content
> 
> Best regards
> 
> John
> 
> 2009/6/4 RON70 <ron_michael70 at yahoo.com>:
>>
>> Right now I am not interested on estimation however trying to convince
>> myself
>> to justify this statement "normalization is always possible if variables
>> arranged properly". I am trying to answer "why and how it is always
>> possible?"
>>
>> PS: we know that having information on C.I. matrix does not improve coef
>> estimation as rate of convergence for C.I. coef are much faster than
>> rest.
>>
>>
>>
>> Pfaff, Bernhard Dr. wrote:
>>>
>>> Dear Ron,
>>>
>>> if I understand you correctly, you have a-priori knowledge about some of
>>> the CI-relations? If so, why don't you compute them in advance and then
>>> work further? This would also reduce the dimension of your VECM.
>>>
>>> Best,
>>> Bernhard
>>>
>>> ps: Incidentally, the returned list element 'beta' of cajorls is
>>> computed
>>> pretty much in sync what you have quoted, i.e., "normalization is always
>>> possible if variables arranged properly".
>>>
>>>
>>>
>>>>Von: r-sig-finance-bounces at stat.math.ethz.ch
>>>>[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von RON70
>>>>Gesendet: Donnerstag, 4. Juni 2009 11:30
>>>>An: r-sig-finance at stat.math.ethz.ch
>>>>Betreff: Re: [R-SIG-Finance] [R-sig-finance] A question on VECM
>>>>
>>>>
>>>>Thanks Bernhard for this reply.
>>>>
>>>>However actually I was thinking there might be some matrix
>>>>property for any
>>>>rxn (rank "r") matrix to equivalently explain in a combination
>>>>of Identity
>>>>and rx(n-r) matrices. Is it so? Actually I got this feeling
>>>>from a statement
>>>>saying that, "normalization is always possible if variables arranged
>>>>properly". Therefore suppose I have some economic theory to
>>>>express C.I.
>>>>vectors in original term i.e. arbitrary C.I. matrix, based on some
>>>>economics. Then I arrange them i.e. do matrix manipulation to make C.I.
>>>>matrix Normalized i.e. let say, I have following original C.I.
>>>>matrix (based
>>>>on some economics) on 10 variables :
>>>>
>>>>n = 10
>>>>r = 4
>>>>C.I.matrix = matrix(rnorm(10*4), 4)
>>>>
>>>>Now I want to make it (I[4], C.I.matrix.modified[4x6] )
>>>>
>>>>Here I am rather interested is there any R function to do this kind of
>>>>"matrix-normalization", not so interested to get a "already
>>>>normalized" C.I.
>>>>matrix.
>>>>
>>>>Is there any?
>>>>
>>>>Thanks
>>>>
>>>>
>>>>
>>>>Pfaff, Bernhard Dr. wrote:
>>>>>
>>>>>>-----Urspr?ngliche Nachricht-----
>>>>>>Von: r-sig-finance-bounces at stat.math.ethz.ch
>>>>>>[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von RON70
>>>>>>Gesendet: Mittwoch, 3. Juni 2009 10:19
>>>>>>An: r-sig-finance at stat.math.ethz.ch
>>>>>>Betreff: [R-SIG-Finance] [R-sig-finance] A question on VECM
>>>>>>
>>>>>>
>>>>>>In my textbook, I found that for a vector error correction
>>>>>>model, the "beta"
>>>>>>matrix i.e. which represents the co-integrating vectors can be
>>>>>>represented
>>>>>>in a speacial matrix wherein first rxr partition is Identity
>>>>>>matrix like :
>>>>>>
>>>>>>beta[rxn] = (I(r), beta[rx(n-r)])
>>>>>>
>>>>>>Is there any R function to do that representation?
>>>>>>
>>>>>
>>>>> Dear Ron?
>>>>>
>>>>> have you considered the CRAN package 'urca' and there the function
>>>>> cajorls()?
>>>>>
>>>>> library(urca)
>>>>> example(cajorls)
>>>>>
>>>>> Best,
>>>>> Bernhard
>>>>>
>>>>>
>>>>>>Regards,
>>>>>>--
>>>>>>View this message in context:
>>>>>>http://www.nabble.com/A-question-on-VECM-tp23847406p23847406.html
>>>>>>Sent from the Rmetrics mailing list archive at Nabble.com.
>>>>>>
>>>>>>_______________________________________________
>>>>>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>>>-- Subscriber-posting only.
>>>>>>-- If you want to post, subscribe first.
>>>>>>
>>>>> *****************************************************************
>>>>> Confidentiality Note: The information contained in this
>>>>...{{dropped:10}}
>>>>>
>>>>> _______________________________________________
>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>> -- Subscriber-posting only.
>>>>> -- If you want to post, subscribe first.
>>>>>
>>>>>
>>>>
>>>>--
>>>>View this message in context:
>>>>http://www.nabble.com/A-question-on-VECM-tp23847406p23866615.html
>>>>Sent from the Rmetrics mailing list archive at Nabble.com.
>>>>
>>>>_______________________________________________
>>>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>-- Subscriber-posting only.
>>>>-- If you want to post, subscribe first.
>>>>
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>>
>>
>> --
>> View this message in context:
>> http://www.nabble.com/A-question-on-VECM-tp23847406p23867519.html
>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
> 
> 
> 
> -- 
> John C Frain, Ph.D.
> Trinity College Dublin
> Dublin 2
> Ireland
> www.tcd.ie/Economics/staff/frainj/home.htm
> mailto:frainj at tcd.ie
> mailto:frainj at gmail.com
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 

-- 
View this message in context: http://www.nabble.com/A-question-on-VECM-tp23847406p23868560.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From ron_michael70 at yahoo.com  Thu Jun  4 14:07:39 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Thu, 4 Jun 2009 05:07:39 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] A question on VECM
In-Reply-To: <23868560.post@talk.nabble.com>
References: <23847406.post@talk.nabble.com>
	<B89F0CE41D45644A97CCC93DF548C1C31ADBDFC9@GBHENXMB02.corp.amvescap.net>
	<23866615.post@talk.nabble.com>
	<B89F0CE41D45644A97CCC93DF548C1C31B02DF6C@GBHENXMB02.corp.amvescap.net>
	<23867519.post@talk.nabble.com>
	<cfdde1650906040439r16cc6389w89c4d71041d25faa@mail.gmail.com>
	<23868560.post@talk.nabble.com>
Message-ID: <23868632.post@talk.nabble.com>


ok ok I got it...........thank you so much John


RON70 wrote:
> 
> Thanks, I did following :
> 
> n = 10
> r = 4
> beta = matrix(rnorm(10*4), 4)
> Q = beta[1:r, 1:r]
> P = solve(Q)
> beta.norm = P %*% beta  # This is the normalized, according to you.
> 
> Now how can I say "beta" and "beta.norm" are indeed equivalent?
> 
> Regards,
> 
> 
> 
> 
> John C. Frain wrote:
>> 
>> Let beta be (r by n) of rank r. (r<n).  Let Q (r by r) be the first r
>> columns of this matrix. Let P = inv(Q).  Then P * pi is of the form
>> you require.  (I presume that Q is always invertible - see
>> Johansen(1995), Likelihood based inference in Cointegrated Vector
>> Autoregressive Models, Oxford.).  In the early days this method was
>> seen as one way of achieving identification of the model. Regrettably,
>> in most cases, it does not have any economic content
>> 
>> Best regards
>> 
>> John
>> 
>> 2009/6/4 RON70 <ron_michael70 at yahoo.com>:
>>>
>>> Right now I am not interested on estimation however trying to convince
>>> myself
>>> to justify this statement "normalization is always possible if variables
>>> arranged properly". I am trying to answer "why and how it is always
>>> possible?"
>>>
>>> PS: we know that having information on C.I. matrix does not improve coef
>>> estimation as rate of convergence for C.I. coef are much faster than
>>> rest.
>>>
>>>
>>>
>>> Pfaff, Bernhard Dr. wrote:
>>>>
>>>> Dear Ron,
>>>>
>>>> if I understand you correctly, you have a-priori knowledge about some
>>>> of
>>>> the CI-relations? If so, why don't you compute them in advance and then
>>>> work further? This would also reduce the dimension of your VECM.
>>>>
>>>> Best,
>>>> Bernhard
>>>>
>>>> ps: Incidentally, the returned list element 'beta' of cajorls is
>>>> computed
>>>> pretty much in sync what you have quoted, i.e., "normalization is
>>>> always
>>>> possible if variables arranged properly".
>>>>
>>>>
>>>>
>>>>>Von: r-sig-finance-bounces at stat.math.ethz.ch
>>>>>[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von RON70
>>>>>Gesendet: Donnerstag, 4. Juni 2009 11:30
>>>>>An: r-sig-finance at stat.math.ethz.ch
>>>>>Betreff: Re: [R-SIG-Finance] [R-sig-finance] A question on VECM
>>>>>
>>>>>
>>>>>Thanks Bernhard for this reply.
>>>>>
>>>>>However actually I was thinking there might be some matrix
>>>>>property for any
>>>>>rxn (rank "r") matrix to equivalently explain in a combination
>>>>>of Identity
>>>>>and rx(n-r) matrices. Is it so? Actually I got this feeling
>>>>>from a statement
>>>>>saying that, "normalization is always possible if variables arranged
>>>>>properly". Therefore suppose I have some economic theory to
>>>>>express C.I.
>>>>>vectors in original term i.e. arbitrary C.I. matrix, based on some
>>>>>economics. Then I arrange them i.e. do matrix manipulation to make C.I.
>>>>>matrix Normalized i.e. let say, I have following original C.I.
>>>>>matrix (based
>>>>>on some economics) on 10 variables :
>>>>>
>>>>>n = 10
>>>>>r = 4
>>>>>C.I.matrix = matrix(rnorm(10*4), 4)
>>>>>
>>>>>Now I want to make it (I[4], C.I.matrix.modified[4x6] )
>>>>>
>>>>>Here I am rather interested is there any R function to do this kind of
>>>>>"matrix-normalization", not so interested to get a "already
>>>>>normalized" C.I.
>>>>>matrix.
>>>>>
>>>>>Is there any?
>>>>>
>>>>>Thanks
>>>>>
>>>>>
>>>>>
>>>>>Pfaff, Bernhard Dr. wrote:
>>>>>>
>>>>>>>-----Urspr?ngliche Nachricht-----
>>>>>>>Von: r-sig-finance-bounces at stat.math.ethz.ch
>>>>>>>[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von RON70
>>>>>>>Gesendet: Mittwoch, 3. Juni 2009 10:19
>>>>>>>An: r-sig-finance at stat.math.ethz.ch
>>>>>>>Betreff: [R-SIG-Finance] [R-sig-finance] A question on VECM
>>>>>>>
>>>>>>>
>>>>>>>In my textbook, I found that for a vector error correction
>>>>>>>model, the "beta"
>>>>>>>matrix i.e. which represents the co-integrating vectors can be
>>>>>>>represented
>>>>>>>in a speacial matrix wherein first rxr partition is Identity
>>>>>>>matrix like :
>>>>>>>
>>>>>>>beta[rxn] = (I(r), beta[rx(n-r)])
>>>>>>>
>>>>>>>Is there any R function to do that representation?
>>>>>>>
>>>>>>
>>>>>> Dear Ron?
>>>>>>
>>>>>> have you considered the CRAN package 'urca' and there the function
>>>>>> cajorls()?
>>>>>>
>>>>>> library(urca)
>>>>>> example(cajorls)
>>>>>>
>>>>>> Best,
>>>>>> Bernhard
>>>>>>
>>>>>>
>>>>>>>Regards,
>>>>>>>--
>>>>>>>View this message in context:
>>>>>>>http://www.nabble.com/A-question-on-VECM-tp23847406p23847406.html
>>>>>>>Sent from the Rmetrics mailing list archive at Nabble.com.
>>>>>>>
>>>>>>>_______________________________________________
>>>>>>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>>>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>>>>-- Subscriber-posting only.
>>>>>>>-- If you want to post, subscribe first.
>>>>>>>
>>>>>> *****************************************************************
>>>>>> Confidentiality Note: The information contained in this
>>>>>...{{dropped:10}}
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>>> -- Subscriber-posting only.
>>>>>> -- If you want to post, subscribe first.
>>>>>>
>>>>>>
>>>>>
>>>>>--
>>>>>View this message in context:
>>>>>http://www.nabble.com/A-question-on-VECM-tp23847406p23866615.html
>>>>>Sent from the Rmetrics mailing list archive at Nabble.com.
>>>>>
>>>>>_______________________________________________
>>>>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>>-- Subscriber-posting only.
>>>>>-- If you want to post, subscribe first.
>>>>>
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>>
>>>
>>> --
>>> View this message in context:
>>> http://www.nabble.com/A-question-on-VECM-tp23847406p23867519.html
>>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>> 
>> 
>> 
>> -- 
>> John C Frain, Ph.D.
>> Trinity College Dublin
>> Dublin 2
>> Ireland
>> www.tcd.ie/Economics/staff/frainj/home.htm
>> mailto:frainj at tcd.ie
>> mailto:frainj at gmail.com
>> 
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>> 
> 
> 

-- 
View this message in context: http://www.nabble.com/A-question-on-VECM-tp23847406p23868632.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From anass.mouhsine at gmail.com  Thu Jun  4 15:16:43 2009
From: anass.mouhsine at gmail.com (anass)
Date: Thu, 4 Jun 2009 15:16:43 +0200
Subject: [R-SIG-Finance] [r-sig-finance] Package Quantmod: reading csv files
Message-ID: <5651742d0906040616vcb8c25cgefbbae7e5d1b044b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090604/9312adba/attachment.pl>

From ggrothendieck at gmail.com  Thu Jun  4 15:36:57 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 4 Jun 2009 09:36:57 -0400
Subject: [R-SIG-Finance] [r-sig-finance] Package Quantmod: reading csv
	files
In-Reply-To: <5651742d0906040616vcb8c25cgefbbae7e5d1b044b@mail.gmail.com>
References: <5651742d0906040616vcb8c25cgefbbae7e5d1b044b@mail.gmail.com>
Message-ID: <971536df0906040636n704c059dmfd0d337bf0f9405a@mail.gmail.com>

Try this (noting that quantmod pulls in zoo and
xts as well):

> Lines <- "Date,Open,High,Low,Close,Volume
+ 2009-01-02 09:00:00,2476,2478,2468,2469,5140
+ 2009-01-02 09:01:00,2469,2469,2459,2464,2998
+ 2009-01-02 09:02:00,2464,2464,2459,2462,1985
+ 2009-01-02 09:03:00,2462,2468,2461,2468,1694
+ 2009-01-02 09:04:00,2468,2469,2466,2467,605
+ 2009-01-02 09:05:00,2467,2468,2466,2468,996
+ 2009-01-02 09:06:00,2468,2469,2467,2469,1241"
> library(quantmod)
> z <- read.zoo(textConnection(Lines), header = TRUE, sep = ",", tz = "")
> x <- as.xts(z)
> chartSeries(x)


On Thu, Jun 4, 2009 at 9:16 AM, anass <anass.mouhsine at gmail.com> wrote:
> Hi all,
>
> I started to work on the package quantmod, but I have trouble reading simple
> csv files.
> The first lines of the file are as follows.
>
> Date,Open,High,Low,Close,Volume
> 2009-01-02 09:00:00,2476,2478,2468,2469,5140
> 2009-01-02 09:01:00,2469,2469,2459,2464,2998
> 2009-01-02 09:02:00,2464,2464,2459,2462,1985
> 2009-01-02 09:03:00,2462,2468,2461,2468,1694
> 2009-01-02 09:04:00,2468,2469,2466,2467,605
> 2009-01-02 09:05:00,2467,2468,2466,2468,996
> 2009-01-02 09:06:00,2468,2469,2467,2469,1241
>
> I read few days ago a thread on the subject, but it seems to me that the
> kinf of file I try to read is easier since the time is given in the first
> column.
>
> What I tried to do is the following:
>
>> t<-getSymbols('STXE',src='csv')
> Error in dimnames(x) <- dn :
> ?length of 'dimnames' [2] not equal to array extent
> In addition: Warning messages:
> 1: In structure(.Internal(as.POSIXct(x, tz)), class = c("POSIXt",
> "POSIXct"), ?:
> ?unknown timezone ' ET'
> 2: In structure(.Internal(as.POSIXct(x, tz)), class = c("POSIXt",
> "POSIXct"), ?:
> ?unknown timezone ' ET'
> 3: In structure(.Internal(as.POSIXct(x, tz)), class = c("POSIXt",
> "POSIXct"), ?:
> ?unknown timezone ' ET'
> 4: In structure(.Internal(as.POSIXct(x, tz)), class = c("POSIXt",
> "POSIXct"), ?:
> ?unknown timezone ' ET'
> 5: In structure(.Internal(as.POSIXct(x, tz)), class = c("POSIXt",
> "POSIXct"), ?:
> ?unknown timezone ' ET'
>
> any thoughts on the subject?
>
> Anass
> --
>
> En toda ocasion, disfruta de la vida
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From josh.m.ulrich at gmail.com  Thu Jun  4 15:59:06 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Thu, 4 Jun 2009 08:59:06 -0500
Subject: [R-SIG-Finance] [r-sig-finance] Package Quantmod: reading csv
	files
In-Reply-To: <5651742d0906040616vcb8c25cgefbbae7e5d1b044b@mail.gmail.com>
References: <5651742d0906040616vcb8c25cgefbbae7e5d1b044b@mail.gmail.com>
Message-ID: <8cca69990906040659of02d9btce821e4e050348e0@mail.gmail.com>

Anass,

getSymbols.csv() seems to expect the data to be in the format pulled
from Yahoo.  The dimnames error is because your data is missing the
"Adjusted" column.  It also seems to force the index to class "Date",
which will not allow intra-day data.

Here's a quick work-around until Jeff can comment:
> x <- read.csv("STXE.csv")
> rownames(x) <- x[,1]; x <- x[,-1]
> y <- as.xts(x)

Best,
Josh
--
http://www.fosstrading.com



On Thu, Jun 4, 2009 at 8:16 AM, anass <anass.mouhsine at gmail.com> wrote:
> Hi all,
>
> I started to work on the package quantmod, but I have trouble reading simple
> csv files.
> The first lines of the file are as follows.
>
> Date,Open,High,Low,Close,Volume
> 2009-01-02 09:00:00,2476,2478,2468,2469,5140
> 2009-01-02 09:01:00,2469,2469,2459,2464,2998
> 2009-01-02 09:02:00,2464,2464,2459,2462,1985
> 2009-01-02 09:03:00,2462,2468,2461,2468,1694
> 2009-01-02 09:04:00,2468,2469,2466,2467,605
> 2009-01-02 09:05:00,2467,2468,2466,2468,996
> 2009-01-02 09:06:00,2468,2469,2467,2469,1241
>
> I read few days ago a thread on the subject, but it seems to me that the
> kinf of file I try to read is easier since the time is given in the first
> column.
>
> What I tried to do is the following:
>
>> t<-getSymbols('STXE',src='csv')
> Error in dimnames(x) <- dn :
> ?length of 'dimnames' [2] not equal to array extent
> In addition: Warning messages:
> 1: In structure(.Internal(as.POSIXct(x, tz)), class = c("POSIXt",
> "POSIXct"), ?:
> ?unknown timezone ' ET'
> 2: In structure(.Internal(as.POSIXct(x, tz)), class = c("POSIXt",
> "POSIXct"), ?:
> ?unknown timezone ' ET'
> 3: In structure(.Internal(as.POSIXct(x, tz)), class = c("POSIXt",
> "POSIXct"), ?:
> ?unknown timezone ' ET'
> 4: In structure(.Internal(as.POSIXct(x, tz)), class = c("POSIXt",
> "POSIXct"), ?:
> ?unknown timezone ' ET'
> 5: In structure(.Internal(as.POSIXct(x, tz)), class = c("POSIXt",
> "POSIXct"), ?:
> ?unknown timezone ' ET'
>
> any thoughts on the subject?
>
> Anass
> --
>
> En toda ocasion, disfruta de la vida
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From anass.mouhsine at gmail.com  Thu Jun  4 16:04:07 2009
From: anass.mouhsine at gmail.com (anass)
Date: Thu, 4 Jun 2009 16:04:07 +0200
Subject: [R-SIG-Finance] [r-sig-finance] Package Quantmod: reading csv
	files
In-Reply-To: <8cca69990906040659of02d9btce821e4e050348e0@mail.gmail.com>
References: <5651742d0906040616vcb8c25cgefbbae7e5d1b044b@mail.gmail.com>
	<8cca69990906040659of02d9btce821e4e050348e0@mail.gmail.com>
Message-ID: <5651742d0906040704y6221533etfdba2424e7f5dff8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090604/8dc1146e/attachment.pl>

From windspeedo99 at gmail.com  Thu Jun  4 16:10:57 2009
From: windspeedo99 at gmail.com (Wind)
Date: Thu, 4 Jun 2009 22:10:57 +0800
Subject: [R-SIG-Finance] retrieving option info from IB
Message-ID: <d718c8210906040710n43a23c16w21599c658b61220a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090604/75eaf89a/attachment.pl>

From jeff.a.ryan at gmail.com  Thu Jun  4 16:28:18 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 4 Jun 2009 09:28:18 -0500
Subject: [R-SIG-Finance] retrieving option info from IB
In-Reply-To: <d718c8210906040710n43a23c16w21599c658b61220a@mail.gmail.com>
References: <d718c8210906040710n43a23c16w21599c658b61220a@mail.gmail.com>
Message-ID: <e8e755250906040728h45d2e4e7lc3861ae04fe6b91f@mail.gmail.com>

Hi Wind,

Most of IBrokers behaves 1:1 with the "official" API from Interactive
Brokers.  That is meant to provide a smaller learning curve when
coming off the official API, but at the cost of being forced to deal
with some of the quirks of the platform as well.

That said, it is *not* a simple wrapper to the API though.  Many
design decisions were made to make it more productive for an R user,
as well as more productive from a trading perspective than the
standard API.  The eWrapper structure and using R in general makes it
more productive than all the other APIs in my opinion.

With respect to the contract information, that is really hit or miss.
I'll look into creating a 'best practices' or FAQ to help pool our
collective wisdom --- maybe a simple wiki would be a good start.

For getting snapshot information, IB's interface (API) is lacking.  I
have that on a list of things to try and put together from what the
API *can* do (it can't do what you want per se), and will be looking
to incorporate that into a future release.

The newest quantmod has a function called "getOptionChain" which pulls
from Yahoo.  Obviously yahoo data caveats apply, but it is a decent
start to getting snapshots.

More documentation for IBrokers and quantmod is coming, as well as a
presentation at the upcoming Rmetrics conference in Switzerland.  For
those not signed up yet, take a look at www.rmetrics.org for more
information.

HTH,
Jeff

On Thu, Jun 4, 2009 at 9:10 AM, Wind <windspeedo99 at gmail.com> wrote:
> I have successfully retrieved price info from IB via IBrokers for stocks and
> simple options. ?The package is very efficient.Yet I could not get info on
> future option.
>
>> oc<-reqContractDetails(tws, twsOption(local="", right="",symbol="QQQQ"))
>> length(oc)
> [1] 1018
>> oc<-reqContractDetails(tws, twsOption(local="",right="",symbol="GC"))
> Error in reqContractDetails(tws, twsOption(local = "", right = "", symbol =
> "GC")) :
> ?Unable to complete ContractDetails request
>> oc<-reqContractDetails(tws,
> twsOption(local="",right="",exch="NYMEX",symbol="GC"))
> Error in reqContractDetails(tws, twsOption(local = "", right = "", exch =
> "NYMEX", ?:
> ?Unable to complete ContractDetails request
>
> By the way, how could we retrieving the option chain, ?just the current bid,
> ask, and size. ?A snap shot only.
> ?I have only find functions for data feed and historical data download.
>
> Any suggestion would be appreciated.
>
> Wind
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From windspeedo99 at gmail.com  Thu Jun  4 16:54:39 2009
From: windspeedo99 at gmail.com (Wind)
Date: Thu, 4 Jun 2009 22:54:39 +0800
Subject: [R-SIG-Finance] retrieving option info from IB
In-Reply-To: <e8e755250906040728h45d2e4e7lc3861ae04fe6b91f@mail.gmail.com>
References: <d718c8210906040710n43a23c16w21599c658b61220a@mail.gmail.com>
	<e8e755250906040728h45d2e4e7lc3861ae04fe6b91f@mail.gmail.com>
Message-ID: <d718c8210906040754s43f5debesba682770871ff0a4@mail.gmail.com>

Thanks Jeff.

On Thu, Jun 4, 2009 at 10:28 PM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
>
> Hi Wind,
>
> ........
> With respect to the contract information, that is really hit or miss.
> I'll look into creating a 'best practices' or FAQ to help pool our
> collective wisdom --- maybe a simple wiki would be a good start.

That's a great idea.

>
> The newest quantmod has a function called "getOptionChain" which pulls
> from Yahoo. ?Obviously yahoo data caveats apply, but it is a decent
> start to getting snapshots.

The only problem with yahoo data is that it only provide options on
stocks. ?Not options on futures.
By the way, I have seen people sell their matlab program with the same
function for $19.99. :)

>
> More documentation for IBrokers and quantmod is coming, as well as a
> presentation at the upcoming Rmetrics conference in Switzerland. ?For
> those not signed up yet, take a look at www.rmetrics.org for more
> information.
>
> HTH,
> Jeff
>
> On Thu, Jun 4, 2009 at 9:10 AM, Wind <windspeedo99 at gmail.com> wrote:
> > I have successfully retrieved price info from IB via IBrokers for stocks and
> > simple options. ?The package is very efficient.Yet I could not get info on
> > future option.
> >
> >> oc<-reqContractDetails(tws, twsOption(local="", right="",symbol="QQQQ"))
> >> length(oc)
> > [1] 1018
> >> oc<-reqContractDetails(tws, twsOption(local="",right="",symbol="GC"))
> > Error in reqContractDetails(tws, twsOption(local = "", right = "", symbol =
> > "GC")) :
> > ?Unable to complete ContractDetails request
> >> oc<-reqContractDetails(tws,
> > twsOption(local="",right="",exch="NYMEX",symbol="GC"))
> > Error in reqContractDetails(tws, twsOption(local = "", right = "", exch =
> > "NYMEX", ?:
> > ?Unable to complete ContractDetails request
> >
> > By the way, how could we retrieving the option chain, ?just the current bid,
> > ask, and size. ?A snap shot only.
> > ?I have only find functions for data feed and historical data download.
> >
> > Any suggestion would be appreciated.
> >
> > Wind
> >
> > ? ? ? ?[[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>
>
>
> --
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com


From jeff.a.ryan at gmail.com  Thu Jun  4 17:08:15 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 4 Jun 2009 10:08:15 -0500
Subject: [R-SIG-Finance] retrieving option info from IB
In-Reply-To: <d718c8210906040754s43f5debesba682770871ff0a4@mail.gmail.com>
References: <d718c8210906040710n43a23c16w21599c658b61220a@mail.gmail.com>
	<e8e755250906040728h45d2e4e7lc3861ae04fe6b91f@mail.gmail.com>
	<d718c8210906040754s43f5debesba682770871ff0a4@mail.gmail.com>
Message-ID: <e8e755250906040808n55b22d99jc2572bf8cb9544d@mail.gmail.com>

>
> The only problem with yahoo data is that it only provide options on
> stocks. ?Not options on futures.
> By the way, I have seen people sell their matlab program with the same
> function for $19.99. :)
>

What I get out of the R community is way more than that. :)

FWIW, I have seen the matlab API go for $299...

For generic software tools I like the "free as in beer" idea.  We are
all better off, aren't we.

Jeff


>>
>> On Thu, Jun 4, 2009 at 9:10 AM, Wind <windspeedo99 at gmail.com> wrote:
>> > I have successfully retrieved price info from IB via IBrokers for stocks and
>> > simple options. ?The package is very efficient.Yet I could not get info on
>> > future option.
>> >
>> >> oc<-reqContractDetails(tws, twsOption(local="", right="",symbol="QQQQ"))
>> >> length(oc)
>> > [1] 1018
>> >> oc<-reqContractDetails(tws, twsOption(local="",right="",symbol="GC"))
>> > Error in reqContractDetails(tws, twsOption(local = "", right = "", symbol =
>> > "GC")) :
>> > ?Unable to complete ContractDetails request
>> >> oc<-reqContractDetails(tws,
>> > twsOption(local="",right="",exch="NYMEX",symbol="GC"))
>> > Error in reqContractDetails(tws, twsOption(local = "", right = "", exch =
>> > "NYMEX", ?:
>> > ?Unable to complete ContractDetails request
>> >
>> > By the way, how could we retrieving the option chain, ?just the current bid,
>> > ask, and size. ?A snap shot only.
>> > ?I have only find functions for data feed and historical data download.
>> >
>> > Any suggestion would be appreciated.
>> >
>> > Wind
>> >
>> > ? ? ? ?[[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> > -- Subscriber-posting only.
>> > -- If you want to post, subscribe first.
>> >
>>
>>
>>
>> --
>> Jeffrey Ryan
>> jeffrey.ryan at insightalgo.com
>>
>> ia: insight algorithmics
>> www.insightalgo.com
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From singularitaet at gmx.net  Thu Jun  4 18:02:21 2009
From: singularitaet at gmx.net (Stefan Grosse)
Date: Thu, 4 Jun 2009 18:02:21 +0200
Subject: [R-SIG-Finance] determine non-linear correlation
In-Reply-To: <90598.5865.qm@web38605.mail.mud.yahoo.com>
References: <90598.5865.qm@web38605.mail.mud.yahoo.com>
Message-ID: <20090604180221.2ed7c992@gmx.net>

On Wed, 3 Jun 2009 12:15:17 -0700 (PDT) Mark Breman
<m.breman at yahoo.com> wrote:

MB> I would like to know if two financial time-series are nonlinear
MB> correlated, and if so, what that correlation function is. Is there
MB> an easy way to do this with R?

Maybe you should be more specific about what you want to do? Test for
nonlinear cointegration? If that is the case:

http://cran.r-project.org/web/views/TimeSeries.html
is a starter. 


Stefan


From windspeedo99 at gmail.com  Thu Jun  4 19:43:59 2009
From: windspeedo99 at gmail.com (Wind)
Date: Fri, 5 Jun 2009 01:43:59 +0800
Subject: [R-SIG-Finance] retrieving option info from IB
In-Reply-To: <e8e755250906040728h45d2e4e7lc3861ae04fe6b91f@mail.gmail.com>
References: <d718c8210906040710n43a23c16w21599c658b61220a@mail.gmail.com>
	<e8e755250906040728h45d2e4e7lc3861ae04fe6b91f@mail.gmail.com>
Message-ID: <d718c8210906041043s27018305k8a860e49500cb664@mail.gmail.com>

I have figured out how to get market data for future options with IBrokers.

>oc<-reqContractDetails(tws, twsContract("GC","FOP",exch="NYMEX",primary="",expiry="",
				strike="",currency="USD",right="",local="",multiplier="100",NULL,NULL,"0"))
>length(oc)
[1] 1480

>temp<-lapply(oc,function(x){if ((x$contract$strike=="1000.0") & (substr(x$contract$expiry,1,6)=="200909")) x else NULL  })
>temp1<-temp[!sapply(temp, is.null)]
>temp2<-temp1[[1]]
>reqMktData(tws, temp2$contract)

<20090605 01:39:56.089000> id=1 symbol= Volume: 41
<20090605 01:39:56.091000> id=1 symbol= highPrice: 55.5
<20090605 01:39:56.093000> id=1 symbol= lowPrice: 51.4
<20090605 01:39:56.094000> id=1 symbol= bidOption: 0.2925249666770512
0.49009769315987844
<20090605 01:39:56.097000> id=1 symbol= askOption: 0.30040373020930494
0.49204912017993013
<20090605 01:39:56.098000> id=1 symbol= lastOption: 0.2930042176111911
0.49021807490565755
<20090605 01:39:56.606000> id=1 symbol= bidOption: 0.2925249666770512 -2
<20090605 01:39:56.607000> id=1 symbol= askOption: 0.30040373020930494 -2
<20090605 01:39:56.610000> id=1 symbol= lastOption: 0.2930042176111911 -2

I guess my codes could be optimized furthur.

Now there is only one issue to be settled: how to get only the current
bid, ask and size for one time instead of the continuous data feed.



On Thu, Jun 4, 2009 at 10:28 PM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> Hi Wind,
>
> Most of IBrokers behaves 1:1 with the "official" API from Interactive
> Brokers. ?That is meant to provide a smaller learning curve when
> coming off the official API, but at the cost of being forced to deal
> with some of the quirks of the platform as well.
>
> That said, it is *not* a simple wrapper to the API though. ?Many
> design decisions were made to make it more productive for an R user,
> as well as more productive from a trading perspective than the
> standard API. ?The eWrapper structure and using R in general makes it
> more productive than all the other APIs in my opinion.
>
> With respect to the contract information, that is really hit or miss.
> I'll look into creating a 'best practices' or FAQ to help pool our
> collective wisdom --- maybe a simple wiki would be a good start.
>
> For getting snapshot information, IB's interface (API) is lacking. ?I
> have that on a list of things to try and put together from what the
> API *can* do (it can't do what you want per se), and will be looking
> to incorporate that into a future release.
>
> The newest quantmod has a function called "getOptionChain" which pulls
> from Yahoo. ?Obviously yahoo data caveats apply, but it is a decent
> start to getting snapshots.
>
> More documentation for IBrokers and quantmod is coming, as well as a
> presentation at the upcoming Rmetrics conference in Switzerland. ?For
> those not signed up yet, take a look at www.rmetrics.org for more
> information.
>
> HTH,
> Jeff
>
> On Thu, Jun 4, 2009 at 9:10 AM, Wind <windspeedo99 at gmail.com> wrote:
>> I have successfully retrieved price info from IB via IBrokers for stocks and
>> simple options. ?The package is very efficient.Yet I could not get info on
>> future option.
>>
>>> oc<-reqContractDetails(tws, twsOption(local="", right="",symbol="QQQQ"))
>>> length(oc)
>> [1] 1018
>>> oc<-reqContractDetails(tws, twsOption(local="",right="",symbol="GC"))
>> Error in reqContractDetails(tws, twsOption(local = "", right = "", symbol =
>> "GC")) :
>> ?Unable to complete ContractDetails request
>>> oc<-reqContractDetails(tws,
>> twsOption(local="",right="",exch="NYMEX",symbol="GC"))
>> Error in reqContractDetails(tws, twsOption(local = "", right = "", exch =
>> "NYMEX", ?:
>> ?Unable to complete ContractDetails request
>>
>> By the way, how could we retrieving the option chain, ?just the current bid,
>> ask, and size. ?A snap shot only.
>> ?I have only find functions for data feed and historical data download.
>>
>> Any suggestion would be appreciated.
>>
>> Wind
>>
>> ? ? ? ?[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> --
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com
>


From cevans at chyden.net  Thu Jun  4 23:34:39 2009
From: cevans at chyden.net (Charles Evans)
Date: Thu, 4 Jun 2009 17:34:39 -0400
Subject: [R-SIG-Finance] subscripting variable names?
Message-ID: <98D66769-E146-4198-8009-F81D3912B1E5@chyden.net>

Hello!

Is there any function that will let me tell R to create plot 1 and  
save it to my hard drive as plot01.png, create plot 2 and save it to  
my hard drive as plot02.png, etc.

I am running several batches of plots that I want to save for use in  
the appendices of my dissertation.  The kludge that I have come up  
with is:

for (i in 1:dim(p)[3]) {
png(width=800,height=640,units="px",bg="white")
plot(residuals(lm(p[,1,i]~n[,1,i],na.action=na.omit)),ylab=i)
}

Then, I use dev.off() to save the top graphic in the stack, rename the  
file on my computer, and repeat.

I have tried using cat() to create indexed filenames, but that did not  
work.  After a couple hours of searching online, I have not been able  
to find a solution.

Any hints would be appreciated.

Yours,

C.Evans

Capital goes where it's welcome and stays where it's well treated.
Walter B. Wriston


From brian at braverock.com  Thu Jun  4 23:42:28 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 04 Jun 2009 16:42:28 -0500
Subject: [R-SIG-Finance] subscripting variable names?
In-Reply-To: <98D66769-E146-4198-8009-F81D3912B1E5@chyden.net>
References: <98D66769-E146-4198-8009-F81D3912B1E5@chyden.net>
Message-ID: <4A283FC4.7030307@braverock.com>

1> there are several good books on R graphics. read them.

2> this is a question for r-help, please post it there after reading the 
posting guidelines

Regards,

   - Brian

Charles Evans wrote:
> Hello!
>
> Is there any function that will let me tell R to create plot 1 and 
> save it to my hard drive as plot01.png, create plot 2 and save it to 
> my hard drive as plot02.png, etc.
>
> I am running several batches of plots that I want to save for use in 
> the appendices of my dissertation.  The kludge that I have come up 
> with is:
>
> for (i in 1:dim(p)[3]) {
> png(width=800,height=640,units="px",bg="white")
> plot(residuals(lm(p[,1,i]~n[,1,i],na.action=na.omit)),ylab=i)
> }
>
> Then, I use dev.off() to save the top graphic in the stack, rename the 
> file on my computer, and repeat.
>
> I have tried using cat() to create indexed filenames, but that did not 
> work.  After a couple hours of searching online, I have not been able 
> to find a solution.
>
> Any hints would be appreciated.
>
> Yours,
>
> C.Evans
>
> Capital goes where it's welcome and stays where it's well treated.
> Walter B. Wriston
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From cedrick at cedrickjohnson.com  Fri Jun  5 03:19:39 2009
From: cedrick at cedrickjohnson.com (Cedrick Johnson)
Date: Thu, 04 Jun 2009 21:19:39 -0400
Subject: [R-SIG-Finance] Generating Monthly Returns from a ton of daily data
Message-ID: <4A2872AB.30700@cedrickjohnson.com>

Howdy-

I have a large blob timeSeries object within R full of theoretical pl 
values. The Data is in daily format and i need to somehow get daily to 
monthly and calculate the return (First and Last Day)..

Here's a sample of my dataset (is.timeSeries = TRUE):

                                           PL1      PL2         PL3
2008-05-01 12:00:00    -533    15467    -623
2008-05-02 12:00:00    -346    -5577    2363
.........
2008-05-30 12:00:00    57        27168   -7850
2008-06-02 12:00:00    1308   -7750    548
2008-06-03 12:00:00    291    20498    -435
.........
2008-06-30 12:00:00    1132   24990    -1405.5
...... this goes on until 5/27/09

So basically what I'm looking to do is calculate each month's returns 
using CalculateReturns() or returns(). In order to do that, I realized 
that i needed to take the time series and convert the daily PL returns 
to monthly, which i did by issuing the following:

Manager3.mnth = to.monthly(Managers[,3], OHLC=FALSE)

I wanted to get PL3's daily returns and then aggregate it into a monthly 
return by running it through returns()and then continue on further by 
doing table.CalendarReturns, etc..

Here's where I am stumped: When I do the to.monthly(), and i set 
OHLC=false, I get the following:


 > Manager3.mnth
GMT
                                                    Managers[, 3].Open 
Managers[, 3].High Managers[, 3].Low Managers[, 3].Close
2008-05-30 13:00:00                  17961.0                  
27879.0                 16564.5                   27879.0
2008-06-30 13:00:00                  22683.5                  
50482.5                 22683.5                   49906.5


I get a OHLC data set back.

Am I approaching this problem the wrong way? For now, I can manually get 
around this by manipulating the data in Excel to achieve monthly info, 
but I envision these datasets becoming large enough that will become a 
huge PITA...

Regards,
Cedrick


From brian at braverock.com  Fri Jun  5 03:46:55 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 04 Jun 2009 20:46:55 -0500
Subject: [R-SIG-Finance] Generating Monthly Returns from a ton of daily
 data
In-Reply-To: <4A2872AB.30700@cedrickjohnson.com>
References: <4A2872AB.30700@cedrickjohnson.com>
Message-ID: <4A28790F.40708@braverock.com>

Cedrick Johnson wrote:
> Howdy-
>
> I have a large blob timeSeries object within R full of theoretical pl 
> values. The Data is in daily format and i need to somehow get daily to 
> monthly and calculate the return (First and Last Day)..
>
> Here's a sample of my dataset (is.timeSeries = TRUE):
>
>                                           PL1      PL2         PL3
> 2008-05-01 12:00:00    -533    15467    -623
> 2008-05-02 12:00:00    -346    -5577    2363
> .........
> 2008-05-30 12:00:00    57        27168   -7850
> 2008-06-02 12:00:00    1308   -7750    548
> 2008-06-03 12:00:00    291    20498    -435
> .........
> 2008-06-30 12:00:00    1132   24990    -1405.5
> ...... this goes on until 5/27/09
>
> So basically what I'm looking to do is calculate each month's returns 
> using CalculateReturns() or returns(). In order to do that, I realized 
> that i needed to take the time series and convert the daily PL returns 
> to monthly, which i did by issuing the following:
>
> Manager3.mnth = to.monthly(Managers[,3], OHLC=FALSE)
>
> I wanted to get PL3's daily returns and then aggregate it into a 
> monthly return by running it through returns()and then continue on 
> further by doing table.CalendarReturns, etc..
>
> Here's where I am stumped: When I do the to.monthly(), and i set 
> OHLC=false, I get the following:
>
>
> > Manager3.mnth
> GMT
>                                                    Managers[, 3].Open 
> Managers[, 3].High Managers[, 3].Low Managers[, 3].Close
> 2008-05-30 13:00:00                  17961.0                  
> 27879.0                 16564.5                   27879.0
> 2008-06-30 13:00:00                  22683.5                  
> 50482.5                 22683.5                   49906.5
>
>
> I get a OHLC data set back.
>
> Am I approaching this problem the wrong way? For now, I can manually 
> get around this by manipulating the data in Excel to achieve monthly 
> info, but I envision these datasets becoming large enough that will 
> become a huge PITA...
>
> Regards,
> Cedrick
So, I think the issue here is that you don't have a price series, which 
to.monthly would help you with, but a p&l series, where each day is 
gains or losses for that day.

What you'll need to do instead is to sum() all the days in each month 
for each column, which will give you
the monthly P&L for each strategy.  Once you have monthly P&L, this can 
be converted to returns by setting a starting wealth value (probably 
your capital amount or notional value of the portfolio or strategy or 
something similar, as I described in another thread a couple weeks 
ago).  You could, of course, also get daily simple returns in a similar 
fashion, then compound them to get a monthly return.

I don't know immediately why setting OHLC=false isn't working correctly, 
but that's a different issue.  I think you need to correct the logic 
issue first, as described above.

I can see general utility from what you're trying to do, so please share 
the solution you come up with.  I'll add it to my (long) list of things 
to do as well, but I don't know when I'd get to it.  Maybe Jeff or Josh 
or Gabor (xts/quantmod/zoo) will have a quick way of doing the date 
subsetting in xts to get the sum() of all the days in each month.

Regards,

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From josh.m.ulrich at gmail.com  Fri Jun  5 04:05:10 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Thu, 4 Jun 2009 21:05:10 -0500
Subject: [R-SIG-Finance] Generating Monthly Returns from a ton of daily
	data
In-Reply-To: <4A28790F.40708@braverock.com>
References: <4A2872AB.30700@cedrickjohnson.com> <4A28790F.40708@braverock.com>
Message-ID: <8cca69990906041905w711f77caq3b0fd247402a910c@mail.gmail.com>

On Thu, Jun 4, 2009 at 8:46 PM, Brian G. Peterson <brian at braverock.com> wrote:
> Cedrick Johnson wrote:
>>
>> Howdy-
>>
>> I have a large blob timeSeries object within R full of theoretical pl
>> values. The Data is in daily format and i need to somehow get daily to
>> monthly and calculate the return (First and Last Day)..
>>
>> Here's a sample of my dataset (is.timeSeries = TRUE):
>>
>> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?PL1 ? ? ?PL2 ? ? ? ? PL3
>> 2008-05-01 12:00:00 ? ?-533 ? ?15467 ? ?-623
>> 2008-05-02 12:00:00 ? ?-346 ? ?-5577 ? ?2363
>> .........
>> 2008-05-30 12:00:00 ? ?57 ? ? ? ?27168 ? -7850
>> 2008-06-02 12:00:00 ? ?1308 ? -7750 ? ?548
>> 2008-06-03 12:00:00 ? ?291 ? ?20498 ? ?-435
>> .........
>> 2008-06-30 12:00:00 ? ?1132 ? 24990 ? ?-1405.5
>> ...... this goes on until 5/27/09
>>
>> So basically what I'm looking to do is calculate each month's returns
>> using CalculateReturns() or returns(). In order to do that, I realized that
>> i needed to take the time series and convert the daily PL returns to
>> monthly, which i did by issuing the following:
>>
>> Manager3.mnth = to.monthly(Managers[,3], OHLC=FALSE)
>>
>> I wanted to get PL3's daily returns and then aggregate it into a monthly
>> return by running it through returns()and then continue on further by doing
>> table.CalendarReturns, etc..
>>
>> Here's where I am stumped: When I do the to.monthly(), and i set
>> OHLC=false, I get the following:
>>
>>
>> > Manager3.mnth
>> GMT
>> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Managers[, 3].Open
>> Managers[, 3].High Managers[, 3].Low Managers[, 3].Close
>> 2008-05-30 13:00:00 ? ? ? ? ? ? ? ? ?17961.0 ? ? ? ? ? ? ? ? ?27879.0
>> ? ? ? ? ? ? 16564.5 ? ? ? ? ? ? ? ? ? 27879.0
>> 2008-06-30 13:00:00 ? ? ? ? ? ? ? ? ?22683.5 ? ? ? ? ? ? ? ? ?50482.5
>> ? ? ? ? ? ? 22683.5 ? ? ? ? ? ? ? ? ? 49906.5
>>
>>
>> I get a OHLC data set back.
>>
>> Am I approaching this problem the wrong way? For now, I can manually get
>> around this by manipulating the data in Excel to achieve monthly info, but I
>> envision these datasets becoming large enough that will become a huge
>> PITA...
>>
>> Regards,
>> Cedrick
>
> So, I think the issue here is that you don't have a price series, which
> to.monthly would help you with, but a p&l series, where each day is gains or
> losses for that day.
>
> What you'll need to do instead is to sum() all the days in each month for
> each column, which will give you
> the monthly P&L for each strategy. ?Once you have monthly P&L, this can be
> converted to returns by setting a starting wealth value (probably your
> capital amount or notional value of the portfolio or strategy or something
> similar, as I described in another thread a couple weeks ago). ?You could,
> of course, also get daily simple returns in a similar fashion, then compound
> them to get a monthly return.
>
> I don't know immediately why setting OHLC=false isn't working correctly, but
> that's a different issue. ?I think you need to correct the logic issue
> first, as described above.
>
> I can see general utility from what you're trying to do, so please share the
> solution you come up with. ?I'll add it to my (long) list of things to do as
> well, but I don't know when I'd get to it. ?Maybe Jeff or Josh or Gabor
> (xts/quantmod/zoo) will have a quick way of doing the date subsetting in xts
> to get the sum() of all the days in each month.
>
> Regards,
>
> ?- Brian
>
> --
> Brian G. Peterson
> http://braverock.com/brian/
> Ph: 773-459-4973
> IM: bgpbraverock
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

Here's some code that does what Brian suggested (using Gabor's style):

library(xts)

Lines <- "2008-05-01 12:00:00,-533,15467,-623
2008-05-02 12:00:00,-346,-5577,2363
2008-05-30 12:00:00,57,27168,-7850
2008-06-02 12:00:00,1308,-7750,548
2008-06-03 12:00:00,291,20498,-435
2008-06-30 12:00:00,1132,24990,-1405.5"

z <- read.zoo(textConnection(Lines),sep=",")
(m <- apply(z,2,function(x) apply.monthly(x,sum)))

HTH,
Josh
--
http://www.fosstrading.com


From cedrick at cedrickjohnson.com  Fri Jun  5 04:19:47 2009
From: cedrick at cedrickjohnson.com (Cedrick Johnson)
Date: Thu, 04 Jun 2009 22:19:47 -0400
Subject: [R-SIG-Finance] Generating Monthly Returns from a ton of daily
 data
In-Reply-To: <8cca69990906041905w711f77caq3b0fd247402a910c@mail.gmail.com>
References: <4A2872AB.30700@cedrickjohnson.com> <4A28790F.40708@braverock.com>
	<8cca69990906041905w711f77caq3b0fd247402a910c@mail.gmail.com>
Message-ID: <4A2880C3.8060408@cedrickjohnson.com>

Thanks Josh/Brian, this gave me a couple ideas - I will pick up the 
troubleshooting in the morning (the pub is calling me)... I'll share 
with the group any solution I come up with...

i have a couple ideas for getting the sum of the monthly PL, perhaps 
involving some fancy xts footwork i.e. (x['2008-05'])and adding to a 
matrix where I just have pure returns so I can use some features in 
performanceanalytics...

I'll also try the other approaches...

many thanks again,
c

Joshua Ulrich wrote:
> On Thu, Jun 4, 2009 at 8:46 PM, Brian G. Peterson <brian at braverock.com> wrote:
>   
>> Cedrick Johnson wrote:
>>     
>>> Howdy-
>>>
>>> I have a large blob timeSeries object within R full of theoretical pl
>>> values. The Data is in daily format and i need to somehow get daily to
>>> monthly and calculate the return (First and Last Day)..
>>>
>>> Here's a sample of my dataset (is.timeSeries = TRUE):
>>>
>>>                                          PL1      PL2         PL3
>>> 2008-05-01 12:00:00    -533    15467    -623
>>> 2008-05-02 12:00:00    -346    -5577    2363
>>> .........
>>> 2008-05-30 12:00:00    57        27168   -7850
>>> 2008-06-02 12:00:00    1308   -7750    548
>>> 2008-06-03 12:00:00    291    20498    -435
>>> .........
>>> 2008-06-30 12:00:00    1132   24990    -1405.5
>>> ...... this goes on until 5/27/09
>>>
>>> So basically what I'm looking to do is calculate each month's returns
>>> using CalculateReturns() or returns(). In order to do that, I realized that
>>> i needed to take the time series and convert the daily PL returns to
>>> monthly, which i did by issuing the following:
>>>
>>> Manager3.mnth = to.monthly(Managers[,3], OHLC=FALSE)
>>>
>>> I wanted to get PL3's daily returns and then aggregate it into a monthly
>>> return by running it through returns()and then continue on further by doing
>>> table.CalendarReturns, etc..
>>>
>>> Here's where I am stumped: When I do the to.monthly(), and i set
>>> OHLC=false, I get the following:
>>>
>>>
>>>       
>>>> Manager3.mnth
>>>>         
>>> GMT
>>>                                                   Managers[, 3].Open
>>> Managers[, 3].High Managers[, 3].Low Managers[, 3].Close
>>> 2008-05-30 13:00:00                  17961.0                  27879.0
>>>             16564.5                   27879.0
>>> 2008-06-30 13:00:00                  22683.5                  50482.5
>>>             22683.5                   49906.5
>>>
>>>
>>> I get a OHLC data set back.
>>>
>>> Am I approaching this problem the wrong way? For now, I can manually get
>>> around this by manipulating the data in Excel to achieve monthly info, but I
>>> envision these datasets becoming large enough that will become a huge
>>> PITA...
>>>
>>> Regards,
>>> Cedrick
>>>       
>> So, I think the issue here is that you don't have a price series, which
>> to.monthly would help you with, but a p&l series, where each day is gains or
>> losses for that day.
>>
>> What you'll need to do instead is to sum() all the days in each month for
>> each column, which will give you
>> the monthly P&L for each strategy.  Once you have monthly P&L, this can be
>> converted to returns by setting a starting wealth value (probably your
>> capital amount or notional value of the portfolio or strategy or something
>> similar, as I described in another thread a couple weeks ago).  You could,
>> of course, also get daily simple returns in a similar fashion, then compound
>> them to get a monthly return.
>>
>> I don't know immediately why setting OHLC=false isn't working correctly, but
>> that's a different issue.  I think you need to correct the logic issue
>> first, as described above.
>>
>> I can see general utility from what you're trying to do, so please share the
>> solution you come up with.  I'll add it to my (long) list of things to do as
>> well, but I don't know when I'd get to it.  Maybe Jeff or Josh or Gabor
>> (xts/quantmod/zoo) will have a quick way of doing the date subsetting in xts
>> to get the sum() of all the days in each month.
>>
>> Regards,
>>
>>  - Brian
>>
>> --
>> Brian G. Peterson
>> http://braverock.com/brian/
>> Ph: 773-459-4973
>> IM: bgpbraverock
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>     
>
> Here's some code that does what Brian suggested (using Gabor's style):
>
> library(xts)
>
> Lines <- "2008-05-01 12:00:00,-533,15467,-623
> 2008-05-02 12:00:00,-346,-5577,2363
> 2008-05-30 12:00:00,57,27168,-7850
> 2008-06-02 12:00:00,1308,-7750,548
> 2008-06-03 12:00:00,291,20498,-435
> 2008-06-30 12:00:00,1132,24990,-1405.5"
>
> z <- read.zoo(textConnection(Lines),sep=",")
> (m <- apply(z,2,function(x) apply.monthly(x,sum)))
>
> HTH,
> Josh
> --
> http://www.fosstrading.com
>


From edd at debian.org  Fri Jun  5 04:24:45 2009
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 4 Jun 2009 21:24:45 -0500
Subject: [R-SIG-Finance] Generating Monthly Returns from a ton of
	daily	data
In-Reply-To: <8cca69990906041905w711f77caq3b0fd247402a910c@mail.gmail.com>
References: <4A2872AB.30700@cedrickjohnson.com> <4A28790F.40708@braverock.com>
	<8cca69990906041905w711f77caq3b0fd247402a910c@mail.gmail.com>
Message-ID: <18984.33261.753796.923449@ron.nulle.part>


On 4 June 2009 at 21:05, Joshua Ulrich wrote:
| Here's some code that does what Brian suggested (using Gabor's style):

It's also easy to do it by hand using basic factor() usage (as I just
explained today to one of our interns :) :


##  first create some dates as a simple sequence, and some pseudo pnl 

R> randomdates <- seq(Sys.Date(), Sys.Date()+90, by=1)
R> randompnl <- zoo( seq(1, length(randomdates)), order.by=randomdates )

##  the key then is to create a factor for the desired aggregation, here by month

R> datefactor <- as.factor( format(index(randompnl), "%m") )

##  and all it takes is a call to tapply() [ or by() ] with the data, factor
##  variable and aggregation function

R> tapply( coredata(randompnl), datefactor, sum )
  06   07   08   09 
 378 1333 2294  181 


I even made it more complicated than necessary by keeping it as zoo objects
etc pp. 

Hth, Dirk

-- 
Three out of two people have difficulties with fractions.


From cedrick at cedrickjohnson.com  Fri Jun  5 05:24:09 2009
From: cedrick at cedrickjohnson.com (Cedrick Johnson)
Date: Thu, 04 Jun 2009 23:24:09 -0400
Subject: [R-SIG-Finance] Generating Monthly Returns from a ton of daily
 data
In-Reply-To: <8cca69990906041905w711f77caq3b0fd247402a910c@mail.gmail.com>
References: <4A2872AB.30700@cedrickjohnson.com> <4A28790F.40708@braverock.com>
	<8cca69990906041905w711f77caq3b0fd247402a910c@mail.gmail.com>
Message-ID: <4A288FD9.4050007@cedrickjohnson.com>

Here's a quick and dirty solution (rife with errors, but based on spot 
checks it looks like i'm in the right ballpark) that I came up with 
after doing a bit more investigation. I'm fairly close, I recall in one 
help file I was reading there was a way to properly get the first and 
last trading days of the month... I'm forsaking previous pub time due to 
my R addiction, so I'll pick this up tomorrow AM, particularly I believe 
a starting point for me to investigate should be the from and to dates 
in the example below (and how to deal with timeseries/xts/zoo and dates, 
etc.).

I ran CalculateReturns before on a mirror object that I created (x) and 
extracted the daily returns for column 3 and did the sum of returns..

 > ts = as.timeSeries(x[,3])
 > head(ts)
GMT
                  TS.1
2009-05-27 0.016053309
2009-05-26 0.005035229
2009-05-22 0.011481779
2009-05-21 0.008237344
2009-05-20 0.002171932
2009-05-19 0.004968058

 > from  = timeSequence(from="2008-05-01", length.out=13, by ="month")
 > to = from + 4*6*24*3600
 > from
GMT
 [1] [2008-05-01] [2008-06-01] [2008-07-01] [2008-08-01] [2008-09-01] 
[2008-10-01] [2008-11-01] [2008-12-01] [2009-01-01] [2009-02-01] 
[2009-03-01] [2009-04-01] [2009-05-01]

 > to
GMT
 [1] [2008-05-25] [2008-06-25] [2008-07-25] [2008-08-25] [2008-09-25] 
[2008-10-25] [2008-11-25] [2008-12-25] [2009-01-25] [2009-02-25] 
[2009-03-25] [2009-04-25] [2009-05-25]

 > test = applySeries(ts, from, to, FUN=sum)
 > timeSeries(test, to)
GMT
                  TS.1
2008-05-25  0.18501584
2008-06-25  0.17309887
2008-07-25  0.09630221
2008-08-25  0.04597184
2008-09-25  0.10668020
2008-10-25  0.14313745
2008-11-25  0.10458655
2008-12-25 -0.04078307
2009-01-25 -0.04049024
2009-02-25  0.12576100
2009-03-25  0.13219515
2009-04-25  0.03148875
2009-05-25  0.02194375

 > ts1 = timeSeries(test, to)
 > table.CalendarReturns(ts1)
     Jan  Feb  Mar Apr  May  Jun Jul Aug  Sep  Oct  Nov  Dec  TS.1
2008  NA   NA   NA  NA 18.5 17.3 9.6 4.6 10.7 14.3 10.5 -4.1 113.7
2009  -4 12.6 13.2 3.1  2.2   NA  NA  NA   NA   NA   NA   NA  28.9


PS: Dirk, just saw the reply in my inbox.. thanks, i'll investigate that 
method as well

-c


From binabina at bellsouth.net  Fri Jun  5 14:53:40 2009
From: binabina at bellsouth.net (zubin)
Date: Fri, 05 Jun 2009 08:53:40 -0400
Subject: [R-SIG-Finance] Interactive Broker API
Message-ID: <4A291554.5020107@bellsouth.net>

Hello, quick question - does the Interactive Broker API provide the 
ability to:

a) execute a simultaneous trade of a couple of stocks and options? 

b) ability to receive REAL TIME data from a few hundred stocks and 
option chains to monitor for a trade?

if not, are there brokers that support this type of trading?

-zubin


From jeff.a.ryan at gmail.com  Fri Jun  5 16:29:21 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 5 Jun 2009 09:29:21 -0500
Subject: [R-SIG-Finance] Interactive Broker API
In-Reply-To: <4A291554.5020107@bellsouth.net>
References: <4A291554.5020107@bellsouth.net>
Message-ID: <e8e755250906050729x75ca7d9cqafde3a1b7ac0fda5@mail.gmail.com>

This is less an R question, and more of an IB question.  In general
you'll always need to the research yourself to find the best answer,
or construct a very detailed (with examples) question for this list to
get useful help.

That said, the IBrokers *R* API to the IB API is relatively complete.
Meaning you can pull RT data and execute orders.  Of course the caveat
coming that you are playing with your money and not mine, and with the
full understanding that there is no support provided or warranty
implied for the R API, or the TWS API for that matter.

To answer you questions:

IB supports many different order types as well as the ability to pull
concurrent symbols. I'd suggest you start looking at the documentation
for the details:

http://www.interactivebrokers.com/en/p.php?f=api&p=b&ib_entity=llc

The IBrokers documentation (which would have answered some of your
questions had you looked) is also useful.

A MAJOR caveat to IB is that they do not claim to be a data vendor.
Their data is aggregated in 1/5 or 1/3 sec. increments, so not truly
tick-level.  Of course, most people are not _really_ needing that
[tick-level] granularity from a _retail_ product, though it always
sounds cool to say you do. :)

IB does (on Windows) offer eSignal as a data supplier -- I don't have
personal experience on this though, but I suspect the data from the
API would then be 'eSignal' data instead of IB.  Again a question for
IB/TWS/API forums, not here.

You can now (with IBrokers and R) pull data from a secondary source
(of your own API/etc) and use IB to execute.  This is probably the
smartest route if you require top-notch feeds.  Coding this isn't
entirely trivial, but is easy enough.  I may (or may not) be making a
DTN interface available, though the structure of the terms on that
from DTN make open-source not possible.  Contact me off list if you
are interested.

An additional point on the data: large quantities of data requires
large bandwidth, which may cost more in hardware and infrastructure
than you are expecting.

HTH
Jeff

On Fri, Jun 5, 2009 at 7:53 AM, zubin<binabina at bellsouth.net> wrote:
> Hello, quick question - does the Interactive Broker API provide the ability
> to:
>
> a) execute a simultaneous trade of a couple of stocks and options?
> b) ability to receive REAL TIME data from a few hundred stocks and option
> chains to monitor for a trade?
>
> if not, are there brokers that support this type of trading?
>
> -zubin
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From wuertz at itp.phys.ethz.ch  Fri Jun  5 16:33:55 2009
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Fri, 05 Jun 2009 16:33:55 +0200
Subject: [R-SIG-Finance] R/Rmetrics ebook and Meielisalp Workshop
Message-ID: <4A292CD3.5020905@itp.phys.ethz.ch>

Dear Mailing List,

We are proud to present our first Rmetrics ebook, entitled 'Portfolio 
Optimization with R/Rmetrics'. It is intended as a reference and user 
guide for the Rmetrics packages for portfolio optimization. For more 
information please visit: http://www.rmetrics.org/ebook.htm.

We would also like to remind everyone that the 3rd R/Rmetrics User and 
Developer Workshop in Computational Finance and Financial Engineering 
(http://www.rmetrics.org/meielisalp.htm) is taking place from June 28th 
- July 2nd, 2009 at Meielisalp, Lake Thune in Switzerland. If you are 
interested, you can still register at 
https://www.rmetrics.org/meielisalp-form.htm.

Kind regards,
Diethelm, Yohan and Andrew


From josh.m.ulrich at gmail.com  Fri Jun  5 16:42:33 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Fri, 5 Jun 2009 09:42:33 -0500
Subject: [R-SIG-Finance] Interactive Broker API
In-Reply-To: <4A291554.5020107@bellsouth.net>
References: <4A291554.5020107@bellsouth.net>
Message-ID: <8cca69990906050742n68d11301j4ff891afd112a4e9@mail.gmail.com>

On Fri, Jun 5, 2009 at 7:53 AM, zubin<binabina at bellsouth.net> wrote:
> Hello, quick question - does the Interactive Broker API provide the ability
> to:
>
> a) execute a simultaneous trade of a couple of stocks and options?
> b) ability to receive REAL TIME data from a few hundred stocks and option
> chains to monitor for a trade?
>
If you generate <= $800 in monthly commissions you will only have 100
simultaneous real-time data feeds available.  For every $8 above that
amount you will have access to 1 additional line of market data.  You
cannot purchase additional lines; you can only get them via the method
above.

> if not, are there brokers that support this type of trading?
>
> -zubin
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

Best,
Josh
--
http://www.fosstrading.com


From herrold at owlriver.com  Fri Jun  5 16:58:54 2009
From: herrold at owlriver.com (R P Herrold)
Date: Fri, 5 Jun 2009 10:58:54 -0400 (EDT)
Subject: [R-SIG-Finance] Interactive Broker API
In-Reply-To: <8cca69990906050742n68d11301j4ff891afd112a4e9@mail.gmail.com>
References: <4A291554.5020107@bellsouth.net>
	<8cca69990906050742n68d11301j4ff891afd112a4e9@mail.gmail.com>
Message-ID: <alpine.LRH.2.00.0906051056520.15431@arj.bjyevire.pbz>

On Fri, 5 Jun 2009, Joshua Ulrich wrote:

> If you generate <= $800 in monthly commissions you will only have 100
> simultaneous real-time data feeds available.  For every $8 above that

Please ... can we stop this thread here? .. this is not 
accurate, and it is out of scope to R.

As to the nature of the inaccuracy, please feel free to 
contact me offlist for a correction.  I have no affiliation 
with IB, other than developing toward and using its API.

thanks

-- Russ herrold


From josh.m.ulrich at gmail.com  Fri Jun  5 17:01:06 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Fri, 5 Jun 2009 10:01:06 -0500
Subject: [R-SIG-Finance] Interactive Broker API
In-Reply-To: <alpine.LRH.2.00.0906051056520.15431@arj.bjyevire.pbz>
References: <4A291554.5020107@bellsouth.net>
	<8cca69990906050742n68d11301j4ff891afd112a4e9@mail.gmail.com>
	<alpine.LRH.2.00.0906051056520.15431@arj.bjyevire.pbz>
Message-ID: <8cca69990906050801u5110faddl9edd7c8dd1b0af64@mail.gmail.com>

On Fri, Jun 5, 2009 at 9:58 AM, R P Herrold<herrold at owlriver.com> wrote:
> On Fri, 5 Jun 2009, Joshua Ulrich wrote:
>
>> If you generate <= $800 in monthly commissions you will only have 100
>> simultaneous real-time data feeds available. ?For every $8 above that
>
> Please ... can we stop this thread here? .. this is not accurate, and it is
> out of scope to R.
>
> As to the nature of the inaccuracy, please feel free to contact me offlist
> for a correction. ?I have no affiliation with IB, other than developing
> toward and using its API.
>
> thanks
>
> -- Russ herrold
>

Russ,

I would appreciate your correction, since that information can
directly from IB support.

Best,
Josh
--
http://www.fosstrading.com


From herrold at owlriver.com  Fri Jun  5 17:46:47 2009
From: herrold at owlriver.com (R P Herrold)
Date: Fri, 5 Jun 2009 11:46:47 -0400 (EDT)
Subject: [R-SIG-Finance] Interactive Broker API
In-Reply-To: <8cca69990906050801u5110faddl9edd7c8dd1b0af64@mail.gmail.com>
References: <4A291554.5020107@bellsouth.net>
	<8cca69990906050742n68d11301j4ff891afd112a4e9@mail.gmail.com>
	<alpine.LRH.2.00.0906051056520.15431@arj.bjyevire.pbz>
	<8cca69990906050801u5110faddl9edd7c8dd1b0af64@mail.gmail.com>
Message-ID: <alpine.LRH.2.00.0906051145540.16684@arj.bjyevire.pbz>

On Fri, 5 Jun 2009, Joshua Ulrich wrote:

>> herrold: 
>> As to the nature of the inaccuracy, please feel free to contact me offlist

> I would appreciate your correction, since that information can
> directly from IB support.

answered offlist

- Russ herrold


From edd at debian.org  Fri Jun  5 18:14:11 2009
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 5 Jun 2009 11:14:11 -0500
Subject: [R-SIG-Finance] Interactive Broker API
In-Reply-To: <alpine.LRH.2.00.0906051145540.16684@arj.bjyevire.pbz>
References: <4A291554.5020107@bellsouth.net>
	<8cca69990906050742n68d11301j4ff891afd112a4e9@mail.gmail.com>
	<alpine.LRH.2.00.0906051056520.15431@arj.bjyevire.pbz>
	<8cca69990906050801u5110faddl9edd7c8dd1b0af64@mail.gmail.com>
	<alpine.LRH.2.00.0906051145540.16684@arj.bjyevire.pbz>
Message-ID: <18985.17491.275235.100309@ron.nulle.part>


On 5 June 2009 at 11:46, R P Herrold wrote:
| On Fri, 5 Jun 2009, Joshua Ulrich wrote:
| 
| >> herrold: 
| >> As to the nature of the inaccuracy, please feel free to contact me offlist
| 
| > I would appreciate your correction, since that information can
| > directly from IB support.
| 
| answered offlist

I consider this to action be against the spirit of this mailing list.  Folks
tend to ask questions in public, and it is good etiquette to respond (or
summarize) publically.

Everything else is leeching. There are enough other places excelling in
that. We really don't need to copy that here.

Dirk

-- 
Three out of two people have difficulties with fractions.


From matthieu.stigler at gmail.com  Fri Jun  5 19:54:27 2009
From: matthieu.stigler at gmail.com (Matthieu Stigler)
Date: Fri, 05 Jun 2009 19:54:27 +0200
Subject: [R-SIG-Finance] determine non-linear correlation
In-Reply-To: <20090604180221.2ed7c992@gmx.net>
References: <90598.5865.qm@web38605.mail.mud.yahoo.com>
	<20090604180221.2ed7c992@gmx.net>
Message-ID: <4A295BD3.80603@gmail.com>

Well the term nonlinear is always a little bit misleading as it can 
include really different alternatives! Can you provide a reference to 
the thesis you were mentioning?

I think one of the question you have to ask for first is whether your 
variables are stationary or not.

If they are, I would try to include in a regression the nonlinearity you 
suspect, and then interpreting individual coefficients or the Rsquared 
as nonlinear correlation. I found actually a similar answer on a similar 
question on:
https://stat.ethz.ch/pipermail/r-help/2008-March/156284.html Note that 
you would maybe have to use HAC covariance estimators if you want to 
make some inference, as you are dealing with time-series, see package 
sandwich.

If the variables are not stationary and I(1), you could indeed check for 
nonlinear cointegration. This is possible in the dev version of package 
tsdyn who allows to estimate and test for threshold cointegration (btw, 
I will make a presentation on this subject at userR 2009). Other types 
of nonlinear cointegration are to my knowledge not implemented in R. You 
can find much literature on smooth transition cointegration, and a 
general treatment is done in Park, Joon Y & Phillips, Peter C B, 2001. 
"Nonlinear Regressions with Integrated Time Series," Econometrica, vol. 
69(1), pages 117-61,

Matthieu

Stefan Grosse a ?crit :
> On Wed, 3 Jun 2009 12:15:17 -0700 (PDT) Mark Breman
> <m.breman at yahoo.com> wrote:
>
> MB> I would like to know if two financial time-series are nonlinear
> MB> correlated, and if so, what that correlation function is. Is there
> MB> an easy way to do this with R?
>
> Maybe you should be more specific about what you want to do? Test for
> nonlinear cointegration? If that is the case:
>
> http://cran.r-project.org/web/views/TimeSeries.html
> is a starter. 
>
>
> Stefan
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From m.breman at yahoo.com  Fri Jun  5 21:07:24 2009
From: m.breman at yahoo.com (Mark Breman)
Date: Fri, 5 Jun 2009 12:07:24 -0700 (PDT)
Subject: [R-SIG-Finance] determine non-linear correlation
In-Reply-To: <147373.60825.qm@web110015.mail.gq1.yahoo.com>
References: <90598.5865.qm@web38605.mail.mud.yahoo.com>
	<147373.60825.qm@web110015.mail.gq1.yahoo.com>
Message-ID: <294235.27726.qm@web38602.mail.mud.yahoo.com>

Here is the thesis I was talking about (see attached pdf file)

I am working on an illustration (example) of the problem I'm trying to solve. I will post it to this group this weekend...

Regards,

-Mark- 




________________________________
From: KAUSHIK BHATTACHARJEE <kabonline07 at yahoo.com>
To: Mark Breman <m.breman at yahoo.com>
Sent: Friday, June 5, 2009 1:18:55 PM
Subject: Re: [R-SIG-Finance] determine non-linear correlation


correlation by definition is a linear measure of assocation. So what is the presice definition of non-linear correlation?
can you give the thesis......I am curious to know....
 Kaushik Bhattacharjee




________________________________
 From: Mark Breman <m.breman at yahoo.com>
To: r-sig-finance at stat.math.ethz.ch
Sent: Thursday, June 4, 2009 12:45:17 AM
Subject: [R-SIG-Finance] determine non-linear correlation

I would like to know if two financial time-series are nonlinear correlated, and if so, what that correlation function is.
Is there an easy way to do this with R?

I have read a thesis about the "high order correlation coefficient to solve the nonlinear correlation problem" but I'm not able to translate this into a solution for my problem. All these statistics are very interesting but also challenging for me...  

Kind regards,

-Mark-


      
    [[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090605/2c209309/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 20001203.pdf
Type: application/pdf
Size: 345537 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090605/2c209309/attachment.pdf>

From brian at braverock.com  Sat Jun  6 01:59:29 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 05 Jun 2009 18:59:29 -0500
Subject: [R-SIG-Finance] Interactive Broker API
In-Reply-To: <e8e755250906050729x75ca7d9cqafde3a1b7ac0fda5@mail.gmail.com>
References: <4A291554.5020107@bellsouth.net>
	<e8e755250906050729x75ca7d9cqafde3a1b7ac0fda5@mail.gmail.com>
Message-ID: <4A29B161.3070801@braverock.com>

Jeff Ryan wrote:
> You can now (with IBrokers and R) pull data from a secondary source
> (of your own API/etc) and use IB to execute.  This is probably the
> smartest route if you require top-notch feeds.  Coding this isn't
> entirely trivial, but is easy enough.  I may (or may not) be making a
> DTN interface available, though the structure of the terms on that
> from DTN make open-source not possible.  Contact me off list if you
> are interested.
>   

Of course, I am not a lawyer, but at least in the United States, courts 
have repeatedly held that API's for a commercial software product cannot 
be held to be "confidential", and that writing your own code to 
interface to those API's is legal.  Thus Samba for connecting to 
windows-style file shares.  As such, an open source connector to the DTN 
API would likely violate no law in the United States.

Regards,

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From jeff.a.ryan at gmail.com  Sat Jun  6 04:32:07 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 5 Jun 2009 21:32:07 -0500
Subject: [R-SIG-Finance] Interactive Broker API
In-Reply-To: <4A29B161.3070801@braverock.com>
References: <4A291554.5020107@bellsouth.net>
	<e8e755250906050729x75ca7d9cqafde3a1b7ac0fda5@mail.gmail.com>
	<4A29B161.3070801@braverock.com>
Message-ID: <e8e755250906051932u778eae15t1bfc31c639040775@mail.gmail.com>

The OSS issue:

>> You can now (with IBrokers and R) pull data from a secondary source
>> (of your own API/etc) and use IB to execute. ?This is probably the
>> smartest route if you require top-notch feeds. ?Coding this isn't
>> entirely trivial, but is easy enough. ?I may (or may not) be making a
>> DTN interface available, though the structure of the terms on that
>> from DTN make open-source not possible. ?Contact me off list if you
>> are interested.

[Very off topic...]

The problem isn't in making it open-source per se.  That I would tend
to agree is OK.  The problem is that DTN requires a API developer
subscription at $x per year, plus they need to _approve_ all API
before use.  This is to protect the integrity of their service I
suspect, as well as the other customers they have.

Essentially the R code would be available, but unusable without this
developer registration 'key'.  If one distributes the key (which
likely is not to be made "public" per the developer agreement --- so
this is moot) then others could use it.

So the basic code could be GPL'd (most likely) but to use it you would
need to pay a developer fee.  It is more economical for each user to
pay one developer to use his code (which is OK --- that is the model
they are employing), than for each to pay the full fee.

A small example of divergence between OSS theory and practice. :)

>>
>
> Of course, I am not a lawyer, but at least in the United States, courts have
> repeatedly held that API's for a commercial software product cannot be held
> to be "confidential", and that writing your own code to interface to those
> API's is legal. ?Thus Samba for connecting to windows-style file shares. ?As
> such, an open source connector to the DTN API would likely violate no law in
> the United States.
>
> Regards,
>
> ?- Brian
>
> --
> Brian G. Peterson
> http://braverock.com/brian/
> Ph: 773-459-4973
> IM: bgpbraverock
>
>
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From m.breman at yahoo.com  Sat Jun  6 04:33:06 2009
From: m.breman at yahoo.com (Mark Breman)
Date: Fri, 5 Jun 2009 19:33:06 -0700 (PDT)
Subject: [R-SIG-Finance] determine non-linear correlation
In-Reply-To: <147373.60825.qm@web110015.mail.gq1.yahoo.com>
References: <90598.5865.qm@web38605.mail.mud.yahoo.com>
	<147373.60825.qm@web110015.mail.gq1.yahoo.com>
Message-ID: <693805.62072.qm@web38605.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090605/247cbdc5/attachment.pl>

From rbali at ufmg.br  Sat Jun  6 05:14:51 2009
From: rbali at ufmg.br (Robert Iquiapaza)
Date: Sat, 6 Jun 2009 00:14:51 -0300
Subject: [R-SIG-Finance] determine non-linear correlation
In-Reply-To: <294235.27726.qm@web38602.mail.mud.yahoo.com>
References: <90598.5865.qm@web38605.mail.mud.yahoo.com><147373.60825.qm@web110015.mail.gq1.yahoo.com>
	<294235.27726.qm@web38602.mail.mud.yahoo.com>
Message-ID: <68188D3311E24FF283A31A1EC1729008@DellPC>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090606/16c1ce1a/attachment.pl>

From m.breman at yahoo.com  Sat Jun  6 12:29:57 2009
From: m.breman at yahoo.com (Mark Breman)
Date: Sat, 6 Jun 2009 03:29:57 -0700 (PDT)
Subject: [R-SIG-Finance] determine non-linear correlation
In-Reply-To: <68188D3311E24FF283A31A1EC1729008@DellPC>
References: <90598.5865.qm@web38605.mail.mud.yahoo.com><147373.60825.qm@web110015.mail.gq1.yahoo.com>
	<294235.27726.qm@web38602.mail.mud.yahoo.com>
	<68188D3311E24FF283A31A1EC1729008@DellPC>
Message-ID: <281400.14352.qm@web38602.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090606/79ae432e/attachment.pl>

From m.breman at yahoo.com  Sat Jun  6 12:29:57 2009
From: m.breman at yahoo.com (Mark Breman)
Date: Sat, 6 Jun 2009 03:29:57 -0700 (PDT)
Subject: [R-SIG-Finance] determine non-linear correlation
In-Reply-To: <68188D3311E24FF283A31A1EC1729008@DellPC>
References: <90598.5865.qm@web38605.mail.mud.yahoo.com><147373.60825.qm@web110015.mail.gq1.yahoo.com>
	<294235.27726.qm@web38602.mail.mud.yahoo.com>
	<68188D3311E24FF283A31A1EC1729008@DellPC>
Message-ID: <281400.14352.qm@web38602.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090606/79ae432e/attachment-0001.pl>

From junkmanu at free.fr  Sat Jun  6 15:21:44 2009
From: junkmanu at free.fr (manulemalin)
Date: Sat, 6 Jun 2009 06:21:44 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] determine non-linear correlation
In-Reply-To: <693805.62072.qm@web38605.mail.mud.yahoo.com>
References: <90598.5865.qm@web38605.mail.mud.yahoo.com>
	<693805.62072.qm@web38605.mail.mud.yahoo.com>
Message-ID: <23901855.post@talk.nabble.com>


things are not that easy
in your example, your data are non stationnary

( it should be clearer if you have a look at the unit roots of your model(s)
)

as already said, having a look at cointegration can be a good start

moreover, ( as already said!), there is not ONE function to determine
nonlinearity; it's up to you to determine, and test, the kind of your
nonlinearities
if you have got no idea / your nonlinearities, you can use nonparametric
modelling, which can work, for instance with neural networks, as a black box
( which can also be misleading...)

hope it helps

regards

manu





Mark Breman-2 wrote:
> 
> Here is the thesis I was talking about:
> www.jos.org.cn/ch/reader/download_pdf.aspx?file_no=20001203&year_id=2000&quarter_id=12&falg=1
> 
> I did some experiments based on the replies to my post which pointed me in
> the right direction (thank you for that):
> 
> let's say I have a timeseries a:
> 
> 
>> a
>            priceA
> 2009-06-01      1
> 2009-06-02      2
> 2009-06-03      3
> 2009-06-04      4
> 
> And a timesieries b:
> 
>> b
>            priceB
> 2009-06-01     11
> 2009-06-02     12
> 2009-06-03     13
> 2009-06-04     14
> 
> priceB is derived from priceA by the simple linear function priceB = 10 +
> priceA.
> As expected all correlation methods agree that timeseries B is highly
> correlated to A:
> 
>> cor(a,b, method="pearson")
>        priceB
> priceA      1
>> cor(a,b, method="kendall")
>        priceB
> priceA      1
>> cor(a,b, method="spearman")
>        priceB
> priceA      1
> 
> Now suppose I have another timeseries F:
> 
>> f
>            priceF
> 2009-06-01      9
> 2009-06-02     24
> 2009-06-03     45
> 2009-06-04     72
> 
> priceF is derived from priceA by the nonlinear function: 3*(priceA^2) +
> 6*priceA.
> If I'm correct than this is known a "second order kwadratic function"
> which is not a linear function. (If you would draw it in a graph it would
> be shown as a parabolic line)
> 
> Now let's look at the different correlation methods for timeseries A and
> F:
> 
>> cor(a,f, method="pearson")
>           priceF
> priceA 0.9919354
>> cor(a,f, method="kendall")
>        priceF
> priceA      1
>> cor(a,f, method="spearman")
>        priceF
> priceA      1
> 
> Note that Pearson obviously does not know about the nonlinear function
> between priceA and priceF as it does not reports a correlation of 1.
> Kendall and Spearman on the other hand seem to know about the nonlinear
> function as they report a correlation of 1 between A and F.
> So my conclusion (correct me if I am wrong): kendall and spearman can
> handle nonlinear functions as the basis for the corellation, pearson can
> not. (one of you already pointed this out in a reply I think)
> 
> I'm still left with one questions thought:
> Is it possible (with R) to obtain the function that forms the basis for
> the correlation as reported by kendall and spearman? I mean: if I have two
> real timeseries which are highly correlated according to kendall and
> spearman (i.e. cor(x,y) = 1) I would like to know the (nonlinear) function
> that form the basis for the correlation.
> 
> Regards,
> 
> -Mark- 
> 
> 
> 
> 
>  
> 
> ________________________________
> 
> 
> Sent: Friday, June 5, 2009 1:18:55 PM
> Subject: Re: [R-SIG-Finance] determine non-linear correlation
> 
> 
> correlation by definition is a linear measure of assocation. So what is
> the presice definition of non-linear correlation?
> can you give the thesis......I am curious to know....
>  Kaushik Bhattacharjee
> 
> 
> 
> 
> ________________________________
> 
> To: r-sig-finance at stat.math.ethz.ch
> Sent: Thursday, June 4, 2009 12:45:17 AM
> Subject: [R-SIG-Finance] determine non-linear correlation
> 
> I would like to know if two financial time-series are nonlinear
> correlated, and if so, what that correlation function is.
> Is there an easy way to do this with R?
> 
> I have read a thesis about the "high order correlation coefficient to
> solve the nonlinear correlation problem" but I'm not able to translate
> this into a solution for my problem. All these statistics are very
> interesting but also challenging for me...  
> 
> Kind regards,
> 
> -Mark-
> 
> 
>       
>     [[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 
>       
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/determine-non-linear-correlation-tp23857603p23901855.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From d2009_ at live.com.pt  Sun Jun  7 15:19:54 2009
From: d2009_ at live.com.pt (Daniel Mail)
Date: Sun, 7 Jun 2009 14:19:54 +0100
Subject: [R-SIG-Finance] Fix ARMA parameters in garchfit
Message-ID: <BAY0-DP1-19471C544C0F1E98DAEEDBB9460@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090607/414aef89/attachment.pl>

From cwrward at gmail.com  Sun Jun  7 22:34:04 2009
From: cwrward at gmail.com (Charles Ward)
Date: Sun, 7 Jun 2009 21:34:04 +0100
Subject: [R-SIG-Finance] fPortfolio and R/Rmetrics
Message-ID: <bd9aa36b0906071334h40c15b7ds2f82d59ab9153657@mail.gmail.com>

Could someone please suggest ways of improving the following two
functions for use in fPortfolio?

(fPortfolio is an extremely powerful and comprehensive package for
constructing and testing portfolios.See the Rmetrics ebook, entitled
'Portfolio Optimization with R/Rmetrics'.
http://www.rmetrics.org/ebook.htm)
The first function aims to replace the matrix of returns used in
running fPortfolio by a matrix of returns in which the mean of each
asset is equal to a forecast specifed by the user.  It works but is
incredibly slow, mainly I think because of the loops used.

?forec=function(x,forc)
{
newx=x
if (length(forc)!=ncol(x)) stop("Forecast columns do not match up with data")
cm=colMeans(x)
nc=ncol(x)
nr=nrow(x)
for (j in 1:nc){for (i in 1:nr)
newx[i,j]=x[i,j]-cm[j]+forc[j]}
newx
}

#Used as in this example:
library(fPortfolio)
lppAssets=100*LPP2005.RET[,c("SBI","SPI","LMI","MPI")]
forc=c(.1,.2,.1,.2)
newAss=forec(lppAssets,forc)
colMeans(lppAssets)
colMeans(newAss)

The second addresses a problem which is common in real estate asset
returns which arises because of the way in which the returns are
generated (through appraisals rather than through market prices). The
function is designed to "de-smooth" the appraisal-based returns. For
references to the problem see, the summary of a report for the
Investment Property Forum in the UK
https://members.ipf.org.uk/membersarealive/downloads/listings1.asp?pid=292

or google "desmoothing real estate returns"

The function requires a column of asset returns  and effectively
removes the AR(1) component.The alpha is the AR(1) coefficient but
users can enter a lower coefficient if preferred for reasons that are
discussed in the literature!
Again improvements would be welcome.e.g reading in a matrix rather
than a single column and specifying say a standard deviation rather
than the alpha.
Thank you for any comments
Charles Ward

desmooth<-function(x1,alpha=0,plot=TRUE,print=TRUE)
{
x1=ts(x1)
x2=x1
tab01=matrix(ncol=4,nrow=4)
rnd<-function(x) {x=round(100000*x)/100000;x}
r1=pacf(x1,plot=FALSE)
if (alpha==0) alpha=r1$ac[1]
for (i in 2:length(x1)) x2[i]=(x1[i]-alpha*x1[i-1])/(1-alpha)
if (plot==TRUE) {plot(x2,col="red",type="l");lines(x1,col="black");
	legend("topleft",c("Original","De-smoothed"), col=1:5,lty=1:1)}
if (print==TRUE) {
	tab01[1,]=c("", "Original","De-Smoothed","Alpha");
	tab01[2,1]="Mean";
	tab01[3,1]="Std. Dev.";
	tab01[2,2]=rnd(mean(x1));tab01[2,3]=rnd(mean(x2));
	tab01[3,2]=rnd(sd(x1));tab01[3,3]=rnd(sd(x2));
	tab01[2,4]="";	
	tab01[3,4]=rnd(alpha);
	dimnames(tab01)<-list(rep("",3),rep("",4));
	print(tab01,quote=FALSE)}
	x2
}

Used as in this example

library(fPortfolio)
PropInd=c(.0093,.0085,.009,.0098,.0084,.01294,.0136,.0165,.0235,.0150,.0178,.0131,
.0233,.0156,.0258,.026,.0284,.0261,.0230,.0235
 ,.0209,.0218,.0241,.0152,.0161)
DesPropInd=desmooth(PropInd)
#or to take a less Efficient Market line
DesPropInd=desmooth(PropInd,alpha=0.4)


From ggrothendieck at gmail.com  Sun Jun  7 23:03:48 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 7 Jun 2009 17:03:48 -0400
Subject: [R-SIG-Finance] fPortfolio and R/Rmetrics
In-Reply-To: <bd9aa36b0906071334h40c15b7ds2f82d59ab9153657@mail.gmail.com>
References: <bd9aa36b0906071334h40c15b7ds2f82d59ab9153657@mail.gmail.com>
Message-ID: <971536df0906071403j2a9eb646t2cfa19f37d7cfb52@mail.gmail.com>

On Sun, Jun 7, 2009 at 4:34 PM, Charles Ward<cwrward at gmail.com> wrote:
> Could someone please suggest ways of improving the following two
> functions for use in fPortfolio?
>
> (fPortfolio is an extremely powerful and comprehensive package for
> constructing and testing portfolios.See the Rmetrics ebook, entitled
> 'Portfolio Optimization with R/Rmetrics'.
> http://www.rmetrics.org/ebook.htm)
> The first function aims to replace the matrix of returns used in
> running fPortfolio by a matrix of returns in which the mean of each
> asset is equal to a forecast specifed by the user. ?It works but is
> incredibly slow, mainly I think because of the loops used.
>
> ?forec=function(x,forc)
> {
> newx=x
> if (length(forc)!=ncol(x)) stop("Forecast columns do not match up with data")
> cm=colMeans(x)
> nc=ncol(x)
> nr=nrow(x)
> for (j in 1:nc){for (i in 1:nr)
> newx[i,j]=x[i,j]-cm[j]+forc[j]}
> newx
> }

 sweep(x, 2, colMeans(x) - forc, "-")

>
> #Used as in this example:
> library(fPortfolio)
> lppAssets=100*LPP2005.RET[,c("SBI","SPI","LMI","MPI")]
> forc=c(.1,.2,.1,.2)
> newAss=forec(lppAssets,forc)
> colMeans(lppAssets)
> colMeans(newAss)

Try this:

colMeans(sweep(lppAssets, 2, colMeans(lppAssets) - forc, "-"))

>
> The second addresses a problem which is common in real estate asset
> returns which arises because of the way in which the returns are
> generated (through appraisals rather than through market prices). The
> function is designed to "de-smooth" the appraisal-based returns. For
> references to the problem see, the summary of a report for the
> Investment Property Forum in the UK
> https://members.ipf.org.uk/membersarealive/downloads/listings1.asp?pid=292
>
> or google "desmoothing real estate returns"
>
> The function requires a column of asset returns ?and effectively
> removes the AR(1) component.The alpha is the AR(1) coefficient but
> users can enter a lower coefficient if preferred for reasons that are
> discussed in the literature!
> Again improvements would be welcome.e.g reading in a matrix rather
> than a single column and specifying say a standard deviation rather
> than the alpha.
> Thank you for any comments
> Charles Ward
>
> desmooth<-function(x1,alpha=0,plot=TRUE,print=TRUE)
> {
> x1=ts(x1)
> x2=x1
> tab01=matrix(ncol=4,nrow=4)
> rnd<-function(x) {x=round(100000*x)/100000;x}
> r1=pacf(x1,plot=FALSE)
> if (alpha==0) alpha=r1$ac[1]
> for (i in 2:length(x1)) x2[i]=(x1[i]-alpha*x1[i-1])/(1-alpha)
> if (plot==TRUE) {plot(x2,col="red",type="l");lines(x1,col="black");
> ? ? ? ?legend("topleft",c("Original","De-smoothed"), col=1:5,lty=1:1)}
> if (print==TRUE) {
> ? ? ? ?tab01[1,]=c("", "Original","De-Smoothed","Alpha");
> ? ? ? ?tab01[2,1]="Mean";
> ? ? ? ?tab01[3,1]="Std. Dev.";
> ? ? ? ?tab01[2,2]=rnd(mean(x1));tab01[2,3]=rnd(mean(x2));
> ? ? ? ?tab01[3,2]=rnd(sd(x1));tab01[3,3]=rnd(sd(x2));
> ? ? ? ?tab01[2,4]="";
> ? ? ? ?tab01[3,4]=rnd(alpha);
> ? ? ? ?dimnames(tab01)<-list(rep("",3),rep("",4));
> ? ? ? ?print(tab01,quote=FALSE)}
> ? ? ? ?x2
> }

if (plot==TRUE) is the same as if (plot)

>
> Used as in this example
>
> library(fPortfolio)
> PropInd=c(.0093,.0085,.009,.0098,.0084,.01294,.0136,.0165,.0235,.0150,.0178,.0131,
> .0233,.0156,.0258,.026,.0284,.0261,.0230,.0235
> ?,.0209,.0218,.0241,.0152,.0161)
> DesPropInd=desmooth(PropInd)
> #or to take a less Efficient Market line
> DesPropInd=desmooth(PropInd,alpha=0.4)
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From d2009_ at live.com.pt  Mon Jun  8 11:10:45 2009
From: d2009_ at live.com.pt (Daniel Mail)
Date: Mon, 8 Jun 2009 10:10:45 +0100
Subject: [R-SIG-Finance] FW:  Fix ARMA parameters in garchfit
In-Reply-To: <Pine.LNX.4.43.0906071111300.13056@hymn11.u.washington.edu>
References: <BAY0-DP1-19471C544C0F1E98DAEEDBB9460@phx.gbl>
	<Pine.LNX.4.43.0906071111300.13056@hymn11.u.washington.edu> 
Message-ID: <BLU112-W22BF0F674B9A953942287B9470@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090608/96a28205/attachment.pl>

From landronimirc at gmail.com  Tue Jun  9 18:35:32 2009
From: landronimirc at gmail.com (Liviu Andronic)
Date: Tue, 9 Jun 2009 18:35:32 +0200
Subject: [R-SIG-Finance] Vector autoregression with Newey-West standard
	errors
Message-ID: <68b1e2610906090935w2cc4bf6agf717a76fe929113a@mail.gmail.com>

Dear all,

I'm currently fitting vector autoregression using VAR() from package
`vars'. It estimates VAR by using OLS, and by default it provides
"naive" standard errors (not HC and not HAC).

> require(vars)
> data(Canada)
> Canada <- (as.data.frame(Canada))
> head(Canada)
      e  prod    rw    U
1 929.6 405.4 386.1 7.53
2 929.8 404.6 388.1 7.70
3 930.3 403.8 390.5 7.47
4 931.4 404.2 394.0 7.27
5 932.7 405.0 396.8 7.37
6 933.6 404.4 400.0 7.13
> temp <- VAR(Canada[,1:3], p = 1, type = "const")
> coef(temp)
$e
        Estimate Std. Error t value  Pr(>|t|)
e.l1    0.923983   0.023492 39.3313 1.202e-53
prod.l1 0.155238   0.028725  5.4043 6.697e-07
rw.l1   0.008184   0.006824  1.1992 2.340e-01
const   5.255418  13.451377  0.3907 6.971e-01

$prod
        Estimate Std. Error t value  Pr(>|t|)
e.l1    -0.06662   0.032275  -2.064 4.229e-02
prod.l1  1.02613   0.039463  26.002 1.777e-40
rw.l1    0.02744   0.009376   2.926 4.478e-03
const   40.29543  18.480012   2.180 3.220e-02

$rw
        Estimate Std. Error t value  Pr(>|t|)
e.l1      0.1568    0.03650   4.296 4.907e-05
prod.l1  -0.1394    0.04463  -3.124 2.496e-03
rw.l1     0.9362    0.01060  88.303 1.012e-80
const   -62.0937   20.89779  -2.971 3.928e-03


However, I would like to obtain Heteroskedasticity-Autocorrelation
Consistent standard errors using NeweyWest() from package `sandwich',
which handles principally `lm' and `glm' objects. I noticed that the
VAR() returns an object of class `varest', which contains a list of
fitted `lm' objects. So I tried to apply NeweyWest() to individual
`lm' components of `varest', unsuccessfully.

> class(temp)
[1] "varest"
> class(temp$varresult$e)
[1] "lm"
> temp.lm <- temp$varresult$e
> class(temp.lm)
[1] "lm"
> require(sandwich)
> NeweyWest(temp.lm)
Error in AA %*% t(X) : requires numeric matrix/vector arguments
In addition: Warning message:
In ar.ols(x, aic = aic, order.max = order.max, na.action = na.action,  :
  model order: 1singularities in the computation of the projection
matrixresults are only valid up to model order0
> vcovHAC(temp.lm)
Error in bread. %*% meat. : non-conformable arguments


I would have expected the above to have worked. For "standard" `lm'
objects, I never had any such issues:
> temp.std.lm <- lm(e ~ prod + rw, data=Canada)
> class(temp.std.lm)
[1] "lm"
> NeweyWest(temp.std.lm)    # the variance-covariance matrix of the estimates
            (Intercept)      prod       rw
(Intercept)   30738.384 -83.52869  7.65428
prod            -83.529   0.23111 -0.02458
rw                7.654  -0.02458  0.00539
> require(lmtest)
> coeftest(temp.std.lm, df = Inf, vcov = NeweyWest)    # NW std errs for regression coefs

z test of coefficients:

            Estimate Std. Error z value Pr(>|z|)
(Intercept) 476.4344   175.3237    2.72  0.00658 **
prod          0.8819     0.4807    1.83  0.06659 .
rw            0.2454     0.0734    3.34  0.00083 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1


Could anyone suggest a way to work around this error? Thank you,
Liviu

PS I am also cc-ing this message to the authors of `vars' and
`sandwich', Bernhard Pfaff and Achim Zeileis, respectively. Perhaps
they could shed some light on the issue.





> sessionInfo()
R version 2.8.1 (2008-12-22)
i686-pc-linux-gnu

locale:
LC_CTYPE=en_GB.UTF-8;LC_NUMERIC=C;LC_TIME=en_GB.UTF-8;LC_COLLATE=en_GB.UTF-8;LC_MONETARY=C;LC_MESSAGES=en_GB.UTF-8;LC_PAPER=en_GB.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_GB.UTF-8;LC_IDENTIFICATION=C

attached base packages:
 [1] grid      splines   tcltk     stats     graphics  grDevices datasets
 [8] utils     methods   base

other attached packages:
 [1] Matrix_0.999375-23              relimp_1.0-1
 [3] dse1_2008.10-1                  tframe_2009.2-1
 [5] setRNG_2008.3-1                 lmtest_0.9-24
 [7] vars_1.4-5                      urca_1.2-2
 [9] strucchange_1.3-7               sandwich_2.2-1
[11] zoo_1.5-6                       MASS_7.2-47
[13] abind_1.1-0                     RcmdrPlugin.TeachingDemos_1.0-3
[15] TeachingDemos_2.3               RcmdrPlugin.HH_1.1-24
[17] HH_2.1-29                       leaps_2.9
[19] multcomp_1.0-8                  mvtnorm_0.9-7
[21] lattice_0.17-25                 RcmdrPlugin.IaeTlse_0.1-3
[23] RcmdrPlugin.Export_0.3-0        Design_2.2-0
[25] survival_2.35-4                 Hmisc_3.6-0
[27] xtable_1.5-5                    Rcmdr_1.4-10
[29] car_1.2-14                      JGR_1.6-4
[31] iplots_1.1-3                    JavaGD_0.5-2
[33] rJava_0.6-2                     fortunes_1.3-6
[35] ctv_0.5-2                       PolynomF_0.93

loaded via a namespace (and not attached):
[1] cluster_1.12.0 nlme_3.1-92    tools_2.8.1


From Achim.Zeileis at wu-wien.ac.at  Tue Jun  9 19:08:32 2009
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 9 Jun 2009 19:08:32 +0200 (CEST)
Subject: [R-SIG-Finance] Vector autoregression with Newey-West standard
	errors
In-Reply-To: <68b1e2610906090935w2cc4bf6agf717a76fe929113a@mail.gmail.com>
References: <68b1e2610906090935w2cc4bf6agf717a76fe929113a@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0906091857540.31897@paninaro.stat-math.wu-wien.ac.at>

On Tue, 9 Jun 2009, Liviu Andronic wrote:

> Dear all,
>
> I'm currently fitting vector autoregression using VAR() from package
> `vars'. It estimates VAR by using OLS, and by default it provides
> "naive" standard errors (not HC and not HAC).

The standard HAC approach only works if you have no lagged dependent 
variables among the regressors. The idea is that either (1) you don't 
model the autoregressive structure at all and just capture it in the 
correction of the standard errors or (2) you model the autoregressive 
structure by the model and need no correction of the standard errors.

> However, I would like to obtain Heteroskedasticity-Autocorrelation
> Consistent standard errors using NeweyWest() from package `sandwich',
> which handles principally `lm' and `glm' objects.

The approach in "sandwich" is fully object-oriented, see
   vignette("sandwich-OOP", package = "sandwich")

> I noticed that the
> VAR() returns an object of class `varest', which contains a list of
> fitted `lm' objects. So I tried to apply NeweyWest() to individual
> `lm' components of `varest', unsuccessfully.
>
>> class(temp)
> [1] "varest"
>> class(temp$varresult$e)
> [1] "lm"
>> temp.lm <- temp$varresult$e
>> class(temp.lm)
> [1] "lm"
>> require(sandwich)
>> NeweyWest(temp.lm)
> Error in AA %*% t(X) : requires numeric matrix/vector arguments
> In addition: Warning message:
> In ar.ols(x, aic = aic, order.max = order.max, na.action = na.action,  :
>  model order: 1singularities in the computation of the projection
> matrixresults are only valid up to model order0
>> vcovHAC(temp.lm)
> Error in bread. %*% meat. : non-conformable arguments
>
>
> I would have expected the above to have worked. For "standard" `lm'
> objects, I never had any such issues:

That's because the internal structure of "varest" objects does not contain 
`standard' "lm" objects... The constant/intercept is special cased and 
hence
   bread(temp$varresult$e)
   meatHAC(temp$varresult$e)
are non-conformable. The warning from ar.ols() is thrown because 
prewhitening is used by default which really doesn't make any sense for 
VARs.

To adapt "sandwich" to a new model class, bread() and estfun() methods 
need to be supplied (see the vignette above). This wouldn't be very 
difficult for "varest" objects, but as pointed out above, I don't think it 
is very useful.

Best,
Z


From matthieu.stigler at gmail.com  Tue Jun  9 19:34:56 2009
From: matthieu.stigler at gmail.com (Matthieu Stigler)
Date: Tue, 09 Jun 2009 19:34:56 +0200
Subject: [R-SIG-Finance] Vector autoregression with Newey-West standard
 errors
In-Reply-To: <Pine.LNX.4.64.0906091857540.31897@paninaro.stat-math.wu-wien.ac.at>
References: <68b1e2610906090935w2cc4bf6agf717a76fe929113a@mail.gmail.com>
	<Pine.LNX.4.64.0906091857540.31897@paninaro.stat-math.wu-wien.ac.at>
Message-ID: <4A2E9D40.4090907@gmail.com>

Hi

I wasn't aware of the fact that HAC is not designed for time series 
model (thanks Achim!). But nevertheless I think that HC is still usable, 
well at least I saw it in couple of papers dealing with times series.

So if you still want to use an HC, two solutions:

A solve the problem (workaround):
The problem is actually in an inconsistency in VAR() when working with 
type=const or both, as in your case.

See from your example:
model.matrix(a)
model.matrix(temp.lm)

to get it working you should replace:
         attr(equation[[colnames(yend)[i]]]$terms, "intercept") <- 1
by:
         attr(equation[[colnames(yend)[i]]]$terms, "intercept") <- NULL

I'll see with Bernhard how to update it.

B: I was working myself on a Eicker -White cov-mat and implemented it in 
a package (dev version of tsDyn) that also contains VAR, so you could 
use this one.

Bests


Achim Zeileis a ?crit :
> On Tue, 9 Jun 2009, Liviu Andronic wrote:
>
>> Dear all,
>>
>> I'm currently fitting vector autoregression using VAR() from package
>> `vars'. It estimates VAR by using OLS, and by default it provides
>> "naive" standard errors (not HC and not HAC).
>
> The standard HAC approach only works if you have no lagged dependent 
> variables among the regressors. The idea is that either (1) you don't 
> model the autoregressive structure at all and just capture it in the 
> correction of the standard errors or (2) you model the autoregressive 
> structure by the model and need no correction of the standard errors.
>
>> However, I would like to obtain Heteroskedasticity-Autocorrelation
>> Consistent standard errors using NeweyWest() from package `sandwich',
>> which handles principally `lm' and `glm' objects.
>
> The approach in "sandwich" is fully object-oriented, see
>   vignette("sandwich-OOP", package = "sandwich")
>
>> I noticed that the
>> VAR() returns an object of class `varest', which contains a list of
>> fitted `lm' objects. So I tried to apply NeweyWest() to individual
>> `lm' components of `varest', unsuccessfully.
>>
>>> class(temp)
>> [1] "varest"
>>> class(temp$varresult$e)
>> [1] "lm"
>>> temp.lm <- temp$varresult$e
>>> class(temp.lm)
>> [1] "lm"
>>> require(sandwich)
>>> NeweyWest(temp.lm)
>> Error in AA %*% t(X) : requires numeric matrix/vector arguments
>> In addition: Warning message:
>> In ar.ols(x, aic = aic, order.max = order.max, na.action = na.action,  :
>>  model order: 1singularities in the computation of the projection
>> matrixresults are only valid up to model order0
>>> vcovHAC(temp.lm)
>> Error in bread. %*% meat. : non-conformable arguments
>>
>>
>> I would have expected the above to have worked. For "standard" `lm'
>> objects, I never had any such issues:
>
> That's because the internal structure of "varest" objects does not 
> contain `standard' "lm" objects... The constant/intercept is special 
> cased and hence
>   bread(temp$varresult$e)
>   meatHAC(temp$varresult$e)
> are non-conformable. The warning from ar.ols() is thrown because 
> prewhitening is used by default which really doesn't make any sense 
> for VARs.
>
> To adapt "sandwich" to a new model class, bread() and estfun() methods 
> need to be supplied (see the vignette above). This wouldn't be very 
> difficult for "varest" objects, but as pointed out above, I don't 
> think it is very useful.
>
> Best,
> Z
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From Achim.Zeileis at wu-wien.ac.at  Tue Jun  9 19:47:58 2009
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 9 Jun 2009 19:47:58 +0200 (CEST)
Subject: [R-SIG-Finance] Vector autoregression with Newey-West standard
 errors
In-Reply-To: <4A2E9D40.4090907@gmail.com>
References: <68b1e2610906090935w2cc4bf6agf717a76fe929113a@mail.gmail.com>
	<Pine.LNX.4.64.0906091857540.31897@paninaro.stat-math.wu-wien.ac.at>
	<4A2E9D40.4090907@gmail.com>
Message-ID: <Pine.LNX.4.64.0906091940340.31897@paninaro.stat-math.wu-wien.ac.at>

On Tue, 9 Jun 2009, Matthieu Stigler wrote:

> Hi
>
> I wasn't aware of the fact that HAC is not designed for time series model 
> (thanks Achim!). But nevertheless I think that HC is still usable, well at 
> least I saw it in couple of papers dealing with times series.
>
> So if you still want to use an HC, two solutions:
>
> A solve the problem (workaround):

No, there is no "problem" at least not from the "sandwich" point of view. 
If you want "sandwich" to cooperate with "varest" objects, you just need 
to provide the appropriate methods (essentially, bread() and estfun()) for 
"varest" objects. This is a clean and non-invasive solution and not very 
difficult to implement given that all this is OLS.

> The problem is actually in an inconsistency in VAR() when working with 
> type=const or both, as in your case.
>
> See from your example:
> model.matrix(a)
> model.matrix(temp.lm)
>
> to get it working you should replace:
>        attr(equation[[colnames(yend)[i]]]$terms, "intercept") <- 1
> by:
>        attr(equation[[colnames(yend)[i]]]$terms, "intercept") <- NULL
>
> I'll see with Bernhard how to update it.
>
> B: I was working myself on a Eicker -White cov-mat and implemented it in a 
> package (dev version of tsDyn) that also contains VAR, so you could use this 
> one.

Thanks for the pointer. My feeling would be that if you provided 
appropriate methods, you would get the HC stuff (or Eicker-White or 
Eicker-Huber-White or White or HCCM or sandwich or ...) for free from 
"sandwich".

As I pointed out in my previous my, this is all documented in
   vignette("sandwich-OOP", package = "sandwich")

Best,
Z

> Bests
>
>
> Achim Zeileis a ?crit :
>> On Tue, 9 Jun 2009, Liviu Andronic wrote:
>> 
>>> Dear all,
>>> 
>>> I'm currently fitting vector autoregression using VAR() from package
>>> `vars'. It estimates VAR by using OLS, and by default it provides
>>> "naive" standard errors (not HC and not HAC).
>> 
>> The standard HAC approach only works if you have no lagged dependent 
>> variables among the regressors. The idea is that either (1) you don't model 
>> the autoregressive structure at all and just capture it in the correction 
>> of the standard errors or (2) you model the autoregressive structure by the 
>> model and need no correction of the standard errors.
>> 
>>> However, I would like to obtain Heteroskedasticity-Autocorrelation
>>> Consistent standard errors using NeweyWest() from package `sandwich',
>>> which handles principally `lm' and `glm' objects.
>> 
>> The approach in "sandwich" is fully object-oriented, see
>>   vignette("sandwich-OOP", package = "sandwich")
>> 
>>> I noticed that the
>>> VAR() returns an object of class `varest', which contains a list of
>>> fitted `lm' objects. So I tried to apply NeweyWest() to individual
>>> `lm' components of `varest', unsuccessfully.
>>> 
>>>> class(temp)
>>> [1] "varest"
>>>> class(temp$varresult$e)
>>> [1] "lm"
>>>> temp.lm <- temp$varresult$e
>>>> class(temp.lm)
>>> [1] "lm"
>>>> require(sandwich)
>>>> NeweyWest(temp.lm)
>>> Error in AA %*% t(X) : requires numeric matrix/vector arguments
>>> In addition: Warning message:
>>> In ar.ols(x, aic = aic, order.max = order.max, na.action = na.action,  :
>>>  model order: 1singularities in the computation of the projection
>>> matrixresults are only valid up to model order0
>>>> vcovHAC(temp.lm)
>>> Error in bread. %*% meat. : non-conformable arguments
>>> 
>>> 
>>> I would have expected the above to have worked. For "standard" `lm'
>>> objects, I never had any such issues:
>> 
>> That's because the internal structure of "varest" objects does not contain 
>> `standard' "lm" objects... The constant/intercept is special cased and 
>> hence
>>   bread(temp$varresult$e)
>>   meatHAC(temp$varresult$e)
>> are non-conformable. The warning from ar.ols() is thrown because 
>> prewhitening is used by default which really doesn't make any sense for 
>> VARs.
>> 
>> To adapt "sandwich" to a new model class, bread() and estfun() methods need 
>> to be supplied (see the vignette above). This wouldn't be very difficult 
>> for "varest" objects, but as pointed out above, I don't think it is very 
>> useful.
>> 
>> Best,
>> Z
>> 
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>
>
>

From m.breman at yahoo.com  Tue Jun  9 20:07:06 2009
From: m.breman at yahoo.com (Mark Breman)
Date: Tue, 9 Jun 2009 11:07:06 -0700 (PDT)
Subject: [R-SIG-Finance] determine non-linear correlation
In-Reply-To: <966138.12192.qm@web110011.mail.gq1.yahoo.com>
References: <90598.5865.qm@web38605.mail.mud.yahoo.com>
	<147373.60825.qm@web110015.mail.gq1.yahoo.com>
	<693805.62072.qm@web38605.mail.mud.yahoo.com>
	<966138.12192.qm@web110011.mail.gq1.yahoo.com>
Message-ID: <215345.99075.qm@web38608.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090609/c69af2b8/attachment.pl>

From seancarmody at gmail.com  Wed Jun 10 09:29:00 2009
From: seancarmody at gmail.com (Sean Carmody)
Date: Wed, 10 Jun 2009 17:29:00 +1000
Subject: [R-SIG-Finance] quantmod error downloading .AORD data
Message-ID: <ce6bbb9d0906100029p4f0199ft57e940a48e2423e2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090610/5ce260a7/attachment.pl>

From m.breman at yahoo.com  Wed Jun 10 10:35:08 2009
From: m.breman at yahoo.com (Mark Breman)
Date: Wed, 10 Jun 2009 01:35:08 -0700 (PDT)
Subject: [R-SIG-Finance] determine non-linear correlation
In-Reply-To: <17045183.1055567.1244608694761.JavaMail.root@vms064.mailsrvcs.net>
References: <17045183.1055567.1244608694761.JavaMail.root@vms064.mailsrvcs.net>
Message-ID: <539937.37275.qm@web38603.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090610/ceab9fdc/attachment.pl>

From philjoubert at yahoo.com  Wed Jun 10 13:49:48 2009
From: philjoubert at yahoo.com (Phil Joubert)
Date: Wed, 10 Jun 2009 04:49:48 -0700 (PDT)
Subject: [R-SIG-Finance] Business day conventions
Message-ID: <579506.84668.qm@web111513.mail.gq1.yahoo.com>


Hi all

I'm looking for a date package which can handle business day conventions, e.g. Mod Following, etc. (http://en.wikipedia.org/wiki/Date_rolling)

Basically I want to be able to generate a sequence of dates from StartDate to EndDate with a given frequency and following a given business day convention. A stub convention would also be useful :)

Can anyone point me in the right direction?

thanks
Phil


From edd at debian.org  Wed Jun 10 15:11:08 2009
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 10 Jun 2009 08:11:08 -0500
Subject: [R-SIG-Finance] Business day conventions
In-Reply-To: <579506.84668.qm@web111513.mail.gq1.yahoo.com>
References: <579506.84668.qm@web111513.mail.gq1.yahoo.com>
Message-ID: <18991.45292.880391.13045@ron.nulle.part>


Phil,

On 10 June 2009 at 04:49, Phil Joubert wrote:
| I'm looking for a date package which can handle business day conventions, e.g. Mod Following, etc. (http://en.wikipedia.org/wiki/Date_rolling)
| 
| Basically I want to be able to generate a sequence of dates from StartDate to EndDate with a given frequency and following a given business day convention. A stub convention would also be useful :)
| 
| Can anyone point me in the right direction?

QuantLib has all the calendar functionality at the C++ level, and we're
slowly exposing more of it. For example, I recently added a businessDay()
function as I needed one. This already understands a bunch of calendars, and
you could try to copy the logic / interface to do something similar for the
different settlement and calendar combinations.

More generally, Khan (as part of his Google Summer of Code project of
extending RQuantLib), is exposing more as he is adding a lot of Fixed Income
functionality.  So you probably want to talk to Khanh (whom I CC'ed).

You can follow this via the R-Forge infrastructure if you're able to work
from source / svn / tarballs.  There will be new packages at some point, we
just don't know when :)

Dirk

-- 
Three out of two people have difficulties with fractions.


From windspeedo99 at gmail.com  Wed Jun 10 15:42:06 2009
From: windspeedo99 at gmail.com (Wind2)
Date: Wed, 10 Jun 2009 06:42:06 -0700 (PDT)
Subject: [R-SIG-Finance] re[R-sig-finance] trieving option info from IB
In-Reply-To: <d718c8210906041043s27018305k8a860e49500cb664@mail.gmail.com>
References: <d718c8210906040710n43a23c16w21599c658b61220a@mail.gmail.com>
	<e8e755250906040728h45d2e4e7lc3861ae04fe6b91f@mail.gmail.com>
	<d718c8210906041043s27018305k8a860e49500cb664@mail.gmail.com>
Message-ID: <23962592.post@talk.nabble.com>


Jeff's IBrokers's great.  
Finnally I have been able to retreive the snapshot data from IB for futures
and future options.   Here is the code in case if anyone is interested in.

>library(IBrokers)
>source("rFunctions/myIBrokersFun.r")

>tws <- twsConnect() # make a new connection to the TWS
>reqCurrentTime(tws) # check the server's timestamp

>futures<-lapply(c("200908","200909","200910"),function(x){twsFuture("CL","NYMEX",x)})
>myWrapper <- eWrapper(symbols=sapply(futures,function(x)
{paste(x$symbol,x$expiry)}))
>reqMktData.snapshot(tws, futures,eventWrapper=myWrapper)

I modified the reqMktData and twsCALLBACK a liittle. The code is in the
myIBrokersFun.r.   A parameter of  numId=length(Contract) has been
transferred to twsCALLBACK by reqMkeData.  So twsCALLBACK could terminate
itself once there has been numId times occurence of
(as.numeric(curMsg)==46).      

It works well for futures.

As for the future option, there is no curMsg==46, so twsCALLBACK would be
terminated because of an error.  That's OK for my task. :)

I guess the problem is that IB does not send an end message for the snapshot
mode.    Maybe IB thinks that the number of messages sent is fixed for each
kind of symbols.   But I am not familiar with IB, so I'd rather wait for the
no message error to terminate the data retrieving progress.

Now I just wonder how people utilize the default output of reqMktData.   The
output seems customized for read by eyes but not for being readed by R and
transferred into matrix or data.frame.          
But if use the eWrapper to modify the output format,  I could not figure out
how to show the symbols in the output.

Any suggestion for reading the output of reqMktData into R data would be
appreciated.
Thanks.




Wind2 wrote:
> 
> I have figured out how to get market data for future options with
> IBrokers.
> 
>>oc<-reqContractDetails(tws,
twsContract("GC","FOP",exch="NYMEX",primary="",expiry="",
> 			
> strike="",currency="USD",right="",local="",multiplier="100",NULL,NULL,"0"))
>>length(oc)
> [1] 1480
> 
>>temp<-lapply(oc,function(x){if ((x$contract$strike=="1000.0") &
(substr(x$contract$expiry,1,6)=="200909")) x else NULL  })
>>temp1<-temp[!sapply(temp, is.null)]
>>temp2<-temp1[[1]]
>>reqMktData(tws, temp2$contract)
> 
> <20090605 01:39:56.089000> id=1 symbol= Volume: 41
> <20090605 01:39:56.091000> id=1 symbol= highPrice: 55.5
> <20090605 01:39:56.093000> id=1 symbol= lowPrice: 51.4
> <20090605 01:39:56.094000> id=1 symbol= bidOption: 0.2925249666770512
> 0.49009769315987844
> <20090605 01:39:56.097000> id=1 symbol= askOption: 0.30040373020930494
> 0.49204912017993013
> <20090605 01:39:56.098000> id=1 symbol= lastOption: 0.2930042176111911
> 0.49021807490565755
> <20090605 01:39:56.606000> id=1 symbol= bidOption: 0.2925249666770512 -2
> <20090605 01:39:56.607000> id=1 symbol= askOption: 0.30040373020930494 -2
> <20090605 01:39:56.610000> id=1 symbol= lastOption: 0.2930042176111911 -2
> 
> I guess my codes could be optimized furthur.
> 
> Now there is only one issue to be settled: how to get only the current
> bid, ask and size for one time instead of the continuous data feed.
> 
> 
> 
> On Thu, Jun 4, 2009 at 10:28 PM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
>> Hi Wind,
>>
>> Most of IBrokers behaves 1:1 with the "official" API from Interactive
>> Brokers. ?That is meant to provide a smaller learning curve when
>> coming off the official API, but at the cost of being forced to deal
>> with some of the quirks of the platform as well.
>>
>> That said, it is *not* a simple wrapper to the API though. ?Many
>> design decisions were made to make it more productive for an R user,
>> as well as more productive from a trading perspective than the
>> standard API. ?The eWrapper structure and using R in general makes it
>> more productive than all the other APIs in my opinion.
>>
>> With respect to the contract information, that is really hit or miss.
>> I'll look into creating a 'best practices' or FAQ to help pool our
>> collective wisdom --- maybe a simple wiki would be a good start.
>>
>> For getting snapshot information, IB's interface (API) is lacking. ?I
>> have that on a list of things to try and put together from what the
>> API *can* do (it can't do what you want per se), and will be looking
>> to incorporate that into a future release.
>>
>> The newest quantmod has a function called "getOptionChain" which pulls
>> from Yahoo. ?Obviously yahoo data caveats apply, but it is a decent
>> start to getting snapshots.
>>
>> More documentation for IBrokers and quantmod is coming, as well as a
>> presentation at the upcoming Rmetrics conference in Switzerland. ?For
>> those not signed up yet, take a look at www.rmetrics.org for more
>> information.
>>
>> HTH,
>> Jeff
>>
>> On Thu, Jun 4, 2009 at 9:10 AM, Wind <windspeedo99 at gmail.com> wrote:
>>> I have successfully retrieved price info from IB via IBrokers for stocks
>>> and
>>> simple options. ?The package is very efficient.Yet I could not get info
>>> on
>>> future option.
>>>
>>>> oc<-reqContractDetails(tws, twsOption(local="",
>>>> right="",symbol="QQQQ"))
>>>> length(oc)
>>> [1] 1018
>>>> oc<-reqContractDetails(tws, twsOption(local="",right="",symbol="GC"))
>>> Error in reqContractDetails(tws, twsOption(local = "", right = "",
>>> symbol =
>>> "GC")) :
>>> ?Unable to complete ContractDetails request
>>>> oc<-reqContractDetails(tws,
>>> twsOption(local="",right="",exch="NYMEX",symbol="GC"))
>>> Error in reqContractDetails(tws, twsOption(local = "", right = "", exch
>>> =
>>> "NYMEX", ?:
>>> ?Unable to complete ContractDetails request
>>>
>>> By the way, how could we retrieving the option chain, ?just the current
>>> bid,
>>> ask, and size. ?A snap shot only.
>>> ?I have only find functions for data feed and historical data download.
>>>
>>> Any suggestion would be appreciated.
>>>
>>> Wind
>>>
>>> ? ? ? ?[[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>>
>>
>> --
>> Jeffrey Ryan
>> jeffrey.ryan at insightalgo.com
>>
>> ia: insight algorithmics
>> www.insightalgo.com
>>
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/retrieving-option-info-from-IB-tp23870812p23962592.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From landronimirc at gmail.com  Wed Jun 10 15:57:27 2009
From: landronimirc at gmail.com (Liviu Andronic)
Date: Wed, 10 Jun 2009 15:57:27 +0200
Subject: [R-SIG-Finance] Vector autoregression with Newey-West standard
	errors
In-Reply-To: <4A2E9D40.4090907@gmail.com>
References: <68b1e2610906090935w2cc4bf6agf717a76fe929113a@mail.gmail.com>
	<Pine.LNX.4.64.0906091857540.31897@paninaro.stat-math.wu-wien.ac.at>
	<4A2E9D40.4090907@gmail.com>
Message-ID: <68b1e2610906100657o6a1c19f5q93a348888d8ef971@mail.gmail.com>

Dear Matthieu,

On Tue, Jun 9, 2009 at 7:34 PM, Matthieu
Stigler<matthieu.stigler at gmail.com> wrote:
> to get it working you should replace:
> ? ? ? ?attr(equation[[colnames(yend)[i]]]$terms, "intercept") <- 1
> by:
> ? ? ? ?attr(equation[[colnames(yend)[i]]]$terms, "intercept") <- NULL
>
I modified the source as suggested, and rebuilt the package, and it
indeed solves the issue for the dummy example that I posted. It does
not, strangely, solve the issue for the real data I'm working on [1];
to be precise, it generates a different error.
[1] http://s000.tinyupload.com/index.php?file_id=00314428626037547016

> require(vars)
> head(tempqdf)
  yldave3 spread5 gdpc1LogRetLag1
1   3.588  0.4850           4.624
2   3.532  0.4454           5.416
3   3.604  0.3874           1.072
4   3.880  0.1964           9.716
5   3.992  0.1086           5.368
6   3.864  0.2300           8.032
> temp <- VAR(tempqdf[,c("yldave3", "spread5", "gdpc1LogRetLag1")], p=1,
+ 	type="const")
> #coef(temp)
> class(temp)
[1] "varest"
> temp.varest.lm <- (temp$varresult$yldave3)
> class(temp.varest.lm)
[1] "lm"
> head(model.matrix(temp.varest.lm))
  yldave3.l1 spread5.l1 gdpc1LogRetLag1.l1 const
1      3.588     0.4850              4.624     1
2      3.532     0.4454              5.416     1
3      3.604     0.3874              1.072     1
4      3.880     0.1964              9.716     1
5      3.992     0.1086              5.368     1
6      3.864     0.2300              8.032     1
> meat(temp.varest.lm)
                   yldave3.l1 spread5.l1 gdpc1LogRetLag1.l1   const
yldave3.l1            138.940    -1.8838           -18.3958 12.0313
spread5.l1             -1.884     3.0398             3.3658  0.1875
gdpc1LogRetLag1.l1    -18.396     3.3658            30.7899 -0.5654
const                  12.031     0.1875            -0.5654  1.1552
> bread(temp.varest.lm)
Error in if (attr(z$terms, "intercept")) sum((f - mean(f))^2) else sum(f^2) :
  argument is of length zero
> vcovHC(temp.varest.lm)
Error in if (attr(z$terms, "intercept")) sum((f - mean(f))^2) else sum(f^2) :
  argument is of length zero


# On JGR, the error above starts popping up after executing the line below.
> temp.varest.lm <- (temp$varresult$yldave3)
> Error in if (attr(z$terms, "intercept")) sum((f - mean(f))^2) else sum(f^2) :
  argument is of length zero

Is there a way to  work around this error?


> B: I was working myself on a Eicker -White cov-mat and implemented it in a
> package (dev version of tsDyn) that also contains VAR, so you could use this
> one.
>
Thank you for the pointer. For the moment I would prefer to stick to
VAR(), mostly for the extensive methods and functions applicable to
varest models.

Thank you,
Liviu


From windspeedo99 at gmail.com  Wed Jun 10 16:12:08 2009
From: windspeedo99 at gmail.com (Wind2)
Date: Wed, 10 Jun 2009 07:12:08 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] quantmod error downloading
	.AORD data
In-Reply-To: <ce6bbb9d0906100029p4f0199ft57e940a48e2423e2@mail.gmail.com>
References: <ce6bbb9d0906100029p4f0199ft57e940a48e2423e2@mail.gmail.com>
Message-ID: <23963209.post@talk.nabble.com>


It seems that google does not supply the csv download link for AORD.
You can check this by yourself.
There is a Download to Spreadsheet link in
http://www.google.com/finance/historical?q=NASDAQ:MSFT
But no such link in
http://www.google.com/finance/historical?q=INDEXASX:.AORD

If you do need AORD data, try this:
getSymbols("^AORD") 




Sean Carmody wrote:
> 
> I have been having trouble getting quantmod to return data for certain
> series. An examle is for ".AORD" (the Australian All Ordinaries index).
> 
> The following call:
> 
> getSymbols(".AORD", src="google")
> 
> results in this error message:
> 
> Error in download.file(paste(google.URL, "q=", Symbols.name,
> "&startdate=",
> :
>   cannot open URL '
> http://finance.google.com/finance/historical?q=.AORD&startdate=Jan+01,+2007&enddate=Jun+10,+2009&output=csv
> '
> In addition: Warning message:
> In download.file(paste(google.URL, "q=", Symbols.name, "&startdate=",  :
>   cannot open: HTTP status was '404 Not Found'
> 
> The problem seems to be with the "output=csv" part of the url, since
> pasting
> http://finance.google.com/finance/historical?q=.AORD&startdate=Jan+01,+2007&enddate=Jun+10,+2009into
> a browser works fine.
> 
> Of course, the same problem doesn't always arise, as with the example from
> the documentation:
> 
> getSymbols("MSFT", src="google")
> 
> Any thoughts would be appreciated.
> 
> Regards,
> Sean.
> 
> -- 
> Sean Carmody
> 
> The Stubborn Mule
> http://www.stubbornmule.net
> http://twitter.com/seancarmody
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/quantmod-error-downloading-.AORD-data-tp23957208p23963209.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From landronimirc at gmail.com  Wed Jun 10 17:14:07 2009
From: landronimirc at gmail.com (Liviu Andronic)
Date: Wed, 10 Jun 2009 17:14:07 +0200
Subject: [R-SIG-Finance] Vector autoregression with Newey-West standard
	errors
In-Reply-To: <4A2FBD20.6050901@gmail.com>
References: <68b1e2610906090935w2cc4bf6agf717a76fe929113a@mail.gmail.com>
	<Pine.LNX.4.64.0906091857540.31897@paninaro.stat-math.wu-wien.ac.at>
	<4A2E9D40.4090907@gmail.com>
	<68b1e2610906100657o6a1c19f5q93a348888d8ef971@mail.gmail.com>
	<4A2FBD20.6050901@gmail.com>
Message-ID: <68b1e2610906100814u39169e0an92a8d149dfa1cbd1@mail.gmail.com>

Dear Matthieu,

On 6/10/09, Matthieu Stigler <matthieu.stigler at gmail.com> wrote:
> by:
>>        attr(equation[[colnames(yend)[i]]]$terms, "intercept") <- 0
>
> instead of:
>>        attr(equation[[colnames(yend)[i]]]$terms, "intercept") <- NULL
>
Replacing "1" (in the published package) with "0" indeed solves my issue. Now,

> coeftest(temp.varest.lm, df = Inf, vcov = vcovHC)

z test of coefficients:

                   Estimate Std. Error z value Pr(>|z|)
yldave3.l1           0.9294     0.0685   13.57   <2e-16 ***
spread5.l1           0.0273     0.1245    0.22     0.83
gdpc1LogRetLag1.l1   0.0632     0.0433    1.46     0.14
const                0.2347     0.3786    0.62     0.54
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1


Thanks a lot for you help. Best,
Liviu


From landronimirc at gmail.com  Wed Jun 10 23:26:52 2009
From: landronimirc at gmail.com (Liviu Andronic)
Date: Wed, 10 Jun 2009 23:26:52 +0200
Subject: [R-SIG-Finance] Vector autoregression with Newey-West standard
	errors
In-Reply-To: <68b1e2610906100814u39169e0an92a8d149dfa1cbd1@mail.gmail.com>
References: <68b1e2610906090935w2cc4bf6agf717a76fe929113a@mail.gmail.com>
	<Pine.LNX.4.64.0906091857540.31897@paninaro.stat-math.wu-wien.ac.at>
	<4A2E9D40.4090907@gmail.com>
	<68b1e2610906100657o6a1c19f5q93a348888d8ef971@mail.gmail.com>
	<4A2FBD20.6050901@gmail.com>
	<68b1e2610906100814u39169e0an92a8d149dfa1cbd1@mail.gmail.com>
Message-ID: <68b1e2610906101426j34ee3778gb206342b0ea7a49@mail.gmail.com>

On 6/10/09, Liviu Andronic <landronimirc at gmail.com> wrote:
> Replacing "1" (in the published package) with "0" indeed solves my issue. Now,
>
Warning, though. This fix can generate incorrect R squared, degrees of
freedom and F-statistics when the `varest' fit is summarised. See the
changelog of the package (for the end 2008).
Liviu


From anass.mouhsine at gmail.com  Thu Jun 11 13:11:32 2009
From: anass.mouhsine at gmail.com (Anass Mouhsine)
Date: Thu, 11 Jun 2009 13:11:32 +0200
Subject: [R-SIG-Finance] How to compare two asynchroneous xts time series?
Message-ID: <4A30E664.3030004@gmail.com>

Hi all,

Suppose I have two xts time series with asynchroneous time index.
I would like for example to calculate a ratio of those two series, but I 
don't know how to get an intersection of the two indices in order to 
avoid errors.

an example of the data is the following

series1

2008-06-02 09:00:00 5007.0
2008-06-02 09:01:00 5010.0
2008-06-02 09:02:00 5014.0
2008-06-02 09:03:00 5012.5
2008-06-02 09:04:00 5013.5
2008-06-02 09:05:00 5009.5
2008-06-02 09:06:00 5007.0
2008-06-02 09:07:00 5006.5
2008-06-02 09:08:00 5008.5
2008-06-02 09:09:00 5004.5

Series2

2008-06-02 09:01:00 7115.0
2008-06-02 09:03:00 7117.0
2008-06-02 09:05:00 7111.0
2008-06-02 09:07:00 7107.0
2008-06-02 09:09:00 7102.5

Any idea?

Thanks in advance


From josh.m.ulrich at gmail.com  Thu Jun 11 16:00:03 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Thu, 11 Jun 2009 09:00:03 -0500
Subject: [R-SIG-Finance] How to compare two asynchroneous xts time
	series?
In-Reply-To: <4A30E664.3030004@gmail.com>
References: <4A30E664.3030004@gmail.com>
Message-ID: <8cca69990906110700y397cecbbv9d20aba82f4e9eb6@mail.gmail.com>

Hi Anass,

Can you provide an example of what you're trying to do and what error
you are receiving?  I'm able to run the code below without error.

> require(xts)
>
> Lines <-
+ "2008-06-02 09:00:00,5007.0
+ 2008-06-02 09:01:00,5010.0
+ 2008-06-02 09:02:00,5014.0
+ 2008-06-02 09:03:00,5012.5
+ 2008-06-02 09:04:00,5013.5
+ 2008-06-02 09:05:00,5009.5
+ 2008-06-02 09:06:00,5007.0
+ 2008-06-02 09:07:00,5006.5
+ 2008-06-02 09:08:00,5008.5
+ 2008-06-02 09:09:00,5004.5"
>
> one <- read.zoo(textConnection(Lines),sep=',',FUN=as.POSIXct)
> one <- as.xts(one)
>
> Lines <-
+ "2008-06-02 09:01:00,7115.0
+ 2008-06-02 09:03:00,7117.0
+ 2008-06-02 09:05:00,7111.0
+ 2008-06-02 09:07:00,7107.0
+ 2008-06-02 09:09:00,7102.5"
>
> two <- read.zoo(textConnection(Lines),sep=',',FUN=as.POSIXct)
> two <- as.xts(two)
>
> one/two
                           e1
2008-06-02 09:01:00 0.7041462
2008-06-02 09:03:00 0.7042996
2008-06-02 09:05:00 0.7044719
2008-06-02 09:07:00 0.7044463
2008-06-02 09:09:00 0.7046111
> two/one
                          e1
2008-06-02 09:01:00 1.420160
2008-06-02 09:03:00 1.419850
2008-06-02 09:05:00 1.419503
2008-06-02 09:07:00 1.419555
2008-06-02 09:09:00 1.419223
>

Best,
Joshua
--
http://www.fosstrading.com



On Thu, Jun 11, 2009 at 6:11 AM, Anass Mouhsine<anass.mouhsine at gmail.com> wrote:
> Hi all,
>
> Suppose I have two xts time series with asynchroneous time index.
> I would like for example to calculate a ratio of those two series, but I
> don't know how to get an intersection of the two indices in order to avoid
> errors.
>
> an example of the data is the following
>
> series1
>
> 2008-06-02 09:00:00 5007.0
> 2008-06-02 09:01:00 5010.0
> 2008-06-02 09:02:00 5014.0
> 2008-06-02 09:03:00 5012.5
> 2008-06-02 09:04:00 5013.5
> 2008-06-02 09:05:00 5009.5
> 2008-06-02 09:06:00 5007.0
> 2008-06-02 09:07:00 5006.5
> 2008-06-02 09:08:00 5008.5
> 2008-06-02 09:09:00 5004.5
>
> Series2
>
> 2008-06-02 09:01:00 7115.0
> 2008-06-02 09:03:00 7117.0
> 2008-06-02 09:05:00 7111.0
> 2008-06-02 09:07:00 7107.0
> 2008-06-02 09:09:00 7102.5
>
> Any idea?
>
> Thanks in advance
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jeff.a.ryan at gmail.com  Thu Jun 11 16:05:50 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 11 Jun 2009 09:05:50 -0500
Subject: [R-SIG-Finance] How to compare two asynchroneous xts time
	series?
In-Reply-To: <8cca69990906110700y397cecbbv9d20aba82f4e9eb6@mail.gmail.com>
References: <4A30E664.3030004@gmail.com>
	<8cca69990906110700y397cecbbv9d20aba82f4e9eb6@mail.gmail.com>
Message-ID: <e8e755250906110705i550f882j2da7017f8fb2ffd4@mail.gmail.com>

Anass,

xts and zoo automatically align series via "merge" when performing Ops
methods (+/-*...etc)

This is what you want in most cases.

See ?merge.xts, ?xts, ?merge.zoo and ?Ops.zoo

HTH
Jeff

On Thu, Jun 11, 2009 at 9:00 AM, Joshua Ulrich<josh.m.ulrich at gmail.com> wrote:
> Hi Anass,
>
> Can you provide an example of what you're trying to do and what error
> you are receiving? ?I'm able to run the code below without error.
>
>> require(xts)
>>
>> Lines <-
> + "2008-06-02 09:00:00,5007.0
> + 2008-06-02 09:01:00,5010.0
> + 2008-06-02 09:02:00,5014.0
> + 2008-06-02 09:03:00,5012.5
> + 2008-06-02 09:04:00,5013.5
> + 2008-06-02 09:05:00,5009.5
> + 2008-06-02 09:06:00,5007.0
> + 2008-06-02 09:07:00,5006.5
> + 2008-06-02 09:08:00,5008.5
> + 2008-06-02 09:09:00,5004.5"
>>
>> one <- read.zoo(textConnection(Lines),sep=',',FUN=as.POSIXct)
>> one <- as.xts(one)
>>
>> Lines <-
> + "2008-06-02 09:01:00,7115.0
> + 2008-06-02 09:03:00,7117.0
> + 2008-06-02 09:05:00,7111.0
> + 2008-06-02 09:07:00,7107.0
> + 2008-06-02 09:09:00,7102.5"
>>
>> two <- read.zoo(textConnection(Lines),sep=',',FUN=as.POSIXct)
>> two <- as.xts(two)
>>
>> one/two
> ? ? ? ? ? ? ? ? ? ? ? ? ? e1
> 2008-06-02 09:01:00 0.7041462
> 2008-06-02 09:03:00 0.7042996
> 2008-06-02 09:05:00 0.7044719
> 2008-06-02 09:07:00 0.7044463
> 2008-06-02 09:09:00 0.7046111
>> two/one
> ? ? ? ? ? ? ? ? ? ? ? ? ?e1
> 2008-06-02 09:01:00 1.420160
> 2008-06-02 09:03:00 1.419850
> 2008-06-02 09:05:00 1.419503
> 2008-06-02 09:07:00 1.419555
> 2008-06-02 09:09:00 1.419223
>>
>
> Best,
> Joshua
> --
> http://www.fosstrading.com
>
>
>
> On Thu, Jun 11, 2009 at 6:11 AM, Anass Mouhsine<anass.mouhsine at gmail.com> wrote:
>> Hi all,
>>
>> Suppose I have two xts time series with asynchroneous time index.
>> I would like for example to calculate a ratio of those two series, but I
>> don't know how to get an intersection of the two indices in order to avoid
>> errors.
>>
>> an example of the data is the following
>>
>> series1
>>
>> 2008-06-02 09:00:00 5007.0
>> 2008-06-02 09:01:00 5010.0
>> 2008-06-02 09:02:00 5014.0
>> 2008-06-02 09:03:00 5012.5
>> 2008-06-02 09:04:00 5013.5
>> 2008-06-02 09:05:00 5009.5
>> 2008-06-02 09:06:00 5007.0
>> 2008-06-02 09:07:00 5006.5
>> 2008-06-02 09:08:00 5008.5
>> 2008-06-02 09:09:00 5004.5
>>
>> Series2
>>
>> 2008-06-02 09:01:00 7115.0
>> 2008-06-02 09:03:00 7117.0
>> 2008-06-02 09:05:00 7111.0
>> 2008-06-02 09:07:00 7107.0
>> 2008-06-02 09:09:00 7102.5
>>
>> Any idea?
>>
>> Thanks in advance
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From anass.mouhsine at gmail.com  Thu Jun 11 16:16:56 2009
From: anass.mouhsine at gmail.com (anass)
Date: Thu, 11 Jun 2009 16:16:56 +0200
Subject: [R-SIG-Finance] How to compare two asynchroneous xts time
	series?
In-Reply-To: <e8e755250906110705i550f882j2da7017f8fb2ffd4@mail.gmail.com>
References: <4A30E664.3030004@gmail.com>
	<8cca69990906110700y397cecbbv9d20aba82f4e9eb6@mail.gmail.com>
	<e8e755250906110705i550f882j2da7017f8fb2ffd4@mail.gmail.com>
Message-ID: <5651742d0906110716k46ea2925l12669a6d42612dcd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090611/36f1e0f8/attachment.pl>

From josh.m.ulrich at gmail.com  Thu Jun 11 16:25:27 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Thu, 11 Jun 2009 09:25:27 -0500
Subject: [R-SIG-Finance] How to compare two asynchroneous xts time
	series?
In-Reply-To: <5651742d0906110716k46ea2925l12669a6d42612dcd@mail.gmail.com>
References: <4A30E664.3030004@gmail.com>
	<8cca69990906110700y397cecbbv9d20aba82f4e9eb6@mail.gmail.com>
	<e8e755250906110705i550f882j2da7017f8fb2ffd4@mail.gmail.com>
	<5651742d0906110716k46ea2925l12669a6d42612dcd@mail.gmail.com>
Message-ID: <8cca69990906110725k7cc58ef2i29c7a3db4c935fd9@mail.gmail.com>

On Thu, Jun 11, 2009 at 9:16 AM, anass<anass.mouhsine at gmail.com> wrote:
> Thx guys,
>
> well the error I got is elswhere and I thought it was due to the asynchrone
> timeseries.
>
> let's assume that I got the intraday ratio, what I do is the following
>
> d<-index(to.daily(ratio))
> for (i in 1:length(d)){
> ?tstart<-paste(d[i], "09:00:00")
> ?tend<-paste(d[i],"17:00:00")
> ts_ratio<-ratio[tstart:tend]

This line is incorrect.  I don't believe the ":" sequence operator in
package:base is defined for character strings (see ?":").  This is
what is causing an error.  What you probably intended is this:
ts_ratio <- ratio[paste(tstart,tend,sep="::")]

> #
> # other operations
> #
> }
>
> and I have this error
> Error in tstart:tend : argument NA / NaN
>
> So I assume if it is not due to asynchrone series, it is due to the
> subsetting.
> Does xts objects accept this kind of subset?
>
>
> On Thu, Jun 11, 2009 at 4:05 PM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
>>
>> Anass,
>>
>> xts and zoo automatically align series via "merge" when performing Ops
>> methods (+/-*...etc)
>>
>> This is what you want in most cases.
>>
>> See ?merge.xts, ?xts, ?merge.zoo and ?Ops.zoo
>>
>> HTH
>> Jeff
>>
>> On Thu, Jun 11, 2009 at 9:00 AM, Joshua Ulrich<josh.m.ulrich at gmail.com>
>> wrote:
>> > Hi Anass,
>> >
>> > Can you provide an example of what you're trying to do and what error
>> > you are receiving? ?I'm able to run the code below without error.
>> >
>> >> require(xts)
>> >>
>> >> Lines <-
>> > + "2008-06-02 09:00:00,5007.0
>> > + 2008-06-02 09:01:00,5010.0
>> > + 2008-06-02 09:02:00,5014.0
>> > + 2008-06-02 09:03:00,5012.5
>> > + 2008-06-02 09:04:00,5013.5
>> > + 2008-06-02 09:05:00,5009.5
>> > + 2008-06-02 09:06:00,5007.0
>> > + 2008-06-02 09:07:00,5006.5
>> > + 2008-06-02 09:08:00,5008.5
>> > + 2008-06-02 09:09:00,5004.5"
>> >>
>> >> one <- read.zoo(textConnection(Lines),sep=',',FUN=as.POSIXct)
>> >> one <- as.xts(one)
>> >>
>> >> Lines <-
>> > + "2008-06-02 09:01:00,7115.0
>> > + 2008-06-02 09:03:00,7117.0
>> > + 2008-06-02 09:05:00,7111.0
>> > + 2008-06-02 09:07:00,7107.0
>> > + 2008-06-02 09:09:00,7102.5"
>> >>
>> >> two <- read.zoo(textConnection(Lines),sep=',',FUN=as.POSIXct)
>> >> two <- as.xts(two)
>> >>
>> >> one/two
>> > ? ? ? ? ? ? ? ? ? ? ? ? ? e1
>> > 2008-06-02 09:01:00 0.7041462
>> > 2008-06-02 09:03:00 0.7042996
>> > 2008-06-02 09:05:00 0.7044719
>> > 2008-06-02 09:07:00 0.7044463
>> > 2008-06-02 09:09:00 0.7046111
>> >> two/one
>> > ? ? ? ? ? ? ? ? ? ? ? ? ?e1
>> > 2008-06-02 09:01:00 1.420160
>> > 2008-06-02 09:03:00 1.419850
>> > 2008-06-02 09:05:00 1.419503
>> > 2008-06-02 09:07:00 1.419555
>> > 2008-06-02 09:09:00 1.419223
>> >>
>> >
>> > Best,
>> > Joshua
>> > --
>> > http://www.fosstrading.com
>> >
>> >
>> >
>> > On Thu, Jun 11, 2009 at 6:11 AM, Anass
>> > Mouhsine<anass.mouhsine at gmail.com> wrote:
>> >> Hi all,
>> >>
>> >> Suppose I have two xts time series with asynchroneous time index.
>> >> I would like for example to calculate a ratio of those two series, but
>> >> I
>> >> don't know how to get an intersection of the two indices in order to
>> >> avoid
>> >> errors.
>> >>
>> >> an example of the data is the following
>> >>
>> >> series1
>> >>
>> >> 2008-06-02 09:00:00 5007.0
>> >> 2008-06-02 09:01:00 5010.0
>> >> 2008-06-02 09:02:00 5014.0
>> >> 2008-06-02 09:03:00 5012.5
>> >> 2008-06-02 09:04:00 5013.5
>> >> 2008-06-02 09:05:00 5009.5
>> >> 2008-06-02 09:06:00 5007.0
>> >> 2008-06-02 09:07:00 5006.5
>> >> 2008-06-02 09:08:00 5008.5
>> >> 2008-06-02 09:09:00 5004.5
>> >>
>> >> Series2
>> >>
>> >> 2008-06-02 09:01:00 7115.0
>> >> 2008-06-02 09:03:00 7117.0
>> >> 2008-06-02 09:05:00 7111.0
>> >> 2008-06-02 09:07:00 7107.0
>> >> 2008-06-02 09:09:00 7102.5
>> >>
>> >> Any idea?
>> >>
>> >> Thanks in advance
>> >>
>> >> _______________________________________________
>> >> R-SIG-Finance at stat.math.ethz.ch mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> >> -- Subscriber-posting only.
>> >> -- If you want to post, subscribe first.
>> >>
>> >
>> > _______________________________________________
>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> > -- Subscriber-posting only.
>> > -- If you want to post, subscribe first.
>> >
>>
>>
>>
>> --
>> Jeffrey Ryan
>> jeffrey.ryan at insightalgo.com
>>
>> ia: insight algorithmics
>> www.insightalgo.com
>
>
>
> --
>
> En toda ocasion, disfruta de la vida
>

Best,
Joshua
--
http://www.fosstrading.com


From anass.mouhsine at gmail.com  Thu Jun 11 16:29:23 2009
From: anass.mouhsine at gmail.com (Anass Mouhsine)
Date: Thu, 11 Jun 2009 16:29:23 +0200
Subject: [R-SIG-Finance] How to compare two asynchroneous xts time
	series?
In-Reply-To: <8cca69990906110725k7cc58ef2i29c7a3db4c935fd9@mail.gmail.com>
References: <4A30E664.3030004@gmail.com>	
	<8cca69990906110700y397cecbbv9d20aba82f4e9eb6@mail.gmail.com>	
	<e8e755250906110705i550f882j2da7017f8fb2ffd4@mail.gmail.com>	
	<5651742d0906110716k46ea2925l12669a6d42612dcd@mail.gmail.com>
	<8cca69990906110725k7cc58ef2i29c7a3db4c935fd9@mail.gmail.com>
Message-ID: <4A3114C3.50904@gmail.com>

Thanks Joshua

A

Joshua Ulrich wrote:
> On Thu, Jun 11, 2009 at 9:16 AM, anass<anass.mouhsine at gmail.com> wrote:
>   
>> Thx guys,
>>
>> well the error I got is elswhere and I thought it was due to the asynchrone
>> timeseries.
>>
>> let's assume that I got the intraday ratio, what I do is the following
>>
>> d<-index(to.daily(ratio))
>> for (i in 1:length(d)){
>>  tstart<-paste(d[i], "09:00:00")
>>  tend<-paste(d[i],"17:00:00")
>> ts_ratio<-ratio[tstart:tend]
>>     
>
> This line is incorrect.  I don't believe the ":" sequence operator in
> package:base is defined for character strings (see ?":").  This is
> what is causing an error.  What you probably intended is this:
> ts_ratio <- ratio[paste(tstart,tend,sep="::")]
>
>   
>> #
>> # other operations
>> #
>> }
>>
>> and I have this error
>> Error in tstart:tend : argument NA / NaN
>>
>> So I assume if it is not due to asynchrone series, it is due to the
>> subsetting.
>> Does xts objects accept this kind of subset?
>>
>>
>> On Thu, Jun 11, 2009 at 4:05 PM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
>>     
>>> Anass,
>>>
>>> xts and zoo automatically align series via "merge" when performing Ops
>>> methods (+/-*...etc)
>>>
>>> This is what you want in most cases.
>>>
>>> See ?merge.xts, ?xts, ?merge.zoo and ?Ops.zoo
>>>
>>> HTH
>>> Jeff
>>>
>>> On Thu, Jun 11, 2009 at 9:00 AM, Joshua Ulrich<josh.m.ulrich at gmail.com>
>>> wrote:
>>>       
>>>> Hi Anass,
>>>>
>>>> Can you provide an example of what you're trying to do and what error
>>>> you are receiving?  I'm able to run the code below without error.
>>>>
>>>>         
>>>>> require(xts)
>>>>>
>>>>> Lines <-
>>>>>           
>>>> + "2008-06-02 09:00:00,5007.0
>>>> + 2008-06-02 09:01:00,5010.0
>>>> + 2008-06-02 09:02:00,5014.0
>>>> + 2008-06-02 09:03:00,5012.5
>>>> + 2008-06-02 09:04:00,5013.5
>>>> + 2008-06-02 09:05:00,5009.5
>>>> + 2008-06-02 09:06:00,5007.0
>>>> + 2008-06-02 09:07:00,5006.5
>>>> + 2008-06-02 09:08:00,5008.5
>>>> + 2008-06-02 09:09:00,5004.5"
>>>>         
>>>>> one <- read.zoo(textConnection(Lines),sep=',',FUN=as.POSIXct)
>>>>> one <- as.xts(one)
>>>>>
>>>>> Lines <-
>>>>>           
>>>> + "2008-06-02 09:01:00,7115.0
>>>> + 2008-06-02 09:03:00,7117.0
>>>> + 2008-06-02 09:05:00,7111.0
>>>> + 2008-06-02 09:07:00,7107.0
>>>> + 2008-06-02 09:09:00,7102.5"
>>>>         
>>>>> two <- read.zoo(textConnection(Lines),sep=',',FUN=as.POSIXct)
>>>>> two <- as.xts(two)
>>>>>
>>>>> one/two
>>>>>           
>>>>                           e1
>>>> 2008-06-02 09:01:00 0.7041462
>>>> 2008-06-02 09:03:00 0.7042996
>>>> 2008-06-02 09:05:00 0.7044719
>>>> 2008-06-02 09:07:00 0.7044463
>>>> 2008-06-02 09:09:00 0.7046111
>>>>         
>>>>> two/one
>>>>>           
>>>>                          e1
>>>> 2008-06-02 09:01:00 1.420160
>>>> 2008-06-02 09:03:00 1.419850
>>>> 2008-06-02 09:05:00 1.419503
>>>> 2008-06-02 09:07:00 1.419555
>>>> 2008-06-02 09:09:00 1.419223
>>>>         
>>>> Best,
>>>> Joshua
>>>> --
>>>> http://www.fosstrading.com
>>>>
>>>>
>>>>
>>>> On Thu, Jun 11, 2009 at 6:11 AM, Anass
>>>> Mouhsine<anass.mouhsine at gmail.com> wrote:
>>>>         
>>>>> Hi all,
>>>>>
>>>>> Suppose I have two xts time series with asynchroneous time index.
>>>>> I would like for example to calculate a ratio of those two series, but
>>>>> I
>>>>> don't know how to get an intersection of the two indices in order to
>>>>> avoid
>>>>> errors.
>>>>>
>>>>> an example of the data is the following
>>>>>
>>>>> series1
>>>>>
>>>>> 2008-06-02 09:00:00 5007.0
>>>>> 2008-06-02 09:01:00 5010.0
>>>>> 2008-06-02 09:02:00 5014.0
>>>>> 2008-06-02 09:03:00 5012.5
>>>>> 2008-06-02 09:04:00 5013.5
>>>>> 2008-06-02 09:05:00 5009.5
>>>>> 2008-06-02 09:06:00 5007.0
>>>>> 2008-06-02 09:07:00 5006.5
>>>>> 2008-06-02 09:08:00 5008.5
>>>>> 2008-06-02 09:09:00 5004.5
>>>>>
>>>>> Series2
>>>>>
>>>>> 2008-06-02 09:01:00 7115.0
>>>>> 2008-06-02 09:03:00 7117.0
>>>>> 2008-06-02 09:05:00 7111.0
>>>>> 2008-06-02 09:07:00 7107.0
>>>>> 2008-06-02 09:09:00 7102.5
>>>>>
>>>>> Any idea?
>>>>>
>>>>> Thanks in advance
>>>>>
>>>>> _______________________________________________
>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>> -- Subscriber-posting only.
>>>>> -- If you want to post, subscribe first.
>>>>>
>>>>>           
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>>         
>>>
>>> --
>>> Jeffrey Ryan
>>> jeffrey.ryan at insightalgo.com
>>>
>>> ia: insight algorithmics
>>> www.insightalgo.com
>>>       
>>
>> --
>>
>> En toda ocasion, disfruta de la vida
>>
>>     
>
> Best,
> Joshua
> --
> http://www.fosstrading.com
>
>


From spencer.graves at prodsyse.com  Thu Jun 11 17:57:44 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Thu, 11 Jun 2009 08:57:44 -0700
Subject: [R-SIG-Finance] How to compare two asynchroneous xts
	time	series?
In-Reply-To: <4A3114C3.50904@gmail.com>
References: <4A30E664.3030004@gmail.com>		<8cca69990906110700y397cecbbv9d20aba82f4e9eb6@mail.gmail.com>		<e8e755250906110705i550f882j2da7017f8fb2ffd4@mail.gmail.com>		<5651742d0906110716k46ea2925l12669a6d42612dcd@mail.gmail.com>	<8cca69990906110725k7cc58ef2i29c7a3db4c935fd9@mail.gmail.com>
	<4A3114C3.50904@gmail.com>
Message-ID: <4A312978.6050905@prodsyse.com>

      One could also smooth the two series first, then compare the 
smooths.  This could be used to interpolate missing values to provide a 
larger sample size for tradition tools. 


      The "fda" package has a number of tools for doing this kind of 
thing.  Learning about this package will soon get easier with the 
scheduled appearance in July of Ramsay, Hooker and Graves (2009) 
Functional Data Analysis with R and Matlab (Springer).  The "fda" 
package contains script files to reproduce all but one of the 78 figures 
in the book.  The script files are available now, and can be found as 
follows: 


 > system.file('scripts', package='fda')
[1] "C:/Users/sgraves/R/R-2.9.0/library/fda/scripts"
 > dir(system.file('scripts', package='fda'))
<snip> 
[17] "fdarm-ch01.R"            "fdarm-ch02.R"          
[19] "fdarm-ch03.R"            "fdarm-ch04.R"          
[21] "fdarm-ch05.R"            "fdarm-ch06.R"          
[23] "fdarm-ch07.R"            "fdarm-ch08.R"          
[25] "fdarm-ch09.R"            "fdarm-ch10.R"          
[27] "fdarm-ch11.R"            "pda.fd.test.R"  


      Hope this helps. 
      Spencer

Anass Mouhsine wrote:
> Thanks Joshua
>
> A
>
> Joshua Ulrich wrote:
>> On Thu, Jun 11, 2009 at 9:16 AM, anass<anass.mouhsine at gmail.com> wrote:
>>  
>>> Thx guys,
>>>
>>> well the error I got is elswhere and I thought it was due to the 
>>> asynchrone
>>> timeseries.
>>>
>>> let's assume that I got the intraday ratio, what I do is the following
>>>
>>> d<-index(to.daily(ratio))
>>> for (i in 1:length(d)){
>>>  tstart<-paste(d[i], "09:00:00")
>>>  tend<-paste(d[i],"17:00:00")
>>> ts_ratio<-ratio[tstart:tend]
>>>     
>>
>> This line is incorrect.  I don't believe the ":" sequence operator in
>> package:base is defined for character strings (see ?":").  This is
>> what is causing an error.  What you probably intended is this:
>> ts_ratio <- ratio[paste(tstart,tend,sep="::")]
>>
>>  
>>> #
>>> # other operations
>>> #
>>> }
>>>
>>> and I have this error
>>> Error in tstart:tend : argument NA / NaN
>>>
>>> So I assume if it is not due to asynchrone series, it is due to the
>>> subsetting.
>>> Does xts objects accept this kind of subset?
>>>
>>>
>>> On Thu, Jun 11, 2009 at 4:05 PM, Jeff Ryan <jeff.a.ryan at gmail.com> 
>>> wrote:
>>>    
>>>> Anass,
>>>>
>>>> xts and zoo automatically align series via "merge" when performing Ops
>>>> methods (+/-*...etc)
>>>>
>>>> This is what you want in most cases.
>>>>
>>>> See ?merge.xts, ?xts, ?merge.zoo and ?Ops.zoo
>>>>
>>>> HTH
>>>> Jeff
>>>>
>>>> On Thu, Jun 11, 2009 at 9:00 AM, Joshua 
>>>> Ulrich<josh.m.ulrich at gmail.com>
>>>> wrote:
>>>>      
>>>>> Hi Anass,
>>>>>
>>>>> Can you provide an example of what you're trying to do and what error
>>>>> you are receiving?  I'm able to run the code below without error.
>>>>>
>>>>>        
>>>>>> require(xts)
>>>>>>
>>>>>> Lines <-
>>>>>>           
>>>>> + "2008-06-02 09:00:00,5007.0
>>>>> + 2008-06-02 09:01:00,5010.0
>>>>> + 2008-06-02 09:02:00,5014.0
>>>>> + 2008-06-02 09:03:00,5012.5
>>>>> + 2008-06-02 09:04:00,5013.5
>>>>> + 2008-06-02 09:05:00,5009.5
>>>>> + 2008-06-02 09:06:00,5007.0
>>>>> + 2008-06-02 09:07:00,5006.5
>>>>> + 2008-06-02 09:08:00,5008.5
>>>>> + 2008-06-02 09:09:00,5004.5"
>>>>>        
>>>>>> one <- read.zoo(textConnection(Lines),sep=',',FUN=as.POSIXct)
>>>>>> one <- as.xts(one)
>>>>>>
>>>>>> Lines <-
>>>>>>           
>>>>> + "2008-06-02 09:01:00,7115.0
>>>>> + 2008-06-02 09:03:00,7117.0
>>>>> + 2008-06-02 09:05:00,7111.0
>>>>> + 2008-06-02 09:07:00,7107.0
>>>>> + 2008-06-02 09:09:00,7102.5"
>>>>>        
>>>>>> two <- read.zoo(textConnection(Lines),sep=',',FUN=as.POSIXct)
>>>>>> two <- as.xts(two)
>>>>>>
>>>>>> one/two
>>>>>>           
>>>>>                           e1
>>>>> 2008-06-02 09:01:00 0.7041462
>>>>> 2008-06-02 09:03:00 0.7042996
>>>>> 2008-06-02 09:05:00 0.7044719
>>>>> 2008-06-02 09:07:00 0.7044463
>>>>> 2008-06-02 09:09:00 0.7046111
>>>>>        
>>>>>> two/one
>>>>>>           
>>>>>                          e1
>>>>> 2008-06-02 09:01:00 1.420160
>>>>> 2008-06-02 09:03:00 1.419850
>>>>> 2008-06-02 09:05:00 1.419503
>>>>> 2008-06-02 09:07:00 1.419555
>>>>> 2008-06-02 09:09:00 1.419223
>>>>>         Best,
>>>>> Joshua
>>>>> -- 
>>>>> http://www.fosstrading.com
>>>>>
>>>>>
>>>>>
>>>>> On Thu, Jun 11, 2009 at 6:11 AM, Anass
>>>>> Mouhsine<anass.mouhsine at gmail.com> wrote:
>>>>>        
>>>>>> Hi all,
>>>>>>
>>>>>> Suppose I have two xts time series with asynchroneous time index.
>>>>>> I would like for example to calculate a ratio of those two 
>>>>>> series, but
>>>>>> I
>>>>>> don't know how to get an intersection of the two indices in order to
>>>>>> avoid
>>>>>> errors.
>>>>>>
>>>>>> an example of the data is the following
>>>>>>
>>>>>> series1
>>>>>>
>>>>>> 2008-06-02 09:00:00 5007.0
>>>>>> 2008-06-02 09:01:00 5010.0
>>>>>> 2008-06-02 09:02:00 5014.0
>>>>>> 2008-06-02 09:03:00 5012.5
>>>>>> 2008-06-02 09:04:00 5013.5
>>>>>> 2008-06-02 09:05:00 5009.5
>>>>>> 2008-06-02 09:06:00 5007.0
>>>>>> 2008-06-02 09:07:00 5006.5
>>>>>> 2008-06-02 09:08:00 5008.5
>>>>>> 2008-06-02 09:09:00 5004.5
>>>>>>
>>>>>> Series2
>>>>>>
>>>>>> 2008-06-02 09:01:00 7115.0
>>>>>> 2008-06-02 09:03:00 7117.0
>>>>>> 2008-06-02 09:05:00 7111.0
>>>>>> 2008-06-02 09:07:00 7107.0
>>>>>> 2008-06-02 09:09:00 7102.5
>>>>>>
>>>>>> Any idea?
>>>>>>
>>>>>> Thanks in advance
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>>> -- Subscriber-posting only.
>>>>>> -- If you want to post, subscribe first.
>>>>>>
>>>>>>           
>>>>> _______________________________________________
>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>> -- Subscriber-posting only.
>>>>> -- If you want to post, subscribe first.
>>>>>
>>>>>         
>>>>
>>>> -- 
>>>> Jeffrey Ryan
>>>> jeffrey.ryan at insightalgo.com
>>>>
>>>> ia: insight algorithmics
>>>> www.insightalgo.com
>>>>       
>>>
>>> -- 
>>>
>>> En toda ocasion, disfruta de la vida
>>>
>>>     
>>
>> Best,
>> Joshua
>> -- 
>> http://www.fosstrading.com
>>
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ICoe at connectcap.com  Thu Jun 11 18:33:33 2009
From: ICoe at connectcap.com (Ian Coe)
Date: Thu, 11 Jun 2009 09:33:33 -0700
Subject: [R-SIG-Finance] prices in usd
Message-ID: <C92D6BF93B8E2A4B96E206B66040B916D6B784@CONNCAPSBS.connectcap.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090611/96bb31d8/attachment.pl>

From ICoe at connectcap.com  Thu Jun 11 18:58:51 2009
From: ICoe at connectcap.com (Ian Coe)
Date: Thu, 11 Jun 2009 09:58:51 -0700
Subject: [R-SIG-Finance] prices in usd
In-Reply-To: <C92D6BF93B8E2A4B96E206B66040B916D6B784@CONNCAPSBS.connectcap.local>
References: <C92D6BF93B8E2A4B96E206B66040B916D6B784@CONNCAPSBS.connectcap.local>
Message-ID: <C92D6BF93B8E2A4B96E206B66040B916D6B787@CONNCAPSBS.connectcap.local>

ADDENDUM: In the past, I've used the getHistoricalData2 function from
Bloomberg, but I'm not sure how to use that from R.

Thanks,
Ian

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Ian Coe
Sent: Thursday, June 11, 2009 9:34 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] prices in usd

Hi, 

    Does anyone know of a simple way to prices in usd?   I can't see to
get blpGetData to take a parameter allowing me to specify the currency.

 

Thanks,

Ian 

 

 

CONFIDENTIALITY NOTICE: This e-mail communication\ (incl...{{dropped:9}}


From Ian_Menezes at syntelinc.com  Fri Jun 12 12:33:51 2009
From: Ian_Menezes at syntelinc.com (Menezes, Ian)
Date: Fri, 12 Jun 2009 16:03:51 +0530
Subject: [R-SIG-Finance] buildModel in quantmod issue
Message-ID: <B5BA7CBCDFC54E489BDE3AAD3EF42B9202D3C3AC@spzcorexch04.SYNTELORG.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090612/5146fdbd/attachment.pl>

From robert at sanctumfi.com  Fri Jun 12 13:25:16 2009
From: robert at sanctumfi.com (Robert Sams)
Date: Fri, 12 Jun 2009 12:25:16 +0100
Subject: [R-SIG-Finance] prices in usd
References: <C92D6BF93B8E2A4B96E206B66040B916D6B784@CONNCAPSBS.connectcap.local>
	<SANCTUMFISERVERumzk00001acc@sanctumfi.com>
Message-ID: <SANCTUMFISERVERSQeS00001c69@sanctumfi.com>

The currency parameter of the COM API is currently not supported in
RBloomberg; support for it is on the wish list.
~R


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Ian Coe
Sent: 11 June 2009 17:59
To: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] prices in usd

ADDENDUM: In the past, I've used the getHistoricalData2 function from
Bloomberg, but I'm not sure how to use that from R.

Thanks,
Ian

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Ian Coe
Sent: Thursday, June 11, 2009 9:34 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] prices in usd

Hi, 

    Does anyone know of a simple way to prices in usd?   I can't see to
get blpGetData to take a parameter allowing me to specify the currency.

 

Thanks,

Ian 

 

 

CONFIDENTIALITY NOTICE: This e-mail communication\ (incl...{{dropped:8}}


From nelson.ana at gmail.com  Fri Jun 12 13:48:29 2009
From: nelson.ana at gmail.com (Ana Nelson)
Date: Fri, 12 Jun 2009 12:48:29 +0100
Subject: [R-SIG-Finance] prices in usd
In-Reply-To: <SANCTUMFISERVERSQeS00001c69@sanctumfi.com>
References: <C92D6BF93B8E2A4B96E206B66040B916D6B784@CONNCAPSBS.connectcap.local>
	<SANCTUMFISERVERumzk00001acc@sanctumfi.com>
	<SANCTUMFISERVERSQeS00001c69@sanctumfi.com>
Message-ID: <a7d6d2740906120448p6117c957j4fc05d278dc6219e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090612/1a29ce40/attachment.pl>

From Bastian.Offermann at zzgmbh.at  Fri Jun 12 16:29:53 2009
From: Bastian.Offermann at zzgmbh.at (Bastian.Offermann at zzgmbh.at)
Date: Fri, 12 Jun 2009 16:29:53 +0200
Subject: [R-SIG-Finance] Real interest rate data
In-Reply-To: <a7d6d2740906120448p6117c957j4fc05d278dc6219e@mail.gmail.com>
Message-ID: <164F0964A312874D9E8F66BA81163455012918E1@mail.palais-coburg.at>

Hi all,

Does anybody know whether there is something comparable to the Penn
World Tables for real interest rates?

Regards

Bastian


From jeff.a.ryan at gmail.com  Fri Jun 12 16:42:46 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 12 Jun 2009 09:42:46 -0500
Subject: [R-SIG-Finance] buildModel in quantmod issue
In-Reply-To: <B5BA7CBCDFC54E489BDE3AAD3EF42B9202D3C3AC@spzcorexch04.SYNTELORG.COM>
References: <B5BA7CBCDFC54E489BDE3AAD3EF42B9202D3C3AC@spzcorexch04.SYNTELORG.COM>
Message-ID: <e8e755250906120742l46622278idb30c979ca1c973e@mail.gmail.com>

Ian,


> When I try to use the buildModel, I seem to get an error.

> ""Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...)
> :
>
> ?0 (non-NA) cases""

The lm error gives a (big) hint.

>
> Using other methods, model specifications etc doesn't seem to help, I
> just get a different error.
>
> This is the code I'm trying:
>
> getSymbols(F)
> a=specifyModel(Next(Op(F)~OpCl(F))
> ?c=buildModel(a,method="lm",training.per=c("2009-01-01","2009-01-01"))

training.per = 1 day (or maybe 0!).  You might want a bit more data to
build the model on. ;)

> Thanks
>

All the above said, specify/build/trade in quantmod isn't entirely
'workable' at the moment.  The most recent version on R-forge (and
making its way to CRAN) restores the previous (think 2007!)
functionality, but it is still not incredibly useful in practice.
(hence it languishing in the development priority list)

HTH,
Jeff
>
>
> Ian
>
>
> Confidential: This electronic message and all contents contain information from Syntel, Inc. which may be privileged, confidential or otherwise protected from disclosure. The information is intended to be for the addressee only. If you are not the addressee, any disclosure, copy, distribution or use of the contents of this message is prohibited. If you have received this electronic message in error, please notify the sender immediately and destroy the original message and all copies.
>
> Confidential: This electronic message and all contents contain information from Syntel, Inc. which may be privileged, confidential or otherwise protected from disclosure. The information is intended to be for the addressee only. If you are not the addressee, any disclosure, copy, distribution or use of the contents of this message is prohibited. If you have received this electronic message in error, please notify the sender immediately and destroy the original message and all copies.
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From ICoe at connectcap.com  Fri Jun 12 17:25:03 2009
From: ICoe at connectcap.com (Ian Coe)
Date: Fri, 12 Jun 2009 08:25:03 -0700
Subject: [R-SIG-Finance] prices in usd
In-Reply-To: <a7d6d2740906120448p6117c957j4fc05d278dc6219e@mail.gmail.com>
References: <C92D6BF93B8E2A4B96E206B66040B916D6B784@CONNCAPSBS.connectcap.local>
	<SANCTUMFISERVERumzk00001acc@sanctumfi.com>
	<SANCTUMFISERVERSQeS00001c69@sanctumfi.com>
	<a7d6d2740906120448p6117c957j4fc05d278dc6219e@mail.gmail.com>
Message-ID: <C92D6BF93B8E2A4B96E206B66040B916D6B801@CONNCAPSBS.connectcap.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090612/0e621218/attachment.pl>

From nelson.ana at gmail.com  Fri Jun 12 17:31:33 2009
From: nelson.ana at gmail.com (Ana Nelson)
Date: Fri, 12 Jun 2009 16:31:33 +0100
Subject: [R-SIG-Finance] prices in usd
In-Reply-To: <C92D6BF93B8E2A4B96E206B66040B916D6B801@CONNCAPSBS.connectcap.local>
References: <C92D6BF93B8E2A4B96E206B66040B916D6B784@CONNCAPSBS.connectcap.local>
	<SANCTUMFISERVERumzk00001acc@sanctumfi.com>
	<SANCTUMFISERVERSQeS00001c69@sanctumfi.com>
	<a7d6d2740906120448p6117c957j4fc05d278dc6219e@mail.gmail.com>
	<C92D6BF93B8E2A4B96E206B66040B916D6B801@CONNCAPSBS.connectcap.local>
Message-ID: <a7d6d2740906120831w7412a3davf73052e460f0b998@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090612/50dae3b6/attachment.pl>

From spencer.graves at prodsyse.com  Fri Jun 12 18:30:06 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Fri, 12 Jun 2009 09:30:06 -0700
Subject: [R-SIG-Finance] prices in usd
In-Reply-To: <a7d6d2740906120831w7412a3davf73052e460f0b998@mail.gmail.com>
References: <C92D6BF93B8E2A4B96E206B66040B916D6B784@CONNCAPSBS.connectcap.local>	<SANCTUMFISERVERumzk00001acc@sanctumfi.com>	<SANCTUMFISERVERSQeS00001c69@sanctumfi.com>	<a7d6d2740906120448p6117c957j4fc05d278dc6219e@mail.gmail.com>	<C92D6BF93B8E2A4B96E206B66040B916D6B801@CONNCAPSBS.connectcap.local>
	<a7d6d2740906120831w7412a3davf73052e460f0b998@mail.gmail.com>
Message-ID: <4A32828E.8090906@prodsyse.com>

Hi, Ana:


How can we get this development version? I encountered a problem just 
now with the standard "install.packages" copied from 
"http://r-forge.r-project.org/R/?group_id=145":


 > install.packages("RBloomberg", repos="http://R-Forge.R-project.org")
Warning message:
In getDependencies(pkgs, dependencies, available, lib) :
package ?RBloomberg? is not available


Thanks for your work in improving this package.
Best Wishes,
Spencer

Ana Nelson wrote:
> Did you look at the example in the link I sent? And did you download the
> version of RBloomberg I specified?
>
>
>
> On Fri, Jun 12, 2009 at 4:25 PM, Ian Coe <ICoe at connectcap.com> wrote:
>
>   
>>  Hi,
>>
>>
>>
>> A example of the code I?m trying ot run is below.  I?ve tried putting in a
>> field that says Currency=?USD?, but that doesn?t seem to do anything to the
>> result.
>>
>> prices<-blpGetData(conn, "GOOG
>> Equity",c("PX_Last"),start=as.chron("2008-01-01"))
>>
>>
>>
>> In VBA, you have the option to use GetHistoricalData2 to have the option to
>> specify currency (see example below).  From the Bloomberg WAPI help file,
>> that is the only function that allows you specify the currency parameter.
>>
>> objDataControl.GetHistoricalData2 ?GOOG Equity?, 1, arrayFields,
>> CDate("2006/01/01"), "USD", CDate("2008/05/30"), Results:=vtResult
>>
>>
>>
>> I looked at the source in BLPGetHistoricalData.R and it looks like the call
>> is being made to the Bloomberg function GetHistoricalData.  I was wondering
>> if I switched all the calls to GetHistoricalData2, if that would allow me to
>> pass a currency parameter.  Perhaps I am missing something?
>>
>>
>>
>> Please let me know if you have any questions.
>>
>>
>>
>> Thanks,
>>
>> Ian
>>
>>
>>
>>
>>
>>
>>
>> *From:* Ana Nelson [mailto:nelson.ana at gmail.com]
>> *Sent:* Friday, June 12, 2009 4:48 AM
>> *To:* r-sig-finance at stat.math.ethz.ch
>> *Cc:* Robert Sams; Ian Coe
>>
>> *Subject:* Re: [R-SIG-Finance] prices in usd
>>
>>
>>
>> This should be possible using an override field, it needs the latest
>> development version of RBloomberg.
>>
>> http://r-forge.r-project.org/projects/rbloomberg/
>>
>> Check out the tests for examples:
>>
>> http://r-forge.r-project.org/plugins/scmsvn/viewcvs.php/trunk/inst/runit-tests/blpGetDataTest.R?rev=21&root=rbloomberg&view=markup
>>
>> Wow, I really need to update those examples. :-)
>>
>> If you have trouble, send me a specific example with tickers etc. and I'll
>> try to come up with working code.
>>
>>
>>
>>  On Fri, Jun 12, 2009 at 12:25 PM, Robert Sams <robert at sanctumfi.com>
>> wrote:
>>
>> The currency parameter of the COM API is currently not supported in
>> RBloomberg; support for it is on the wish list.
>> ~R
>>
>>
>>
>> -----Original Message-----
>> From: r-sig-finance-bounces at stat.math.ethz.ch
>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Ian Coe
>>
>> Sent: 11 June 2009 17:59
>> To: r-sig-finance at stat.math.ethz.ch
>>
>> Subject: Re: [R-SIG-Finance] prices in usd
>>
>> ADDENDUM: In the past, I've used the getHistoricalData2 function from
>> Bloomberg, but I'm not sure how to use that from R.
>>
>> Thanks,
>> Ian
>>
>> -----Original Message-----
>> From: r-sig-finance-bounces at stat.math.ethz.ch
>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Ian Coe
>> Sent: Thursday, June 11, 2009 9:34 AM
>> To: r-sig-finance at stat.math.ethz.ch
>> Subject: [R-SIG-Finance] prices in usd
>>
>> Hi,
>>
>>    Does anyone know of a simple way to prices in usd?   I can't see to
>> get blpGetData to take a parameter allowing me to specify the currency.
>>
>>
>>
>> Thanks,
>>
>> Ian
>>
>>
>>
>>
>>  CONFIDENTIALITY NOTICE: This e-mail communication\ (incl...{{dropped:8}}
>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>
>>
>>     
>
> 	[[alternative HTML version deleted]]
>
>   
> ------------------------------------------------------------------------
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From nelson.ana at gmail.com  Fri Jun 12 18:38:55 2009
From: nelson.ana at gmail.com (Ana Nelson)
Date: Fri, 12 Jun 2009 17:38:55 +0100
Subject: [R-SIG-Finance] prices in usd
In-Reply-To: <4A32828E.8090906@prodsyse.com>
References: <C92D6BF93B8E2A4B96E206B66040B916D6B784@CONNCAPSBS.connectcap.local>
	<SANCTUMFISERVERumzk00001acc@sanctumfi.com>
	<SANCTUMFISERVERSQeS00001c69@sanctumfi.com>
	<a7d6d2740906120448p6117c957j4fc05d278dc6219e@mail.gmail.com>
	<C92D6BF93B8E2A4B96E206B66040B916D6B801@CONNCAPSBS.connectcap.local>
	<a7d6d2740906120831w7412a3davf73052e460f0b998@mail.gmail.com>
	<4A32828E.8090906@prodsyse.com>
Message-ID: <a7d6d2740906120938w34763067l95ff35bb50bf537d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090612/b3fa8fdf/attachment.pl>

From nelson.ana at gmail.com  Fri Jun 12 18:51:15 2009
From: nelson.ana at gmail.com (Ana Nelson)
Date: Fri, 12 Jun 2009 17:51:15 +0100
Subject: [R-SIG-Finance] prices in usd
In-Reply-To: <4A32828E.8090906@prodsyse.com>
References: <C92D6BF93B8E2A4B96E206B66040B916D6B784@CONNCAPSBS.connectcap.local>
	<SANCTUMFISERVERumzk00001acc@sanctumfi.com>
	<SANCTUMFISERVERSQeS00001c69@sanctumfi.com>
	<a7d6d2740906120448p6117c957j4fc05d278dc6219e@mail.gmail.com>
	<C92D6BF93B8E2A4B96E206B66040B916D6B801@CONNCAPSBS.connectcap.local>
	<a7d6d2740906120831w7412a3davf73052e460f0b998@mail.gmail.com>
	<4A32828E.8090906@prodsyse.com>
Message-ID: <a7d6d2740906120951y72e212cbt36f4d061d24f9174@mail.gmail.com>

It might be that r-forge always builds for the latest version of R so, you
may need to upgrade to R 2.9 for that to work. I haven't tested with 2.9,
though, so I don't know if all the dependent libraries have been updated
yet.

You can try the attached which I just created by zipping up my currently
installed version, compiled for R 2.8.



On Fri, Jun 12, 2009 at 5:30 PM, spencerg <spencer.graves at prodsyse.com>wrote:

> Hi, Ana:
>
>
> How can we get this development version? I encountered a problem just now
> with the standard "install.packages" copied from "
> http://r-forge.r-project.org/R/?group_id=145":
>
>
> > install.packages("RBloomberg", repos="http://R-Forge.R-project.org")
> Warning message:
> In getDependencies(pkgs, dependencies, available, lib) :
> package ?RBloomberg? is not available
>
>
> Thanks for your work in improving this package.
> Best Wishes,
> Spencer
>
> Ana Nelson wrote:
>
>> Did you look at the example in the link I sent? And did you download the
>> version of RBloomberg I specified?
>>
>>
>>
>> On Fri, Jun 12, 2009 at 4:25 PM, Ian Coe <ICoe at connectcap.com> wrote:
>>
>>
>>
>>>  Hi,
>>>
>>>
>>>
>>> A example of the code I?m trying ot run is below.  I?ve tried putting in
>>> a
>>> field that says Currency=?USD?, but that doesn?t seem to do anything to
>>> the
>>> result.
>>>
>>> prices<-blpGetData(conn, "GOOG
>>> Equity",c("PX_Last"),start=as.chron("2008-01-01"))
>>>
>>>
>>>
>>> In VBA, you have the option to use GetHistoricalData2 to have the option
>>> to
>>> specify currency (see example below).  From the Bloomberg WAPI help file,
>>> that is the only function that allows you specify the currency parameter.
>>>
>>> objDataControl.GetHistoricalData2 ?GOOG Equity?, 1, arrayFields,
>>> CDate("2006/01/01"), "USD", CDate("2008/05/30"), Results:=vtResult
>>>
>>>
>>>
>>> I looked at the source in BLPGetHistoricalData.R and it looks like the
>>> call
>>> is being made to the Bloomberg function GetHistoricalData.  I was
>>> wondering
>>> if I switched all the calls to GetHistoricalData2, if that would allow me
>>> to
>>> pass a currency parameter.  Perhaps I am missing something?
>>>
>>>
>>>
>>> Please let me know if you have any questions.
>>>
>>>
>>>
>>> Thanks,
>>>
>>> Ian
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> *From:* Ana Nelson [mailto:nelson.ana at gmail.com]
>>> *Sent:* Friday, June 12, 2009 4:48 AM
>>> *To:* r-sig-finance at stat.math.ethz.ch
>>> *Cc:* Robert Sams; Ian Coe
>>>
>>> *Subject:* Re: [R-SIG-Finance] prices in usd
>>>
>>>
>>>
>>> This should be possible using an override field, it needs the latest
>>> development version of RBloomberg.
>>>
>>> http://r-forge.r-project.org/projects/rbloomberg/
>>>
>>> Check out the tests for examples:
>>>
>>>
>>> http://r-forge.r-project.org/plugins/scmsvn/viewcvs.php/trunk/inst/runit-tests/blpGetDataTest.R?rev=21&root=rbloomberg&view=markup
>>>
>>> Wow, I really need to update those examples. :-)
>>>
>>> If you have trouble, send me a specific example with tickers etc. and
>>> I'll
>>> try to come up with working code.
>>>
>>>
>>>
>>>  On Fri, Jun 12, 2009 at 12:25 PM, Robert Sams <robert at sanctumfi.com>
>>> wrote:
>>>
>>> The currency parameter of the COM API is currently not supported in
>>> RBloomberg; support for it is on the wish list.
>>> ~R
>>>
>>>
>>>
>>> -----Original Message-----
>>> From: r-sig-finance-bounces at stat.math.ethz.ch
>>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Ian Coe
>>>
>>> Sent: 11 June 2009 17:59
>>> To: r-sig-finance at stat.math.ethz.ch
>>>
>>> Subject: Re: [R-SIG-Finance] prices in usd
>>>
>>> ADDENDUM: In the past, I've used the getHistoricalData2 function from
>>> Bloomberg, but I'm not sure how to use that from R.
>>>
>>> Thanks,
>>> Ian
>>>
>>> -----Original Message-----
>>> From: r-sig-finance-bounces at stat.math.ethz.ch
>>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Ian Coe
>>> Sent: Thursday, June 11, 2009 9:34 AM
>>> To: r-sig-finance at stat.math.ethz.ch
>>> Subject: [R-SIG-Finance] prices in usd
>>>
>>> Hi,
>>>
>>>   Does anyone know of a simple way to prices in usd?   I can't see to
>>> get blpGetData to take a parameter allowing me to specify the currency.
>>>
>>>
>>>
>>> Thanks,
>>>
>>> Ian
>>>
>>>
>>>
>>>
>>>  CONFIDENTIALITY NOTICE: This e-mail communication\ (incl...{{dropped:8}}
>>>
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>>
>>>
>>>
>>>
>>
>>        [[alternative HTML version deleted]]
>>
>>  ------------------------------------------------------------------------
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090612/d2b0414f/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: RBloomberg.zip
Type: application/zip
Size: 47762 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090612/d2b0414f/attachment.zip>

From spencer.graves at prodsyse.com  Fri Jun 12 18:59:35 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Fri, 12 Jun 2009 09:59:35 -0700
Subject: [R-SIG-Finance] prices in usd
In-Reply-To: <a7d6d2740906120938w34763067l95ff35bb50bf537d@mail.gmail.com>
References: <C92D6BF93B8E2A4B96E206B66040B916D6B784@CONNCAPSBS.connectcap.local>	
	<SANCTUMFISERVERumzk00001acc@sanctumfi.com>	
	<SANCTUMFISERVERSQeS00001c69@sanctumfi.com>	
	<a7d6d2740906120448p6117c957j4fc05d278dc6219e@mail.gmail.com>	
	<C92D6BF93B8E2A4B96E206B66040B916D6B801@CONNCAPSBS.connectcap.local>	
	<a7d6d2740906120831w7412a3davf73052e460f0b998@mail.gmail.com>	
	<4A32828E.8090906@prodsyse.com>
	<a7d6d2740906120938w34763067l95ff35bb50bf537d@mail.gmail.com>
Message-ID: <4A328977.4000805@prodsyse.com>

Hi, Ana: 


      I just clicked on "Windows binary (.zip)" on 
"http://r-forge.r-project.org/R/?group_id=145", as you suggested, and 
got "PAGE NOT FOUND".  I was able to download the *.tar.gz version, 
which is fine for anyone who knows how to install it from that version. 


      This sounds like an issue to report to "R-Forge at R-Project.org".  
I'll forward this to them separately. 


      Best Wishes,
      Spencer


Ana Nelson wrote:
> Not sure what the issue is, but can you try downloading the Windows zip
> version from that page and installing it manually?
>
> Use the "Install package(s) from local zip files..." option in the Packages
> menu.
>
>
>
> On Fri, Jun 12, 2009 at 5:30 PM, spencerg <spencer.graves at prodsyse.com>wrote:
>
>   
>> Hi, Ana:
>>
>>
>> How can we get this development version? I encountered a problem just now
>> with the standard "install.packages" copied from "
>> http://r-forge.r-project.org/R/?group_id=145":
>>
>>
>>     
>>> install.packages("RBloomberg", repos="http://R-Forge.R-project.org")
>>>       
>> Warning message:
>> In getDependencies(pkgs, dependencies, available, lib) :
>> package ?RBloomberg? is not available
>>
>>
>> Thanks for your work in improving this package.
>> Best Wishes,
>> Spencer
>>
>> Ana Nelson wrote:
>>
>>     
>>> Did you look at the example in the link I sent? And did you download the
>>> version of RBloomberg I specified?
>>>
>>>
>>>
>>> On Fri, Jun 12, 2009 at 4:25 PM, Ian Coe <ICoe at connectcap.com> wrote:
>>>
>>>
>>>
>>>       
>>>>  Hi,
>>>>
>>>>
>>>>
>>>> A example of the code I?m trying ot run is below.  I?ve tried putting in
>>>> a
>>>> field that says Currency=?USD?, but that doesn?t seem to do anything to
>>>> the
>>>> result.
>>>>
>>>> prices<-blpGetData(conn, "GOOG
>>>> Equity",c("PX_Last"),start=as.chron("2008-01-01"))
>>>>
>>>>
>>>>
>>>> In VBA, you have the option to use GetHistoricalData2 to have the option
>>>> to
>>>> specify currency (see example below).  From the Bloomberg WAPI help file,
>>>> that is the only function that allows you specify the currency parameter.
>>>>
>>>> objDataControl.GetHistoricalData2 ?GOOG Equity?, 1, arrayFields,
>>>> CDate("2006/01/01"), "USD", CDate("2008/05/30"), Results:=vtResult
>>>>
>>>>
>>>>
>>>> I looked at the source in BLPGetHistoricalData.R and it looks like the
>>>> call
>>>> is being made to the Bloomberg function GetHistoricalData.  I was
>>>> wondering
>>>> if I switched all the calls to GetHistoricalData2, if that would allow me
>>>> to
>>>> pass a currency parameter.  Perhaps I am missing something?
>>>>
>>>>
>>>>
>>>> Please let me know if you have any questions.
>>>>
>>>>
>>>>
>>>> Thanks,
>>>>
>>>> Ian
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> *From:* Ana Nelson [mailto:nelson.ana at gmail.com]
>>>> *Sent:* Friday, June 12, 2009 4:48 AM
>>>> *To:* r-sig-finance at stat.math.ethz.ch
>>>> *Cc:* Robert Sams; Ian Coe
>>>>
>>>> *Subject:* Re: [R-SIG-Finance] prices in usd
>>>>
>>>>
>>>>
>>>> This should be possible using an override field, it needs the latest
>>>> development version of RBloomberg.
>>>>
>>>> http://r-forge.r-project.org/projects/rbloomberg/
>>>>
>>>> Check out the tests for examples:
>>>>
>>>>
>>>> http://r-forge.r-project.org/plugins/scmsvn/viewcvs.php/trunk/inst/runit-tests/blpGetDataTest.R?rev=21&root=rbloomberg&view=markup
>>>>
>>>> Wow, I really need to update those examples. :-)
>>>>
>>>> If you have trouble, send me a specific example with tickers etc. and
>>>> I'll
>>>> try to come up with working code.
>>>>
>>>>
>>>>
>>>>  On Fri, Jun 12, 2009 at 12:25 PM, Robert Sams <robert at sanctumfi.com>
>>>> wrote:
>>>>
>>>> The currency parameter of the COM API is currently not supported in
>>>> RBloomberg; support for it is on the wish list.
>>>> ~R
>>>>
>>>>
>>>>
>>>> -----Original Message-----
>>>> From: r-sig-finance-bounces at stat.math.ethz.ch
>>>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Ian Coe
>>>>
>>>> Sent: 11 June 2009 17:59
>>>> To: r-sig-finance at stat.math.ethz.ch
>>>>
>>>> Subject: Re: [R-SIG-Finance] prices in usd
>>>>
>>>> ADDENDUM: In the past, I've used the getHistoricalData2 function from
>>>> Bloomberg, but I'm not sure how to use that from R.
>>>>
>>>> Thanks,
>>>> Ian
>>>>
>>>> -----Original Message-----
>>>> From: r-sig-finance-bounces at stat.math.ethz.ch
>>>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Ian Coe
>>>> Sent: Thursday, June 11, 2009 9:34 AM
>>>> To: r-sig-finance at stat.math.ethz.ch
>>>> Subject: [R-SIG-Finance] prices in usd
>>>>
>>>> Hi,
>>>>
>>>>   Does anyone know of a simple way to prices in usd?   I can't see to
>>>> get blpGetData to take a parameter allowing me to specify the currency.
>>>>
>>>>
>>>>
>>>> Thanks,
>>>>
>>>> Ian
>>>>
>>>>
>>>>
>>>>
>>>>  CONFIDENTIALITY NOTICE: This e-mail communication\ (incl...{{dropped:8}}
>>>>
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>         
>>>        [[alternative HTML version deleted]]
>>>
>>>  ------------------------------------------------------------------------
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>>       
>>     
>
>


From spencer.graves at prodsyse.com  Fri Jun 12 19:04:59 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Fri, 12 Jun 2009 10:04:59 -0700
Subject: [R-SIG-Finance] prices in usd
In-Reply-To: <a7d6d2740906120951y72e212cbt36f4d061d24f9174@mail.gmail.com>
References: <C92D6BF93B8E2A4B96E206B66040B916D6B784@CONNCAPSBS.connectcap.local>	
	<SANCTUMFISERVERumzk00001acc@sanctumfi.com>	
	<SANCTUMFISERVERSQeS00001c69@sanctumfi.com>	
	<a7d6d2740906120448p6117c957j4fc05d278dc6219e@mail.gmail.com>	
	<C92D6BF93B8E2A4B96E206B66040B916D6B801@CONNCAPSBS.connectcap.local>	
	<a7d6d2740906120831w7412a3davf73052e460f0b998@mail.gmail.com>	
	<4A32828E.8090906@prodsyse.com>
	<a7d6d2740906120951y72e212cbt36f4d061d24f9174@mail.gmail.com>
Message-ID: <4A328ABB.1090209@prodsyse.com>

I've been using version 2.9.0 for some time: 

 sessionInfo()
R version 2.9.0 (2009-04-17)
i386-pc-mingw32

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United 
States.1252;LC_MONETARY=English_United 
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base    

Ana Nelson wrote:
> It might be that r-forge always builds for the latest version of R so, you
> may need to upgrade to R 2.9 for that to work. I haven't tested with 2.9,
> though, so I don't know if all the dependent libraries have been updated
> yet.
>
> You can try the attached which I just created by zipping up my currently
> installed version, compiled for R 2.8.
>
>
>
> On Fri, Jun 12, 2009 at 5:30 PM, spencerg <spencer.graves at prodsyse.com>wrote:
>
>   
>> Hi, Ana:
>>
>>
>> How can we get this development version? I encountered a problem just now
>> with the standard "install.packages" copied from "
>> http://r-forge.r-project.org/R/?group_id=145":
>>
>>
>>     
>>> install.packages("RBloomberg", repos="http://R-Forge.R-project.org")
>>>       
>> Warning message:
>> In getDependencies(pkgs, dependencies, available, lib) :
>> package ?RBloomberg? is not available
>>
>>
>> Thanks for your work in improving this package.
>> Best Wishes,
>> Spencer
>>
>> Ana Nelson wrote:
>>
>>     
>>> Did you look at the example in the link I sent? And did you download the
>>> version of RBloomberg I specified?
>>>
>>>
>>>
>>> On Fri, Jun 12, 2009 at 4:25 PM, Ian Coe <ICoe at connectcap.com> wrote:
>>>
>>>
>>>
>>>       
>>>>  Hi,
>>>>
>>>>
>>>>
>>>> A example of the code I?m trying ot run is below.  I?ve tried putting in
>>>> a
>>>> field that says Currency=?USD?, but that doesn?t seem to do anything to
>>>> the
>>>> result.
>>>>
>>>> prices<-blpGetData(conn, "GOOG
>>>> Equity",c("PX_Last"),start=as.chron("2008-01-01"))
>>>>
>>>>
>>>>
>>>> In VBA, you have the option to use GetHistoricalData2 to have the option
>>>> to
>>>> specify currency (see example below).  From the Bloomberg WAPI help file,
>>>> that is the only function that allows you specify the currency parameter.
>>>>
>>>> objDataControl.GetHistoricalData2 ?GOOG Equity?, 1, arrayFields,
>>>> CDate("2006/01/01"), "USD", CDate("2008/05/30"), Results:=vtResult
>>>>
>>>>
>>>>
>>>> I looked at the source in BLPGetHistoricalData.R and it looks like the
>>>> call
>>>> is being made to the Bloomberg function GetHistoricalData.  I was
>>>> wondering
>>>> if I switched all the calls to GetHistoricalData2, if that would allow me
>>>> to
>>>> pass a currency parameter.  Perhaps I am missing something?
>>>>
>>>>
>>>>
>>>> Please let me know if you have any questions.
>>>>
>>>>
>>>>
>>>> Thanks,
>>>>
>>>> Ian
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> *From:* Ana Nelson [mailto:nelson.ana at gmail.com]
>>>> *Sent:* Friday, June 12, 2009 4:48 AM
>>>> *To:* r-sig-finance at stat.math.ethz.ch
>>>> *Cc:* Robert Sams; Ian Coe
>>>>
>>>> *Subject:* Re: [R-SIG-Finance] prices in usd
>>>>
>>>>
>>>>
>>>> This should be possible using an override field, it needs the latest
>>>> development version of RBloomberg.
>>>>
>>>> http://r-forge.r-project.org/projects/rbloomberg/
>>>>
>>>> Check out the tests for examples:
>>>>
>>>>
>>>> http://r-forge.r-project.org/plugins/scmsvn/viewcvs.php/trunk/inst/runit-tests/blpGetDataTest.R?rev=21&root=rbloomberg&view=markup
>>>>
>>>> Wow, I really need to update those examples. :-)
>>>>
>>>> If you have trouble, send me a specific example with tickers etc. and
>>>> I'll
>>>> try to come up with working code.
>>>>
>>>>
>>>>
>>>>  On Fri, Jun 12, 2009 at 12:25 PM, Robert Sams <robert at sanctumfi.com>
>>>> wrote:
>>>>
>>>> The currency parameter of the COM API is currently not supported in
>>>> RBloomberg; support for it is on the wish list.
>>>> ~R
>>>>
>>>>
>>>>
>>>> -----Original Message-----
>>>> From: r-sig-finance-bounces at stat.math.ethz.ch
>>>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Ian Coe
>>>>
>>>> Sent: 11 June 2009 17:59
>>>> To: r-sig-finance at stat.math.ethz.ch
>>>>
>>>> Subject: Re: [R-SIG-Finance] prices in usd
>>>>
>>>> ADDENDUM: In the past, I've used the getHistoricalData2 function from
>>>> Bloomberg, but I'm not sure how to use that from R.
>>>>
>>>> Thanks,
>>>> Ian
>>>>
>>>> -----Original Message-----
>>>> From: r-sig-finance-bounces at stat.math.ethz.ch
>>>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Ian Coe
>>>> Sent: Thursday, June 11, 2009 9:34 AM
>>>> To: r-sig-finance at stat.math.ethz.ch
>>>> Subject: [R-SIG-Finance] prices in usd
>>>>
>>>> Hi,
>>>>
>>>>   Does anyone know of a simple way to prices in usd?   I can't see to
>>>> get blpGetData to take a parameter allowing me to specify the currency.
>>>>
>>>>
>>>>
>>>> Thanks,
>>>>
>>>> Ian
>>>>
>>>>
>>>>
>>>>
>>>>  CONFIDENTIALITY NOTICE: This e-mail communication\ (incl...{{dropped:8}}
>>>>
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>         
>>>        [[alternative HTML version deleted]]
>>>
>>>  ------------------------------------------------------------------------
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>>       
>>     
>
>


From nelson.ana at gmail.com  Fri Jun 12 19:25:54 2009
From: nelson.ana at gmail.com (Ana Nelson)
Date: Fri, 12 Jun 2009 18:25:54 +0100
Subject: [R-SIG-Finance] prices in usd
In-Reply-To: <4A328ABB.1090209@prodsyse.com>
References: <C92D6BF93B8E2A4B96E206B66040B916D6B784@CONNCAPSBS.connectcap.local>
	<SANCTUMFISERVERumzk00001acc@sanctumfi.com>
	<SANCTUMFISERVERSQeS00001c69@sanctumfi.com>
	<a7d6d2740906120448p6117c957j4fc05d278dc6219e@mail.gmail.com>
	<C92D6BF93B8E2A4B96E206B66040B916D6B801@CONNCAPSBS.connectcap.local>
	<a7d6d2740906120831w7412a3davf73052e460f0b998@mail.gmail.com>
	<4A32828E.8090906@prodsyse.com>
	<a7d6d2740906120951y72e212cbt36f4d061d24f9174@mail.gmail.com>
	<4A328ABB.1090209@prodsyse.com>
Message-ID: <a7d6d2740906121025s515d5a6jc5bcc34b56617730@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090612/f8f8468e/attachment.pl>

From breman.mark at gmail.com  Fri Jun 12 19:27:19 2009
From: breman.mark at gmail.com (Mark Breman)
Date: Fri, 12 Jun 2009 19:27:19 +0200
Subject: [R-SIG-Finance] Return.calculate strange results?
Message-ID: <5e6a2e670906121027wed30e6ck52d063dc7965c454@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090612/1585fe7c/attachment.pl>

From ICoe at connectcap.com  Fri Jun 12 19:41:52 2009
From: ICoe at connectcap.com (Ian Coe)
Date: Fri, 12 Jun 2009 10:41:52 -0700
Subject: [R-SIG-Finance] prices in usd
In-Reply-To: <a7d6d2740906120831w7412a3davf73052e460f0b998@mail.gmail.com>
References: <C92D6BF93B8E2A4B96E206B66040B916D6B784@CONNCAPSBS.connectcap.local>
	<SANCTUMFISERVERumzk00001acc@sanctumfi.com>
	<SANCTUMFISERVERSQeS00001c69@sanctumfi.com>
	<a7d6d2740906120448p6117c957j4fc05d278dc6219e@mail.gmail.com>
	<C92D6BF93B8E2A4B96E206B66040B916D6B801@CONNCAPSBS.connectcap.local>
	<a7d6d2740906120831w7412a3davf73052e460f0b998@mail.gmail.com>
Message-ID: <C92D6BF93B8E2A4B96E206B66040B916D6B80B@CONNCAPSBS.connectcap.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090612/67848873/attachment.pl>

From rbali at ufmg.br  Fri Jun 12 21:18:27 2009
From: rbali at ufmg.br (Robert Iquiapaza)
Date: Fri, 12 Jun 2009 16:18:27 -0300
Subject: [R-SIG-Finance] Return.calculate strange results?
In-Reply-To: <5e6a2e670906121027wed30e6ck52d063dc7965c454@mail.gmail.com>
References: <5e6a2e670906121027wed30e6ck52d063dc7965c454@mail.gmail.com>
Message-ID: <85FB32D392414397BE6A4A27D99B5C6D@DellPC>

Mark

Using library(timeSeries)
returns(A, method = "discrete")
returns(A, method = "continuous")

or PerformanceAnalytics version 0.9.7.1

require(PerformanceAnalytics)
Return.calculate(z, method="simple")
               Column
2007-01-04 0.02219570
Return.calculate(z, method="compound")
               Column
2007-01-04 0.02195296

I got the same results.

The continuous or compound is computed with ln(85.66/83.8)

The error could be in the way you call the data. Also it appears that some 
packages have extrange behavior when you only have two observations.

This is the result with timeSeries
> returns(A, method = "discrete")
                    x
2007-01-04         NA        #lock the data
2007-01-04 0.02219570
>returns(A, method = "continuous")
                    x
2007-01-03         NA
           0.02195296     #no data

with tree observations gets better
> returns(A, method = "continuous")
                     x
2007-01-03          NA
2007-01-04  0.02195296
2007-01-05 -0.01363444


--------------------------------------------------
From: "Mark Breman" <breman.mark at gmail.com>
Sent: Friday, June 12, 2009 2:27 PM
To: <r-sig-finance at stat.math.ethz.ch>
Subject: [R-SIG-Finance] Return.calculate strange results?

> I'm getting strange results from the Return.calculate() function in the
> PerformanceAnalytics package:
> I have a timeseries A with price data:
>
>> A
>                      AAPL.Close
> 2007-01-03      83.80
> 2007-01-04      85.66
>
> The simple method of Return.calculate() gives me the following returns:
>
>> Return.calculate(A, method="simple")
>                   AAPL.Close
> 2007-01-03   -0.02171375
> 2007-01-04          NA
>
> Isn' t that weird? I would expect a simple return on the 2007-01-04 row 
> of:
> (85.66 / 83.80) -1 = 0.02219570 and a NA value for the 2007-01-03 row.
>
> The compound method of the function also gives a strange result:
>
>> Return.calculate(A, method="compound")
>                   AAPL.Close
> 2007-01-03         NA
> 2007-01-04   0.02195296
>
> On the 2007-01-04 row I would expect the same return as the simple method
> (because there is only one period): (85.66 / 83.80) -1 = 0.02219570
> Instead it gives me 0.02195296.
>
> Am I doing something wrong here or is the function broken?
>
> Regards,
>
> -Mark-
>
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From brian at braverock.com  Fri Jun 12 22:47:58 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 12 Jun 2009 15:47:58 -0500
Subject: [R-SIG-Finance] prices in usd
In-Reply-To: <C92D6BF93B8E2A4B96E206B66040B916D6B80B@CONNCAPSBS.connectcap.local>
References: <C92D6BF93B8E2A4B96E206B66040B916D6B784@CONNCAPSBS.connectcap.local>	<SANCTUMFISERVERumzk00001acc@sanctumfi.com>	<SANCTUMFISERVERSQeS00001c69@sanctumfi.com>	<a7d6d2740906120448p6117c957j4fc05d278dc6219e@mail.gmail.com>	<C92D6BF93B8E2A4B96E206B66040B916D6B801@CONNCAPSBS.connectcap.local>	<a7d6d2740906120831w7412a3davf73052e460f0b998@mail.gmail.com>
	<C92D6BF93B8E2A4B96E206B66040B916D6B80B@CONNCAPSBS.connectcap.local>
Message-ID: <4A32BEFE.4080703@braverock.com>

Ian Coe wrote:
> I used the update feature from within R, so I should have the most
> up-to-date one available from CRAN.  I am having the same issue as
> Spencer when trying to download the windows binaries.
>   
Ana specified "the latest development version".  That will require that 
you install from r-forge, not CRAN.  Instructions on how to do this are 
on the r-forge page.

Regards,

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From ssmith88 at umd.edu  Sat Jun 13 00:14:27 2009
From: ssmith88 at umd.edu (ssmith88 at umd.edu)
Date: Fri, 12 Jun 2009 18:14:27 -0400 (EDT)
Subject: [R-SIG-Finance] portfolioFrontier nonsense
Message-ID: <20090612181427.AIY24688@po7.mail.umd.edu>

I'm attempting a very basic optimization with fPortfolio using the default constraints.  Yet, the portfolio frontier that is generated is nonsense.  As the return increases, the risk decreases!  Any help would be greatly appreciated.  Below is my code:



library(fPortfolio)
#Gets date,price, and return data from excel spreadsheet
#------------------------------------------------------------
sp500Reuters<-read.csv("C:\\data\\sp500Reuters.csv")
spgsciReuters<-read.csv("C:\\data\\spgsciReuters.csv")
msciWorldReuters<-read.csv("C:\\data\\msciWorldReuters.csv")
cshfReuters<-read.csv("C:\\data\\cshfReuters.csv")

# Creates return Matrix, timeDate object, and timeSeries object
#------------------------------------------------------------
returnMatrix<-cbind(sp500Reuters[(2:185),3],msciWorldReuters[(2:185),3],spgsciReuters[(2:185),3],cshfReuters[(2:185),3])
colNames<-c("US Equity","World Equity","Commodities","Hedge Fund")
colnames(returnMatrix)<-colNames
returnSeries<-as.timeSeries(returnMatrix)
timeObject<-timeSequence(from="1994-1-31",to="2009-4-30",by="month",format="%Y-%m-%d",FinCenter="GMT")
dataSet<-timeSeries(returnSeries,timeObject,units=NULL,format=NULL,zone="GMT",FinCenter="GMT")
------------------------------------------------------------
# Creates portfolioData object and generates frontier
portfolioData<-portfolioData(dataSet,spec=portfolioSpec())
newFrontier<-portfolioFrontier(portfolioData)

#Note that I've also tried 
newFrontier<-portfolioFrontier(dataSet)

In both cases I get an output such as the following:

Target Return and Risks:
     mean     mu    Cov  Sigma   CVaR    VaR
1  0.0380 0.0380 0.5285 0.5285 1.3375 0.9721
13 0.0500 0.0500 0.4391 0.4391 1.0920 0.7691
25 0.0620 0.0620 0.3609 0.3609 0.8590 0.5488
37 0.0740 0.0740 0.3014 0.3014 0.6797 0.3874
50 0.0869 0.0869 0.2732 0.2732 0.6036 0.4280


Thanks for any help in advance.


From nelson.ana at gmail.com  Sat Jun 13 01:02:44 2009
From: nelson.ana at gmail.com (Ana Nelson)
Date: Sat, 13 Jun 2009 00:02:44 +0100
Subject: [R-SIG-Finance] prices in usd
In-Reply-To: <4A32BEFE.4080703@braverock.com>
References: <C92D6BF93B8E2A4B96E206B66040B916D6B784@CONNCAPSBS.connectcap.local>
	<SANCTUMFISERVERumzk00001acc@sanctumfi.com>
	<SANCTUMFISERVERSQeS00001c69@sanctumfi.com>
	<a7d6d2740906120448p6117c957j4fc05d278dc6219e@mail.gmail.com>
	<C92D6BF93B8E2A4B96E206B66040B916D6B801@CONNCAPSBS.connectcap.local>
	<a7d6d2740906120831w7412a3davf73052e460f0b998@mail.gmail.com>
	<C92D6BF93B8E2A4B96E206B66040B916D6B80B@CONNCAPSBS.connectcap.local>
	<4A32BEFE.4080703@braverock.com>
Message-ID: <a7d6d2740906121602g4528083cn73fdac620374e3bd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090613/cafa6fc7/attachment.pl>

From wuertz at itp.phys.ethz.ch  Sat Jun 13 01:58:48 2009
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Sat, 13 Jun 2009 01:58:48 +0200
Subject: [R-SIG-Finance] portfolioFrontier nonsense
In-Reply-To: <20090612181427.AIY24688@po7.mail.umd.edu>
References: <20090612181427.AIY24688@po7.mail.umd.edu>
Message-ID: <4A32EBB8.2020107@itp.phys.ethz.ch>

ssmith88 at umd.edu wrote:
> I'm attempting a very basic optimization with fPortfolio using the default constraints.  Yet, the portfolio frontier that is generated is nonsense.  As the return increases, the risk decreases!  

Let me start with a comparison: When you fill up your Ferrari with water 
instead
of gas, then the "return in speed decreases" and the "risk of damage 
increases".
This is a problem arising from the kind of fuel you use and not of the 
Ferrari.

I think it is a data problem - when you send me your data, I will try to 
fnd out
what was possibly going wrong.

Kind regards
Diethelm



> Any help would be greatly appreciated.  Below is my code:
>
>
>
> library(fPortfolio)
> #Gets date,price, and return data from excel spreadsheet
> #------------------------------------------------------------
> sp500Reuters<-read.csv("C:\\data\\sp500Reuters.csv")
> spgsciReuters<-read.csv("C:\\data\\spgsciReuters.csv")
> msciWorldReuters<-read.csv("C:\\data\\msciWorldReuters.csv")
> cshfReuters<-read.csv("C:\\data\\cshfReuters.csv")
>
> # Creates return Matrix, timeDate object, and timeSeries object
> #------------------------------------------------------------
> returnMatrix<-cbind(sp500Reuters[(2:185),3],msciWorldReuters[(2:185),3],spgsciReuters[(2:185),3],cshfReuters[(2:185),3])
> colNames<-c("US Equity","World Equity","Commodities","Hedge Fund")
> colnames(returnMatrix)<-colNames
> returnSeries<-as.timeSeries(returnMatrix)
> timeObject<-timeSequence(from="1994-1-31",to="2009-4-30",by="month",format="%Y-%m-%d",FinCenter="GMT")
> dataSet<-timeSeries(returnSeries,timeObject,units=NULL,format=NULL,zone="GMT",FinCenter="GMT")
> ------------------------------------------------------------
> # Creates portfolioData object and generates frontier
> portfolioData<-portfolioData(dataSet,spec=portfolioSpec())
> newFrontier<-portfolioFrontier(portfolioData)
>
> #Note that I've also tried 
> newFrontier<-portfolioFrontier(dataSet)
>
> In both cases I get an output such as the following:
>
> Target Return and Risks:
>      mean     mu    Cov  Sigma   CVaR    VaR
> 1  0.0380 0.0380 0.5285 0.5285 1.3375 0.9721
> 13 0.0500 0.0500 0.4391 0.4391 1.0920 0.7691
> 25 0.0620 0.0620 0.3609 0.3609 0.8590 0.5488
> 37 0.0740 0.0740 0.3014 0.3014 0.6797 0.3874
> 50 0.0869 0.0869 0.2732 0.2732 0.6036 0.4280
>
>
> Thanks for any help in advance.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From ssmith88 at umd.edu  Sat Jun 13 20:45:46 2009
From: ssmith88 at umd.edu (ssmith88 at umd.edu)
Date: Sat, 13 Jun 2009 14:45:46 -0400 (EDT)
Subject: [R-SIG-Finance] portfolioFrontier nonsense
In-Reply-To: <EF227F563ECB4D45951329B08A97E2DF42EC70@SMSG037.ad-its.credit-agricole.fr>
References: <20090612181427.AIY24688@po7.mail.umd.edu>
	<4A32EBB8.2020107@itp.phys.ethz.ch>
	<EF227F563ECB4D45951329B08A97E2DF42EC70@SMSG037.ad-its.credit-agricole.fr>
Message-ID: <20090613144546.AIY59223@po7.mail.umd.edu>

The number of points doesn't change anything.  I think its a data issue, as the routine works fine for any of the sample data that comes with the package.


From spencer.graves at prodsyse.com  Mon Jun 15 02:18:47 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Sun, 14 Jun 2009 17:18:47 -0700
Subject: [R-SIG-Finance] Business day conventions
In-Reply-To: <18991.45292.880391.13045@ron.nulle.part>
References: <579506.84668.qm@web111513.mail.gq1.yahoo.com>
	<18991.45292.880391.13045@ron.nulle.part>
Message-ID: <4A359367.6090807@prodsyse.com>

Hi, Dirk:


1. How do the RQuantLib capabilities compare on this issue with the 
capabilities available in Rmetrics, including the holiday calendars such 
as holidayNYSE and holidayZURICH as well as the timeSequence and 
seq.timeDate functions in the "timeDate" package?


2. R-Forge did not want to give me the current RQuantLib version from 
R-Forge:


 > install.packages("RQuantLib", repos="http://R-Forge.R-project.org")
Warning message:
In getDependencies(pkgs, dependencies, available, lib) :
package ?RQuantLib? is not available


I recently encountered a similar problem trying to access the latest 
version of "RBloomberg" on R-Forge. In that case, the package was "not 
compiling because there is an
out-of-date dependency."


Thanks for all your contributions to the R project.

Best Wishes,
Spencer


Dirk Eddelbuettel wrote:
> Phil,
>
> On 10 June 2009 at 04:49, Phil Joubert wrote:
> | I'm looking for a date package which can handle business day conventions, e.g. Mod Following, etc. (http://en.wikipedia.org/wiki/Date_rolling)
> | 
> | Basically I want to be able to generate a sequence of dates from StartDate to EndDate with a given frequency and following a given business day convention. A stub convention would also be useful :)
> | 
> | Can anyone point me in the right direction?
>
> QuantLib has all the calendar functionality at the C++ level, and we're
> slowly exposing more of it. For example, I recently added a businessDay()
> function as I needed one. This already understands a bunch of calendars, and
> you could try to copy the logic / interface to do something similar for the
> different settlement and calendar combinations.
>
> More generally, Khan (as part of his Google Summer of Code project of
> extending RQuantLib), is exposing more as he is adding a lot of Fixed Income
> functionality.  So you probably want to talk to Khanh (whom I CC'ed).
>
> You can follow this via the R-Forge infrastructure if you're able to work
> from source / svn / tarballs.  There will be new packages at some point, we
> just don't know when :)
>
> Dirk
>
>


From spencer.graves at prodsyse.com  Mon Jun 15 02:39:23 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Sun, 14 Jun 2009 17:39:23 -0700
Subject: [R-SIG-Finance] Real interest rate data
In-Reply-To: <164F0964A312874D9E8F66BA81163455012918E1@mail.palais-coburg.at>
References: <164F0964A312874D9E8F66BA81163455012918E1@mail.palais-coburg.at>
Message-ID: <4A35983B.801@prodsyse.com>

      I did not see a reply on this, so I will offer a comment:  The 
short answer is that there probably is something comparable, available 
for purchase from various sources.  I tried Google, which led me to 
various sources, including IMF statistics 
(http://www.imfstatistics.org/imf/IFSIntRa.htm).  If they provide access 
to data, that might be the most comprehensive available.  However, I 
could not find an appropriate download button in the few seconds I 
devoted to this.  Other, less comprehensive data seems to be available 
from U. Mich (http://www.lib.umich.edu/govdocs/stecon.html). 


      Hope this helps. 
      Spencer Graves


Bastian.Offermann at zzgmbh.at wrote:
> Hi all,
>
> Does anybody know whether there is something comparable to the Penn
> World Tables for real interest rates?
>
> Regards
>
> Bastian
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From enrique.bengoechea at credit-suisse.com  Mon Jun 15 09:32:54 2009
From: enrique.bengoechea at credit-suisse.com (=?iso-8859-1?Q?Bengoechea_Bartolom=E9_Enrique_=28SIES_73=29?=)
Date: Mon, 15 Jun 2009 09:32:54 +0200
Subject: [R-SIG-Finance] portfolioFrontier nonsense
In-Reply-To: <mailman.1.1244887201.11671.r-sig-finance@stat.math.ethz.ch>
Message-ID: <19811401A1D8174CB3EAD7F6072E9B50023410EF@chsa1025.share.beluni.net>

Hi,

This is perfectly possible, depending on the data. For example, optimizing with 6-months data at the beginning of this year with indexes representing major world asset classes would give you an efficient frontier with negative slope (as return increases, risk decreases), as the only asset class with positive returns was money-market.

Enrique

------------------------------

Message: 18
Date: Fri, 12 Jun 2009 18:14:27 -0400 (EDT)
From: <ssmith88 at umd.edu>
Subject: [R-SIG-Finance] portfolioFrontier nonsense
To: r-sig-finance at stat.math.ethz.ch
Message-ID: <20090612181427.AIY24688 at po7.mail.umd.edu>
Content-Type: text/plain; charset=us-ascii

I'm attempting a very basic optimization with fPortfolio using the default constraints.  Yet, the portfolio frontier that is generated is nonsense.  As the return increases, the risk decreases!  Any help would be greatly appreciated.  Below is my code:


From edd at debian.org  Mon Jun 15 12:49:25 2009
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 15 Jun 2009 05:49:25 -0500
Subject: [R-SIG-Finance] Business day conventions
In-Reply-To: <4A359367.6090807@prodsyse.com>
References: <579506.84668.qm@web111513.mail.gq1.yahoo.com>
	<18991.45292.880391.13045@ron.nulle.part>
	<4A359367.6090807@prodsyse.com>
Message-ID: <18998.10037.414259.974750@ron.nulle.part>


Spencer,

On 14 June 2009 at 17:18, spencerg wrote:
| Hi, Dirk:
| 
| 1. How do the RQuantLib capabilities compare on this issue with the 
| capabilities available in Rmetrics, including the holiday calendars such 
| as holidayNYSE and holidayZURICH as well as the timeSequence and 
| seq.timeDate functions in the "timeDate" package?

Favourably.  Quantlib calendaring support is very complete. 

It is also orthogonal to RMetrics and provides a second source of information
/ implementation which can be a good thing.
 
| 2. R-Forge did not want to give me the current RQuantLib version from 
| R-Forge:
| 
| 
|  > install.packages("RQuantLib", repos="http://R-Forge.R-project.org")
| Warning message:
| In getDependencies(pkgs, dependencies, available, lib) :
| package ?RQuantLib? is not available
| 
| 
| I recently encountered a similar problem trying to access the latest 
| version of "RBloomberg" on R-Forge. In that case, the package was "not 
| compiling because there is an
| out-of-date dependency."

Yes, so take the source, read the README (and hence install the required
Quantlib libraries, which may require installing Boost) and install it
locally.  That is a bit of work which is why the r-forge build system does not have
it. 

The CRAN host simply uses my Debian builds of Quantlib for Linux; and I help
Uwe with a Windows build but there are only so many hours in the day so I
haven't done that for Stefan and the Windows side of R-forge. ]
 
| Thanks for all your contributions to the R project.

Always a pleasure.

Dirk

| 
| Best Wishes,
| Spencer
| 
| 
| Dirk Eddelbuettel wrote:
| > Phil,
| >
| > On 10 June 2009 at 04:49, Phil Joubert wrote:
| > | I'm looking for a date package which can handle business day conventions, e.g. Mod Following, etc. (http://en.wikipedia.org/wiki/Date_rolling)
| > | 
| > | Basically I want to be able to generate a sequence of dates from StartDate to EndDate with a given frequency and following a given business day convention. A stub convention would also be useful :)
| > | 
| > | Can anyone point me in the right direction?
| >
| > QuantLib has all the calendar functionality at the C++ level, and we're
| > slowly exposing more of it. For example, I recently added a businessDay()
| > function as I needed one. This already understands a bunch of calendars, and
| > you could try to copy the logic / interface to do something similar for the
| > different settlement and calendar combinations.
| >
| > More generally, Khan (as part of his Google Summer of Code project of
| > extending RQuantLib), is exposing more as he is adding a lot of Fixed Income
| > functionality.  So you probably want to talk to Khanh (whom I CC'ed).
| >
| > You can follow this via the R-Forge infrastructure if you're able to work
| > from source / svn / tarballs.  There will be new packages at some point, we
| > just don't know when :)
| >
| > Dirk
| >
| >
| 
| _______________________________________________
| R-SIG-Finance at stat.math.ethz.ch mailing list
| https://stat.ethz.ch/mailman/listinfo/r-sig-finance
| -- Subscriber-posting only.
| -- If you want to post, subscribe first.

-- 
Three out of two people have difficulties with fractions.


From kabonline07 at yahoo.com  Mon Jun 15 13:24:38 2009
From: kabonline07 at yahoo.com (KAUSHIK BHATTACHARJEE)
Date: Mon, 15 Jun 2009 04:24:38 -0700 (PDT)
Subject: [R-SIG-Finance] Hi this is not a R-problem per se but an
	econometric problem of course
Message-ID: <518087.56134.qm@web110004.mail.gq1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090615/ad3bcdeb/attachment.pl>

From brian at braverock.com  Mon Jun 15 14:33:36 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 15 Jun 2009 07:33:36 -0500
Subject: [R-SIG-Finance] Hi this is not a R-problem per se but
 an	econometric problem of course
In-Reply-To: <518087.56134.qm@web110004.mail.gq1.yahoo.com>
References: <518087.56134.qm@web110004.mail.gq1.yahoo.com>
Message-ID: <4A363FA0.2090903@braverock.com>

 From your description of the system you are trying to estimate, it 
seems that you should be looking at unit roots and cointegration instead 
of the pure *LS methods.

Regards,

  - Brian


KAUSHIK BHATTACHARJEE wrote:
> Dear All,
> I have 4 dependent  time series variables --y1t,y2t,y3t,y4t..
> For any fixed 't', they occer in sequence..first y4t, then y3t,...last y1t.
>   So I have a model like this.....
> y4t= y3tlag1+y2tlag1+y1tlag1+y4tlag1+ error4t
> y3t=  y4t +     y2tlag1+y1tlag1+ error3t
> y2t= y3t + y4t + y2tlag1+ y1tlag1+ error2t
> y1t= y2t + y3t + y4t + y1tag1 + error1t
>
> considering it a triangular (or recusrive--as mentioned in Maddala) system--using OLS I am getting some results. However, when I am considering them occuring for a given time period 't'--so trying tro estimate them jointly by  using 2 Stage Least Squares  (or 3SLS)...I am getting entirely different results in the sense that all the coefficients that are significant in case of OLS are insignificant in 2SLS.
>
> My uestion is : Just because I am changing the method of estimation , why the parameter estimates are  changing so much?
> Any comments/ reference so that where to look for?
> Even it it is model misspecification / non-inclusion of imp variables...it is true for both the case.. So why this difference?
>  
>  Kaushik Bhattacharjee
>   
-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From nands31 at gmail.com  Mon Jun 15 15:44:20 2009
From: nands31 at gmail.com (Subhrangshu Nandi)
Date: Mon, 15 Jun 2009 08:44:20 -0500
Subject: [R-SIG-Finance] Sharpe ratio in tseries
Message-ID: <de69a2b90906150644s4eec37a2kdf905a724c208174@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090615/2efb1570/attachment.pl>

From ggrothendieck at gmail.com  Mon Jun 15 15:57:04 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 15 Jun 2009 09:57:04 -0400
Subject: [R-SIG-Finance] Sharpe ratio in tseries
In-Reply-To: <de69a2b90906150644s4eec37a2kdf905a724c208174@mail.gmail.com>
References: <de69a2b90906150644s4eec37a2kdf905a724c208174@mail.gmail.com>
Message-ID: <971536df0906150657v5b82eebctf91f386c55b0d534@mail.gmail.com>

Try this:

library(tseries)
sharpe


On Mon, Jun 15, 2009 at 9:44 AM, Subhrangshu Nandi<nands31 at gmail.com> wrote:
> Does anyone know what formula the function "sharpe" in package "tseries"
> uses?If we have daily observations as follows:
>
> 1/2/2007 0 1/3/2007 -87.8269 1/4/2007 -214.031 1/5/2007 -295.833 1/8/2007
> -125.284 1/9/2007 213.2553 1/10/2007 -106.514 1/11/2007 -298.365 1/12/2007
> -333.594 1/15/2007 -61.0625 1/16/2007 -346.113 1/17/2007 -505.241 1/18/2007
> 139.586 1/19/2007 -312.135 1/22/2007 -176.663 1/23/2007 -256.924 1/24/2007
> -469.091 1/25/2007 -426.754 1/26/2007 -367.663 1/29/2007 -186.82 1/30/2007
> -1257.42 1/31/2007 -411.755 1/2/2008 -1082.18 1/3/2008 -661.405 1/4/2008
> 699.0845 1/7/2008 -1862.33 1/8/2008 -92.5218 1/9/2008 121.3522 1/10/2008
> -505.142 1/11/2008 54.88142 1/14/2008 -208.322 1/15/2008 -324.522 1/16/2008
> -157.523 1/17/2008 220.3509 1/18/2008 -419.522 1/21/2008 816.2032 1/22/2008
> -5523.15 1/23/2008 -6.2435 1/24/2008 -2015.79 1/25/2008 -758.78 1/28/2008
> -834.751 1/29/2008 -558.338 1/30/2008 -1736.82 1/31/2008 -939.335
>
> Mean -492.297 SD 960.0048
> Annualized Sharpe should be Mean*sqrt(253)/SD = -8.1569. However, the
> function "sharpe" yields -0.22784. Could anyone tell me what I am missing?
>
> Thanks,
> -Nandi
>
> --
> I'm a great believer in luck, and I find the harder I work the more I have
> of it. ?~Thomas Jefferson
>
> Subhrangshu Nandi
> High Frequency Trading
> Greater Chicago Area
> Office:(312) 601-8096
> EFax: (703) 852-7405
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From nands31 at gmail.com  Mon Jun 15 16:01:40 2009
From: nands31 at gmail.com (Subhrangshu Nandi)
Date: Mon, 15 Jun 2009 09:01:40 -0500
Subject: [R-SIG-Finance] Sharpe ratio in tseries
In-Reply-To: <de69a2b90906150644s4eec37a2kdf905a724c208174@mail.gmail.com>
References: <de69a2b90906150644s4eec37a2kdf905a724c208174@mail.gmail.com>
Message-ID: <de69a2b90906150701s6e8fa671y28b3409ec65e7c2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090615/ff01ec98/attachment.pl>

From jeff.a.ryan at gmail.com  Mon Jun 15 16:10:16 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Mon, 15 Jun 2009 09:10:16 -0500
Subject: [R-SIG-Finance] Sharpe ratio in tseries
In-Reply-To: <de69a2b90906150701s6e8fa671y28b3409ec65e7c2@mail.gmail.com>
References: <de69a2b90906150644s4eec37a2kdf905a724c208174@mail.gmail.com>
	<de69a2b90906150701s6e8fa671y28b3409ec65e7c2@mail.gmail.com>
Message-ID: <e8e755250906150710h4cf30957icd8da102a86aed58@mail.gmail.com>

Nandi,

Take a look at the functions in PerformanceAnalytics.

> library(PerformanceAnalytics)
Loading required package: zoo

Package PerformanceAnalytics (0.9.7.1) loaded.
Econometric tools for performance and risk analysis.
(c) 2004-2008 Peter Carl, Brian G. Peterson. License: GPL
http://braverock.com/R/


Attaching package: 'PerformanceAnalytics'


        The following object(s) are masked from package:graphics :

         legend

> ?SharpeRatio
> SharpeRatio
function (Ra, rf = 0)
{
    Ra = checkData(Ra, method = "zoo")
    Ra.excess = Return.excess(Ra, rf)
    return(mean(Ra.excess)/sd(Ra.excess))
}
<environment: namespace:PerformanceAnalytics>

That and others should do what you are expecting.

HTH
Jeff

On Mon, Jun 15, 2009 at 9:01 AM, Subhrangshu Nandi<nands31 at gmail.com> wrote:
> I think I know the reason behind the difference. When a vector of returns
> (x) is passed to the function sharpe(tseries), it returns the sharpe of *
> diff(x)* and not *x*. I'm not sure why the function was constructed in this
> manner. Shouldnt the formula of annualized sharpe me Mean*Sqrt(253)/SD, for
> a daily return series?
> Thanks for your help.
> -Nandi
>
> On Mon, Jun 15, 2009 at 8:44 AM, Subhrangshu Nandi <nands31 at gmail.com>wrote:
>
>> Does anyone know what formula the function "sharpe" in package "tseries"
>> uses?If we have daily observations as follows:
>>
>> 1/2/2007 0 1/3/2007 -87.8269 1/4/2007 -214.031 1/5/2007 -295.833 1/8/2007
>> -125.284 1/9/2007 213.2553 1/10/2007 -106.514 1/11/2007 -298.365 1/12/2007
>> -333.594 1/15/2007 -61.0625 1/16/2007 -346.113 1/17/2007 -505.241 1/18/2007
>> 139.586 1/19/2007 -312.135 1/22/2007 -176.663 1/23/2007 -256.924 1/24/2007
>> -469.091 1/25/2007 -426.754 1/26/2007 -367.663 1/29/2007 -186.82 1/30/2007
>> -1257.42 1/31/2007 -411.755 1/2/2008 -1082.18 1/3/2008 -661.405 1/4/2008
>> 699.0845 1/7/2008 -1862.33 1/8/2008 -92.5218 1/9/2008 121.3522 1/10/2008
>> -505.142 1/11/2008 54.88142 1/14/2008 -208.322 1/15/2008 -324.522 1/16/2008
>> -157.523 1/17/2008 220.3509 1/18/2008 -419.522 1/21/2008 816.2032 1/22/2008
>> -5523.15 1/23/2008 -6.2435 1/24/2008 -2015.79 1/25/2008 -758.78 1/28/2008
>> -834.751 1/29/2008 -558.338 1/30/2008 -1736.82 1/31/2008 -939.335
>>
>> Mean -492.297 SD 960.0048
>> Annualized Sharpe should be Mean*sqrt(253)/SD = -8.1569. However, the
>> function "sharpe" yields -0.22784. Could anyone tell me what I am missing?
>>
>> Thanks,
>> -Nandi
>>
>> --
>> I'm a great believer in luck, and I find the harder I work the more I have
>> of it. ?~Thomas Jefferson
>>
>> Subhrangshu Nandi
>> High Frequency Trading
>> Greater Chicago Area
>> Office:(312) 601-8096
>> EFax: (703) 852-7405
>>
>
>
>
> --
> I'm a great believer in luck, and I find the harder I work the more I have
> of it. ?~Thomas Jefferson
>
> Subhrangshu Nandi
> High Frequency Trading
> Greater Chicago Area
> Office:(312) 601-8096
> EFax: (703) 852-7405
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From windspeedo99 at gmail.com  Mon Jun 15 18:04:53 2009
From: windspeedo99 at gmail.com (Wind)
Date: Tue, 16 Jun 2009 00:04:53 +0800
Subject: [R-SIG-Finance] Speed optimization on minutes distribution
	calculation
Message-ID: <d718c8210906150904r6aefb423k216a06a3a140b365@mail.gmail.com>

I want to plot the distribution of volume of the future  CLN9 along
the 24 hours axis.   The following codes could complete the task.  But
it is very time consuming when sapply(mins,function(x)
{mean(hqm[which(format(index(hqm),"%H:%M")==x),5])}).
Any suggestion for codes with better performance would be highly appreciated.


The data hqm has been retrieved from IB via IBrokers.

> head(hqm[,5])
                    CLN9.Volume
2009-05-25 06:00:00          17
2009-05-25 06:01:00           2
2009-05-25 06:02:00          11
2009-05-25 06:03:00          26
2009-05-25 06:04:00          20
2009-05-25 06:05:00           5
> tail(hqm[,5])
                    CLN9.Volume
2009-06-15 21:51:00        1050
2009-06-15 21:52:00         807
2009-06-15 21:53:00         782
2009-06-15 21:54:00         385
2009-06-15 21:55:00         562
2009-06-15 21:56:00         423
>mins<-unlist(lapply(0:23,function(h){sapply(0:59,function(m){paste(sprintf("%02d",h),sprintf("%02d",m),sep=":")})}))
> head(mins)
[1] "00:00" "00:01" "00:02" "00:03" "00:04" "00:05"
> tail(mins)
[1] "23:54" "23:55" "23:56" "23:57" "23:58" "23:59"

>temp<-sapply(mins,function(x) {mean(hqm[which(format(index(hqm),"%H:%M")==x),5])})
> head(temp)
   00:00    00:01    00:02    00:03    00:04    00:05
279.1333 284.9333 247.8667 176.3333 278.8667 179.0667
> tail(temp)
   23:54    23:55    23:56    23:57    23:58    23:59
250.2667 312.7333 318.9333 210.8000 258.2000 232.8667
>plot(temp)


From comtech.usa at gmail.com  Mon Jun 15 18:20:28 2009
From: comtech.usa at gmail.com (Michael)
Date: Mon, 15 Jun 2009 09:20:28 -0700
Subject: [R-SIG-Finance] hands-on book on financial time series with R?
Message-ID: <b1f16d9d0906150920h78a93633j4535370883bb7e42@mail.gmail.com>

Hi all,

I am looking for pointers to good books on financial time series with
R, so I could do some experiments while learning financial time
series...

Thanks a lot?


From andyzhu35 at yahoo.com  Mon Jun 15 18:37:46 2009
From: andyzhu35 at yahoo.com (Andy Zhu)
Date: Mon, 15 Jun 2009 09:37:46 -0700 (PDT)
Subject: [R-SIG-Finance] portfolioFrontier nonsense
Message-ID: <160323.80821.qm@web56206.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090615/9446fd93/attachment.pl>

From Bastian.Offermann at zzgmbh.at  Mon Jun 15 18:38:10 2009
From: Bastian.Offermann at zzgmbh.at (Bastian.Offermann at zzgmbh.at)
Date: Mon, 15 Jun 2009 18:38:10 +0200
Subject: [R-SIG-Finance] hands-on book on financial time series with R?
In-Reply-To: <b1f16d9d0906150920h78a93633j4535370883bb7e42@mail.gmail.com>
Message-ID: <164F0964A312874D9E8F66BA81163455012918F1@mail.palais-coburg.at>

R:

Time Series Analysis: With Applications in R (Springer Texts in Statistics) by Jonathan D. Cryer and Kung-Sik Chan 


Not R, but S-Plus

Modeling Financial Time Series with S-PLUS(r) by Eric Zivot and Jiahui Wang 
Statistical Analysis of Financial Data in S-PLUS by Rene A. Carmona 

-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Michael
Gesendet: Montag, 15. Juni 2009 18:20
An: r-sig-finance at stat.math.ethz.ch
Betreff: [R-SIG-Finance] hands-on book on financial time series with R?


Hi all,

I am looking for pointers to good books on financial time series with R, so I could do some experiments while learning financial time series...

Thanks a lot?

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From rbali at ufmg.br  Mon Jun 15 18:56:31 2009
From: rbali at ufmg.br (Robert Iquiapaza)
Date: Mon, 15 Jun 2009 13:56:31 -0300
Subject: [R-SIG-Finance] hands-on book on financial time series with R?
In-Reply-To: <b1f16d9d0906150920h78a93633j4535370883bb7e42@mail.gmail.com>
References: <b1f16d9d0906150920h78a93633j4535370883bb7e42@mail.gmail.com>
Message-ID: <9AAE974EB1554061B64C5265F8F4D8BF@DellPC>

Look at 

http://www.r-project.org/doc/bib/R-books.html

Robert

--------------------------------------------------
From: "Michael" <comtech.usa at gmail.com>
Sent: Monday, June 15, 2009 1:20 PM
To: <r-sig-finance at stat.math.ethz.ch>
Subject: [R-SIG-Finance] hands-on book on financial time series with R?

> Hi all,
> 
> I am looking for pointers to good books on financial time series with
> R, so I could do some experiments while learning financial time
> series...
> 
> Thanks a lot?
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From spencer.graves at prodsyse.com  Mon Jun 15 19:15:18 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Mon, 15 Jun 2009 10:15:18 -0700
Subject: [R-SIG-Finance] hands-on book on financial time series with R?
In-Reply-To: <164F0964A312874D9E8F66BA81163455012918F1@mail.palais-coburg.at>
References: <164F0964A312874D9E8F66BA81163455012918F1@mail.palais-coburg.at>
Message-ID: <4A3681A6.8040400@prodsyse.com>

Also: Wuertz, et al. (2009) Portfolio Optimization with R/Rmetrics
(ebook, obtainable for 88 CHF / ~ 84 USD from
http://www.rmetrics.org/ebook.htm).

This is a companion to the Rmetrics project, so it covers that and
perhaps not other time series capabilities in R.

Spencer

Bastian.Offermann at zzgmbh.at wrote:
> R:
>
> Time Series Analysis: With Applications in R (Springer Texts in Statistics) by Jonathan D. Cryer and Kung-Sik Chan 
>
>
> Not R, but S-Plus
>
> Modeling Financial Time Series with S-PLUS(r) by Eric Zivot and Jiahui Wang 
> Statistical Analysis of Financial Data in S-PLUS by Rene A. Carmona 
>
> -----Urspr?ngliche Nachricht-----
> Von: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Michael
> Gesendet: Montag, 15. Juni 2009 18:20
> An: r-sig-finance at stat.math.ethz.ch
> Betreff: [R-SIG-Finance] hands-on book on financial time series with R?
>
>
> Hi all,
>
> I am looking for pointers to good books on financial time series with R, so I could do some experiments while learning financial time series...
>
> Thanks a lot?
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From malcolm.croucher at gmail.com  Mon Jun 15 20:30:44 2009
From: malcolm.croucher at gmail.com (malcolm Crouch)
Date: Mon, 15 Jun 2009 20:30:44 +0200
Subject: [R-SIG-Finance] Finance Data
Message-ID: <386fa5610906151130p1e4a8abdgd1b6e6efd2ebd54a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090615/241985cc/attachment.pl>

From bnersesian at yahoo.com  Mon Jun 15 20:49:28 2009
From: bnersesian at yahoo.com (burke nersesian)
Date: Mon, 15 Jun 2009 11:49:28 -0700 (PDT)
Subject: [R-SIG-Finance] Basic Mean Variance Optimization
Message-ID: <196436.69222.qm@web50907.mail.re2.yahoo.com>


Need single period weighting for simple portfolio (minimal assumptions and constraints).
Each asset has a variance = 1.0 and expected return = 1.0 and the covariance with each of the other assets in the portfolio is > 0.0 and < 1.0,
(in other words, the weighting will only be dependent on the covariance matrix to produce in essence the minimum variance portfolio). 
If possible, I would prefer each weight to be positive. The portfolio contains hundreds of assets and must be calculated thousands of times so R seems too slow for the job.
Does anyone know of any C function in say, GSL, BLAS or any other library that takes a Convariance Matrix as input and returns a weight vector as output?
The solution doesn't have to be guaranteed to be robust, if it dominates the equal weighted portfolio that's sufficient.
Any ideas will help, thank you all so much!


From jeff.a.ryan at gmail.com  Mon Jun 15 20:52:21 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Mon, 15 Jun 2009 13:52:21 -0500
Subject: [R-SIG-Finance] Finance Data
In-Reply-To: <386fa5610906151130p1e4a8abdgd1b6e6efd2ebd54a@mail.gmail.com>
References: <386fa5610906151130p1e4a8abdgd1b6e6efd2ebd54a@mail.gmail.com>
Message-ID: <e8e755250906151152x4bad25ffj8f3117be444d358d@mail.gmail.com>

Try FRED

http://research.stlouisfed.org/fred2/search/vehicle/1
http://research.stlouisfed.org/fred2/search/retail+sales/1

library(quantmod)
> getSymbols("RETAIL", src="FRED")
[1] "RETAIL"
> head(RETAIL)
           RETAIL
1947-01-01   9583
1947-02-01   9852
1947-03-01   9769
1947-04-01   9947
1947-05-01  10061
1947-06-01  10146
>

HTH
Jeff

On Mon, Jun 15, 2009 at 1:30 PM, malcolm
Crouch<malcolm.croucher at gmail.com> wrote:
> Hi ,
>
> I am looking for US GDP Figures , non-pharm Payrolls , retail sales , motor
> vehicle sales ect.
>
> Does anyone know where i can find data like that ?
>
> i have found :
>
> http://www.bls.gov/data/#employment ? ?- ?Labour Data
> http://www.economicindicators.gov/ ?- Some Data
> http://www.bea.gov/ ? -- Economic Data
>
> im still struggling to find a csv or excel data source for payrolls Motor
> Vehicle Sales and Retail Sales.
>
> Regards
>
> Malcolm
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From ssmith88 at umd.edu  Mon Jun 15 22:48:16 2009
From: ssmith88 at umd.edu (ssmith88 at umd.edu)
Date: Mon, 15 Jun 2009 16:48:16 -0400 (EDT)
Subject: [R-SIG-Finance] portfolioFrontier nonsense
In-Reply-To: <19811401A1D8174CB3EAD7F6072E9B50023410EF@chsa1025.share.beluni.net>
References: <mailman.1.1244887201.11671.r-sig-finance@stat.math.ethz.ch>
	<19811401A1D8174CB3EAD7F6072E9B50023410EF@chsa1025.share.beluni.net>
Message-ID: <20090615164816.AIZ59967@po7.mail.umd.edu>

I think you are correct.  It turns out if I eliminate the hedge fund data the frontier looks normal.  Yet, I still haven't quite figured out what it was about the hedge fund data that changed the frontier so much.


From brian at braverock.com  Tue Jun 16 02:00:15 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 15 Jun 2009 19:00:15 -0500
Subject: [R-SIG-Finance] Speed optimization on minutes
	distribution	calculation
In-Reply-To: <d718c8210906150904r6aefb423k216a06a3a140b365@mail.gmail.com>
References: <d718c8210906150904r6aefb423k216a06a3a140b365@mail.gmail.com>
Message-ID: <4A36E08F.2000308@braverock.com>

It seems that the slow part is all the character string manipulation.  
This would be slow in almost any programming language.   Honestly, I am 
always annoyed by useless axes in charts that simply count from 1 to n.  
A time axis at least has some real meaning, and avoids the useless 
rewriting of character strings.

You should be able to get a meaningful, readable axis using the 
periodicity() function in xts without the string manipulation.

Regards,

    - Brian

Wind wrote:
> I want to plot the distribution of volume of the future  CLN9 along
> the 24 hours axis.   The following codes could complete the task.  But
> it is very time consuming when sapply(mins,function(x)
> {mean(hqm[which(format(index(hqm),"%H:%M")==x),5])}).
> Any suggestion for codes with better performance would be highly appreciated.
>
>
> The data hqm has been retrieved from IB via IBrokers.
>
>   
>> head(hqm[,5])
>>     
>                     CLN9.Volume
> 2009-05-25 06:00:00          17
> 2009-05-25 06:01:00           2
> 2009-05-25 06:02:00          11
> 2009-05-25 06:03:00          26
> 2009-05-25 06:04:00          20
> 2009-05-25 06:05:00           5
>   
>> tail(hqm[,5])
>>     
>                     CLN9.Volume
> 2009-06-15 21:51:00        1050
> 2009-06-15 21:52:00         807
> 2009-06-15 21:53:00         782
> 2009-06-15 21:54:00         385
> 2009-06-15 21:55:00         562
> 2009-06-15 21:56:00         423
>   
>> mins<-unlist(lapply(0:23,function(h){sapply(0:59,function(m){paste(sprintf("%02d",h),sprintf("%02d",m),sep=":")})}))
>> head(mins)
>>     
> [1] "00:00" "00:01" "00:02" "00:03" "00:04" "00:05"
>   
>> tail(mins)
>>     
> [1] "23:54" "23:55" "23:56" "23:57" "23:58" "23:59"
>
>   
>> temp<-sapply(mins,function(x) {mean(hqm[which(format(index(hqm),"%H:%M")==x),5])})
>> head(temp)
>>     
>    00:00    00:01    00:02    00:03    00:04    00:05
> 279.1333 284.9333 247.8667 176.3333 278.8667 179.0667
>   
>> tail(temp)
>>     
>    23:54    23:55    23:56    23:57    23:58    23:59
> 250.2667 312.7333 318.9333 210.8000 258.2000 232.8667
>   
>> plot(temp)
>>     
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>   


-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From windspeedo99 at gmail.com  Tue Jun 16 04:27:12 2009
From: windspeedo99 at gmail.com (Wind)
Date: Tue, 16 Jun 2009 10:27:12 +0800
Subject: [R-SIG-Finance] Speed optimization on minutes distribution
	calculation
In-Reply-To: <4A36E08F.2000308@braverock.com>
References: <d718c8210906150904r6aefb423k216a06a3a140b365@mail.gmail.com>
	<4A36E08F.2000308@braverock.com>
Message-ID: <d718c8210906151927y5f9e9912j69e3fd91b9075b09@mail.gmail.com>

periodicity() function in xts is a good tool for axis manipulation.

Maybe I should not use character string methods to complie the
distribution of minutes volume, as Brian suggested.   But what
function should be used for such task in R?  I've tried in kdb+ , it
is  somewhat simple and quick enough with select and xbar function.
But I am not familiar with R.  Maybe there is some functions for this
specific task I don't know.

Thanks Brian.


On Tue, Jun 16, 2009 at 8:00 AM, Brian G. Peterson<brian at braverock.com> wrote:
> It seems that the slow part is all the character string manipulation. ?This
> would be slow in almost any programming language. ? Honestly, I am always
> annoyed by useless axes in charts that simply count from 1 to n. ?A time
> axis at least has some real meaning, and avoids the useless rewriting of
> character strings.
>
> You should be able to get a meaningful, readable axis using the
> periodicity() function in xts without the string manipulation.
>
> Regards,
>
> ? - Brian
>
> Wind wrote:
>>
>> I want to plot the distribution of volume of the future ?CLN9 along
>> the 24 hours axis. ? The following codes could complete the task. ?But
>> it is very time consuming when sapply(mins,function(x)
>> {mean(hqm[which(format(index(hqm),"%H:%M")==x),5])}).
>> Any suggestion for codes with better performance would be highly
>> appreciated.
>>
>>
>> The data hqm has been retrieved from IB via IBrokers.
>>
>>
>>>
>>> head(hqm[,5])
>>>
>>
>> ? ? ? ? ? ? ? ? ? ?CLN9.Volume
>> 2009-05-25 06:00:00 ? ? ? ? ?17
>> 2009-05-25 06:01:00 ? ? ? ? ? 2
>> 2009-05-25 06:02:00 ? ? ? ? ?11
>> 2009-05-25 06:03:00 ? ? ? ? ?26
>> 2009-05-25 06:04:00 ? ? ? ? ?20
>> 2009-05-25 06:05:00 ? ? ? ? ? 5
>>
>>>
>>> tail(hqm[,5])
>>>
>>
>> ? ? ? ? ? ? ? ? ? ?CLN9.Volume
>> 2009-06-15 21:51:00 ? ? ? ?1050
>> 2009-06-15 21:52:00 ? ? ? ? 807
>> 2009-06-15 21:53:00 ? ? ? ? 782
>> 2009-06-15 21:54:00 ? ? ? ? 385
>> 2009-06-15 21:55:00 ? ? ? ? 562
>> 2009-06-15 21:56:00 ? ? ? ? 423
>>
>>>
>>>
>>> mins<-unlist(lapply(0:23,function(h){sapply(0:59,function(m){paste(sprintf("%02d",h),sprintf("%02d",m),sep=":")})}))
>>> head(mins)
>>>
>>
>> [1] "00:00" "00:01" "00:02" "00:03" "00:04" "00:05"
>>
>>>
>>> tail(mins)
>>>
>>
>> [1] "23:54" "23:55" "23:56" "23:57" "23:58" "23:59"
>>
>>
>>>
>>> temp<-sapply(mins,function(x)
>>> {mean(hqm[which(format(index(hqm),"%H:%M")==x),5])})
>>> head(temp)
>>>
>>
>> ? 00:00 ? ?00:01 ? ?00:02 ? ?00:03 ? ?00:04 ? ?00:05
>> 279.1333 284.9333 247.8667 176.3333 278.8667 179.0667
>>
>>>
>>> tail(temp)
>>>
>>
>> ? 23:54 ? ?23:55 ? ?23:56 ? ?23:57 ? ?23:58 ? ?23:59
>> 250.2667 312.7333 318.9333 210.8000 258.2000 232.8667
>>
>>>
>>> plot(temp)
>>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
> --
> Brian G. Peterson
> http://braverock.com/brian/
> Ph: 773-459-4973
> IM: bgpbraverock
>
>
>


From jeff.a.ryan at gmail.com  Tue Jun 16 05:24:39 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Mon, 15 Jun 2009 22:24:39 -0500
Subject: [R-SIG-Finance] Speed optimization on minutes distribution
	calculation
In-Reply-To: <d718c8210906151927y5f9e9912j69e3fd91b9075b09@mail.gmail.com>
References: <d718c8210906150904r6aefb423k216a06a3a140b365@mail.gmail.com>
	<4A36E08F.2000308@braverock.com>
	<d718c8210906151927y5f9e9912j69e3fd91b9075b09@mail.gmail.com>
Message-ID: <e8e755250906152024q75047202t61a17b7eb27aa9ae@mail.gmail.com>

I think you want something like ?aggregate.zoo

I didn't pull actual volume data, but here is an example that will
show what you can do:

library(xts)  ## only used for the sequence and to leverage
aggregate.zoo internally.

## generate a sequence of POSIXct 1 mo @ 1min
x <- timeBasedSeq('20090515/20090615 12:00')

## convert to POSIXlt and turn into HHMM numeric format
hm <- as.POSIXlt(x)$min + as.POSIXlt(x)$hour * 100

##  your original "Volume" column (here a simple xts object with each
min having Vol=1000)
##  There are 32 observations at each minute in 00:00--12:00 and 31
for 12:01--23:59
xx <- xts(rep(1000,length(x)), x)

##  using 'aggregate' to apply sum to the matching times
ax <- aggregate(xx, as.factor(hm), sum)

head(ax)

0 32000
1 32000
2 32000
3 32000
4 32000
5 32000
> tail(ax)

2354 31000
2355 31000
2356 31000
2357 31000
2358 31000
2359 31000

I haven't had a chance to actually test this, but at the very least it
should provide a start for you.

And the above is very fast:

 system.time(ax <- aggregate(xx, as.factor(hm), sum))
   user  system elapsed
  0.058   0.015   0.073

HTH
Jeff
On Mon, Jun 15, 2009 at 9:27 PM, Wind<windspeedo99 at gmail.com> wrote:
> periodicity() function in xts is a good tool for axis manipulation.
>
> Maybe I should not use character string methods to complie the
> distribution of minutes volume, as Brian suggested. ? But what
> function should be used for such task in R? ?I've tried in kdb+ , it
> is ?somewhat simple and quick enough with select and xbar function.
> But I am not familiar with R. ?Maybe there is some functions for this
> specific task I don't know.
>
> Thanks Brian.
>
>
> On Tue, Jun 16, 2009 at 8:00 AM, Brian G. Peterson<brian at braverock.com> wrote:
>> It seems that the slow part is all the character string manipulation. ?This
>> would be slow in almost any programming language. ? Honestly, I am always
>> annoyed by useless axes in charts that simply count from 1 to n. ?A time
>> axis at least has some real meaning, and avoids the useless rewriting of
>> character strings.
>>
>> You should be able to get a meaningful, readable axis using the
>> periodicity() function in xts without the string manipulation.
>>
>> Regards,
>>
>> ? - Brian
>>
>> Wind wrote:
>>>
>>> I want to plot the distribution of volume of the future ?CLN9 along
>>> the 24 hours axis. ? The following codes could complete the task. ?But
>>> it is very time consuming when sapply(mins,function(x)
>>> {mean(hqm[which(format(index(hqm),"%H:%M")==x),5])}).
>>> Any suggestion for codes with better performance would be highly
>>> appreciated.
>>>
>>>
>>> The data hqm has been retrieved from IB via IBrokers.
>>>
>>>
>>>>
>>>> head(hqm[,5])
>>>>
>>>
>>> ? ? ? ? ? ? ? ? ? ?CLN9.Volume
>>> 2009-05-25 06:00:00 ? ? ? ? ?17
>>> 2009-05-25 06:01:00 ? ? ? ? ? 2
>>> 2009-05-25 06:02:00 ? ? ? ? ?11
>>> 2009-05-25 06:03:00 ? ? ? ? ?26
>>> 2009-05-25 06:04:00 ? ? ? ? ?20
>>> 2009-05-25 06:05:00 ? ? ? ? ? 5
>>>
>>>>
>>>> tail(hqm[,5])
>>>>
>>>
>>> ? ? ? ? ? ? ? ? ? ?CLN9.Volume
>>> 2009-06-15 21:51:00 ? ? ? ?1050
>>> 2009-06-15 21:52:00 ? ? ? ? 807
>>> 2009-06-15 21:53:00 ? ? ? ? 782
>>> 2009-06-15 21:54:00 ? ? ? ? 385
>>> 2009-06-15 21:55:00 ? ? ? ? 562
>>> 2009-06-15 21:56:00 ? ? ? ? 423
>>>
>>>>
>>>>
>>>> mins<-unlist(lapply(0:23,function(h){sapply(0:59,function(m){paste(sprintf("%02d",h),sprintf("%02d",m),sep=":")})}))
>>>> head(mins)
>>>>
>>>
>>> [1] "00:00" "00:01" "00:02" "00:03" "00:04" "00:05"
>>>
>>>>
>>>> tail(mins)
>>>>
>>>
>>> [1] "23:54" "23:55" "23:56" "23:57" "23:58" "23:59"
>>>
>>>
>>>>
>>>> temp<-sapply(mins,function(x)
>>>> {mean(hqm[which(format(index(hqm),"%H:%M")==x),5])})
>>>> head(temp)
>>>>
>>>
>>> ? 00:00 ? ?00:01 ? ?00:02 ? ?00:03 ? ?00:04 ? ?00:05
>>> 279.1333 284.9333 247.8667 176.3333 278.8667 179.0667
>>>
>>>>
>>>> tail(temp)
>>>>
>>>
>>> ? 23:54 ? ?23:55 ? ?23:56 ? ?23:57 ? ?23:58 ? ?23:59
>>> 250.2667 312.7333 318.9333 210.8000 258.2000 232.8667
>>>
>>>>
>>>> plot(temp)
>>>>
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>>
>> --
>> Brian G. Peterson
>> http://braverock.com/brian/
>> Ph: 773-459-4973
>> IM: bgpbraverock
>>
>>
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From jeff.a.ryan at gmail.com  Tue Jun 16 05:45:17 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Mon, 15 Jun 2009 22:45:17 -0500
Subject: [R-SIG-Finance] Speed optimization on minutes distribution
	calculation
In-Reply-To: <e8e755250906152024q75047202t61a17b7eb27aa9ae@mail.gmail.com>
References: <d718c8210906150904r6aefb423k216a06a3a140b365@mail.gmail.com>
	<4A36E08F.2000308@braverock.com>
	<d718c8210906151927y5f9e9912j69e3fd91b9075b09@mail.gmail.com>
	<e8e755250906152024q75047202t61a17b7eb27aa9ae@mail.gmail.com>
Message-ID: <e8e755250906152045t629c66edod6004f56f6d73b3b@mail.gmail.com>

An actual example using IBrokers/IB

NQ <- reqHistoricalData(tws,
          twsFUT("NQ","GLOBEX","200909"),
          useRTH="0", bar="1 min", dur="5 D")

str(NQ)
An 'xts' object from 2009-06-09 15:30:00 to 2009-06-15 22:33:00 containing:
  Data: num [1:5910, 1:8] 1510 1510 1510 1510 1510 1510 1510 1510 1510 1510 ...
 - attr(*, "dimnames")=List of 2
  ..$ : NULL
  ..$ : chr [1:8] "NQU9.Open" "NQU9.High" "NQU9.Low" "NQU9.Close" ...
  Indexed by objects of class: [POSIXt,POSIXct] TZ: America/Chicago
  xts Attributes:
List of 4
 $ from   : chr "20090611  04:33:46"
 $ to     : chr "20090616  04:33:46"
 $ src    : chr "IB"
 $ updated: POSIXct[1:1], format: "2009-06-15 22:33:46.46141"

nqi <- index(NQ)
hm <- as.POSIXlt(nqi)$min + as.POSIXlt(nqi)$hour*100
NQV <- aggregate(Vo(NQ), as.factor(hm), sum)

barplot(NQV)

The axis/chart leaves a lot to be desired, but once again that should
be enough to set you on the right path.

HTH
Jeff

On Mon, Jun 15, 2009 at 10:24 PM, Jeff Ryan<jeff.a.ryan at gmail.com> wrote:
> I think you want something like ?aggregate.zoo
>
> I didn't pull actual volume data, but here is an example that will
> show what you can do:
>
> library(xts) ?## only used for the sequence and to leverage
> aggregate.zoo internally.
>
> ## generate a sequence of POSIXct 1 mo @ 1min
> x <- timeBasedSeq('20090515/20090615 12:00')
>
> ## convert to POSIXlt and turn into HHMM numeric format
> hm <- as.POSIXlt(x)$min + as.POSIXlt(x)$hour * 100
>
> ## ?your original "Volume" column (here a simple xts object with each
> min having Vol=1000)
> ## ?There are 32 observations at each minute in 00:00--12:00 and 31
> for 12:01--23:59
> xx <- xts(rep(1000,length(x)), x)
>
> ## ?using 'aggregate' to apply sum to the matching times
> ax <- aggregate(xx, as.factor(hm), sum)
>
> head(ax)
>
> 0 32000
> 1 32000
> 2 32000
> 3 32000
> 4 32000
> 5 32000
>> tail(ax)
>
> 2354 31000
> 2355 31000
> 2356 31000
> 2357 31000
> 2358 31000
> 2359 31000
>
> I haven't had a chance to actually test this, but at the very least it
> should provide a start for you.
>
> And the above is very fast:
>
> ?system.time(ax <- aggregate(xx, as.factor(hm), sum))
> ? user ?system elapsed
> ?0.058 ? 0.015 ? 0.073
>
> HTH
> Jeff
> On Mon, Jun 15, 2009 at 9:27 PM, Wind<windspeedo99 at gmail.com> wrote:
>> periodicity() function in xts is a good tool for axis manipulation.
>>
>> Maybe I should not use character string methods to complie the
>> distribution of minutes volume, as Brian suggested. ? But what
>> function should be used for such task in R? ?I've tried in kdb+ , it
>> is ?somewhat simple and quick enough with select and xbar function.
>> But I am not familiar with R. ?Maybe there is some functions for this
>> specific task I don't know.
>>
>> Thanks Brian.
>>
>>
>> On Tue, Jun 16, 2009 at 8:00 AM, Brian G. Peterson<brian at braverock.com> wrote:
>>> It seems that the slow part is all the character string manipulation. ?This
>>> would be slow in almost any programming language. ? Honestly, I am always
>>> annoyed by useless axes in charts that simply count from 1 to n. ?A time
>>> axis at least has some real meaning, and avoids the useless rewriting of
>>> character strings.
>>>
>>> You should be able to get a meaningful, readable axis using the
>>> periodicity() function in xts without the string manipulation.
>>>
>>> Regards,
>>>
>>> ? - Brian
>>>
>>> Wind wrote:
>>>>
>>>> I want to plot the distribution of volume of the future ?CLN9 along
>>>> the 24 hours axis. ? The following codes could complete the task. ?But
>>>> it is very time consuming when sapply(mins,function(x)
>>>> {mean(hqm[which(format(index(hqm),"%H:%M")==x),5])}).
>>>> Any suggestion for codes with better performance would be highly
>>>> appreciated.
>>>>
>>>>
>>>> The data hqm has been retrieved from IB via IBrokers.
>>>>
>>>>
>>>>>
>>>>> head(hqm[,5])
>>>>>
>>>>
>>>> ? ? ? ? ? ? ? ? ? ?CLN9.Volume
>>>> 2009-05-25 06:00:00 ? ? ? ? ?17
>>>> 2009-05-25 06:01:00 ? ? ? ? ? 2
>>>> 2009-05-25 06:02:00 ? ? ? ? ?11
>>>> 2009-05-25 06:03:00 ? ? ? ? ?26
>>>> 2009-05-25 06:04:00 ? ? ? ? ?20
>>>> 2009-05-25 06:05:00 ? ? ? ? ? 5
>>>>
>>>>>
>>>>> tail(hqm[,5])
>>>>>
>>>>
>>>> ? ? ? ? ? ? ? ? ? ?CLN9.Volume
>>>> 2009-06-15 21:51:00 ? ? ? ?1050
>>>> 2009-06-15 21:52:00 ? ? ? ? 807
>>>> 2009-06-15 21:53:00 ? ? ? ? 782
>>>> 2009-06-15 21:54:00 ? ? ? ? 385
>>>> 2009-06-15 21:55:00 ? ? ? ? 562
>>>> 2009-06-15 21:56:00 ? ? ? ? 423
>>>>
>>>>>
>>>>>
>>>>> mins<-unlist(lapply(0:23,function(h){sapply(0:59,function(m){paste(sprintf("%02d",h),sprintf("%02d",m),sep=":")})}))
>>>>> head(mins)
>>>>>
>>>>
>>>> [1] "00:00" "00:01" "00:02" "00:03" "00:04" "00:05"
>>>>
>>>>>
>>>>> tail(mins)
>>>>>
>>>>
>>>> [1] "23:54" "23:55" "23:56" "23:57" "23:58" "23:59"
>>>>
>>>>
>>>>>
>>>>> temp<-sapply(mins,function(x)
>>>>> {mean(hqm[which(format(index(hqm),"%H:%M")==x),5])})
>>>>> head(temp)
>>>>>
>>>>
>>>> ? 00:00 ? ?00:01 ? ?00:02 ? ?00:03 ? ?00:04 ? ?00:05
>>>> 279.1333 284.9333 247.8667 176.3333 278.8667 179.0667
>>>>
>>>>>
>>>>> tail(temp)
>>>>>
>>>>
>>>> ? 23:54 ? ?23:55 ? ?23:56 ? ?23:57 ? ?23:58 ? ?23:59
>>>> 250.2667 312.7333 318.9333 210.8000 258.2000 232.8667
>>>>
>>>>>
>>>>> plot(temp)
>>>>>
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>
>>>
>>> --
>>> Brian G. Peterson
>>> http://braverock.com/brian/
>>> Ph: 773-459-4973
>>> IM: bgpbraverock
>>>
>>>
>>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> --
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From ssmith88 at umd.edu  Tue Jun 16 08:59:57 2009
From: ssmith88 at umd.edu (ssmith88 at umd.edu)
Date: Tue, 16 Jun 2009 02:59:57 -0400 (EDT)
Subject: [R-SIG-Finance] SPS and QLPM portfolios
Message-ID: <20090616025957.AIZ81634@po7.mail.umd.edu>

Does anyone have any sample code on how to implement an optimization with SPS or QLPM portfolios?  Is there a way to specify all 5 parameters for the SPS risk measure?  Thanks very much.

Scott Smith


From windspeedo99 at gmail.com  Tue Jun 16 09:12:16 2009
From: windspeedo99 at gmail.com (Wind)
Date: Tue, 16 Jun 2009 15:12:16 +0800
Subject: [R-SIG-Finance] Speed optimization on minutes distribution
	calculation
In-Reply-To: <e8e755250906152045t629c66edod6004f56f6d73b3b@mail.gmail.com>
References: <d718c8210906150904r6aefb423k216a06a3a140b365@mail.gmail.com>
	<4A36E08F.2000308@braverock.com>
	<d718c8210906151927y5f9e9912j69e3fd91b9075b09@mail.gmail.com>
	<e8e755250906152024q75047202t61a17b7eb27aa9ae@mail.gmail.com>
	<e8e755250906152045t629c66edod6004f56f6d73b3b@mail.gmail.com>
Message-ID: <d718c8210906160012x5842ae48q513095421b75e240@mail.gmail.com>

> nqi <- index(NQ)
> hm <- as.POSIXlt(nqi)$min + as.POSIXlt(nqi)$hour*100
> NQV <- aggregate(Vo(NQ), as.factor(hm), sum)

That's exactly what I need.
The speed is amazing.  As quick as kdb+,  according to subjective judement.
Frankly speaking,  I never imagined the speed of R  could match kdb+.

By the way, it's also a great idea of creating a demand based database
system using getSymbols which mentioned in your ppt in the RFin 2009.
 Something like a hybrid of cloud computing and advanced quickest
database.    Before that, sometimes I used to store the data in a
database like kdb+ and retrieve them to R for furthur analysis.

Thanks Jeff and Brian.


On Tue, Jun 16, 2009 at 11:45 AM, Jeff Ryan<jeff.a.ryan at gmail.com> wrote:
> An actual example using IBrokers/IB
>
> NQ <- reqHistoricalData(tws,
> ? ? ? ? ?twsFUT("NQ","GLOBEX","200909"),
> ? ? ? ? ?useRTH="0", bar="1 min", dur="5 D")
>
> str(NQ)
> An 'xts' object from 2009-06-09 15:30:00 to 2009-06-15 22:33:00 containing:
> ?Data: num [1:5910, 1:8] 1510 1510 1510 1510 1510 1510 1510 1510 1510 1510 ...
> ?- attr(*, "dimnames")=List of 2
> ?..$ : NULL
> ?..$ : chr [1:8] "NQU9.Open" "NQU9.High" "NQU9.Low" "NQU9.Close" ...
> ?Indexed by objects of class: [POSIXt,POSIXct] TZ: America/Chicago
> ?xts Attributes:
> List of 4
> ?$ from ? : chr "20090611 ?04:33:46"
> ?$ to ? ? : chr "20090616 ?04:33:46"
> ?$ src ? ?: chr "IB"
> ?$ updated: POSIXct[1:1], format: "2009-06-15 22:33:46.46141"
>
> nqi <- index(NQ)
> hm <- as.POSIXlt(nqi)$min + as.POSIXlt(nqi)$hour*100
> NQV <- aggregate(Vo(NQ), as.factor(hm), sum)
>
> barplot(NQV)
>
> The axis/chart leaves a lot to be desired, but once again that should
> be enough to set you on the right path.
>
> HTH
> Jeff
>
> On Mon, Jun 15, 2009 at 10:24 PM, Jeff Ryan<jeff.a.ryan at gmail.com> wrote:
>> I think you want something like ?aggregate.zoo
>>
>> I didn't pull actual volume data, but here is an example that will
>> show what you can do:
>>
>> library(xts) ?## only used for the sequence and to leverage
>> aggregate.zoo internally.
>>
>> ## generate a sequence of POSIXct 1 mo @ 1min
>> x <- timeBasedSeq('20090515/20090615 12:00')
>>
>> ## convert to POSIXlt and turn into HHMM numeric format
>> hm <- as.POSIXlt(x)$min + as.POSIXlt(x)$hour * 100
>>
>> ## ?your original "Volume" column (here a simple xts object with each
>> min having Vol=1000)
>> ## ?There are 32 observations at each minute in 00:00--12:00 and 31
>> for 12:01--23:59
>> xx <- xts(rep(1000,length(x)), x)
>>
>> ## ?using 'aggregate' to apply sum to the matching times
>> ax <- aggregate(xx, as.factor(hm), sum)
>>
>> head(ax)
>>
>> 0 32000
>> 1 32000
>> 2 32000
>> 3 32000
>> 4 32000
>> 5 32000
>>> tail(ax)
>>
>> 2354 31000
>> 2355 31000
>> 2356 31000
>> 2357 31000
>> 2358 31000
>> 2359 31000
>>
>> I haven't had a chance to actually test this, but at the very least it
>> should provide a start for you.
>>
>> And the above is very fast:
>>
>> ?system.time(ax <- aggregate(xx, as.factor(hm), sum))
>> ? user ?system elapsed
>> ?0.058 ? 0.015 ? 0.073
>>
>> HTH
>> Jeff
>> On Mon, Jun 15, 2009 at 9:27 PM, Wind<windspeedo99 at gmail.com> wrote:
>>> periodicity() function in xts is a good tool for axis manipulation.
>>>
>>> Maybe I should not use character string methods to complie the
>>> distribution of minutes volume, as Brian suggested. ? But what
>>> function should be used for such task in R? ?I've tried in kdb+ , it
>>> is ?somewhat simple and quick enough with select and xbar function.
>>> But I am not familiar with R. ?Maybe there is some functions for this
>>> specific task I don't know.
>>>
>>> Thanks Brian.
>>>
>>>
>>> On Tue, Jun 16, 2009 at 8:00 AM, Brian G. Peterson<brian at braverock.com> wrote:
>>>> It seems that the slow part is all the character string manipulation. ?This
>>>> would be slow in almost any programming language. ? Honestly, I am always
>>>> annoyed by useless axes in charts that simply count from 1 to n. ?A time
>>>> axis at least has some real meaning, and avoids the useless rewriting of
>>>> character strings.
>>>>
>>>> You should be able to get a meaningful, readable axis using the
>>>> periodicity() function in xts without the string manipulation.
>>>>
>>>> Regards,
>>>>
>>>> ? - Brian
>>>>
>>>> Wind wrote:
>>>>>
>>>>> I want to plot the distribution of volume of the future ?CLN9 along
>>>>> the 24 hours axis. ? The following codes could complete the task. ?But
>>>>> it is very time consuming when sapply(mins,function(x)
>>>>> {mean(hqm[which(format(index(hqm),"%H:%M")==x),5])}).
>>>>> Any suggestion for codes with better performance would be highly
>>>>> appreciated.
>>>>>
>>>>>
>>>>> The data hqm has been retrieved from IB via IBrokers.
>>>>>
>>>>>
>>>>>>
>>>>>> head(hqm[,5])
>>>>>>
>>>>>
>>>>> ? ? ? ? ? ? ? ? ? ?CLN9.Volume
>>>>> 2009-05-25 06:00:00 ? ? ? ? ?17
>>>>> 2009-05-25 06:01:00 ? ? ? ? ? 2
>>>>> 2009-05-25 06:02:00 ? ? ? ? ?11
>>>>> 2009-05-25 06:03:00 ? ? ? ? ?26
>>>>> 2009-05-25 06:04:00 ? ? ? ? ?20
>>>>> 2009-05-25 06:05:00 ? ? ? ? ? 5
>>>>>
>>>>>>
>>>>>> tail(hqm[,5])
>>>>>>
>>>>>
>>>>> ? ? ? ? ? ? ? ? ? ?CLN9.Volume
>>>>> 2009-06-15 21:51:00 ? ? ? ?1050
>>>>> 2009-06-15 21:52:00 ? ? ? ? 807
>>>>> 2009-06-15 21:53:00 ? ? ? ? 782
>>>>> 2009-06-15 21:54:00 ? ? ? ? 385
>>>>> 2009-06-15 21:55:00 ? ? ? ? 562
>>>>> 2009-06-15 21:56:00 ? ? ? ? 423
>>>>>
>>>>>>
>>>>>>
>>>>>> mins<-unlist(lapply(0:23,function(h){sapply(0:59,function(m){paste(sprintf("%02d",h),sprintf("%02d",m),sep=":")})}))
>>>>>> head(mins)
>>>>>>
>>>>>
>>>>> [1] "00:00" "00:01" "00:02" "00:03" "00:04" "00:05"
>>>>>
>>>>>>
>>>>>> tail(mins)
>>>>>>
>>>>>
>>>>> [1] "23:54" "23:55" "23:56" "23:57" "23:58" "23:59"
>>>>>
>>>>>
>>>>>>
>>>>>> temp<-sapply(mins,function(x)
>>>>>> {mean(hqm[which(format(index(hqm),"%H:%M")==x),5])})
>>>>>> head(temp)
>>>>>>
>>>>>
>>>>> ? 00:00 ? ?00:01 ? ?00:02 ? ?00:03 ? ?00:04 ? ?00:05
>>>>> 279.1333 284.9333 247.8667 176.3333 278.8667 179.0667
>>>>>
>>>>>>
>>>>>> tail(temp)
>>>>>>
>>>>>
>>>>> ? 23:54 ? ?23:55 ? ?23:56 ? ?23:57 ? ?23:58 ? ?23:59
>>>>> 250.2667 312.7333 318.9333 210.8000 258.2000 232.8667
>>>>>
>>>>>>
>>>>>> plot(temp)
>>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>> -- Subscriber-posting only.
>>>>> -- If you want to post, subscribe first.
>>>>>
>>>>
>>>>
>>>> --
>>>> Brian G. Peterson
>>>> http://braverock.com/brian/
>>>> Ph: 773-459-4973
>>>> IM: bgpbraverock
>>>>
>>>>
>>>>
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>>
>>
>> --
>> Jeffrey Ryan
>> jeffrey.ryan at insightalgo.com
>>
>> ia: insight algorithmics
>> www.insightalgo.com
>>
>
>
>
> --
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com
>


From Bernhard_Pfaff at fra.invesco.com  Tue Jun 16 09:31:07 2009
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Tue, 16 Jun 2009 08:31:07 +0100
Subject: [R-SIG-Finance] hands-on book on financial time series with R?
In-Reply-To: <b1f16d9d0906150920h78a93633j4535370883bb7e42@mail.gmail.com>
References: <b1f16d9d0906150920h78a93633j4535370883bb7e42@mail.gmail.com>
Message-ID: <B89F0CE41D45644A97CCC93DF548C1C31B3C5E8D@GBHENXMB02.corp.amvescap.net>

Hello Michael,

though I dislike self-advertisment, but the following one might also be useful for your purposes:

Bernhard Pfaff. Analysis of Integrated and Cointegrated Time Series with R, Second Edition. Springer, New York, 2nd edition, 2008. ISBN 978-0-387-75966-1.

Best,
Bernhard

 

>-----Urspr?ngliche Nachricht-----
>Von: r-sig-finance-bounces at stat.math.ethz.ch 
>[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Michael
>Gesendet: Montag, 15. Juni 2009 18:20
>An: r-sig-finance at stat.math.ethz.ch
>Betreff: [R-SIG-Finance] hands-on book on financial time series with R?
>
>Hi all,
>
>I am looking for pointers to good books on financial time series with
>R, so I could do some experiments while learning financial time
>series...
>
>Thanks a lot?
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only.
>-- If you want to post, subscribe first.
>
*****************************************************************
Confidentiality Note: The information contained in this message,
and any attachments, may contain confidential and/or privileged
material. It is intended solely for the person(s) or entity to
which it is addressed. Any review, retransmission, dissemination,
or taking of any action in reliance upon this information by
persons or entities other than the intended recipient(s) is
prohibited. If you received this in error, please contact the
sender and delete the material from any computer.
*****************************************************************

From brian at braverock.com  Tue Jun 16 17:39:04 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 16 Jun 2009 10:39:04 -0500
Subject: [R-SIG-Finance] CQG Gateway for R
Message-ID: <4A37BC98.7080800@braverock.com>

Does anyone have information on a CQG gateway for R like RBloomberg 
provides?

Alternately, can the RBloomberg COM client connect to COM on a remote 
machine?  We have a Bloomberg server license but only a limited number 
of terminals, so we have access to the data, but I'd like to avoid 
having to write a custom importer from our database right now if I can 
avoid it.

It seems as though it should be possible to access a remote COM server 
using the COMCreate function (*thus the D in DCOM), as long as you have 
user permissions on that machine.  Any pointers would be greatly 
appreciated.

Thanks,

   - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From josh at i2pi.com  Tue Jun 16 17:55:31 2009
From: josh at i2pi.com (Joshua Reich)
Date: Tue, 16 Jun 2009 11:55:31 -0400
Subject: [R-SIG-Finance] Fundamental analysis library?
Message-ID: <ff45355b0906160855ld5dab8aq21dfdbde4585e93e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090616/f2451702/attachment.pl>

From edd at debian.org  Tue Jun 16 17:56:04 2009
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 16 Jun 2009 10:56:04 -0500
Subject: [R-SIG-Finance] CQG Gateway for R
In-Reply-To: <4A37BC98.7080800@braverock.com>
References: <4A37BC98.7080800@braverock.com>
Message-ID: <18999.49300.674460.123426@ron.nulle.part>


<insert 'well what do I know about Windows' disclaimer here>

On 16 June 2009 at 10:39, Brian G. Peterson wrote:
| Does anyone have information on a CQG gateway for R like RBloomberg 
| provides?
| 
| Alternately, can the RBloomberg COM client connect to COM on a remote 
| machine?  We have a Bloomberg server license but only a limited number 
| of terminals, so we have access to the data, but I'd like to avoid 
| having to write a custom importer from our database right now if I can 
| avoid it.

When all you have is a hammer :)   I have used Rserve for this.  Rserve on
windows sucks big time -- no multiplexing, only one connection at a time --
but for answering the occassional call to run RBloomberg it may work fine. [1]
Should work best in conjunction with something like vnc or nx as you may need to
restart the beast every now and then as RBloomberg run-time errors are likely
to take down Rserve.

[1] Not that I'd do something like this which could well be in violation of
the terms of service yada yada. 

| It seems as though it should be possible to access a remote COM server 
| using the COMCreate function (*thus the D in DCOM), as long as you have 
| user permissions on that machine.  Any pointers would be greatly 
| appreciated.

That's not my hammer so I leave you with that.

Dirk

-- 
Three out of two people have difficulties with fractions.


From jdlong at gmail.com  Wed Jun 17 14:25:43 2009
From: jdlong at gmail.com (James Long)
Date: Wed, 17 Jun 2009 07:25:43 -0500
Subject: [R-SIG-Finance] Download and parse CME data
Message-ID: <240be9270906170525j3a6f7b83k8e77b1cd2c5bc5b8@mail.gmail.com>

I have the need to download and parse daily futures and options data
from the CME FTP server. Before I get deep into it I wanted to see if
any of you have or have seen code that does this. Ultimately it would
make some sense to hack together a library for this.

-JD Long

-- 
Sent from my mobile device


From brian at braverock.com  Wed Jun 17 15:11:33 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 17 Jun 2009 08:11:33 -0500
Subject: [R-SIG-Finance] Download and parse CME data
In-Reply-To: <240be9270906170525j3a6f7b83k8e77b1cd2c5bc5b8@mail.gmail.com>
References: <240be9270906170525j3a6f7b83k8e77b1cd2c5bc5b8@mail.gmail.com>
Message-ID: <4A38EB85.6000105@braverock.com>

I believe that the best approach to this will be to create a getSymbols 
call for quantmod to minimize the amount of "infrastructure" work 
needed.  This should be just a few lines of code in a getSymbols.CME 
function.  I'm sure Jeff would include such a function in future 
releases of quantmod.

I think that the other business wrappers on when to download, and how to 
automate that download, are not really a problem for inside R, and will 
likely vary significantly from shop to shop.  If the data is retrievable 
via getSymbols, each individual or firm should be able to adapt the 
timing to their needs.  Certainly it would be easy enough to write a 
sample R batch file that would do the download that could be run via 
cron or a similar task scheduler.

Regards,

  - Brian

James Long wrote:
> I have the need to download and parse daily futures and options data
> from the CME FTP server. Before I get deep into it I wanted to see if
> any of you have or have seen code that does this. Ultimately it would
> make some sense to hack together a library for this.
>
> -JD Long
>
>   


-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From nelson.ana at gmail.com  Wed Jun 17 18:29:20 2009
From: nelson.ana at gmail.com (Ana Nelson)
Date: Wed, 17 Jun 2009 17:29:20 +0100
Subject: [R-SIG-Finance] prices in usd
In-Reply-To: <a7d6d2740906121602g4528083cn73fdac620374e3bd@mail.gmail.com>
References: <C92D6BF93B8E2A4B96E206B66040B916D6B784@CONNCAPSBS.connectcap.local>
	<SANCTUMFISERVERumzk00001acc@sanctumfi.com>
	<SANCTUMFISERVERSQeS00001c69@sanctumfi.com>
	<a7d6d2740906120448p6117c957j4fc05d278dc6219e@mail.gmail.com>
	<C92D6BF93B8E2A4B96E206B66040B916D6B801@CONNCAPSBS.connectcap.local>
	<a7d6d2740906120831w7412a3davf73052e460f0b998@mail.gmail.com>
	<C92D6BF93B8E2A4B96E206B66040B916D6B80B@CONNCAPSBS.connectcap.local>
	<4A32BEFE.4080703@braverock.com>
	<a7d6d2740906121602g4528083cn73fdac620374e3bd@mail.gmail.com>
Message-ID: <a7d6d2740906170929s3be0af11p78f278d615bd9913@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090617/6061a12a/attachment.pl>

From marco.zanella at inbox.com  Wed Jun 17 18:41:27 2009
From: marco.zanella at inbox.com (Zanella Marco)
Date: Wed, 17 Jun 2009 08:41:27 -0800
Subject: [R-SIG-Finance] Vasicek model estimation via linear regression
Message-ID: <D3E89AEE850.00000381marco.zanella@inbox.com>

Hi,
I have to check mean reversion with a Vasicek model for a time series. As you certainly know Vasicek process is discribed by following formula:

dXt = a(b-Xt)dt + sdWt   (1) 

http://en.wikipedia.org/wiki/Vasicek_model

To estimate the parameters in my data I can use this expression:

Xt - Xt-1 = a(b-Xt-1)Dt + se   (2) 

where:
Xt: time series at time t
Xt-1: time series at time t-1
a: unknown parameter
b: unknown parameter
Dt: in my case I can assimilate it = 1
S: standard deviation
e: error ~N(0,1)

Basically, I want to estimate unknown parameters a and b using a linear regression. Usually I work on linear regression with lm() function but I don't undertand how formulate my (2) model in to lm command. Can anyone give me some suggestions?

Thanks in advance.

Regards,

Marco

____________________________________________________________
FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!


From knguyen at cs.umb.edu  Wed Jun 17 19:53:56 2009
From: knguyen at cs.umb.edu (Khanh Nguyen)
Date: Wed, 17 Jun 2009 13:53:56 -0400
Subject: [R-SIG-Finance] zoo plotting - invalid 'ylim' value
Message-ID: <2871c9e10906171053k597b467fi7fe0398140eddbd3@mail.gmail.com>

Hi all,

I am very new to 'zoo', so this could probably be a trivia question,
but I got the above error when I plot my zoo object

>plot(stb.foreign) #this work fine

> stb.foreign$room.con.lai
 2009-05-28  2009-05-29  2009-06-01  2009-06-02  2009-06-03
2009-06-04  2009-06-05
11104740    10772400    10775110    10397160    10037930     9986480
 10003780
 2009-06-08  2009-06-09  2009-06-10  2009-06-11  2009-06-12
2009-06-15  2009-06-16
10163340    10545920    11020540    10720430    10484040    10502080
 10932930
 2009-06-17
11201700

> plot(stb.foreign$room.con.lai)
Error in plot.window(...) : invalid 'ylim' value
>

Thanks

-k


From ggrothendieck at gmail.com  Wed Jun 17 19:58:02 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 17 Jun 2009 13:58:02 -0400
Subject: [R-SIG-Finance] zoo plotting - invalid 'ylim' value
In-Reply-To: <2871c9e10906171053k597b467fi7fe0398140eddbd3@mail.gmail.com>
References: <2871c9e10906171053k597b467fi7fe0398140eddbd3@mail.gmail.com>
Message-ID: <971536df0906171058h6202a05fsc31eaa555eb7bb15@mail.gmail.com>

What is the result of

dput(stb.foreign)



On Wed, Jun 17, 2009 at 1:53 PM, Khanh Nguyen<knguyen at cs.umb.edu> wrote:
> Hi all,
>
> I am very new to 'zoo', so this could probably be a trivia question,
> but I got the above error when I plot my zoo object
>
>>plot(stb.foreign) #this work fine
>
>> stb.foreign$room.con.lai
> ?2009-05-28 ?2009-05-29 ?2009-06-01 ?2009-06-02 ?2009-06-03
> 2009-06-04 ?2009-06-05
> 11104740 ? ?10772400 ? ?10775110 ? ?10397160 ? ?10037930 ? ? 9986480
> ?10003780
> ?2009-06-08 ?2009-06-09 ?2009-06-10 ?2009-06-11 ?2009-06-12
> 2009-06-15 ?2009-06-16
> 10163340 ? ?10545920 ? ?11020540 ? ?10720430 ? ?10484040 ? ?10502080
> ?10932930
> ?2009-06-17
> 11201700
>
>> plot(stb.foreign$room.con.lai)
> Error in plot.window(...) : invalid 'ylim' value
>>
>
> Thanks
>
> -k
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From Derek.Schaeffer at barclaysglobal.com  Wed Jun 17 21:40:24 2009
From: Derek.Schaeffer at barclaysglobal.com (Schaeffer, Derek BGI SF)
Date: Wed, 17 Jun 2009 12:40:24 -0700
Subject: [R-SIG-Finance] Vasicek model estimation via linear regression
In-Reply-To: <D3E89AEE850.00000381marco.zanella@inbox.com>
References: <D3E89AEE850.00000381marco.zanella@inbox.com>
Message-ID: <E4829836F2C14E469E197134608CF80E03DCD6F3@calnte2k046.insidelive.net>

 
Hi Marco,

The discrete time Vasicek model at partition size dt (e.g. 1/252) is
given by:

X(t) - X(t-dt) = [a*b] *  dt - [b * X(t-dt)] * dt + s_e * e,   e ~
N(0,1)

Run the following regression:

X(t) = A + B X(t-1) + v,  v ~ N(0,s_v^2)

The continuous-time parameters can be recovered from the regression
estimates using:

b   = -A/B
a   = -ln(1 + B)/dt
s_e = s_v * sqrt(2 * ln(1 + B) / dt) / sqrt( (1+B)^2 - 1)
 
Best,
Derek M. Schaeffer, Ph.D. 
Principal 
Senior Research Officer

TEL   415 894 6427
CELL  415 516 9558
FAX   415 618 1824
derek.schaeffer at barclaysglobal.com

Barclays Global Investors 400 Howard Street San Francisco, CA 94105



-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Zanella
Marco
Sent: Wednesday, June 17, 2009 9:41 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Vasicek model estimation via linear regression

Hi,
I have to check mean reversion with a Vasicek model for a time series.
As you certainly know Vasicek process is discribed by following formula:

dXt = a(b-Xt)dt + sdWt   (1) 

http://en.wikipedia.org/wiki/Vasicek_model

To estimate the parameters in my data I can use this expression:

Xt - Xt-1 = a(b-Xt-1)Dt + se   (2) 

where:
Xt: time series at time t
Xt-1: time series at time t-1
a: unknown parameter
b: unknown parameter
Dt: in my case I can assimilate it = 1
S: standard deviation
e: error ~N(0,1)

Basically, I want to estimate unknown parameters a and b using a linear
regression. Usually I work on linear regression with lm() function but I
don't undertand how formulate my (2) model in to lm command. Can anyone
give me some suggestions?

Thanks in advance.

Regards,

Marco

____________________________________________________________


_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


--

This message and any attachments are confidential, propr...{{dropped:14}}


From knguyen at cs.umb.edu  Wed Jun 17 22:08:00 2009
From: knguyen at cs.umb.edu (Khanh Nguyen)
Date: Wed, 17 Jun 2009 16:08:00 -0400
Subject: [R-SIG-Finance] chart.PerformanceAnalytics(),
	character string is not in a standard 	unambiguous format
Message-ID: <2871c9e10906171308t75a89704j59a76bfad87c921@mail.gmail.com>

Hi all,

I ran into this error "Error in fromchar(x) : character string is not
in a standard unambiguous format"" when I run
charts.PerformanceSummary(). I guess that I probably messed up the
dates in my data somehow.

> z <- Return.calculate(stb.agg$STB.Close, method=c("compound", "simple")) #stb.agg is my monthly price history
> head(z)
                 STB
Aug 2006  0.01250428
Sep 2006  0.05904603
Oct 2006 -0.05970026
Nov 2006  0.05846646
Dec 2006  0.09671211
Jan 2007  0.26135075

> dput(z)
structure(c(0.0125042761200924, 0.059046028746296, -0.0597002649488925,
0.0584664584999639, 0.0967121147037853, 0.261350754509795, 0.084761454787754,
0.281618134192335, -0.050301925509296, 0.127002576668413, -0.229431460030798,
-0.191458836140774, -0.0185289083290332, 0.268858114855369,
-0.0363007156717821,
0.0147123648065453, -0.0448028245368759, -0.0153917235834742,
-0.225195932349637, -0.327933538473546, -0.102059501725719, -0.313061498255964,
-0.112165043352160, 0.214704682668345, 0.269509471872135, -0.261070603226270,
-0.0877756108823857, -0.0811901547300131, -0.0883691504500903,
-0.0501497836854714, -0.167573700159399, 0.090384061468269, 0.259511195485085,
0.232622295268754, 0.360441426734209), .Dim = c(35L, 1L), .Dimnames = list(
    c("Aug 2006", "Sep 2006", "Oct 2006", "Nov 2006", "Dec 2006",
    "Jan 2007", "Feb 2007", "Mar 2007", "Apr 2007", "May 2007",
    "Jun 2007", "Jul 2007", "Aug 2007", "Sep 2007", "Oct 2007",
    "Nov 2007", "Dec 2007", "Jan 2008", "Feb 2008", "Mar 2008",
    "Apr 2008", "May 2008", "Jun 2008", "Jul 2008", "Aug 2008",
    "Sep 2008", "Oct 2008", "Nov 2008", "Dec 2008", "Jan 2009",
    "Feb 2009", "Mar 2009", "Apr 2009", "May 2009", "Jun 2009"
    ), "STB"), index = structure(c(2006.58333333333, 2006.66666666667,
2006.75, 2006.83333333333, 2006.91666666667, 2007, 2007.08333333333,
2007.16666666667, 2007.25, 2007.33333333333, 2007.41666666667,
2007.5, 2007.58333333333, 2007.66666666667, 2007.75, 2007.83333333333,
2007.91666666667, 2008, 2008.08333333333, 2008.16666666667, 2008.25,
2008.33333333333, 2008.41666666667, 2008.5, 2008.58333333333,
2008.66666666667, 2008.75, 2008.83333333333, 2008.91666666667,
2009, 2009.08333333333, 2009.16666666667, 2009.25, 2009.33333333333,
2009.41666666667), class = "yearmon"), class = "zoo")

> charts.PerformanceSummary(z)
Error in fromchar(x) :
  character string is not in a standard unambiguous format


Thanks.

-k


From ezivot at u.washington.edu  Wed Jun 17 22:06:48 2009
From: ezivot at u.washington.edu (Eric Zivot)
Date: Wed, 17 Jun 2009 13:06:48 -0700 (PDT)
Subject: [R-SIG-Finance] Vasicek model estimation via linear regression
In-Reply-To: <E4829836F2C14E469E197134608CF80E03DCD6F3@calnte2k046.insidelive.net>
Message-ID: <Pine.LNX.4.43.0906171306480.3871@hymn14.u.washington.edu>

The Euler discretization of the Vasicek model is misspecified (i.e., it is not the exact discretization based on the true transition density) and so the least squares estimates will be biased (see Andrew Lo's Econometric Theory paper on estimating continuous time models from their discrete time counterparts, and Broze, Scaillet and Zakoian's Journal of Empirical Finance paper on estimating continuous time models from their discretized counterparts). This bias is typically not too large. You can eliminate this bias using an indirect inference estimation technique (see the book by Gourieroux and Monfort for an example applied to the Vasicek model). Alternatively, you can estimate the Vasicek model by exact mle using the code in the nice sde package (see also the accompanying book for more explanation). 
ez

****************************************************************
*  Eric Zivot                  			               *
*  Professor and Gary Waterman Distinguished Scholar           *
*  Department of Economics                                     *
*  Adjunct Professor of Finance                                *
*  Adjunct Professor of Statistics
*  Box 353330                  email:  ezivot at u.washington.edu *
*  University of Washington    phone:  206-543-6715            *
*  Seattle, WA 98195-3330                                      *                                                           *
*  www:  http://faculty.washington.edu/ezivot                  *
****************************************************************

On Wed, 17 Jun 2009, Schaeffer, Derek BGI SF wrote:

>
> Hi Marco,
>
> The discrete time Vasicek model at partition size dt (e.g. 1/252) is
> given by:
>
> X(t) - X(t-dt) = [a*b] *  dt - [b * X(t-dt)] * dt + s_e * e,   e ~
> N(0,1)
>
> Run the following regression:
>
> X(t) = A + B X(t-1) + v,  v ~ N(0,s_v^2)
>
> The continuous-time parameters can be recovered from the regression
> estimates using:
>
> b   = -A/B
> a   = -ln(1 + B)/dt
> s_e = s_v * sqrt(2 * ln(1 + B) / dt) / sqrt( (1+B)^2 - 1)
>
> Best,
> Derek M. Schaeffer, Ph.D.
> Principal
> Senior Research Officer
>
> TEL   415 894 6427
> CELL  415 516 9558
> FAX   415 618 1824
> derek.schaeffer at barclaysglobal.com
>
> Barclays Global Investors 400 Howard Street San Francisco, CA 94105
>
>
>
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Zanella
> Marco
> Sent: Wednesday, June 17, 2009 9:41 AM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] Vasicek model estimation via linear regression
>
> Hi,
> I have to check mean reversion with a Vasicek model for a time series.
> As you certainly know Vasicek process is discribed by following formula:
>
> dXt = a(b-Xt)dt + sdWt   (1)
>
> http://en.wikipedia.org/wiki/Vasicek_model
>
> To estimate the parameters in my data I can use this expression:
>
> Xt - Xt-1 = a(b-Xt-1)Dt + se   (2)
>
> where:
> Xt: time series at time t
> Xt-1: time series at time t-1
> a: unknown parameter
> b: unknown parameter
> Dt: in my case I can assimilate it = 1
> S: standard deviation
> e: error ~N(0,1)
>
> Basically, I want to estimate unknown parameters a and b using a linear
> regression. Usually I work on linear regression with lm() function but I
> don't undertand how formulate my (2) model in to lm command. Can anyone
> give me some suggestions?
>
> Thanks in advance.
>
> Regards,
>
> Marco
>
> ____________________________________________________________
>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>
> --
>
> This message and any attachments are confidential, pro...{{dropped:10}}


From markleeds at verizon.net  Wed Jun 17 22:24:05 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Wed, 17 Jun 2009 15:24:05 -0500 (CDT)
Subject: [R-SIG-Finance] Vasicek model estimation via linear regression
Message-ID: <2055736369.93853.1245270245770.JavaMail.root@vms125.mailsrvcs.net>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090617/52a9fd3e/attachment.html>

From knguyen at cs.umb.edu  Wed Jun 17 22:53:54 2009
From: knguyen at cs.umb.edu (Khanh Nguyen)
Date: Wed, 17 Jun 2009 16:53:54 -0400
Subject: [R-SIG-Finance] chart.PerformanceAnalytics(),
	character string is 	not in a standard unambiguous format
In-Reply-To: <Pine.LNX.4.43.0906171347130.22583@hymn13.u.washington.edu>
References: <2871c9e10906171308t75a89704j59a76bfad87c921@mail.gmail.com>
	<Pine.LNX.4.43.0906171347130.22583@hymn13.u.washington.edu>
Message-ID: <2871c9e10906171353u2fc0d751iec51bfd5c6768f2c@mail.gmail.com>

Thank you Dr. Zivot,

I realized it has to do with the dates, but it is a nuisance though
because the my object is created by Return.calculate(), which is part
of PerformanceAnalytics.

-k

On Wed, Jun 17, 2009 at 4:47 PM, Eric Zivot<ezivot at u.washington.edu> wrote:
> I get this error a lot too. The problem is specifying dates as with just
> month and year information. My guess is that chart.PerfromanceAnalytics() is
> trying to create a zoo object with your date information using the as.Date()
> function to create the time index. Date class wants dates in the form
> mm/dd/yyyy format. It does not recognize date characters with just month and
> year. For this, it is best to use the yearmon class in zoo explicitly.
>
> ****************************************************************
> * ?Eric Zivot ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?*
> * ?Professor and Gary Waterman Distinguished Scholar ? ? ? ? ? *
> * ?Department of Economics ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? *
> * ?Adjunct Professor of Finance ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?*
> * ?Adjunct Professor of Statistics
> * ?Box 353330 ? ? ? ? ? ? ? ? ?email: ?ezivot at u.washington.edu *
> * ?University of Washington ? ?phone: ?206-543-6715 ? ? ? ? ? ?*
> * ?Seattle, WA 98195-3330 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?*
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? *
> * ?www: ?http://faculty.washington.edu/ezivot ? ? ? ? ? ? ? ? ?*
> ****************************************************************
>
> On Wed, 17 Jun 2009, Khanh Nguyen wrote:
>
>> Hi all,
>>
>> I ran into this error "Error in fromchar(x) : character string is not
>> in a standard unambiguous format"" when I run
>> charts.PerformanceSummary(). I guess that I probably messed up the
>> dates in my data somehow.
>>
>>> z <- Return.calculate(stb.agg$STB.Close, method=c("compound", "simple"))
>>> #stb.agg is my monthly price history
>>> head(z)
>>
>> ? ? ? ? ? ? ? ?STB
>> Aug 2006 ?0.01250428
>> Sep 2006 ?0.05904603
>> Oct 2006 -0.05970026
>> Nov 2006 ?0.05846646
>> Dec 2006 ?0.09671211
>> Jan 2007 ?0.26135075
>>
>>> dput(z)
>>
>> structure(c(0.0125042761200924, 0.059046028746296, -0.0597002649488925,
>> 0.0584664584999639, 0.0967121147037853, 0.261350754509795,
>> 0.084761454787754,
>> 0.281618134192335, -0.050301925509296, 0.127002576668413,
>> -0.229431460030798,
>> -0.191458836140774, -0.0185289083290332, 0.268858114855369,
>> -0.0363007156717821,
>> 0.0147123648065453, -0.0448028245368759, -0.0153917235834742,
>> -0.225195932349637, -0.327933538473546, -0.102059501725719,
>> -0.313061498255964,
>> -0.112165043352160, 0.214704682668345, 0.269509471872135,
>> -0.261070603226270,
>> -0.0877756108823857, -0.0811901547300131, -0.0883691504500903,
>> -0.0501497836854714, -0.167573700159399, 0.090384061468269,
>> 0.259511195485085,
>> 0.232622295268754, 0.360441426734209), .Dim = c(35L, 1L), .Dimnames =
>> list(
>> ? c("Aug 2006", "Sep 2006", "Oct 2006", "Nov 2006", "Dec 2006",
>> ? "Jan 2007", "Feb 2007", "Mar 2007", "Apr 2007", "May 2007",
>> ? "Jun 2007", "Jul 2007", "Aug 2007", "Sep 2007", "Oct 2007",
>> ? "Nov 2007", "Dec 2007", "Jan 2008", "Feb 2008", "Mar 2008",
>> ? "Apr 2008", "May 2008", "Jun 2008", "Jul 2008", "Aug 2008",
>> ? "Sep 2008", "Oct 2008", "Nov 2008", "Dec 2008", "Jan 2009",
>> ? "Feb 2009", "Mar 2009", "Apr 2009", "May 2009", "Jun 2009"
>> ? ), "STB"), index = structure(c(2006.58333333333, 2006.66666666667,
>> 2006.75, 2006.83333333333, 2006.91666666667, 2007, 2007.08333333333,
>> 2007.16666666667, 2007.25, 2007.33333333333, 2007.41666666667,
>> 2007.5, 2007.58333333333, 2007.66666666667, 2007.75, 2007.83333333333,
>> 2007.91666666667, 2008, 2008.08333333333, 2008.16666666667, 2008.25,
>> 2008.33333333333, 2008.41666666667, 2008.5, 2008.58333333333,
>> 2008.66666666667, 2008.75, 2008.83333333333, 2008.91666666667,
>> 2009, 2009.08333333333, 2009.16666666667, 2009.25, 2009.33333333333,
>> 2009.41666666667), class = "yearmon"), class = "zoo")
>>
>>> charts.PerformanceSummary(z)
>>
>> Error in fromchar(x) :
>> ?character string is not in a standard unambiguous format
>>
>>
>> Thanks.
>>
>> -k
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>


From brian at braverock.com  Wed Jun 17 22:55:53 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 17 Jun 2009 15:55:53 -0500
Subject: [R-SIG-Finance] chart.PerformanceAnalytics(),
 character string is not in a standard 	unambiguous format
In-Reply-To: <2871c9e10906171308t75a89704j59a76bfad87c921@mail.gmail.com>
References: <2871c9e10906171308t75a89704j59a76bfad87c921@mail.gmail.com>
Message-ID: <4A395859.5060102@braverock.com>

Is your date index a Date class, or a character string?  If it is of 
class Date or YearMon, I wouldn't anticipate any problems, but if your 
row names are characters, the error would be because the charting 
function doesn't know how to create the x axis labels.

For example, as.xts(z) would likely give you the same error if you have 
character strings for the dates.

Regards,

    - Brian

Khanh Nguyen wrote:
> Hi all,
>
> I ran into this error "Error in fromchar(x) : character string is not
> in a standard unambiguous format"" when I run
> charts.PerformanceSummary(). I guess that I probably messed up the
> dates in my data somehow.
>
>   
>> z <- Return.calculate(stb.agg$STB.Close, method=c("compound", "simple")) #stb.agg is my monthly price history
>> head(z)
>>     
>                  STB
> Aug 2006  0.01250428
> Sep 2006  0.05904603
> Oct 2006 -0.05970026
> Nov 2006  0.05846646
> Dec 2006  0.09671211
> Jan 2007  0.26135075
>
>   
>> dput(z)
>>     
<...>
>   
>> charts.PerformanceSummary(z)
>>     
> Error in fromchar(x) :
>   character string is not in a standard unambiguous format
>
>   
-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From brian at braverock.com  Wed Jun 17 23:00:10 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 17 Jun 2009 16:00:10 -0500
Subject: [R-SIG-Finance] chart.PerformanceAnalytics(),
 character string is 	not in a standard unambiguous format
In-Reply-To: <2871c9e10906171353u2fc0d751iec51bfd5c6768f2c@mail.gmail.com>
References: <2871c9e10906171308t75a89704j59a76bfad87c921@mail.gmail.com>	<Pine.LNX.4.43.0906171347130.22583@hymn13.u.washington.edu>
	<2871c9e10906171353u2fc0d751iec51bfd5c6768f2c@mail.gmail.com>
Message-ID: <4A39595A.50601@braverock.com>

Return.calculate doesn't change your index.

We're attempting to address this in the version of code we're trying to 
get ready to release to CRAN by using xts internally.  This will fail 
more quickly and obviously "up front" rather than getting further into a 
process.

You should be able to apply class Date or YearMon to your index using 
the appropriate format string.

Regards,

   - Brian

Khanh Nguyen wrote:
> Thank you Dr. Zivot,
>
> I realized it has to do with the dates, but it is a nuisance though
> because the my object is created by Return.calculate(), which is part
> of PerformanceAnalytics.
>
> -k
>
> On Wed, Jun 17, 2009 at 4:47 PM, Eric Zivot<ezivot at u.washington.edu> wrote:
>   
>> I get this error a lot too. The problem is specifying dates as with just
>> month and year information. My guess is that chart.PerfromanceAnalytics() is
>> trying to create a zoo object with your date information using the as.Date()
>> function to create the time index. Date class wants dates in the form
>> mm/dd/yyyy format. It does not recognize date characters with just month and
>> year. For this, it is best to use the yearmon class in zoo explicitly.
>>
>> ****************************************************************
>> *  Eric Zivot                                                  *
>> *  Professor and Gary Waterman Distinguished Scholar           *
>> *  Department of Economics                                     *
>> *  Adjunct Professor of Finance                                *
>> *  Adjunct Professor of Statistics
>> *  Box 353330                  email:  ezivot at u.washington.edu *
>> *  University of Washington    phone:  206-543-6715            *
>> *  Seattle, WA 98195-3330                                      *
>>                                               *
>> *  www:  http://faculty.washington.edu/ezivot                  *
>> ****************************************************************
>>
>> On Wed, 17 Jun 2009, Khanh Nguyen wrote:
>>
>>     
>>> Hi all,
>>>
>>> I ran into this error "Error in fromchar(x) : character string is not
>>> in a standard unambiguous format"" when I run
>>> charts.PerformanceSummary(). I guess that I probably messed up the
>>> dates in my data somehow.
>>>
>>>       
>>>> z <- Return.calculate(stb.agg$STB.Close, method=c("compound", "simple"))
>>>> #stb.agg is my monthly price history
>>>> head(z)
>>>>         
>>>                STB
>>> Aug 2006  0.01250428
>>> Sep 2006  0.05904603
>>> Oct 2006 -0.05970026
>>> Nov 2006  0.05846646
>>> Dec 2006  0.09671211
>>> Jan 2007  0.26135075
>>>
>>>       
>>>> dput(z)
>>>>         
>>> structure(c(0.0125042761200924, 0.059046028746296, -0.0597002649488925,
>>> 0.0584664584999639, 0.0967121147037853, 0.261350754509795,
>>> 0.084761454787754,
>>> 0.281618134192335, -0.050301925509296, 0.127002576668413,
>>> -0.229431460030798,
>>> -0.191458836140774, -0.0185289083290332, 0.268858114855369,
>>> -0.0363007156717821,
>>> 0.0147123648065453, -0.0448028245368759, -0.0153917235834742,
>>> -0.225195932349637, -0.327933538473546, -0.102059501725719,
>>> -0.313061498255964,
>>> -0.112165043352160, 0.214704682668345, 0.269509471872135,
>>> -0.261070603226270,
>>> -0.0877756108823857, -0.0811901547300131, -0.0883691504500903,
>>> -0.0501497836854714, -0.167573700159399, 0.090384061468269,
>>> 0.259511195485085,
>>> 0.232622295268754, 0.360441426734209), .Dim = c(35L, 1L), .Dimnames =
>>> list(
>>>   c("Aug 2006", "Sep 2006", "Oct 2006", "Nov 2006", "Dec 2006",
>>>   "Jan 2007", "Feb 2007", "Mar 2007", "Apr 2007", "May 2007",
>>>   "Jun 2007", "Jul 2007", "Aug 2007", "Sep 2007", "Oct 2007",
>>>   "Nov 2007", "Dec 2007", "Jan 2008", "Feb 2008", "Mar 2008",
>>>   "Apr 2008", "May 2008", "Jun 2008", "Jul 2008", "Aug 2008",
>>>   "Sep 2008", "Oct 2008", "Nov 2008", "Dec 2008", "Jan 2009",
>>>   "Feb 2009", "Mar 2009", "Apr 2009", "May 2009", "Jun 2009"
>>>   ), "STB"), index = structure(c(2006.58333333333, 2006.66666666667,
>>> 2006.75, 2006.83333333333, 2006.91666666667, 2007, 2007.08333333333,
>>> 2007.16666666667, 2007.25, 2007.33333333333, 2007.41666666667,
>>> 2007.5, 2007.58333333333, 2007.66666666667, 2007.75, 2007.83333333333,
>>> 2007.91666666667, 2008, 2008.08333333333, 2008.16666666667, 2008.25,
>>> 2008.33333333333, 2008.41666666667, 2008.5, 2008.58333333333,
>>> 2008.66666666667, 2008.75, 2008.83333333333, 2008.91666666667,
>>> 2009, 2009.08333333333, 2009.16666666667, 2009.25, 2009.33333333333,
>>> 2009.41666666667), class = "yearmon"), class = "zoo")
>>>
>>>       
>>>> charts.PerformanceSummary(z)
>>>>         
>>> Error in fromchar(x) :
>>>  character string is not in a standard unambiguous format
>>>
>>>
>>> Thanks.
>>>
>>> -k
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>>       
>>
>>     
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>   


-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From peter at braverock.com  Wed Jun 17 23:21:33 2009
From: peter at braverock.com (Peter Carl)
Date: Wed, 17 Jun 2009 16:21:33 -0500 (CDT)
Subject: [R-SIG-Finance] chart.PerformanceAnalytics(),
 character string is not in a standard 	unambiguous format
In-Reply-To: <2871c9e10906171308t75a89704j59a76bfad87c921@mail.gmail.com>
References: <2871c9e10906171308t75a89704j59a76bfad87c921@mail.gmail.com>
Message-ID: <57201.64.190.216.194.1245273693.squirrel@mail.braverock.com>

Try:
> index(z) = as.Date(index(z))

That should revert your dates into a standard %F ISO-standard format.

Should be fixed in the next release.  Sorry for the inconvenience.

pcc
-- 
Peter Carl
http://www.braverock.com/~peter

> Hi all,
>
> I ran into this error "Error in fromchar(x) : character string is not
> in a standard unambiguous format"" when I run
> charts.PerformanceSummary(). I guess that I probably messed up the
> dates in my data somehow.
>
>> z <- Return.calculate(stb.agg$STB.Close, method=c("compound", "simple"))
>> #stb.agg is my monthly price history
>> head(z)
>                  STB
> Aug 2006  0.01250428
> Sep 2006  0.05904603
> Oct 2006 -0.05970026
> Nov 2006  0.05846646
> Dec 2006  0.09671211
> Jan 2007  0.26135075
>
>> dput(z)
> structure(c(0.0125042761200924, 0.059046028746296, -0.0597002649488925,
> 0.0584664584999639, 0.0967121147037853, 0.261350754509795,
> 0.084761454787754,
> 0.281618134192335, -0.050301925509296, 0.127002576668413,
> -0.229431460030798,
> -0.191458836140774, -0.0185289083290332, 0.268858114855369,
> -0.0363007156717821,
> 0.0147123648065453, -0.0448028245368759, -0.0153917235834742,
> -0.225195932349637, -0.327933538473546, -0.102059501725719,
> -0.313061498255964,
> -0.112165043352160, 0.214704682668345, 0.269509471872135,
> -0.261070603226270,
> -0.0877756108823857, -0.0811901547300131, -0.0883691504500903,
> -0.0501497836854714, -0.167573700159399, 0.090384061468269,
> 0.259511195485085,
> 0.232622295268754, 0.360441426734209), .Dim = c(35L, 1L), .Dimnames =
> list(
>     c("Aug 2006", "Sep 2006", "Oct 2006", "Nov 2006", "Dec 2006",
>     "Jan 2007", "Feb 2007", "Mar 2007", "Apr 2007", "May 2007",
>     "Jun 2007", "Jul 2007", "Aug 2007", "Sep 2007", "Oct 2007",
>     "Nov 2007", "Dec 2007", "Jan 2008", "Feb 2008", "Mar 2008",
>     "Apr 2008", "May 2008", "Jun 2008", "Jul 2008", "Aug 2008",
>     "Sep 2008", "Oct 2008", "Nov 2008", "Dec 2008", "Jan 2009",
>     "Feb 2009", "Mar 2009", "Apr 2009", "May 2009", "Jun 2009"
>     ), "STB"), index = structure(c(2006.58333333333, 2006.66666666667,
> 2006.75, 2006.83333333333, 2006.91666666667, 2007, 2007.08333333333,
> 2007.16666666667, 2007.25, 2007.33333333333, 2007.41666666667,
> 2007.5, 2007.58333333333, 2007.66666666667, 2007.75, 2007.83333333333,
> 2007.91666666667, 2008, 2008.08333333333, 2008.16666666667, 2008.25,
> 2008.33333333333, 2008.41666666667, 2008.5, 2008.58333333333,
> 2008.66666666667, 2008.75, 2008.83333333333, 2008.91666666667,
> 2009, 2009.08333333333, 2009.16666666667, 2009.25, 2009.33333333333,
> 2009.41666666667), class = "yearmon"), class = "zoo")
>
>> charts.PerformanceSummary(z)
> Error in fromchar(x) :
>   character string is not in a standard unambiguous format
>
>
> Thanks.
>
> -k
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ggrothendieck at gmail.com  Wed Jun 17 23:35:30 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 17 Jun 2009 17:35:30 -0400
Subject: [R-SIG-Finance] zoo plotting - invalid 'ylim' value
In-Reply-To: <971536df0906171058h6202a05fsc31eaa555eb7bb15@mail.gmail.com>
References: <2871c9e10906171053k597b467fi7fe0398140eddbd3@mail.gmail.com> 
	<971536df0906171058h6202a05fsc31eaa555eb7bb15@mail.gmail.com>
Message-ID: <971536df0906171435l49a0f1fya5585b542de67d21@mail.gmail.com>

Just to follow up the thread so that it is not left dangling
the problem was that the object in question had character
rather than numeric data in it.

On Wed, Jun 17, 2009 at 1:58 PM, Gabor
Grothendieck<ggrothendieck at gmail.com> wrote:
> What is the result of
>
> dput(stb.foreign)
>
>
>
> On Wed, Jun 17, 2009 at 1:53 PM, Khanh Nguyen<knguyen at cs.umb.edu> wrote:
>> Hi all,
>>
>> I am very new to 'zoo', so this could probably be a trivia question,
>> but I got the above error when I plot my zoo object
>>
>>>plot(stb.foreign) #this work fine
>>
>>> stb.foreign$room.con.lai
>> ?2009-05-28 ?2009-05-29 ?2009-06-01 ?2009-06-02 ?2009-06-03
>> 2009-06-04 ?2009-06-05
>> 11104740 ? ?10772400 ? ?10775110 ? ?10397160 ? ?10037930 ? ? 9986480
>> ?10003780
>> ?2009-06-08 ?2009-06-09 ?2009-06-10 ?2009-06-11 ?2009-06-12
>> 2009-06-15 ?2009-06-16
>> 10163340 ? ?10545920 ? ?11020540 ? ?10720430 ? ?10484040 ? ?10502080
>> ?10932930
>> ?2009-06-17
>> 11201700
>>
>>> plot(stb.foreign$room.con.lai)
>> Error in plot.window(...) : invalid 'ylim' value
>>>
>>
>> Thanks
>>
>> -k
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>


From knguyen at cs.umb.edu  Thu Jun 18 00:01:16 2009
From: knguyen at cs.umb.edu (Khanh Nguyen)
Date: Wed, 17 Jun 2009 18:01:16 -0400
Subject: [R-SIG-Finance] chart.PerformanceAnalytics(),
	character string is 	not in a standard unambiguous format
In-Reply-To: <57201.64.190.216.194.1245273693.squirrel@mail.braverock.com>
References: <2871c9e10906171308t75a89704j59a76bfad87c921@mail.gmail.com>
	<57201.64.190.216.194.1245273693.squirrel@mail.braverock.com>
Message-ID: <2871c9e10906171501x70dd699o2699ead91f191a0@mail.gmail.com>

Hi all,

Thank you very much for all the help. Really appreciate it.

On Wed, Jun 17, 2009 at 5:21 PM, Peter Carl<peter at braverock.com> wrote:
> Try:
>> index(z) = as.Date(index(z))
>
> That should revert your dates into a standard %F ISO-standard format.

this still doesn't do it. But finally I got my way. in case others
experience the same problem, this is what I do

> stb.agg <- aggregate(STB, as.yearmon, tail, 1) #my STB comes from quantmod's getSymbols(), wonder whether it is the cause of all problems...
> y <- Return.calculate(stb.agg$STB.Close, method=c("compound", "simple"))

> newx <- xts(coredata(y), order.by=as.Date(index(y)))     # thanks Dirk !

> newy <- zoo(newx)
# I dont know why, but without the above I get an errors, and only one
chart is printed instead of 3
# like the one here
http://cran.r-project.org/web/packages/PerformanceAnalytics/vignettes/PA-charts.pdf,
page 9,
# > charts.PerformanceSummary(newx)
# Error in FUN(newX[, i], ...) : The selected column is not numeric
# In addition: Warning message:
# In max(i) : no non-missing arguments to max; returning -Inf

> charts.PerformanceSummary(newy) #pretty charts !

Cheers,

-k


From spencer.graves at prodsyse.com  Thu Jun 18 04:43:11 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Wed, 17 Jun 2009 20:43:11 -0600
Subject: [R-SIG-Finance] zoo plotting - invalid 'ylim' value
In-Reply-To: <971536df0906171435l49a0f1fya5585b542de67d21@mail.gmail.com>
References: <2871c9e10906171053k597b467fi7fe0398140eddbd3@mail.gmail.com>
	<971536df0906171058h6202a05fsc31eaa555eb7bb15@mail.gmail.com>
	<971536df0906171435l49a0f1fya5585b542de67d21@mail.gmail.com>
Message-ID: <4A39A9BF.6020904@prodsyse.com>

Hi, Gabor: 


      I just lost perhaps a day's work due to that very problem:  I 
couldn't get read.zoo to work, so I used read.table, which created 
factors, because some of my numeric columns included, ", NA", which got 
translated as a level " NA" (note this three character code with a space 
before the NA).  Then I did "as.matrix" of the data.frame that I thought 
was numeric;  the result was a zoo object with coredata of class 
character rather than numeric. 


      Best Wishes,
      Spencer
          

Gabor Grothendieck wrote:
> Just to follow up the thread so that it is not left dangling
> the problem was that the object in question had character
> rather than numeric data in it.
>
> On Wed, Jun 17, 2009 at 1:58 PM, Gabor
> Grothendieck<ggrothendieck at gmail.com> wrote:
>   
>> What is the result of
>>
>> dput(stb.foreign)
>>
>>
>>
>> On Wed, Jun 17, 2009 at 1:53 PM, Khanh Nguyen<knguyen at cs.umb.edu> wrote:
>>     
>>> Hi all,
>>>
>>> I am very new to 'zoo', so this could probably be a trivia question,
>>> but I got the above error when I plot my zoo object
>>>
>>>       
>>>> plot(stb.foreign) #this work fine
>>>>         
>>>> stb.foreign$room.con.lai
>>>>         
>>>  2009-05-28  2009-05-29  2009-06-01  2009-06-02  2009-06-03
>>> 2009-06-04  2009-06-05
>>> 11104740    10772400    10775110    10397160    10037930     9986480
>>>  10003780
>>>  2009-06-08  2009-06-09  2009-06-10  2009-06-11  2009-06-12
>>> 2009-06-15  2009-06-16
>>> 10163340    10545920    11020540    10720430    10484040    10502080
>>>  10932930
>>>  2009-06-17
>>> 11201700
>>>
>>>       
>>>> plot(stb.foreign$room.con.lai)
>>>>         
>>> Error in plot.window(...) : invalid 'ylim' value
>>>       
>>> Thanks
>>>
>>> -k
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>>       
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From ggrothendieck at gmail.com  Thu Jun 18 04:58:43 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 17 Jun 2009 22:58:43 -0400
Subject: [R-SIG-Finance] zoo plotting - invalid 'ylim' value
In-Reply-To: <4A39A9BF.6020904@prodsyse.com>
References: <2871c9e10906171053k597b467fi7fe0398140eddbd3@mail.gmail.com> 
	<971536df0906171058h6202a05fsc31eaa555eb7bb15@mail.gmail.com> 
	<971536df0906171435l49a0f1fya5585b542de67d21@mail.gmail.com> 
	<4A39A9BF.6020904@prodsyse.com>
Message-ID: <971536df0906171958h1bf536b4k9fc04d3e71475e10@mail.gmail.com>

Check out the na.strings= argument:

> Lines <- "3, NA
+ 4,6
+ 5, NA"
> library(zoo)
> z <- read.zoo(textConnection(Lines), sep = ",", na.strings = " NA")
> str(z)
?zoo? series from 3 to 5
  Data: int [1:3] NA 6 NA
  Index:  int [1:3] 3 4 5


On Wed, Jun 17, 2009 at 10:43 PM, spencerg<spencer.graves at prodsyse.com> wrote:
> Hi, Gabor:
>
> ? ? I just lost perhaps a day's work due to that very problem: ?I couldn't
> get read.zoo to work, so I used read.table, which created factors, because
> some of my numeric columns included, ", NA", which got translated as a level
> " NA" (note this three character code with a space before the NA). ?Then I
> did "as.matrix" of the data.frame that I thought was numeric; ?the result
> was a zoo object with coredata of class character rather than numeric.
>
> ? ? Best Wishes,
> ? ? Spencer
>
> Gabor Grothendieck wrote:
>>
>> Just to follow up the thread so that it is not left dangling
>> the problem was that the object in question had character
>> rather than numeric data in it.
>>
>> On Wed, Jun 17, 2009 at 1:58 PM, Gabor
>> Grothendieck<ggrothendieck at gmail.com> wrote:
>>
>>>
>>> What is the result of
>>>
>>> dput(stb.foreign)
>>>
>>>
>>>
>>> On Wed, Jun 17, 2009 at 1:53 PM, Khanh Nguyen<knguyen at cs.umb.edu> wrote:
>>>
>>>>
>>>> Hi all,
>>>>
>>>> I am very new to 'zoo', so this could probably be a trivia question,
>>>> but I got the above error when I plot my zoo object
>>>>
>>>>
>>>>>
>>>>> plot(stb.foreign) #this work fine
>>>>> ? ? ? ?stb.foreign$room.con.lai
>>>>>
>>>>
>>>> ?2009-05-28 ?2009-05-29 ?2009-06-01 ?2009-06-02 ?2009-06-03
>>>> 2009-06-04 ?2009-06-05
>>>> 11104740 ? ?10772400 ? ?10775110 ? ?10397160 ? ?10037930 ? ? 9986480
>>>> ?10003780
>>>> ?2009-06-08 ?2009-06-09 ?2009-06-10 ?2009-06-11 ?2009-06-12
>>>> 2009-06-15 ?2009-06-16
>>>> 10163340 ? ?10545920 ? ?11020540 ? ?10720430 ? ?10484040 ? ?10502080
>>>> ?10932930
>>>> ?2009-06-17
>>>> 11201700
>>>>
>>>>
>>>>>
>>>>> plot(stb.foreign$room.con.lai)
>>>>>
>>>>
>>>> Error in plot.window(...) : invalid 'ylim' value
>>>> ? ? ?Thanks
>>>>
>>>> -k
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>
>
>


From edd at debian.org  Thu Jun 18 05:08:33 2009
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 17 Jun 2009 22:08:33 -0500
Subject: [R-SIG-Finance] zoo plotting - invalid 'ylim' value
In-Reply-To: <4A39A9BF.6020904@prodsyse.com>
References: <2871c9e10906171053k597b467fi7fe0398140eddbd3@mail.gmail.com>
	<971536df0906171058h6202a05fsc31eaa555eb7bb15@mail.gmail.com>
	<971536df0906171435l49a0f1fya5585b542de67d21@mail.gmail.com>
	<4A39A9BF.6020904@prodsyse.com>
Message-ID: <19001.44977.84181.817455@ron.nulle.part>


Spencer,

On 17 June 2009 at 20:43, spencerg wrote:
|       I just lost perhaps a day's work due to that very problem:  I 
| couldn't get read.zoo to work, so I used read.table, which created 
| factors, because some of my numeric columns included, ", NA", which got 
| translated as a level " NA" (note this three character code with a space 
| before the NA).  Then I did "as.matrix" of the data.frame that I thought 
| was numeric;  the result was a zoo object with coredata of class 
| character rather than numeric. 

Very painful.

But that may be a good reminder that at the end of the day, almost all data
reading methods wrap around read.table --- i.e. read.csv "merely" sets the
separator option (and some others), read.zoo "merely" manages to set an index
option of appropriate type etc pp.   

Why does this matter?  Because read.table respect the (reasonably new) option
stringsAsFactors.  From ?read.table:

     read.table(file, header = FALSE, sep = "", quote = "\"'",
                dec = ".", row.names, col.names,
                as.is = !stringsAsFactors,
                        ^^^^^^^^^^^^^^^^^
[...]
                stringsAsFactors = default.stringsAsFactors(),
                fileEncoding = "", encoding = "unknown")

[...]

stringsAsFactors: logical: should character vectors be converted to
          factors?  Note that this is overridden bu 'as.is' and
          'colClasses', both of which allow finer control.


and from ?default.stringsAsFactors :


stringsAsFactors: logical: should character vectors be converted to
          factors?  The 'factory-fresh' default is 'TRUE', but this can
          be changed by setting 'options(stringsAsFactors = FALSE)'.


It may be worthwhile to set this option in ~/.Rprofile

Hth, Dirk



-- 
Three out of two people have difficulties with fractions.


From ggrothendieck at gmail.com  Thu Jun 18 05:11:17 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 17 Jun 2009 23:11:17 -0400
Subject: [R-SIG-Finance] zoo plotting - invalid 'ylim' value
In-Reply-To: <971536df0906171958h1bf536b4k9fc04d3e71475e10@mail.gmail.com>
References: <2871c9e10906171053k597b467fi7fe0398140eddbd3@mail.gmail.com> 
	<971536df0906171058h6202a05fsc31eaa555eb7bb15@mail.gmail.com> 
	<971536df0906171435l49a0f1fya5585b542de67d21@mail.gmail.com> 
	<4A39A9BF.6020904@prodsyse.com>
	<971536df0906171958h1bf536b4k9fc04d3e71475e10@mail.gmail.com>
Message-ID: <971536df0906172011i3116976g7a20c32b8d218b8b@mail.gmail.com>

Or perhaps even better, colClasses = "numeric"

> z <- read.zoo(textConnection(Lines), sep = ",", colClasses = "numeric")
> str(z)
?zoo? series from 3 to 5
  Data: num [1:3] NA 6 NA
  Index:  num [1:3] 3 4 5


On Wed, Jun 17, 2009 at 10:58 PM, Gabor
Grothendieck<ggrothendieck at gmail.com> wrote:
> Check out the na.strings= argument:
>
>> Lines <- "3, NA
> + 4,6
> + 5, NA"
>> library(zoo)
>> z <- read.zoo(textConnection(Lines), sep = ",", na.strings = " NA")
>> str(z)
> ?zoo? series from 3 to 5
> ?Data: int [1:3] NA 6 NA
> ?Index: ?int [1:3] 3 4 5
>
>
> On Wed, Jun 17, 2009 at 10:43 PM, spencerg<spencer.graves at prodsyse.com> wrote:
>> Hi, Gabor:
>>
>> ? ? I just lost perhaps a day's work due to that very problem: ?I couldn't
>> get read.zoo to work, so I used read.table, which created factors, because
>> some of my numeric columns included, ", NA", which got translated as a level
>> " NA" (note this three character code with a space before the NA). ?Then I
>> did "as.matrix" of the data.frame that I thought was numeric; ?the result
>> was a zoo object with coredata of class character rather than numeric.
>>
>> ? ? Best Wishes,
>> ? ? Spencer
>>
>> Gabor Grothendieck wrote:
>>>
>>> Just to follow up the thread so that it is not left dangling
>>> the problem was that the object in question had character
>>> rather than numeric data in it.
>>>
>>> On Wed, Jun 17, 2009 at 1:58 PM, Gabor
>>> Grothendieck<ggrothendieck at gmail.com> wrote:
>>>
>>>>
>>>> What is the result of
>>>>
>>>> dput(stb.foreign)
>>>>
>>>>
>>>>
>>>> On Wed, Jun 17, 2009 at 1:53 PM, Khanh Nguyen<knguyen at cs.umb.edu> wrote:
>>>>
>>>>>
>>>>> Hi all,
>>>>>
>>>>> I am very new to 'zoo', so this could probably be a trivia question,
>>>>> but I got the above error when I plot my zoo object
>>>>>
>>>>>
>>>>>>
>>>>>> plot(stb.foreign) #this work fine
>>>>>> ? ? ? ?stb.foreign$room.con.lai
>>>>>>
>>>>>
>>>>> ?2009-05-28 ?2009-05-29 ?2009-06-01 ?2009-06-02 ?2009-06-03
>>>>> 2009-06-04 ?2009-06-05
>>>>> 11104740 ? ?10772400 ? ?10775110 ? ?10397160 ? ?10037930 ? ? 9986480
>>>>> ?10003780
>>>>> ?2009-06-08 ?2009-06-09 ?2009-06-10 ?2009-06-11 ?2009-06-12
>>>>> 2009-06-15 ?2009-06-16
>>>>> 10163340 ? ?10545920 ? ?11020540 ? ?10720430 ? ?10484040 ? ?10502080
>>>>> ?10932930
>>>>> ?2009-06-17
>>>>> 11201700
>>>>>
>>>>>
>>>>>>
>>>>>> plot(stb.foreign$room.con.lai)
>>>>>>
>>>>>
>>>>> Error in plot.window(...) : invalid 'ylim' value
>>>>> ? ? ?Thanks
>>>>>
>>>>> -k
>>>>>
>>>>> _______________________________________________
>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>> -- Subscriber-posting only.
>>>>> -- If you want to post, subscribe first.
>>>>>
>>>>>
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>>
>>
>>
>


From markleeds at verizon.net  Wed Jun 17 21:55:34 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Wed, 17 Jun 2009 14:55:34 -0500 (CDT)
Subject: [R-SIG-Finance] Vasicek model estimation via linear regression
Message-ID: <1122248846.92150.1245268534653.JavaMail.root@vms125.mailsrvcs.net>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090617/d58038a6/attachment.html>

From nelson.ana at gmail.com  Thu Jun 18 16:01:06 2009
From: nelson.ana at gmail.com (Ana Nelson)
Date: Thu, 18 Jun 2009 15:01:06 +0100
Subject: [R-SIG-Finance] RBloomberg with rcom
Message-ID: <a7d6d2740906180701x2a3e5666t3e9920fe2074fc7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090618/7f832262/attachment.pl>

From rohitgar at andrew.cmu.edu  Thu Jun 18 16:41:07 2009
From: rohitgar at andrew.cmu.edu (Rohit Garg)
Date: Thu, 18 Jun 2009 10:41:07 -0400
Subject: [R-SIG-Finance] Least Square estimate of Multi-variate time series
	data
Message-ID: <84cfd2942c1215ba41c34ad74166ad09.squirrel@webmail.andrew.cmu.edu>

Hi All,

Which package do I need to use if I am handling multi-variate time series
data? More specifically, I want to find the least square estimate of
multi-variate time series.

Thank you for your time and help.
Regards,
Rohit


From john.kerpel at gmail.com  Thu Jun 18 16:44:19 2009
From: john.kerpel at gmail.com (John Kerpel)
Date: Thu, 18 Jun 2009 09:44:19 -0500
Subject: [R-SIG-Finance] Least Square estimate of Multi-variate time
	series data
In-Reply-To: <84cfd2942c1215ba41c34ad74166ad09.squirrel@webmail.andrew.cmu.edu>
References: <84cfd2942c1215ba41c34ad74166ad09.squirrel@webmail.andrew.cmu.edu>
Message-ID: <6555fd730906180744v1d5b3415xa0e71416f3002261@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090618/08a6ae10/attachment.pl>

From bogaso.christofer at gmail.com  Thu Jun 18 21:58:14 2009
From: bogaso.christofer at gmail.com (Bogaso)
Date: Thu, 18 Jun 2009 12:58:14 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Downloading data from specific
	website
Message-ID: <24099586.post@talk.nabble.com>


Hi,

I need to download some econometric data from http://www.jmulti.de/
The path is :
Datasets -> Download data (2nd link) -> e6.dat

I prefer the downloaded data as a zoo object. How can I develop some R
function to do that? Your help will be highly appreciated.

Regards,
-- 
View this message in context: http://www.nabble.com/Downloading-data-from-specific-website-tp24099586p24099586.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From cedrick at cedrickjohnson.com  Thu Jun 18 22:30:13 2009
From: cedrick at cedrickjohnson.com (Cedrick Johnson)
Date: Thu, 18 Jun 2009 16:30:13 -0400
Subject: [R-SIG-Finance] [R-sig-finance] Downloading data from specific
 website
In-Reply-To: <24099586.post@talk.nabble.com>
References: <24099586.post@talk.nabble.com>
Message-ID: <4A3AA3D5.1000505@cedrickjohnson.com>

While my knowledge of converting things and all things xts/zoo 
conversion and applying dates is *severely* lacking, I borrowed this 
from quantmod getSymbols source code and should get you in the ballpark:


tmp = tempfile()
download.file("http://www.jmulti.de//download/datasets/e6.dat", 
destfile=tmp,quiet=FALSE)
dataset = read.delim(tmp, header=TRUE, skip=10, sep="\t")
unlink(tmp)

from here, you can turn 'dataset' into whatever you would like. Someone 
more experienced could share in how to handle the header which seems to 
be a bit screwed up, as evidenced by my output here
 > head(dataset)
                            Dp.............R
1 -0.00313258          0.083               
2 0.0188713            0.083     


Hope that at least points you in the right direction.. Looks like from 
the explanation that the data is quarterly.

-cj


Bogaso wrote:
> Hi,
>
> I need to download some econometric data from http://www.jmulti.de/
> The path is :
> Datasets -> Download data (2nd link) -> e6.dat
>
> I prefer the downloaded data as a zoo object. How can I develop some R
> function to do that? Your help will be highly appreciated.
>
> Regards,
>


From ggrothendieck at gmail.com  Thu Jun 18 22:48:23 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 18 Jun 2009 16:48:23 -0400
Subject: [R-SIG-Finance] [R-sig-finance] Downloading data from specific
	website
In-Reply-To: <24099586.post@talk.nabble.com>
References: <24099586.post@talk.nabble.com>
Message-ID: <971536df0906181348p5302b309u4a0ba894ad3e1abb@mail.gmail.com>

Try this.  First we read in the entire page line by line into Lines
and find the index, ix, of the line starting with <.   st is its
value as a character string after stripping off junk.  Then we
re-read the page from the text we already downloaded but this
time as a table skipping the first ix lines and finally we
convert it to zoo using as.yearqtr(st) as the start quarter.

> library(zoo)
> URL <- "http://www.jmulti.de//download/datasets/e6.dat"
> Lines <- readLines(URL)
> ix <- grep("^<", Lines)
> st <- sub("[<> ]", "", Lines[ix])
> DF <- read.table(textConnection(Lines), skip = ix, header = TRUE)
> z <- zooreg(as.matrix(DF), start = as.yearqtr(st), freq = 4)
> head(z)
                  Dp     R
1972 Q2 -0.003132580 0.083
1972 Q3  0.018871300 0.083
1972 Q4  0.024803600 0.087
1973 Q1  0.016277600 0.087
1973 Q2  0.000289679 0.102
1973 Q3  0.016829000 0.098

On Thu, Jun 18, 2009 at 3:58 PM, Bogaso<bogaso.christofer at gmail.com> wrote:
>
> Hi,
>
> I need to download some econometric data from http://www.jmulti.de/
> The path is :
> Datasets -> Download data (2nd link) -> e6.dat
>
> I prefer the downloaded data as a zoo object. How can I develop some R
> function to do that? Your help will be highly appreciated.
>
> Regards,
> --
> View this message in context: http://www.nabble.com/Downloading-data-from-specific-website-tp24099586p24099586.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From comtech.usa at gmail.com  Sat Jun 20 23:54:10 2009
From: comtech.usa at gmail.com (Michael)
Date: Sat, 20 Jun 2009 14:54:10 -0700
Subject: [R-SIG-Finance] how to compute the daily return?
Message-ID: <b1f16d9d0906201454o55ed7e55g8f00f93c19fbd23e@mail.gmail.com>

Hi all,

If I have wealth number on the following days.... what's the standard
way of computing the daily return and growth rate? What's a standard
way of defining growth rate?

(360 day convention, and how about holidays and weekends)?

1/1/1999
1/4/1999
1/5/1999
1/6/1999
1/7/1999
1/8/1999
1/11/1999
1/12/1999
1/13/1999
1/14/1999
1/15/1999
1/18/1999


Thanks a lot!


From knguyen at cs.umb.edu  Sun Jun 21 00:11:28 2009
From: knguyen at cs.umb.edu (Khanh Nguyen)
Date: Sat, 20 Jun 2009 18:11:28 -0400
Subject: [R-SIG-Finance] how to compute the daily return?
In-Reply-To: <b1f16d9d0906201454o55ed7e55g8f00f93c19fbd23e@mail.gmail.com>
References: <b1f16d9d0906201454o55ed7e55g8f00f93c19fbd23e@mail.gmail.com>
Message-ID: <2871c9e10906201511kb9cba39ma545a5508a4dd055@mail.gmail.com>

You can take a look here

http://cran.r-project.org/web/packages/zoo/vignettes/zoo-quickref.pdf

there is a 'Price and Returns' section.

Moreover, package PerformanceAnalytics has a 'Return.Calculate' method
that I think you can use.

-k

On Sat, Jun 20, 2009 at 5:54 PM, Michael<comtech.usa at gmail.com> wrote:
> Hi all,
>
> If I have wealth number on the following days.... what's the standard
> way of computing the daily return and growth rate? What's a standard
> way of defining growth rate?
>
> (360 day convention, and how about holidays and weekends)?
>
> 1/1/1999
> 1/4/1999
> 1/5/1999
> 1/6/1999
> 1/7/1999
> 1/8/1999
> 1/11/1999
> 1/12/1999
> 1/13/1999
> 1/14/1999
> 1/15/1999
> 1/18/1999
>
>
> Thanks a lot!
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From bogaso.christofer at gmail.com  Sun Jun 21 10:05:20 2009
From: bogaso.christofer at gmail.com (Bogaso)
Date: Sun, 21 Jun 2009 01:05:20 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Downloading data from specific
	website
In-Reply-To: <971536df0906181348p5302b309u4a0ba894ad3e1abb@mail.gmail.com>
References: <24099586.post@talk.nabble.com>
	<971536df0906181348p5302b309u4a0ba894ad3e1abb@mail.gmail.com>
Message-ID: <24132322.post@talk.nabble.com>


Thanks everyone it is working. However interested to know, how you define the
URL i.e. "http://www.jmulti.de//download/datasets/e6.dat"?

If I go 1st "http://www.jmulti.de" and then click on "Datasets" and then
"Download data, 2nd link", I see no change in the address bar. What is the
magic here? Can you please clarify me? If I choose to go with "Download
data, 1st link" then what would be the strategy?

Thanks and regards,



Gabor Grothendieck wrote:
> 
> Try this.  First we read in the entire page line by line into Lines
> and find the index, ix, of the line starting with <.   st is its
> value as a character string after stripping off junk.  Then we
> re-read the page from the text we already downloaded but this
> time as a table skipping the first ix lines and finally we
> convert it to zoo using as.yearqtr(st) as the start quarter.
> 
>> library(zoo)
>> URL <- "http://www.jmulti.de//download/datasets/e6.dat"
>> Lines <- readLines(URL)
>> ix <- grep("^<", Lines)
>> st <- sub("[<> ]", "", Lines[ix])
>> DF <- read.table(textConnection(Lines), skip = ix, header = TRUE)
>> z <- zooreg(as.matrix(DF), start = as.yearqtr(st), freq = 4)
>> head(z)
>                   Dp     R
> 1972 Q2 -0.003132580 0.083
> 1972 Q3  0.018871300 0.083
> 1972 Q4  0.024803600 0.087
> 1973 Q1  0.016277600 0.087
> 1973 Q2  0.000289679 0.102
> 1973 Q3  0.016829000 0.098
> 
> On Thu, Jun 18, 2009 at 3:58 PM, Bogaso<bogaso.christofer at gmail.com>
> wrote:
>>
>> Hi,
>>
>> I need to download some econometric data from http://www.jmulti.de/
>> The path is :
>> Datasets -> Download data (2nd link) -> e6.dat
>>
>> I prefer the downloaded data as a zoo object. How can I develop some R
>> function to do that? Your help will be highly appreciated.
>>
>> Regards,
>> --
>> View this message in context:
>> http://www.nabble.com/Downloading-data-from-specific-website-tp24099586p24099586.html
>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/Downloading-data-from-specific-website-tp24099586p24132322.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From comtech.usa at gmail.com  Sun Jun 21 08:42:11 2009
From: comtech.usa at gmail.com (Michael)
Date: Sat, 20 Jun 2009 23:42:11 -0700
Subject: [R-SIG-Finance] how to read in this time series csv file with both
	dates and times?
Message-ID: <b1f16d9d0906202342y2098c619td8db4b921c9db0dc@mail.gmail.com>

Hi all,

I want to read in this "csv" file,

3/1/2009 23:00:00,123.76,123.94,123.7
3/2/2009 0:00:00,123.85,124.16,123.85
3/2/2009 1:00:00,124.11,124.15,124.06
3/2/2009 2:00:00,124.14,124.32,124.12
3/2/2009 3:00:00,124.2,124.21,124.11
3/2/2009 4:00:00,124.16,124.18,123.94
3/2/2009 5:00:00,124.01,124.2,123.97

And I got the following error message, what could be wrong?

Thanks a lot!


z <- read.zoo("prices.csv", header = TRUE, sep = ",", FUN =
as.chron(format=c(dates = "m/d/y", times = "h:m:s")))
Error in inherits(x, "chron") : element 1 is empty;
   the part of the args list of '.Internal' being evaluated was:
   (x, what, which)


From ggrothendieck at gmail.com  Sun Jun 21 17:21:15 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 21 Jun 2009 11:21:15 -0400
Subject: [R-SIG-Finance] how to read in this time series csv file with
	both dates and times?
In-Reply-To: <b1f16d9d0906202342y2098c619td8db4b921c9db0dc@mail.gmail.com>
References: <b1f16d9d0906202342y2098c619td8db4b921c9db0dc@mail.gmail.com>
Message-ID: <971536df0906210821w432f6664wf42f90feb35b3a00@mail.gmail.com>

There are several things wrong here:

1. FUN= must be a function but its not in the posted code

2. as.chron() uses % codes in its format as described in
?strptime, not the format style for chron().

See R News 4/1 for more on dates and times and also
see the examples in ?read.zoo

Here are two ways to do it.  The first uses as.chron()
and the second defines a custom function, toChron,
that uses chron().

library(zoo)
library(chron)

Lines <- "3/1/2009 23:00:00,123.76,123.94,123.7
3/2/2009 0:00:00,123.85,124.16,123.85
3/2/2009 1:00:00,124.11,124.15,124.06
3/2/2009 2:00:00,124.14,124.32,124.12
3/2/2009 3:00:00,124.2,124.21,124.11
3/2/2009 4:00:00,124.16,124.18,123.94
3/2/2009 5:00:00,124.01,124.2,123.97"

# 1
read.zoo(textConnection(Lines), sep = ",", FUN = as.chron,
	format = "%m/%d/%Y %H:%M:%S")

# 2
toChron <- function(x) chron(sub(" .*", "", x), sub(".* ", "", x))
read.zoo(textConnection(Lines), sep = ",", FUN = toChron)

On Sun, Jun 21, 2009 at 2:42 AM, Michael<comtech.usa at gmail.com> wrote:
> Hi all,
>
> I want to read in this "csv" file,
>
> 3/1/2009 23:00:00,123.76,123.94,123.7
> 3/2/2009 0:00:00,123.85,124.16,123.85
> 3/2/2009 1:00:00,124.11,124.15,124.06
> 3/2/2009 2:00:00,124.14,124.32,124.12
> 3/2/2009 3:00:00,124.2,124.21,124.11
> 3/2/2009 4:00:00,124.16,124.18,123.94
> 3/2/2009 5:00:00,124.01,124.2,123.97
>
> And I got the following error message, what could be wrong?
>
> Thanks a lot!
>
>
> z <- read.zoo("prices.csv", header = TRUE, sep = ",", FUN =
> as.chron(format=c(dates = "m/d/y", times = "h:m:s")))
> Error in inherits(x, "chron") : element 1 is empty;
> ? the part of the args list of '.Internal' being evaluated was:
> ? (x, what, which)
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ggrothendieck at gmail.com  Sun Jun 21 17:32:43 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 21 Jun 2009 11:32:43 -0400
Subject: [R-SIG-Finance] [R-sig-finance] Downloading data from specific
	website
In-Reply-To: <24132322.post@talk.nabble.com>
References: <24099586.post@talk.nabble.com>
	<971536df0906181348p5302b309u4a0ba894ad3e1abb@mail.gmail.com> 
	<24132322.post@talk.nabble.com>
Message-ID: <971536df0906210832v672da71auf6820e77231c8395@mail.gmail.com>

On the datasets page right click e2.dat and choose Copy Link Location
or Copy Shortcut depending on which browser you use.   Then paste
that into your text editor.

On Sun, Jun 21, 2009 at 4:05 AM, Bogaso<bogaso.christofer at gmail.com> wrote:
>
> Thanks everyone it is working. However interested to know, how you define the
> URL i.e. "http://www.jmulti.de//download/datasets/e6.dat"?
>
> If I go 1st "http://www.jmulti.de" and then click on "Datasets" and then
> "Download data, 2nd link", I see no change in the address bar. What is the
> magic here? Can you please clarify me? If I choose to go with "Download
> data, 1st link" then what would be the strategy?
>
> Thanks and regards,
>
>
>
> Gabor Grothendieck wrote:
>>
>> Try this. ?First we read in the entire page line by line into Lines
>> and find the index, ix, of the line starting with <. ? st is its
>> value as a character string after stripping off junk. ?Then we
>> re-read the page from the text we already downloaded but this
>> time as a table skipping the first ix lines and finally we
>> convert it to zoo using as.yearqtr(st) as the start quarter.
>>
>>> library(zoo)
>>> URL <- "http://www.jmulti.de//download/datasets/e6.dat"
>>> Lines <- readLines(URL)
>>> ix <- grep("^<", Lines)
>>> st <- sub("[<> ]", "", Lines[ix])
>>> DF <- read.table(textConnection(Lines), skip = ix, header = TRUE)
>>> z <- zooreg(as.matrix(DF), start = as.yearqtr(st), freq = 4)
>>> head(z)
>> ? ? ? ? ? ? ? ? ? Dp ? ? R
>> 1972 Q2 -0.003132580 0.083
>> 1972 Q3 ?0.018871300 0.083
>> 1972 Q4 ?0.024803600 0.087
>> 1973 Q1 ?0.016277600 0.087
>> 1973 Q2 ?0.000289679 0.102
>> 1973 Q3 ?0.016829000 0.098
>>
>> On Thu, Jun 18, 2009 at 3:58 PM, Bogaso<bogaso.christofer at gmail.com>
>> wrote:
>>>
>>> Hi,
>>>
>>> I need to download some econometric data from http://www.jmulti.de/
>>> The path is :
>>> Datasets -> Download data (2nd link) -> e6.dat
>>>
>>> I prefer the downloaded data as a zoo object. How can I develop some R
>>> function to do that? Your help will be highly appreciated.
>>>
>>> Regards,
>>> --
>>> View this message in context:
>>> http://www.nabble.com/Downloading-data-from-specific-website-tp24099586p24099586.html
>>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>
>
> --
> View this message in context: http://www.nabble.com/Downloading-data-from-specific-website-tp24099586p24132322.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From bogaso.christofer at gmail.com  Sun Jun 21 17:42:22 2009
From: bogaso.christofer at gmail.com (Bogaso)
Date: Sun, 21 Jun 2009 08:42:22 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] How to create seasonal variable for
	zoo object.
Message-ID: <24135743.post@talk.nabble.com>


Hi all, how to create a seasonal variable for a zoo object? For example I
have following dataset :

rm(list = ls())
URL <- "http://www.jmulti.de//download/datasets/e6.dat"
dat1 <- readLines(URL)
dat11 <- dat1[-c(1:11)]
dat12 <- read.table(textConnection(dat11))
library(zoo)
TS.dat <- zooreg(dat12[,c(2,1)], start = 1972.2, frequency = 4)


I want to create seasonal variable for each quarter for "TS.dat"
-- 
View this message in context: http://www.nabble.com/How-to-create-seasonal-variable-for-zoo-object.-tp24135743p24135743.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From ggrothendieck at gmail.com  Sun Jun 21 17:51:34 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 21 Jun 2009 11:51:34 -0400
Subject: [R-SIG-Finance] [R-sig-finance] How to create seasonal variable
	for zoo object.
In-Reply-To: <24135743.post@talk.nabble.com>
References: <24135743.post@talk.nabble.com>
Message-ID: <971536df0906210851k4016c3a2jf1c7b6994349c140@mail.gmail.com>

On Sun, Jun 21, 2009 at 11:42 AM, Bogaso<bogaso.christofer at gmail.com> wrote:
>
> Hi all, how to create a seasonal variable for a zoo object? For example I
> have following dataset :
>
> rm(list = ls())

Please don't post code like that. Someone could download it and find out
you had wiped out their workspace.

> URL <- "http://www.jmulti.de//download/datasets/e6.dat"
> dat1 <- readLines(URL)
> dat11 <- dat1[-c(1:11)]
> dat12 <- read.table(textConnection(dat11))
> library(zoo)
> TS.dat <- zooreg(dat12[,c(2,1)], start = 1972.2, frequency = 4)
>
>
> I want to create seasonal variable for each quarter for "TS.dat"

Its not clear what is meant by "seasonal variable" but perhaps you
are looking for cycle:

cbind(TS.dat, cycle = cycle(TS.dat))

> --
> View this message in context: http://www.nabble.com/How-to-create-seasonal-variable-for-zoo-object.-tp24135743p24135743.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From bogaso.christofer at gmail.com  Sun Jun 21 18:00:19 2009
From: bogaso.christofer at gmail.com (Bogaso)
Date: Sun, 21 Jun 2009 09:00:19 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] How to create seasonal variable
	for zoo object.
In-Reply-To: <971536df0906210851k4016c3a2jf1c7b6994349c140@mail.gmail.com>
References: <24135743.post@talk.nabble.com>
	<971536df0906210851k4016c3a2jf1c7b6994349c140@mail.gmail.com>
Message-ID: <24135934.post@talk.nabble.com>


I am really sorry to post the code "rm(list = ls())". I admit my mistake :(

Actually I wanted to create a Quarterly dummy variable for that data. I am
looking for a direct approach to create that for a zoo object.

Regards,


Gabor Grothendieck wrote:
> 
> On Sun, Jun 21, 2009 at 11:42 AM, Bogaso<bogaso.christofer at gmail.com>
> wrote:
>>
>> Hi all, how to create a seasonal variable for a zoo object? For example I
>> have following dataset :
>>
>> rm(list = ls())
> 
> Please don't post code like that. Someone could download it and find out
> you had wiped out their workspace.
> 
>> URL <- "http://www.jmulti.de//download/datasets/e6.dat"
>> dat1 <- readLines(URL)
>> dat11 <- dat1[-c(1:11)]
>> dat12 <- read.table(textConnection(dat11))
>> library(zoo)
>> TS.dat <- zooreg(dat12[,c(2,1)], start = 1972.2, frequency = 4)
>>
>>
>> I want to create seasonal variable for each quarter for "TS.dat"
> 
> Its not clear what is meant by "seasonal variable" but perhaps you
> are looking for cycle:
> 
> cbind(TS.dat, cycle = cycle(TS.dat))
> 
>> --
>> View this message in context:
>> http://www.nabble.com/How-to-create-seasonal-variable-for-zoo-object.-tp24135743p24135743.html
>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/How-to-create-seasonal-variable-for-zoo-object.-tp24135743p24135934.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From ggrothendieck at gmail.com  Sun Jun 21 18:17:27 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 21 Jun 2009 12:17:27 -0400
Subject: [R-SIG-Finance] [R-sig-finance] How to create seasonal variable
	for zoo object.
In-Reply-To: <971536df0906210851k4016c3a2jf1c7b6994349c140@mail.gmail.com>
References: <24135743.post@talk.nabble.com>
	<971536df0906210851k4016c3a2jf1c7b6994349c140@mail.gmail.com>
Message-ID: <971536df0906210917u3b7dce32v6541a706272555b9@mail.gmail.com>

Try this:

DF.dat <- read.table(URL, skip = 11)
TS2.dat <- zooreg(DF.dat[2:1], start = 1972.25, frequency = 4)

cyc <- factor(cycle(TS2.dat))
m <- model.matrix(~ cyc - 1)
TS3.dat <- cbind(TS2.dat, m)


On Sun, Jun 21, 2009 at 11:51 AM, Gabor
Grothendieck<ggrothendieck at gmail.com> wrote:
> On Sun, Jun 21, 2009 at 11:42 AM, Bogaso<bogaso.christofer at gmail.com> wrote:
>>
>> Hi all, how to create a seasonal variable for a zoo object? For example I
>> have following dataset :
>>
>> rm(list = ls())
>
> Please don't post code like that. Someone could download it and find out
> you had wiped out their workspace.
>
>> URL <- "http://www.jmulti.de//download/datasets/e6.dat"
>> dat1 <- readLines(URL)
>> dat11 <- dat1[-c(1:11)]
>> dat12 <- read.table(textConnection(dat11))
>> library(zoo)
>> TS.dat <- zooreg(dat12[,c(2,1)], start = 1972.2, frequency = 4)
>>
>>
>> I want to create seasonal variable for each quarter for "TS.dat"
>
> Its not clear what is meant by "seasonal variable" but perhaps you
> are looking for cycle:
>
> cbind(TS.dat, cycle = cycle(TS.dat))
>
>> --
>> View this message in context: http://www.nabble.com/How-to-create-seasonal-variable-for-zoo-object.-tp24135743p24135743.html
>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>


From anura at utequip.com  Mon Jun 22 02:52:35 2009
From: anura at utequip.com (Anura Karunaratne)
Date: Sun, 21 Jun 2009 17:52:35 -0700
Subject: [R-SIG-Finance] How to pass user name and password via code
Message-ID: <f443a77d0906211752u4e1e86a7l43f90be68efa3c24@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090621/ffd9b781/attachment.pl>

From josh.m.ulrich at gmail.com  Mon Jun 22 05:46:55 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Sun, 21 Jun 2009 22:46:55 -0500
Subject: [R-SIG-Finance] How to pass user name and password via code
In-Reply-To: <f443a77d0906211752u4e1e86a7l43f90be68efa3c24@mail.gmail.com>
References: <f443a77d0906211752u4e1e86a7l43f90be68efa3c24@mail.gmail.com>
Message-ID: <8cca69990906212046w63e2db2asdced180cdfb37abc@mail.gmail.com>

Anura,

Since this isn't a finance-related question, you'll probably receive a
better response from R-help.  That said, you may find an answer to
your question by searching the mailing list archives for "RCurl" or by
looking at the RCurl documentation.

Best,
Josh
--
http://www.fosstrading.com



On Sun, Jun 21, 2009 at 7:52 PM, Anura Karunaratne<anura at utequip.com> wrote:
> Hi
>
> I need to download stock market data from the following website
> http://www.cse.lk/welcome.htm
> but this site need username and password to access historical data.
> My problem is how do i pass these via code so that i can automate this.
>
> Thanks and Regards
>
> Anura
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From anass.mouhsine at gmail.com  Mon Jun 22 14:23:59 2009
From: anass.mouhsine at gmail.com (anass)
Date: Mon, 22 Jun 2009 14:23:59 +0200
Subject: [R-SIG-Finance] How to compare two asynchroneous xts time
	series?
In-Reply-To: <4A312978.6050905@prodsyse.com>
References: <4A30E664.3030004@gmail.com>
	<8cca69990906110700y397cecbbv9d20aba82f4e9eb6@mail.gmail.com>
	<e8e755250906110705i550f882j2da7017f8fb2ffd4@mail.gmail.com>
	<5651742d0906110716k46ea2925l12669a6d42612dcd@mail.gmail.com>
	<8cca69990906110725k7cc58ef2i29c7a3db4c935fd9@mail.gmail.com>
	<4A3114C3.50904@gmail.com> <4A312978.6050905@prodsyse.com>
Message-ID: <5651742d0906220523k3a3b78f6y2cf675b064be2770@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090622/630e95b7/attachment.pl>

From ggrothendieck at gmail.com  Mon Jun 22 14:44:36 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 22 Jun 2009 08:44:36 -0400
Subject: [R-SIG-Finance] How to compare two asynchroneous xts time
	series?
In-Reply-To: <5651742d0906220523k3a3b78f6y2cf675b064be2770@mail.gmail.com>
References: <4A30E664.3030004@gmail.com>
	<8cca69990906110700y397cecbbv9d20aba82f4e9eb6@mail.gmail.com> 
	<e8e755250906110705i550f882j2da7017f8fb2ffd4@mail.gmail.com> 
	<5651742d0906110716k46ea2925l12669a6d42612dcd@mail.gmail.com> 
	<8cca69990906110725k7cc58ef2i29c7a3db4c935fd9@mail.gmail.com> 
	<4A3114C3.50904@gmail.com> <4A312978.6050905@prodsyse.com> 
	<5651742d0906220523k3a3b78f6y2cf675b064be2770@mail.gmail.com>
Message-ID: <971536df0906220544p17f95005i787cf7c6b54208ad@mail.gmail.com>

The dyn package facilitates regression of time series
by automatically intersecting the components.  It
currently does not work with xts but its easy to
convert those to zoo just prior to performing the
regression. Just preface lm with dyn$ as shown.
It also works with glm and other regression functions
that are sufficiently similar to lm.

> Lines1 <- "2008-06-02 09:00:00,5007.0
+ 2008-06-02 09:01:00,5010.0
+ 2008-06-02 09:02:00,5014.0
+ 2008-06-02 09:03:00,5012.5
+ 2008-06-02 09:04:00,5013.5
+ 2008-06-02 09:05:00,5009.5
+ 2008-06-02 09:06:00,5007.0
+ 2008-06-02 09:07:00,5006.5
+ 2008-06-02 09:08:00,5008.5
+ 2008-06-02 09:09:00,5004.5"
>
> Lines2 <- "2008-06-02 09:01:00,7115.0
+ 2008-06-02 09:03:00,7117.0
+ 2008-06-02 09:05:00,7111.0
+ 2008-06-02 09:07:00,7107.0
+ 2008-06-02 09:09:00,7102.5"
>
> library(xts)
> z1 <- read.zoo(textConnection(Lines1), sep = ",", tz = "")
> x1 <- as.xts(z1)
> z2 <- read.zoo(textConnection(Lines2), sep = ",", tz = "")
> x2 <- as.xts(z2)
>
> # convert xts to zoo
> Z1 <- zoo(coredata(x1), time(x1))
> Z2 <- zoo(coredata(x2), time(x2))
> library(dyn)
> dyn$lm(Z2 ~ Z1)

Call:
lm(formula = dyn(Z2 ~ Z1))

Coefficients:
(Intercept)           Z1
  -2120.912        1.843



On Mon, Jun 22, 2009 at 8:23 AM, anass<anass.mouhsine at gmail.com> wrote:
> Hi again,
>
> Suppose the problem evolves a little.
> I would like to know how to perform a regression on two xts objects with
> different lengths.
> Let's take the same example:
> series1
>
> 2008-06-02 09:00:00 5007.0
> 2008-06-02 09:01:00 5010.0
> 2008-06-02 09:02:00 5014.0
> 2008-06-02 09:03:00 5012.5
> 2008-06-02 09:04:00 5013.5
> 2008-06-02 09:05:00 5009.5
> 2008-06-02 09:06:00 5007.0
> 2008-06-02 09:07:00 5006.5
> 2008-06-02 09:08:00 5008.5
> 2008-06-02 09:09:00 5004.5
>
> Series2
>
> 2008-06-02 09:01:00 7115.0
> 2008-06-02 09:03:00 7117.0
> 2008-06-02 09:05:00 7111.0
> 2008-06-02 09:07:00 7107.0
> 2008-06-02 09:09:00 7102.5
>
>
>>lm(Series2~Series1)
> Error in model.frame.default(formula = Series2 ~ Series1, drop.unused.levels
> = TRUE) :
> ?variable lengths differ (found for 'Series1')
>
> While performing the regression the usual way, I've got the error described
> before.
>
> Any thoughts?
>
> Anass
>
>
>
> On Thu, Jun 11, 2009 at 5:57 PM, spencerg <spencer.graves at prodsyse.com>wrote:
>
>> ? ? One could also smooth the two series first, then compare the smooths.
>> ?This could be used to interpolate missing values to provide a larger sample
>> size for tradition tools.
>>
>> ? ? The "fda" package has a number of tools for doing this kind of thing.
>> ?Learning about this package will soon get easier with the scheduled
>> appearance in July of Ramsay, Hooker and Graves (2009) Functional Data
>> Analysis with R and Matlab (Springer). ?The "fda" package contains script
>> files to reproduce all but one of the 78 figures in the book. ?The script
>> files are available now, and can be found as follows:
>>
>> > system.file('scripts', package='fda')
>> [1] "C:/Users/sgraves/R/R-2.9.0/library/fda/scripts"
>> > dir(system.file('scripts', package='fda'))
>> <snip> [17] "fdarm-ch01.R" ? ? ? ? ? ?"fdarm-ch02.R" ? ? ? ? ?[19]
>> "fdarm-ch03.R" ? ? ? ? ? ?"fdarm-ch04.R" ? ? ? ? ?[21] "fdarm-ch05.R"
>> ? ? ?"fdarm-ch06.R" ? ? ? ? ?[23] "fdarm-ch07.R" ? ? ? ? ? ?"fdarm-ch08.R"
>> ? ? ? ?[25] "fdarm-ch09.R" ? ? ? ? ? ?"fdarm-ch10.R" ? ? ? ? ?[27]
>> "fdarm-ch11.R" ? ? ? ? ? ?"pda.fd.test.R"
>>
>> ? ? Hope this helps. ? ? Spencer
>>
>>
>> Anass Mouhsine wrote:
>>
>>> Thanks Joshua
>>>
>>> A
>>>
>>> Joshua Ulrich wrote:
>>>
>>>> On Thu, Jun 11, 2009 at 9:16 AM, anass<anass.mouhsine at gmail.com> wrote:
>>>>
>>>>
>>>>> Thx guys,
>>>>>
>>>>> well the error I got is elswhere and I thought it was due to the
>>>>> asynchrone
>>>>> timeseries.
>>>>>
>>>>> let's assume that I got the intraday ratio, what I do is the following
>>>>>
>>>>> d<-index(to.daily(ratio))
>>>>> for (i in 1:length(d)){
>>>>> ?tstart<-paste(d[i], "09:00:00")
>>>>> ?tend<-paste(d[i],"17:00:00")
>>>>> ts_ratio<-ratio[tstart:tend]
>>>>>
>>>>>
>>>>
>>>> This line is incorrect. ?I don't believe the ":" sequence operator in
>>>> package:base is defined for character strings (see ?":"). ?This is
>>>> what is causing an error. ?What you probably intended is this:
>>>> ts_ratio <- ratio[paste(tstart,tend,sep="::")]
>>>>
>>>>
>>>>
>>>>> #
>>>>> # other operations
>>>>> #
>>>>> }
>>>>>
>>>>> and I have this error
>>>>> Error in tstart:tend : argument NA / NaN
>>>>>
>>>>> So I assume if it is not due to asynchrone series, it is due to the
>>>>> subsetting.
>>>>> Does xts objects accept this kind of subset?
>>>>>
>>>>>
>>>>> On Thu, Jun 11, 2009 at 4:05 PM, Jeff Ryan <jeff.a.ryan at gmail.com>
>>>>> wrote:
>>>>>
>>>>>
>>>>>> Anass,
>>>>>>
>>>>>> xts and zoo automatically align series via "merge" when performing Ops
>>>>>> methods (+/-*...etc)
>>>>>>
>>>>>> This is what you want in most cases.
>>>>>>
>>>>>> See ?merge.xts, ?xts, ?merge.zoo and ?Ops.zoo
>>>>>>
>>>>>> HTH
>>>>>> Jeff
>>>>>>
>>>>>> On Thu, Jun 11, 2009 at 9:00 AM, Joshua Ulrich<josh.m.ulrich at gmail.com
>>>>>> >
>>>>>> wrote:
>>>>>>
>>>>>>
>>>>>>> Hi Anass,
>>>>>>>
>>>>>>> Can you provide an example of what you're trying to do and what error
>>>>>>> you are receiving? ?I'm able to run the code below without error.
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>> require(xts)
>>>>>>>>
>>>>>>>> Lines <-
>>>>>>>>
>>>>>>>>
>>>>>>> + "2008-06-02 09:00:00,5007.0
>>>>>>> + 2008-06-02 09:01:00,5010.0
>>>>>>> + 2008-06-02 09:02:00,5014.0
>>>>>>> + 2008-06-02 09:03:00,5012.5
>>>>>>> + 2008-06-02 09:04:00,5013.5
>>>>>>> + 2008-06-02 09:05:00,5009.5
>>>>>>> + 2008-06-02 09:06:00,5007.0
>>>>>>> + 2008-06-02 09:07:00,5006.5
>>>>>>> + 2008-06-02 09:08:00,5008.5
>>>>>>> + 2008-06-02 09:09:00,5004.5"
>>>>>>>
>>>>>>>
>>>>>>>> one <- read.zoo(textConnection(Lines),sep=',',FUN=as.POSIXct)
>>>>>>>> one <- as.xts(one)
>>>>>>>>
>>>>>>>> Lines <-
>>>>>>>>
>>>>>>>>
>>>>>>> + "2008-06-02 09:01:00,7115.0
>>>>>>> + 2008-06-02 09:03:00,7117.0
>>>>>>> + 2008-06-02 09:05:00,7111.0
>>>>>>> + 2008-06-02 09:07:00,7107.0
>>>>>>> + 2008-06-02 09:09:00,7102.5"
>>>>>>>
>>>>>>>
>>>>>>>> two <- read.zoo(textConnection(Lines),sep=',',FUN=as.POSIXct)
>>>>>>>> two <- as.xts(two)
>>>>>>>>
>>>>>>>> one/two
>>>>>>>>
>>>>>>>>
>>>>>>> ? ? ? ? ? ? ? ? ? ? ? ? ?e1
>>>>>>> 2008-06-02 09:01:00 0.7041462
>>>>>>> 2008-06-02 09:03:00 0.7042996
>>>>>>> 2008-06-02 09:05:00 0.7044719
>>>>>>> 2008-06-02 09:07:00 0.7044463
>>>>>>> 2008-06-02 09:09:00 0.7046111
>>>>>>>
>>>>>>>
>>>>>>>> two/one
>>>>>>>>
>>>>>>>>
>>>>>>> ? ? ? ? ? ? ? ? ? ? ? ? e1
>>>>>>> 2008-06-02 09:01:00 1.420160
>>>>>>> 2008-06-02 09:03:00 1.419850
>>>>>>> 2008-06-02 09:05:00 1.419503
>>>>>>> 2008-06-02 09:07:00 1.419555
>>>>>>> 2008-06-02 09:09:00 1.419223
>>>>>>> ? ? ? ?Best,
>>>>>>> Joshua
>>>>>>> --
>>>>>>> http://www.fosstrading.com
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> On Thu, Jun 11, 2009 at 6:11 AM, Anass
>>>>>>> Mouhsine<anass.mouhsine at gmail.com> wrote:
>>>>>>>
>>>>>>>
>>>>>>>> Hi all,
>>>>>>>>
>>>>>>>> Suppose I have two xts time series with asynchroneous time index.
>>>>>>>> I would like for example to calculate a ratio of those two series,
>>>>>>>> but
>>>>>>>> I
>>>>>>>> don't know how to get an intersection of the two indices in order to
>>>>>>>> avoid
>>>>>>>> errors.
>>>>>>>>
>>>>>>>> an example of the data is the following
>>>>>>>>
>>>>>>>> series1
>>>>>>>>
>>>>>>>> 2008-06-02 09:00:00 5007.0
>>>>>>>> 2008-06-02 09:01:00 5010.0
>>>>>>>> 2008-06-02 09:02:00 5014.0
>>>>>>>> 2008-06-02 09:03:00 5012.5
>>>>>>>> 2008-06-02 09:04:00 5013.5
>>>>>>>> 2008-06-02 09:05:00 5009.5
>>>>>>>> 2008-06-02 09:06:00 5007.0
>>>>>>>> 2008-06-02 09:07:00 5006.5
>>>>>>>> 2008-06-02 09:08:00 5008.5
>>>>>>>> 2008-06-02 09:09:00 5004.5
>>>>>>>>
>>>>>>>> Series2
>>>>>>>>
>>>>>>>> 2008-06-02 09:01:00 7115.0
>>>>>>>> 2008-06-02 09:03:00 7117.0
>>>>>>>> 2008-06-02 09:05:00 7111.0
>>>>>>>> 2008-06-02 09:07:00 7107.0
>>>>>>>> 2008-06-02 09:09:00 7102.5
>>>>>>>>
>>>>>>>> Any idea?
>>>>>>>>
>>>>>>>> Thanks in advance
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>>>>> -- Subscriber-posting only.
>>>>>>>> -- If you want to post, subscribe first.
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>>>> -- Subscriber-posting only.
>>>>>>> -- If you want to post, subscribe first.
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>
>>>>>> --
>>>>>> Jeffrey Ryan
>>>>>> jeffrey.ryan at insightalgo.com
>>>>>>
>>>>>> ia: insight algorithmics
>>>>>> www.insightalgo.com
>>>>>>
>>>>>>
>>>>>
>>>>> --
>>>>>
>>>>> En toda ocasion, disfruta de la vida
>>>>>
>>>>>
>>>>>
>>>>
>>>> Best,
>>>> Joshua
>>>> --
>>>> http://www.fosstrading.com
>>>>
>>>>
>>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>>
>>
>
>
> --
>
> Every man has his own destiny: the only imperative is to follow it, to
> accept it, no matter where it leads him.(H.Miller)
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ggrothendieck at gmail.com  Mon Jun 22 15:09:53 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 22 Jun 2009 09:09:53 -0400
Subject: [R-SIG-Finance] How to compare two asynchroneous xts time
	series?
In-Reply-To: <971536df0906220544p17f95005i787cf7c6b54208ad@mail.gmail.com>
References: <4A30E664.3030004@gmail.com>
	<8cca69990906110700y397cecbbv9d20aba82f4e9eb6@mail.gmail.com> 
	<e8e755250906110705i550f882j2da7017f8fb2ffd4@mail.gmail.com> 
	<5651742d0906110716k46ea2925l12669a6d42612dcd@mail.gmail.com> 
	<8cca69990906110725k7cc58ef2i29c7a3db4c935fd9@mail.gmail.com> 
	<4A3114C3.50904@gmail.com> <4A312978.6050905@prodsyse.com> 
	<5651742d0906220523k3a3b78f6y2cf675b064be2770@mail.gmail.com> 
	<971536df0906220544p17f95005i787cf7c6b54208ad@mail.gmail.com>
Message-ID: <971536df0906220609j11b83c61vf2fc85fc34c6eebe@mail.gmail.com>

Another way is to explicitly do the merge yourself
in which case you can do it all in xts:

lm(x2 ~ x1, merge(x1, x2))

On Mon, Jun 22, 2009 at 8:44 AM, Gabor
Grothendieck<ggrothendieck at gmail.com> wrote:
> The dyn package facilitates regression of time series
> by automatically intersecting the components. ?It
> currently does not work with xts but its easy to
> convert those to zoo just prior to performing the
> regression. Just preface lm with dyn$ as shown.
> It also works with glm and other regression functions
> that are sufficiently similar to lm.
>
>> Lines1 <- "2008-06-02 09:00:00,5007.0
> + 2008-06-02 09:01:00,5010.0
> + 2008-06-02 09:02:00,5014.0
> + 2008-06-02 09:03:00,5012.5
> + 2008-06-02 09:04:00,5013.5
> + 2008-06-02 09:05:00,5009.5
> + 2008-06-02 09:06:00,5007.0
> + 2008-06-02 09:07:00,5006.5
> + 2008-06-02 09:08:00,5008.5
> + 2008-06-02 09:09:00,5004.5"
>>
>> Lines2 <- "2008-06-02 09:01:00,7115.0
> + 2008-06-02 09:03:00,7117.0
> + 2008-06-02 09:05:00,7111.0
> + 2008-06-02 09:07:00,7107.0
> + 2008-06-02 09:09:00,7102.5"
>>
>> library(xts)
>> z1 <- read.zoo(textConnection(Lines1), sep = ",", tz = "")
>> x1 <- as.xts(z1)
>> z2 <- read.zoo(textConnection(Lines2), sep = ",", tz = "")
>> x2 <- as.xts(z2)
>>
>> # convert xts to zoo
>> Z1 <- zoo(coredata(x1), time(x1))
>> Z2 <- zoo(coredata(x2), time(x2))
>> library(dyn)
>> dyn$lm(Z2 ~ Z1)
>
> Call:
> lm(formula = dyn(Z2 ~ Z1))
>
> Coefficients:
> (Intercept) ? ? ? ? ? Z1
> ?-2120.912 ? ? ? ?1.843
>
>
>
> On Mon, Jun 22, 2009 at 8:23 AM, anass<anass.mouhsine at gmail.com> wrote:
>> Hi again,
>>
>> Suppose the problem evolves a little.
>> I would like to know how to perform a regression on two xts objects with
>> different lengths.
>> Let's take the same example:
>> series1
>>
>> 2008-06-02 09:00:00 5007.0
>> 2008-06-02 09:01:00 5010.0
>> 2008-06-02 09:02:00 5014.0
>> 2008-06-02 09:03:00 5012.5
>> 2008-06-02 09:04:00 5013.5
>> 2008-06-02 09:05:00 5009.5
>> 2008-06-02 09:06:00 5007.0
>> 2008-06-02 09:07:00 5006.5
>> 2008-06-02 09:08:00 5008.5
>> 2008-06-02 09:09:00 5004.5
>>
>> Series2
>>
>> 2008-06-02 09:01:00 7115.0
>> 2008-06-02 09:03:00 7117.0
>> 2008-06-02 09:05:00 7111.0
>> 2008-06-02 09:07:00 7107.0
>> 2008-06-02 09:09:00 7102.5
>>
>>
>>>lm(Series2~Series1)
>> Error in model.frame.default(formula = Series2 ~ Series1, drop.unused.levels
>> = TRUE) :
>> ?variable lengths differ (found for 'Series1')
>>
>> While performing the regression the usual way, I've got the error described
>> before.
>>
>> Any thoughts?
>>
>> Anass
>>
>>
>>
>> On Thu, Jun 11, 2009 at 5:57 PM, spencerg <spencer.graves at prodsyse.com>wrote:
>>
>>> ? ? One could also smooth the two series first, then compare the smooths.
>>> ?This could be used to interpolate missing values to provide a larger sample
>>> size for tradition tools.
>>>
>>> ? ? The "fda" package has a number of tools for doing this kind of thing.
>>> ?Learning about this package will soon get easier with the scheduled
>>> appearance in July of Ramsay, Hooker and Graves (2009) Functional Data
>>> Analysis with R and Matlab (Springer). ?The "fda" package contains script
>>> files to reproduce all but one of the 78 figures in the book. ?The script
>>> files are available now, and can be found as follows:
>>>
>>> > system.file('scripts', package='fda')
>>> [1] "C:/Users/sgraves/R/R-2.9.0/library/fda/scripts"
>>> > dir(system.file('scripts', package='fda'))
>>> <snip> [17] "fdarm-ch01.R" ? ? ? ? ? ?"fdarm-ch02.R" ? ? ? ? ?[19]
>>> "fdarm-ch03.R" ? ? ? ? ? ?"fdarm-ch04.R" ? ? ? ? ?[21] "fdarm-ch05.R"
>>> ? ? ?"fdarm-ch06.R" ? ? ? ? ?[23] "fdarm-ch07.R" ? ? ? ? ? ?"fdarm-ch08.R"
>>> ? ? ? ?[25] "fdarm-ch09.R" ? ? ? ? ? ?"fdarm-ch10.R" ? ? ? ? ?[27]
>>> "fdarm-ch11.R" ? ? ? ? ? ?"pda.fd.test.R"
>>>
>>> ? ? Hope this helps. ? ? Spencer
>>>
>>>
>>> Anass Mouhsine wrote:
>>>
>>>> Thanks Joshua
>>>>
>>>> A
>>>>
>>>> Joshua Ulrich wrote:
>>>>
>>>>> On Thu, Jun 11, 2009 at 9:16 AM, anass<anass.mouhsine at gmail.com> wrote:
>>>>>
>>>>>
>>>>>> Thx guys,
>>>>>>
>>>>>> well the error I got is elswhere and I thought it was due to the
>>>>>> asynchrone
>>>>>> timeseries.
>>>>>>
>>>>>> let's assume that I got the intraday ratio, what I do is the following
>>>>>>
>>>>>> d<-index(to.daily(ratio))
>>>>>> for (i in 1:length(d)){
>>>>>> ?tstart<-paste(d[i], "09:00:00")
>>>>>> ?tend<-paste(d[i],"17:00:00")
>>>>>> ts_ratio<-ratio[tstart:tend]
>>>>>>
>>>>>>
>>>>>
>>>>> This line is incorrect. ?I don't believe the ":" sequence operator in
>>>>> package:base is defined for character strings (see ?":"). ?This is
>>>>> what is causing an error. ?What you probably intended is this:
>>>>> ts_ratio <- ratio[paste(tstart,tend,sep="::")]
>>>>>
>>>>>
>>>>>
>>>>>> #
>>>>>> # other operations
>>>>>> #
>>>>>> }
>>>>>>
>>>>>> and I have this error
>>>>>> Error in tstart:tend : argument NA / NaN
>>>>>>
>>>>>> So I assume if it is not due to asynchrone series, it is due to the
>>>>>> subsetting.
>>>>>> Does xts objects accept this kind of subset?
>>>>>>
>>>>>>
>>>>>> On Thu, Jun 11, 2009 at 4:05 PM, Jeff Ryan <jeff.a.ryan at gmail.com>
>>>>>> wrote:
>>>>>>
>>>>>>
>>>>>>> Anass,
>>>>>>>
>>>>>>> xts and zoo automatically align series via "merge" when performing Ops
>>>>>>> methods (+/-*...etc)
>>>>>>>
>>>>>>> This is what you want in most cases.
>>>>>>>
>>>>>>> See ?merge.xts, ?xts, ?merge.zoo and ?Ops.zoo
>>>>>>>
>>>>>>> HTH
>>>>>>> Jeff
>>>>>>>
>>>>>>> On Thu, Jun 11, 2009 at 9:00 AM, Joshua Ulrich<josh.m.ulrich at gmail.com
>>>>>>> >
>>>>>>> wrote:
>>>>>>>
>>>>>>>
>>>>>>>> Hi Anass,
>>>>>>>>
>>>>>>>> Can you provide an example of what you're trying to do and what error
>>>>>>>> you are receiving? ?I'm able to run the code below without error.
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>> require(xts)
>>>>>>>>>
>>>>>>>>> Lines <-
>>>>>>>>>
>>>>>>>>>
>>>>>>>> + "2008-06-02 09:00:00,5007.0
>>>>>>>> + 2008-06-02 09:01:00,5010.0
>>>>>>>> + 2008-06-02 09:02:00,5014.0
>>>>>>>> + 2008-06-02 09:03:00,5012.5
>>>>>>>> + 2008-06-02 09:04:00,5013.5
>>>>>>>> + 2008-06-02 09:05:00,5009.5
>>>>>>>> + 2008-06-02 09:06:00,5007.0
>>>>>>>> + 2008-06-02 09:07:00,5006.5
>>>>>>>> + 2008-06-02 09:08:00,5008.5
>>>>>>>> + 2008-06-02 09:09:00,5004.5"
>>>>>>>>
>>>>>>>>
>>>>>>>>> one <- read.zoo(textConnection(Lines),sep=',',FUN=as.POSIXct)
>>>>>>>>> one <- as.xts(one)
>>>>>>>>>
>>>>>>>>> Lines <-
>>>>>>>>>
>>>>>>>>>
>>>>>>>> + "2008-06-02 09:01:00,7115.0
>>>>>>>> + 2008-06-02 09:03:00,7117.0
>>>>>>>> + 2008-06-02 09:05:00,7111.0
>>>>>>>> + 2008-06-02 09:07:00,7107.0
>>>>>>>> + 2008-06-02 09:09:00,7102.5"
>>>>>>>>
>>>>>>>>
>>>>>>>>> two <- read.zoo(textConnection(Lines),sep=',',FUN=as.POSIXct)
>>>>>>>>> two <- as.xts(two)
>>>>>>>>>
>>>>>>>>> one/two
>>>>>>>>>
>>>>>>>>>
>>>>>>>> ? ? ? ? ? ? ? ? ? ? ? ? ?e1
>>>>>>>> 2008-06-02 09:01:00 0.7041462
>>>>>>>> 2008-06-02 09:03:00 0.7042996
>>>>>>>> 2008-06-02 09:05:00 0.7044719
>>>>>>>> 2008-06-02 09:07:00 0.7044463
>>>>>>>> 2008-06-02 09:09:00 0.7046111
>>>>>>>>
>>>>>>>>
>>>>>>>>> two/one
>>>>>>>>>
>>>>>>>>>
>>>>>>>> ? ? ? ? ? ? ? ? ? ? ? ? e1
>>>>>>>> 2008-06-02 09:01:00 1.420160
>>>>>>>> 2008-06-02 09:03:00 1.419850
>>>>>>>> 2008-06-02 09:05:00 1.419503
>>>>>>>> 2008-06-02 09:07:00 1.419555
>>>>>>>> 2008-06-02 09:09:00 1.419223
>>>>>>>> ? ? ? ?Best,
>>>>>>>> Joshua
>>>>>>>> --
>>>>>>>> http://www.fosstrading.com
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> On Thu, Jun 11, 2009 at 6:11 AM, Anass
>>>>>>>> Mouhsine<anass.mouhsine at gmail.com> wrote:
>>>>>>>>
>>>>>>>>
>>>>>>>>> Hi all,
>>>>>>>>>
>>>>>>>>> Suppose I have two xts time series with asynchroneous time index.
>>>>>>>>> I would like for example to calculate a ratio of those two series,
>>>>>>>>> but
>>>>>>>>> I
>>>>>>>>> don't know how to get an intersection of the two indices in order to
>>>>>>>>> avoid
>>>>>>>>> errors.
>>>>>>>>>
>>>>>>>>> an example of the data is the following
>>>>>>>>>
>>>>>>>>> series1
>>>>>>>>>
>>>>>>>>> 2008-06-02 09:00:00 5007.0
>>>>>>>>> 2008-06-02 09:01:00 5010.0
>>>>>>>>> 2008-06-02 09:02:00 5014.0
>>>>>>>>> 2008-06-02 09:03:00 5012.5
>>>>>>>>> 2008-06-02 09:04:00 5013.5
>>>>>>>>> 2008-06-02 09:05:00 5009.5
>>>>>>>>> 2008-06-02 09:06:00 5007.0
>>>>>>>>> 2008-06-02 09:07:00 5006.5
>>>>>>>>> 2008-06-02 09:08:00 5008.5
>>>>>>>>> 2008-06-02 09:09:00 5004.5
>>>>>>>>>
>>>>>>>>> Series2
>>>>>>>>>
>>>>>>>>> 2008-06-02 09:01:00 7115.0
>>>>>>>>> 2008-06-02 09:03:00 7117.0
>>>>>>>>> 2008-06-02 09:05:00 7111.0
>>>>>>>>> 2008-06-02 09:07:00 7107.0
>>>>>>>>> 2008-06-02 09:09:00 7102.5
>>>>>>>>>
>>>>>>>>> Any idea?
>>>>>>>>>
>>>>>>>>> Thanks in advance
>>>>>>>>>
>>>>>>>>> _______________________________________________
>>>>>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>>>>>> -- Subscriber-posting only.
>>>>>>>>> -- If you want to post, subscribe first.
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>>>>> -- Subscriber-posting only.
>>>>>>>> -- If you want to post, subscribe first.
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>> --
>>>>>>> Jeffrey Ryan
>>>>>>> jeffrey.ryan at insightalgo.com
>>>>>>>
>>>>>>> ia: insight algorithmics
>>>>>>> www.insightalgo.com
>>>>>>>
>>>>>>>
>>>>>>
>>>>>> --
>>>>>>
>>>>>> En toda ocasion, disfruta de la vida
>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>> Best,
>>>>> Joshua
>>>>> --
>>>>> http://www.fosstrading.com
>>>>>
>>>>>
>>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>>
>>>
>>
>>
>> --
>>
>> Every man has his own destiny: the only imperative is to follow it, to
>> accept it, no matter where it leads him.(H.Miller)
>>
>> ? ? ? ?[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>


From ICoe at connectcap.com  Mon Jun 22 16:49:32 2009
From: ICoe at connectcap.com (Ian Coe)
Date: Mon, 22 Jun 2009 07:49:32 -0700
Subject: [R-SIG-Finance] RBloomberg with rcom
In-Reply-To: <a7d6d2740906180701x2a3e5666t3e9920fe2074fc7@mail.gmail.com>
References: <a7d6d2740906180701x2a3e5666t3e9920fe2074fc7@mail.gmail.com>
Message-ID: <C92D6BF93B8E2A4B96E206B66040B916D6BB28@CONNCAPSBS.connectcap.local>

Hi,
  I have been keeping an eye out for the 0.2-1 release, but it appears
that r-forge has not displayed it yet.

  Has anyone else been able to install it from r-forge yet?  If so,
would you mind sending me your steps?

  When I try to install it, this is the error I get.

> install.packages("RBloomberg", repos="http://R-Forge.R-project.org")
Warning message:
In getDependencies(pkgs, dependencies, available, lib) :
  package 'RBloomberg' is not available
   

Thanks,
Ian 

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Ana Nelson
Sent: Thursday, June 18, 2009 7:01 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] RBloomberg with rcom

I now have a working RBloomberg using rcom instead of RDCOMClient. If
you
know how to compile an R package for windows, you can get the source
from
subversion now. Otherwise, hopefully tomorrow you can just install a
pre-compiled version after tonight's batch build on r-forge. I have
bumped
the version number to 0.2-1.

Again, this is not an official release of RBloomberg, it's just an
experiment to see if rcom is a viable choice.

rcom itself is GPL v2, but rcom requires that you install statconnDCOM
which
is not open source and so:

1) it can not be redistributed
2) you have to put up with a splash screen for a second or two each time
you
initiate a connection
3) you must cite the authors in any publication for which you used
statconnDCOM

On the other hand, statconnDCOM is free and looks like a very actively
used
and developed component.

If you find issues, post here, email me directly or create a ticket at
r-forge:
http://r-forge.r-project.org/tracker/?group_id=145

You can install R bundled with statconnDCOM by downloading RAndFriends:
http://rcom.univie.ac.at/

Alternatively if you install the rcom package, it comes with a function
which will download and install statconnDCOM for you. Please refer to
the
output when installing rcom.


Finally, this is probably not a fully backwards-compatible version of
RBloomberg. I have made some changes in date handling to simplify
things,
POSIX dates are now used and chron is no longer a dependency. There are
other minor changes which may affect you, but nothing drastic. Take a
look
at the examples in the runit test directory to see how things should
work.
You can also run the tests to make sure things are working as they
should.

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From nelson.ana at gmail.com  Mon Jun 22 17:05:30 2009
From: nelson.ana at gmail.com (Ana Nelson)
Date: Mon, 22 Jun 2009 16:05:30 +0100
Subject: [R-SIG-Finance] RBloomberg with rcom
In-Reply-To: <C92D6BF93B8E2A4B96E206B66040B916D6BB28@CONNCAPSBS.connectcap.local>
References: <a7d6d2740906180701x2a3e5666t3e9920fe2074fc7@mail.gmail.com>
	<C92D6BF93B8E2A4B96E206B66040B916D6BB28@CONNCAPSBS.connectcap.local>
Message-ID: <a7d6d2740906220805r377bec41pf6f3a2cd5e91acad@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090622/39a2184e/attachment.pl>

From kagba2006 at yahoo.com  Mon Jun 22 17:58:53 2009
From: kagba2006 at yahoo.com (FMH)
Date: Mon, 22 Jun 2009 08:58:53 -0700 (PDT)
Subject: [R-SIG-Finance] standard error and p-value for the estimated
	parameter in AR model
Message-ID: <662396.91672.qm@web38307.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090622/3bed1b08/attachment.pl>

From comtech.usa at gmail.com  Mon Jun 22 18:02:10 2009
From: comtech.usa at gmail.com (Michael)
Date: Mon, 22 Jun 2009 09:02:10 -0700
Subject: [R-SIG-Finance] how to read in this time series csv file with
	both dates and times?
In-Reply-To: <971536df0906210821w432f6664wf42f90feb35b3a00@mail.gmail.com>
References: <b1f16d9d0906202342y2098c619td8db4b921c9db0dc@mail.gmail.com>
	<971536df0906210821w432f6664wf42f90feb35b3a00@mail.gmail.com>
Message-ID: <b1f16d9d0906220902n3fa21913n2bc025936a4c6565@mail.gmail.com>

Thanks so much and it worked wonders.

Is it possible to select the rows based on some sort of wildcards,
such as selecting out all the rows containing "3:00:00"?

On Sun, Jun 21, 2009 at 8:21 AM, Gabor
Grothendieck<ggrothendieck at gmail.com> wrote:
> There are several things wrong here:
>
> 1. FUN= must be a function but its not in the posted code
>
> 2. as.chron() uses % codes in its format as described in
> ?strptime, not the format style for chron().
>
> See R News 4/1 for more on dates and times and also
> see the examples in ?read.zoo
>
> Here are two ways to do it. ?The first uses as.chron()
> and the second defines a custom function, toChron,
> that uses chron().
>
> library(zoo)
> library(chron)
>
> Lines <- "3/1/2009 23:00:00,123.76,123.94,123.7
> 3/2/2009 0:00:00,123.85,124.16,123.85
> 3/2/2009 1:00:00,124.11,124.15,124.06
> 3/2/2009 2:00:00,124.14,124.32,124.12
> 3/2/2009 3:00:00,124.2,124.21,124.11
> 3/2/2009 4:00:00,124.16,124.18,123.94
> 3/2/2009 5:00:00,124.01,124.2,123.97"
>
> # 1
> read.zoo(textConnection(Lines), sep = ",", FUN = as.chron,
> ? ? ? ?format = "%m/%d/%Y %H:%M:%S")
>
> # 2
> toChron <- function(x) chron(sub(" .*", "", x), sub(".* ", "", x))
> read.zoo(textConnection(Lines), sep = ",", FUN = toChron)
>
> On Sun, Jun 21, 2009 at 2:42 AM, Michael<comtech.usa at gmail.com> wrote:
>> Hi all,
>>
>> I want to read in this "csv" file,
>>
>> 3/1/2009 23:00:00,123.76,123.94,123.7
>> 3/2/2009 0:00:00,123.85,124.16,123.85
>> 3/2/2009 1:00:00,124.11,124.15,124.06
>> 3/2/2009 2:00:00,124.14,124.32,124.12
>> 3/2/2009 3:00:00,124.2,124.21,124.11
>> 3/2/2009 4:00:00,124.16,124.18,123.94
>> 3/2/2009 5:00:00,124.01,124.2,123.97
>>
>> And I got the following error message, what could be wrong?
>>
>> Thanks a lot!
>>
>>
>> z <- read.zoo("prices.csv", header = TRUE, sep = ",", FUN =
>> as.chron(format=c(dates = "m/d/y", times = "h:m:s")))
>> Error in inherits(x, "chron") : element 1 is empty;
>> ? the part of the args list of '.Internal' being evaluated was:
>> ? (x, what, which)
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>


From ggrothendieck at gmail.com  Mon Jun 22 18:11:30 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 22 Jun 2009 12:11:30 -0400
Subject: [R-SIG-Finance] how to read in this time series csv file with
	both dates and times?
In-Reply-To: <b1f16d9d0906220902n3fa21913n2bc025936a4c6565@mail.gmail.com>
References: <b1f16d9d0906202342y2098c619td8db4b921c9db0dc@mail.gmail.com> 
	<971536df0906210821w432f6664wf42f90feb35b3a00@mail.gmail.com> 
	<b1f16d9d0906220902n3fa21913n2bc025936a4c6565@mail.gmail.com>
Message-ID: <971536df0906220911s639dc15agcd3fd64010593b41@mail.gmail.com>

Here are a couple of possibilities assuming z is the result
of read.zoo:

> z[hours(time(z)) == 3,]
                       V2     V3     V4
(03/02/09 03:00:00) 124.2 124.21 124.11
> grep("03:00:00", time(z))
[1] 5
> z[grep("03:00:00", time(z)),]
                       V2     V3     V4
(03/02/09 03:00:00) 124.2 124.21 124.11


On Mon, Jun 22, 2009 at 12:02 PM, Michael<comtech.usa at gmail.com> wrote:
> Thanks so much and it worked wonders.
>
> Is it possible to select the rows based on some sort of wildcards,
> such as selecting out all the rows containing "3:00:00"?
>
> On Sun, Jun 21, 2009 at 8:21 AM, Gabor
> Grothendieck<ggrothendieck at gmail.com> wrote:
>> There are several things wrong here:
>>
>> 1. FUN= must be a function but its not in the posted code
>>
>> 2. as.chron() uses % codes in its format as described in
>> ?strptime, not the format style for chron().
>>
>> See R News 4/1 for more on dates and times and also
>> see the examples in ?read.zoo
>>
>> Here are two ways to do it. ?The first uses as.chron()
>> and the second defines a custom function, toChron,
>> that uses chron().
>>
>> library(zoo)
>> library(chron)
>>
>> Lines <- "3/1/2009 23:00:00,123.76,123.94,123.7
>> 3/2/2009 0:00:00,123.85,124.16,123.85
>> 3/2/2009 1:00:00,124.11,124.15,124.06
>> 3/2/2009 2:00:00,124.14,124.32,124.12
>> 3/2/2009 3:00:00,124.2,124.21,124.11
>> 3/2/2009 4:00:00,124.16,124.18,123.94
>> 3/2/2009 5:00:00,124.01,124.2,123.97"
>>
>> # 1
>> read.zoo(textConnection(Lines), sep = ",", FUN = as.chron,
>> ? ? ? ?format = "%m/%d/%Y %H:%M:%S")
>>
>> # 2
>> toChron <- function(x) chron(sub(" .*", "", x), sub(".* ", "", x))
>> read.zoo(textConnection(Lines), sep = ",", FUN = toChron)
>>
>> On Sun, Jun 21, 2009 at 2:42 AM, Michael<comtech.usa at gmail.com> wrote:
>>> Hi all,
>>>
>>> I want to read in this "csv" file,
>>>
>>> 3/1/2009 23:00:00,123.76,123.94,123.7
>>> 3/2/2009 0:00:00,123.85,124.16,123.85
>>> 3/2/2009 1:00:00,124.11,124.15,124.06
>>> 3/2/2009 2:00:00,124.14,124.32,124.12
>>> 3/2/2009 3:00:00,124.2,124.21,124.11
>>> 3/2/2009 4:00:00,124.16,124.18,123.94
>>> 3/2/2009 5:00:00,124.01,124.2,123.97
>>>
>>> And I got the following error message, what could be wrong?
>>>
>>> Thanks a lot!
>>>
>>>
>>> z <- read.zoo("prices.csv", header = TRUE, sep = ",", FUN =
>>> as.chron(format=c(dates = "m/d/y", times = "h:m:s")))
>>> Error in inherits(x, "chron") : element 1 is empty;
>>> ? the part of the args list of '.Internal' being evaluated was:
>>> ? (x, what, which)
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>


From matthieu.stigler at gmail.com  Mon Jun 22 21:23:59 2009
From: matthieu.stigler at gmail.com (Matthieu Stigler)
Date: Mon, 22 Jun 2009 21:23:59 +0200
Subject: [R-SIG-Finance] standard error and p-value for the estimated
 parameter in AR model
In-Reply-To: <662396.91672.qm@web38307.mail.mud.yahoo.com>
References: <662396.91672.qm@web38307.mail.mud.yahoo.com>
Message-ID: <4A3FDA4F.8090905@gmail.com>

Hi

as you can see:

methods(class="ar")

there is no summary() nor confint() function for class ar :-(

But if you check values returnd by ar:

str(ar(lh))


you see there is: asy.var.coef
so with:

sqrt(diag(ar(lh)$asy.var.coef))


You get standard errors and can compute the corresponding p-values.

Mat

FMH a ?crit :
> Dear All,
>
> I used an  AR(1) model to explain the process of the stationary residual and have used an 'ar' command in R. From the results, i tried to extract the standard error and p-value for the estimated parameter, but unfortunately, i never find any way to extract  it from the output. 
>
> What i did was, i assigned the residuals into the 'residual' object in R and used an 'ar' function as below. 
>
>   
>> residual <- residuals
>> ar(residual, aic = TRUE,  method = "mle", order.max = 1) 
>>     
>
> Could someone help me to extract the stadard error and the p-value for the estimated parameter, please?
>
> Thank you
>
> Fir
>
>
>       
> 	[[alternative HTML version deleted]]
>
>   
> ------------------------------------------------------------------------
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From markleeds at verizon.net  Mon Jun 22 21:36:24 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Mon, 22 Jun 2009 14:36:24 -0500 (CDT)
Subject: [R-SIG-Finance] standard error and p-value for the estimated
 parameter in AR model
Message-ID: <1463852721.296934.1245699384061.JavaMail.root@vms226.mailsrvcs.net>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090622/45ac4d0b/attachment.html>

From matthieu.stigler at gmail.com  Mon Jun 22 22:23:28 2009
From: matthieu.stigler at gmail.com (Matthieu Stigler)
Date: Mon, 22 Jun 2009 22:23:28 +0200
Subject: [R-SIG-Finance] standard error and p-value for the estimated
 parameter in AR model
In-Reply-To: <1463852721.296934.1245699384061.JavaMail.root@vms226.mailsrvcs.net>
References: <1463852721.296934.1245699384061.JavaMail.root@vms226.mailsrvcs.net>
Message-ID: <4A3FE840.2020509@gmail.com>

Hi

Yes dependance of regressor and errors has the effect that your 
estimator is biased. Hamilton (p 215) discusses the case of AR() with 
iid errors:

"the OLS coefficient gives a biased estimate in case of an autoregression 
and the standard t and F statistic can only be justified asymptotically. "


So as you point right out, normal distribution instead of student should 
be used for the p-values! (I'm not sure whether student distribution 
can't be used if you make the assumption that the errors are Gaussian. )

Note however that those results are derived for the OLS estimator, which 
is not the estimator by default in ar().

For small sample p-values, bootstrap methods could be used. Introductory 
discussion can be found in Maddala p 323 (available on google books, 
type: "the procedure for the generation of the bootstrap samples").

Matthieu

markleeds at verizon.net a ?crit :
> hi matthew: maybe someone can say more including yourself but one 
> doesn't have independence of error term
> and regressor in an AR so I'm not certain that the t-test in the arima 
> model is valid ?  I imagine that hamilton or
> some other book must talk about the validity of the assumptions  but I 
> don't have them in my apt at the moment.
>
>
>
> On Jun 22, 2009, *Matthieu Stigler* <matthieu.stigler at gmail.com> wrote:
>
>     Hi
>
>     as you can see:
>
>     methods(class="ar")
>
>     there is no summary() nor confint() function for class ar :-(
>
>     But if you check values returnd by ar:
>
>     str(ar(lh))
>
>
>     you see there is: asy.var.coef
>     so with:
>
>     sqrt(diag(ar(lh)$asy.var.coef))
>
>
>     You get standard errors and can compute the corresponding p-values.
>
>     Mat
>
>     FMH a ?crit :
>     > Dear All,
>     >
>     > I used an AR(1) model to explain the process of the stationary
>     residual and have used an 'ar' command in R. From the results, i
>     tried to extract the standard error and p-value for the estimated
>     parameter, but unfortunately, i never find any way to extract it
>     from the output.
>     >
>     > What i did was, i assigned the residuals into the 'residual'
>     object in R and used an 'ar' function as below.
>     >
>     >
>     >> residual <- residuals
>     >> ar(residual, aic = TRUE, method = "mle", order.max = 1)
>     >>
>     >
>     > Could someone help me to extract the stadard error and the
>     p-value for the estimated parameter, please?
>     >
>     > Thank you
>     >
>     > Fir
>     >
>     >
>     >
>     > [[alternative HTML version deleted]]
>     >
>     >
>     >
>     ------------------------------------------------------------------------
>     >
>     > _______________________________________________
>     > R-SIG-Finance at stat.math.ethz.ch
>     <mailto:R-SIG-Finance at stat.math.ethz.ch> mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>     > -- Subscriber-posting only.
>     > -- If you want to post, subscribe first.
>
>     _______________________________________________
>     R-SIG-Finance at stat.math.ethz.ch
>     <mailto:R-SIG-Finance at stat.math.ethz.ch> mailing list
>     https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>     -- Subscriber-posting only.
>     -- If you want to post, subscribe first.
>


From markleeds at verizon.net  Mon Jun 22 22:48:48 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Mon, 22 Jun 2009 15:48:48 -0500 (CDT)
Subject: [R-SIG-Finance] standard error and p-value for the estimated
 parameter in AR model
Message-ID: <132451753.301359.1245703728163.JavaMail.root@vms226.mailsrvcs.net>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090622/f7e0014d/attachment.html>

From samkemp at predictedmarkets.com  Tue Jun 23 12:36:36 2009
From: samkemp at predictedmarkets.com (Samuel Kemp)
Date: Tue, 23 Jun 2009 11:36:36 +0100
Subject: [R-SIG-Finance] CQG API
Message-ID: <010001c9f3ee$7cb67c70$76237550$@com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090623/b6b09ec6/attachment.pl>

From roboso at gmail.com  Tue Jun 23 12:51:48 2009
From: roboso at gmail.com (Roberto Osorio)
Date: Tue, 23 Jun 2009 03:51:48 -0700
Subject: [R-SIG-Finance] Asynchronous xts time series
Message-ID: <69139884-7B27-491E-9E1A-08B5E132E716@gmail.com>

Look for "asynchroneous".

When detecting a misspelling of an important keyword in the subject  
line, it is a good idea to fix the misspelling under a new subject to  
allow for proper search from the archives.

Roberto


From brian at braverock.com  Tue Jun 23 13:21:33 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 23 Jun 2009 06:21:33 -0500
Subject: [R-SIG-Finance] CQG API
In-Reply-To: <010001c9f3ee$7cb67c70$76237550$@com>
References: <010001c9f3ee$7cb67c70$76237550$@com>
Message-ID: <4A40BABD.6090207@braverock.com>

Samuel Kemp wrote:
> Has anyone used R to interface with the CQG API and can give me some
> pointers on how they did it? 
>
> It would be could to have a CQG interface that is similar to the RBloomberg
> package.
>   
Sam,

I asked this exact question a week ago, and got no answers on CQG, as a 
quick list archive search would have shown.

Now, as for pointers, the docs for the CQG COM API are all available online:

http://www.cqg.com/Products/CQG-API/Help-for-API.aspx

I believe that the model will end up being a combination of approaches 
taken by Ana Nelson and the RBloomberg team and approach taken by Jeff 
Ryan for IBrokers.  The CQG functionality is closer to IBrokers, but, 
alas, the API is a COM API, so accessing it from R will require rcom or 
RDCOMclient, as per RBloomberg.

This is pretty far down on my list of priorities right now, though it is 
definitely on the list, I just don't have time to write it now.  I'll 
help in any way that I can.  I make the assumption that you would 
contribute any effort that you put into this back to the community so 
that this question doesn't need to get asked again.

Regards,

     - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From matthieu.stigler at gmail.com  Tue Jun 23 14:14:04 2009
From: matthieu.stigler at gmail.com (Matthieu Stigler)
Date: Tue, 23 Jun 2009 14:14:04 +0200
Subject: [R-SIG-Finance] standard error and p-value for the estimated
 parameter in AR model
In-Reply-To: <803264.33792.qm@web38302.mail.mud.yahoo.com>
References: <662396.91672.qm@web38307.mail.mud.yahoo.com>
	<4A3FDA4F.8090905@gmail.com>
	<803264.33792.qm@web38302.mail.mud.yahoo.com>
Message-ID: <4A40C70C.805@gmail.com>

without code or output it is difficult to answer your question. Please 
show the specifical series where the problem occurs, show what print(a) 
and str(a) gives.


Matthieu

PS: Please send your question on the mailing list and eventually cc to 
people

FMH a ?crit :
> Hi Matthieu,
>  
> Thank you for you advice.
>  
> Actually, I have 3000 series' of the residuals and have used an AR(1) 
> model for each series. As mentioned before, i 'd like to compute the 
> p-value for the parameter in each series.
>  
> When i started using 'for' loop function, the program suddenly stopped 
> at the second series. I then used the command suggested 
> sqrt(diag(ar(residual)$asy.var.coef)) for this second series series to 
> find out the problem and the results show 'NULL ' output. To check 
> whether my coding is correct, i then apply the same code to the first  
> and third series, and there is no problem, where it successfully gives 
> me the standard error for the parameter. This indicates that there is 
> something wrong with the second series in computing the standard error 
> for the parameter. I tried to plot the second residuals series, but 
> there is no sign of problem graphically.
>  
> Maybe you or someone could give some advice on this matter?
>  
> Thank you
>  
> Fir
>
> ------------------------------------------------------------------------
> *From:* Matthieu Stigler <matthieu.stigler at gmail.com>
> *To:* r-sig-finance at stat.math.ethz.ch
> *Cc:* FMH <kagba2006 at yahoo.com>
> *Sent:* Monday, June 22, 2009 8:23:59 PM
> *Subject:* Re: [R-SIG-Finance] standard error and p-value for the 
> estimated parameter in AR model
>
> Hi
>
> as you can see:
>
> methods(class="ar")
>
> there is no summary() nor confint() function for class ar :-(
>
> But if you check values returnd by ar:
>
> str(ar(lh))
>
>
> you see there is: asy.var.coef
> so with:
>
> sqrt(diag(ar(lh)$asy.var.coef))
>
>
> You get standard errors and can compute the corresponding p-values.
>
> Mat
>
> FMH a ?crit :
> > Dear All,
> >
> > I used an  AR(1) model to explain the process of the stationary 
> residual and have used an 'ar' command in R. From the results, i tried 
> to extract the standard error and p-value for the estimated parameter, 
> but unfortunately, i never find any way to extract  it from the output.
> > What i did was, i assigned the residuals into the 'residual' object 
> in R and used an 'ar' function as below.
> > 
> >> residual <- residuals
> >> ar(residual, aic = TRUE,  method = "mle", order.max = 1)   
> >
> > Could someone help me to extract the stadard error and the p-value 
> for the estimated parameter, please?
> >
> > Thank you
> >
> > Fir
> >
> >
> >          [[alternative HTML version deleted]]
> >
> >  
> ------------------------------------------------------------------------
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch 
> <mailto:R-SIG-Finance at stat.math.ethz.ch> mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
>
>


From jewellsean at gmail.com  Tue Jun 23 21:59:39 2009
From: jewellsean at gmail.com (Sean Jewell)
Date: Tue, 23 Jun 2009 15:59:39 -0400
Subject: [R-SIG-Finance] Performance Analytics
Message-ID: <8048bdf90906231259w55520b2cud6d7c3a2296c4f4d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090623/ec624297/attachment.pl>

From ssmith88 at umd.edu  Tue Jun 23 22:29:23 2009
From: ssmith88 at umd.edu (ssmith88 at umd.edu)
Date: Tue, 23 Jun 2009 16:29:23 -0400 (EDT)
Subject: [R-SIG-Finance] Performance Analytics
In-Reply-To: <8048bdf90906231259w55520b2cud6d7c3a2296c4f4d@mail.gmail.com>
References: <8048bdf90906231259w55520b2cud6d7c3a2296c4f4d@mail.gmail.com>
Message-ID: <20090623162923.AJF43543@po7.mail.umd.edu>

Sean,

I don't know much about the performance analytics package, but here is an alternative.  Load the fPortfolio package and convert to a time series object with as.timeSeries().  Next use the cumulate() function to get the cumulated returns and the plot() function for a graph.

Scott Smith


From brian at braverock.com  Tue Jun 23 23:29:35 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 23 Jun 2009 16:29:35 -0500
Subject: [R-SIG-Finance] Performance Analytics
In-Reply-To: <8048bdf90906231259w55520b2cud6d7c3a2296c4f4d@mail.gmail.com>
References: <8048bdf90906231259w55520b2cud6d7c3a2296c4f4d@mail.gmail.com>
Message-ID: <4A41493F.6010809@braverock.com>

Sean Jewell wrote:
> Error in fromchar(x) :
>   character string is not in a standard unambiguous format
>   
PerformanceAnalytics tries to be generous in what it accepts, but it 
seems that you are using a data.frame without rownames and not (any) 
time series class (we prefer xts for most things because it is a time 
series class that is as fast as matrix even for large data sets).  As 
such, your first column looks like the character string of the 
timestamp.  If you made those the rownames instead, everything would 
likely work fine.  The other (better) option is to convert your data to 
xts (or some other time series class) before attempting your analysis.  
This is good advice in any case, as data.frame should really only be 
used for factor data.

Regards,

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From rhelpacc at gmail.com  Wed Jun 24 03:58:45 2009
From: rhelpacc at gmail.com (R_help Help)
Date: Tue, 23 Jun 2009 21:58:45 -0400
Subject: [R-SIG-Finance] Backtesting framework package
Message-ID: <ad1ead5f0906231858s1574f0d6t246a32f353126f2f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090623/c3d81311/attachment.pl>

From josh.m.ulrich at gmail.com  Wed Jun 24 04:46:25 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Tue, 23 Jun 2009 21:46:25 -0500
Subject: [R-SIG-Finance] Backtesting framework package
In-Reply-To: <ad1ead5f0906231858s1574f0d6t246a32f353126f2f@mail.gmail.com>
References: <ad1ead5f0906231858s1574f0d6t246a32f353126f2f@mail.gmail.com>
Message-ID: <8cca69990906231946i414f3671h735805e16a329a0@mail.gmail.com>

The blotter package probably provides the closest to what you're looking for.
http://r-forge.r-project.org/projects/blotter

Best,
Josh
--
http://www.fosstrading.com



On Tue, Jun 23, 2009 at 8:58 PM, R_help Help<rhelpacc at gmail.com> wrote:
> Hi,
>
> i am wondering if anyone could recommend a package that has a framework for
> backtesting? Thank you.
>
> adschai
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ksspriggs at gmail.com  Wed Jun 24 16:52:09 2009
From: ksspriggs at gmail.com (Kenneth Spriggs)
Date: Wed, 24 Jun 2009 09:52:09 -0500
Subject: [R-SIG-Finance] in xts behavior of to.minutes() and to.period()
Message-ID: <82b060880906240752t182f29f1m9004ca09c20c1ad9@mail.gmail.com>

Both VI and WMP are derived from the same xts object - they have the
same rows.  When I use to.minutes() on VI it yields 618 rows but for
WMP it yields 614 rows.
(Same thing if I use to.period() but that's not surprising.)  You can
see both the start times and end times are the same...

> nrow(VI); nrow(WMP)
[1] 401600
[1] 401600

> head(VI, 1); tail(VI, 1)
                        LastPrice
2009-06-22 21:41:54.869         0
                        LastPrice
2009-06-23 18:59:59.735     25689

> head(WMP, 1); tail(WMP, 1)
                        BidPrice
2009-06-22 21:41:54.869       NA
                        BidPrice
2009-06-23 18:59:59.735 115.4746

> VI_minute2 <- to.minutes(VI,2)
> WMP_minute2 <- to.minutes(WMP, 2)
> nrow(VI_minute2); nrow(WMP_minute2)
[1] 618
[1] 614

> VI_minute2p <- to.period(VI, period = "minutes", k=2)
> WMP_minute2p <- to.period(WMP, period = "minutes", k=2)
> nrow(VI_minute2p); nrow(WMP_minute2p)
[1] 618
[1] 614


From josh.m.ulrich at gmail.com  Wed Jun 24 17:02:12 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Wed, 24 Jun 2009 10:02:12 -0500
Subject: [R-SIG-Finance] in xts behavior of to.minutes() and to.period()
In-Reply-To: <82b060880906240752t182f29f1m9004ca09c20c1ad9@mail.gmail.com>
References: <82b060880906240752t182f29f1m9004ca09c20c1ad9@mail.gmail.com>
Message-ID: <8cca69990906240802mdeb0929qdb82274fb22387c2@mail.gmail.com>

Hi Kenneth,

to.period() removes NAs before aggregating.  So I would guess WMP has
4 rows with NA, and those same values in VI are zero.

HTH,
Josh
--
http://www.fosstrading.com



On Wed, Jun 24, 2009 at 9:52 AM, Kenneth Spriggs<ksspriggs at gmail.com> wrote:
> Both VI and WMP are derived from the same xts object - they have the
> same rows. ?When I use to.minutes() on VI it yields 618 rows but for
> WMP it yields 614 rows.
> (Same thing if I use to.period() but that's not surprising.) ?You can
> see both the start times and end times are the same...
>
>> nrow(VI); nrow(WMP)
> [1] 401600
> [1] 401600
>
>> head(VI, 1); tail(VI, 1)
> ? ? ? ? ? ? ? ? ? ? ? ?LastPrice
> 2009-06-22 21:41:54.869 ? ? ? ? 0
> ? ? ? ? ? ? ? ? ? ? ? ?LastPrice
> 2009-06-23 18:59:59.735 ? ? 25689
>
>> head(WMP, 1); tail(WMP, 1)
> ? ? ? ? ? ? ? ? ? ? ? ?BidPrice
> 2009-06-22 21:41:54.869 ? ? ? NA
> ? ? ? ? ? ? ? ? ? ? ? ?BidPrice
> 2009-06-23 18:59:59.735 115.4746
>
>> VI_minute2 <- to.minutes(VI,2)
>> WMP_minute2 <- to.minutes(WMP, 2)
>> nrow(VI_minute2); nrow(WMP_minute2)
> [1] 618
> [1] 614
>
>> VI_minute2p <- to.period(VI, period = "minutes", k=2)
>> WMP_minute2p <- to.period(WMP, period = "minutes", k=2)
>> nrow(VI_minute2p); nrow(WMP_minute2p)
> [1] 618
> [1] 614
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jeff.a.ryan at gmail.com  Wed Jun 24 17:02:49 2009
From: jeff.a.ryan at gmail.com (J Ryan)
Date: Wed, 24 Jun 2009 10:02:49 -0500
Subject: [R-SIG-Finance] in xts behavior of to.minutes() and to.period()
In-Reply-To: <82b060880906240752t182f29f1m9004ca09c20c1ad9@mail.gmail.com>
References: <82b060880906240752t182f29f1m9004ca09c20c1ad9@mail.gmail.com>
Message-ID: <86F0B5DB-CA76-43E8-8FBF-51AD8330C48F@gmail.com>

Hi Ken,

Can you check if the index is identical?

identical(index(VI),index(WMP))

As the process is deterministic, something beside the data must be  
different.


Jeff



Jeffrey A. Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com

On Jun 24, 2009, at 9:52 AM, Kenneth Spriggs <ksspriggs at gmail.com>  
wrote:

> Both VI and WMP are derived from the same xts object - they have the
> same rows.  When I use to.minutes() on VI it yields 618 rows but for
> WMP it yields 614 rows.
> (Same thing if I use to.period() but that's not surprising.)  You can
> see both the start times and end times are the same...
>
>> nrow(VI); nrow(WMP)
> [1] 401600
> [1] 401600
>
>> head(VI, 1); tail(VI, 1)
>                        LastPrice
> 2009-06-22 21:41:54.869         0
>                        LastPrice
> 2009-06-23 18:59:59.735     25689
>
>> head(WMP, 1); tail(WMP, 1)
>                        BidPrice
> 2009-06-22 21:41:54.869       NA
>                        BidPrice
> 2009-06-23 18:59:59.735 115.4746
>
>> VI_minute2 <- to.minutes(VI,2)
>> WMP_minute2 <- to.minutes(WMP, 2)
>> nrow(VI_minute2); nrow(WMP_minute2)
> [1] 618
> [1] 614
>
>> VI_minute2p <- to.period(VI, period = "minutes", k=2)
>> WMP_minute2p <- to.period(WMP, period = "minutes", k=2)
>> nrow(VI_minute2p); nrow(WMP_minute2p)
> [1] 618
> [1] 614
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From ksspriggs at gmail.com  Wed Jun 24 17:22:47 2009
From: ksspriggs at gmail.com (Kenneth Spriggs)
Date: Wed, 24 Jun 2009 10:22:47 -0500
Subject: [R-SIG-Finance] in xts behavior of to.minutes() and to.period()
In-Reply-To: <8cca69990906240802mdeb0929qdb82274fb22387c2@mail.gmail.com>
References: <82b060880906240752t182f29f1m9004ca09c20c1ad9@mail.gmail.com>
	<8cca69990906240802mdeb0929qdb82274fb22387c2@mail.gmail.com>
Message-ID: <82b060880906240822s5a9429ccx8aaed5de460e5ca9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090624/5cdafa1b/attachment.pl>

From josh.m.ulrich at gmail.com  Wed Jun 24 17:27:26 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Wed, 24 Jun 2009 10:27:26 -0500
Subject: [R-SIG-Finance] in xts behavior of to.minutes() and to.period()
In-Reply-To: <82b060880906240822s5a9429ccx8aaed5de460e5ca9@mail.gmail.com>
References: <82b060880906240752t182f29f1m9004ca09c20c1ad9@mail.gmail.com>
	<8cca69990906240802mdeb0929qdb82274fb22387c2@mail.gmail.com>
	<82b060880906240822s5a9429ccx8aaed5de460e5ca9@mail.gmail.com>
Message-ID: <8cca69990906240827m5691c7eaj93f5037f957b4253@mail.gmail.com>

That won't provide the result you're looking for.  It is testing if
the result of is.na() is a character string, so it will always be
FALSE because is.na() returns logical values (either TRUE or FALSE, no
quotes).  Try this instead:

nrow(is.na(VI) == FALSE)
or
sum(is.na(VI))

The results of head()/tail() in your previous email show that WMP has
at least one NA value.

Best,
Josh
--
http://www.fosstrading.com



On Wed, Jun 24, 2009 at 10:22 AM, Kenneth Spriggs<ksspriggs at gmail.com> wrote:
> Hi Josh,
>
>> nrow(is.na(VI) == 'FALSE')
> [1] 401600
>
>> nrow(is.na(WMP) == 'FALSE')
> [1] 401600
>
>>
>
>
> On Wed, Jun 24, 2009 at 10:02 AM, Joshua Ulrich <josh.m.ulrich at gmail.com>
> wrote:
>>
>> Hi Kenneth,
>>
>> to.period() removes NAs before aggregating. ?So I would guess WMP has
>> 4 rows with NA, and those same values in VI are zero.
>>
>> HTH,
>> Josh
>> --
>> http://www.fosstrading.com
>>
>>
>>
>> On Wed, Jun 24, 2009 at 9:52 AM, Kenneth Spriggs<ksspriggs at gmail.com>
>> wrote:
>> > Both VI and WMP are derived from the same xts object - they have the
>> > same rows. ?When I use to.minutes() on VI it yields 618 rows but for
>> > WMP it yields 614 rows.
>> > (Same thing if I use to.period() but that's not surprising.) ?You can
>> > see both the start times and end times are the same...
>> >
>> >> nrow(VI); nrow(WMP)
>> > [1] 401600
>> > [1] 401600
>> >
>> >> head(VI, 1); tail(VI, 1)
>> > ? ? ? ? ? ? ? ? ? ? ? ?LastPrice
>> > 2009-06-22 21:41:54.869 ? ? ? ? 0
>> > ? ? ? ? ? ? ? ? ? ? ? ?LastPrice
>> > 2009-06-23 18:59:59.735 ? ? 25689
>> >
>> >> head(WMP, 1); tail(WMP, 1)
>> > ? ? ? ? ? ? ? ? ? ? ? ?BidPrice
>> > 2009-06-22 21:41:54.869 ? ? ? NA
>> > ? ? ? ? ? ? ? ? ? ? ? ?BidPrice
>> > 2009-06-23 18:59:59.735 115.4746
>> >
>> >> VI_minute2 <- to.minutes(VI,2)
>> >> WMP_minute2 <- to.minutes(WMP, 2)
>> >> nrow(VI_minute2); nrow(WMP_minute2)
>> > [1] 618
>> > [1] 614
>> >
>> >> VI_minute2p <- to.period(VI, period = "minutes", k=2)
>> >> WMP_minute2p <- to.period(WMP, period = "minutes", k=2)
>> >> nrow(VI_minute2p); nrow(WMP_minute2p)
>> > [1] 618
>> > [1] 614
>> >
>> > _______________________________________________
>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> > -- Subscriber-posting only.
>> > -- If you want to post, subscribe first.
>> >
>
>


From ksspriggs at gmail.com  Wed Jun 24 17:30:34 2009
From: ksspriggs at gmail.com (Kenneth Spriggs)
Date: Wed, 24 Jun 2009 10:30:34 -0500
Subject: [R-SIG-Finance] in xts behavior of to.minutes() and to.period()
In-Reply-To: <8cca69990906240827m5691c7eaj93f5037f957b4253@mail.gmail.com>
References: <82b060880906240752t182f29f1m9004ca09c20c1ad9@mail.gmail.com>
	<8cca69990906240802mdeb0929qdb82274fb22387c2@mail.gmail.com>
	<82b060880906240822s5a9429ccx8aaed5de460e5ca9@mail.gmail.com>
	<8cca69990906240827m5691c7eaj93f5037f957b4253@mail.gmail.com>
Message-ID: <82b060880906240830iaf07701l30cf93a6b1f43843@mail.gmail.com>

Oh yeah, oops but it still isn't adding up...

> sum(is.na(VI))
[1] 0

> sum(is.na(WMP))
[1] 1973



On Wed, Jun 24, 2009 at 10:27 AM, Joshua Ulrich<josh.m.ulrich at gmail.com> wrote:
> That won't provide the result you're looking for. ?It is testing if
> the result of is.na() is a character string, so it will always be
> FALSE because is.na() returns logical values (either TRUE or FALSE, no
> quotes). ?Try this instead:
>
> nrow(is.na(VI) == FALSE)
> or
> sum(is.na(VI))
>
> The results of head()/tail() in your previous email show that WMP has
> at least one NA value.
>
> Best,
> Josh
> --
> http://www.fosstrading.com
>
>
>
> On Wed, Jun 24, 2009 at 10:22 AM, Kenneth Spriggs<ksspriggs at gmail.com> wrote:
>> Hi Josh,
>>
>>> nrow(is.na(VI) == 'FALSE')
>> [1] 401600
>>
>>> nrow(is.na(WMP) == 'FALSE')
>> [1] 401600
>>
>>>
>>
>>
>> On Wed, Jun 24, 2009 at 10:02 AM, Joshua Ulrich <josh.m.ulrich at gmail.com>
>> wrote:
>>>
>>> Hi Kenneth,
>>>
>>> to.period() removes NAs before aggregating. ?So I would guess WMP has
>>> 4 rows with NA, and those same values in VI are zero.
>>>
>>> HTH,
>>> Josh
>>> --
>>> http://www.fosstrading.com
>>>
>>>
>>>
>>> On Wed, Jun 24, 2009 at 9:52 AM, Kenneth Spriggs<ksspriggs at gmail.com>
>>> wrote:
>>> > Both VI and WMP are derived from the same xts object - they have the
>>> > same rows. ?When I use to.minutes() on VI it yields 618 rows but for
>>> > WMP it yields 614 rows.
>>> > (Same thing if I use to.period() but that's not surprising.) ?You can
>>> > see both the start times and end times are the same...
>>> >
>>> >> nrow(VI); nrow(WMP)
>>> > [1] 401600
>>> > [1] 401600
>>> >
>>> >> head(VI, 1); tail(VI, 1)
>>> > ? ? ? ? ? ? ? ? ? ? ? ?LastPrice
>>> > 2009-06-22 21:41:54.869 ? ? ? ? 0
>>> > ? ? ? ? ? ? ? ? ? ? ? ?LastPrice
>>> > 2009-06-23 18:59:59.735 ? ? 25689
>>> >
>>> >> head(WMP, 1); tail(WMP, 1)
>>> > ? ? ? ? ? ? ? ? ? ? ? ?BidPrice
>>> > 2009-06-22 21:41:54.869 ? ? ? NA
>>> > ? ? ? ? ? ? ? ? ? ? ? ?BidPrice
>>> > 2009-06-23 18:59:59.735 115.4746
>>> >
>>> >> VI_minute2 <- to.minutes(VI,2)
>>> >> WMP_minute2 <- to.minutes(WMP, 2)
>>> >> nrow(VI_minute2); nrow(WMP_minute2)
>>> > [1] 618
>>> > [1] 614
>>> >
>>> >> VI_minute2p <- to.period(VI, period = "minutes", k=2)
>>> >> WMP_minute2p <- to.period(WMP, period = "minutes", k=2)
>>> >> nrow(VI_minute2p); nrow(WMP_minute2p)
>>> > [1] 618
>>> > [1] 614
>>> >
>>> > _______________________________________________
>>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> > -- Subscriber-posting only.
>>> > -- If you want to post, subscribe first.
>>> >
>>
>>
>


From jeff.a.ryan at gmail.com  Wed Jun 24 17:35:25 2009
From: jeff.a.ryan at gmail.com (J Ryan)
Date: Wed, 24 Jun 2009 10:35:25 -0500
Subject: [R-SIG-Finance] in xts behavior of to.minutes() and to.period()
In-Reply-To: <82b060880906240830iaf07701l30cf93a6b1f43843@mail.gmail.com>
References: <82b060880906240752t182f29f1m9004ca09c20c1ad9@mail.gmail.com>
	<8cca69990906240802mdeb0929qdb82274fb22387c2@mail.gmail.com>
	<82b060880906240822s5a9429ccx8aaed5de460e5ca9@mail.gmail.com>
	<8cca69990906240827m5691c7eaj93f5037f957b4253@mail.gmail.com>
	<82b060880906240830iaf07701l30cf93a6b1f43843@mail.gmail.com>
Message-ID: <1DF08EC1-B395-4063-AA18-D85FEF109539@gmail.com>

That is reasonable.

You are missing observations *within* the 2 minutes. Somewhere entire  
minute blocks are missing in the raw data, and there is no bar to  
calculate

You'll have to merge again the results to make the index consistent.

HTH
Jeff


Jeffrey A. Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com

On Jun 24, 2009, at 10:30 AM, Kenneth Spriggs <ksspriggs at gmail.com>  
wrote:

> Oh yeah, oops but it still isn't adding up...
>
>> sum(is.na(VI))
> [1] 0
>
>> sum(is.na(WMP))
> [1] 1973
>
>
>
> On Wed, Jun 24, 2009 at 10:27 AM, Joshua Ulrich<josh.m.ulrich at gmail.com 
> > wrote:
>> That won't provide the result you're looking for.  It is testing if
>> the result of is.na() is a character string, so it will always be
>> FALSE because is.na() returns logical values (either TRUE or FALSE,  
>> no
>> quotes).  Try this instead:
>>
>> nrow(is.na(VI) == FALSE)
>> or
>> sum(is.na(VI))
>>
>> The results of head()/tail() in your previous email show that WMP has
>> at least one NA value.
>>
>> Best,
>> Josh
>> --
>> http://www.fosstrading.com
>>
>>
>>
>> On Wed, Jun 24, 2009 at 10:22 AM, Kenneth  
>> Spriggs<ksspriggs at gmail.com> wrote:
>>> Hi Josh,
>>>
>>>> nrow(is.na(VI) == 'FALSE')
>>> [1] 401600
>>>
>>>> nrow(is.na(WMP) == 'FALSE')
>>> [1] 401600
>>>
>>>>
>>>
>>>
>>> On Wed, Jun 24, 2009 at 10:02 AM, Joshua Ulrich <josh.m.ulrich at gmail.com 
>>> >
>>> wrote:
>>>>
>>>> Hi Kenneth,
>>>>
>>>> to.period() removes NAs before aggregating.  So I would guess WMP  
>>>> has
>>>> 4 rows with NA, and those same values in VI are zero.
>>>>
>>>> HTH,
>>>> Josh
>>>> --
>>>> http://www.fosstrading.com
>>>>
>>>>
>>>>
>>>> On Wed, Jun 24, 2009 at 9:52 AM, Kenneth Spriggs<ksspriggs at gmail.com 
>>>> >
>>>> wrote:
>>>>> Both VI and WMP are derived from the same xts object - they have  
>>>>> the
>>>>> same rows.  When I use to.minutes() on VI it yields 618 rows but  
>>>>> for
>>>>> WMP it yields 614 rows.
>>>>> (Same thing if I use to.period() but that's not surprising.)   
>>>>> You can
>>>>> see both the start times and end times are the same...
>>>>>
>>>>>> nrow(VI); nrow(WMP)
>>>>> [1] 401600
>>>>> [1] 401600
>>>>>
>>>>>> head(VI, 1); tail(VI, 1)
>>>>>                        LastPrice
>>>>> 2009-06-22 21:41:54.869         0
>>>>>                        LastPrice
>>>>> 2009-06-23 18:59:59.735     25689
>>>>>
>>>>>> head(WMP, 1); tail(WMP, 1)
>>>>>                        BidPrice
>>>>> 2009-06-22 21:41:54.869       NA
>>>>>                        BidPrice
>>>>> 2009-06-23 18:59:59.735 115.4746
>>>>>
>>>>>> VI_minute2 <- to.minutes(VI,2)
>>>>>> WMP_minute2 <- to.minutes(WMP, 2)
>>>>>> nrow(VI_minute2); nrow(WMP_minute2)
>>>>> [1] 618
>>>>> [1] 614
>>>>>
>>>>>> VI_minute2p <- to.period(VI, period = "minutes", k=2)
>>>>>> WMP_minute2p <- to.period(WMP, period = "minutes", k=2)
>>>>>> nrow(VI_minute2p); nrow(WMP_minute2p)
>>>>> [1] 618
>>>>> [1] 614
>>>>>
>>>>> _______________________________________________
>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>> -- Subscriber-posting only.
>>>>> -- If you want to post, subscribe first.
>>>>>
>>>
>>>
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From ksspriggs at gmail.com  Wed Jun 24 17:47:32 2009
From: ksspriggs at gmail.com (Kenneth Spriggs)
Date: Wed, 24 Jun 2009 10:47:32 -0500
Subject: [R-SIG-Finance] in xts behavior of to.minutes() and to.period()
In-Reply-To: <1DF08EC1-B395-4063-AA18-D85FEF109539@gmail.com>
References: <82b060880906240752t182f29f1m9004ca09c20c1ad9@mail.gmail.com>
	<8cca69990906240802mdeb0929qdb82274fb22387c2@mail.gmail.com>
	<82b060880906240822s5a9429ccx8aaed5de460e5ca9@mail.gmail.com>
	<8cca69990906240827m5691c7eaj93f5037f957b4253@mail.gmail.com>
	<82b060880906240830iaf07701l30cf93a6b1f43843@mail.gmail.com>
	<1DF08EC1-B395-4063-AA18-D85FEF109539@gmail.com>
Message-ID: <82b060880906240847g10406f7cr4ae997716e03108f@mail.gmail.com>

Thanks.  That makes sense.  I guess I should've taken the Warning more
seriously:

Warning messages:
1: In to.period(x, "minutes", k = k, name = name, ...) :
 missing values removed from data
: In to.period(WMP, period = "minutes", k = 2) :
 missing values removed from data




On Wed, Jun 24, 2009 at 10:35 AM, J Ryan<jeff.a.ryan at gmail.com> wrote:
> That is reasonable.
>
> You are missing observations *within* the 2 minutes. Somewhere entire minute
> blocks are missing in the raw data, and there is no bar to calculate
>
> You'll have to merge again the results to make the index consistent.
>
> HTH
> Jeff
>
>
> Jeffrey A. Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com
>
> On Jun 24, 2009, at 10:30 AM, Kenneth Spriggs <ksspriggs at gmail.com> wrote:
>
>> Oh yeah, oops but it still isn't adding up...
>>
>>> sum(is.na(VI))
>>
>> [1] 0
>>
>>> sum(is.na(WMP))
>>
>> [1] 1973
>>
>>
>>
>> On Wed, Jun 24, 2009 at 10:27 AM, Joshua Ulrich<josh.m.ulrich at gmail.com>
>> wrote:
>>>
>>> That won't provide the result you're looking for. ?It is testing if
>>> the result of is.na() is a character string, so it will always be
>>> FALSE because is.na() returns logical values (either TRUE or FALSE, no
>>> quotes). ?Try this instead:
>>>
>>> nrow(is.na(VI) == FALSE)
>>> or
>>> sum(is.na(VI))
>>>
>>> The results of head()/tail() in your previous email show that WMP has
>>> at least one NA value.
>>>
>>> Best,
>>> Josh
>>> --
>>> http://www.fosstrading.com
>>>
>>>
>>>
>>> On Wed, Jun 24, 2009 at 10:22 AM, Kenneth Spriggs<ksspriggs at gmail.com>
>>> wrote:
>>>>
>>>> Hi Josh,
>>>>
>>>>> nrow(is.na(VI) == 'FALSE')
>>>>
>>>> [1] 401600
>>>>
>>>>> nrow(is.na(WMP) == 'FALSE')
>>>>
>>>> [1] 401600
>>>>
>>>>>
>>>>
>>>>
>>>> On Wed, Jun 24, 2009 at 10:02 AM, Joshua Ulrich
>>>> <josh.m.ulrich at gmail.com>
>>>> wrote:
>>>>>
>>>>> Hi Kenneth,
>>>>>
>>>>> to.period() removes NAs before aggregating. ?So I would guess WMP has
>>>>> 4 rows with NA, and those same values in VI are zero.
>>>>>
>>>>> HTH,
>>>>> Josh
>>>>> --
>>>>> http://www.fosstrading.com
>>>>>
>>>>>
>>>>>
>>>>> On Wed, Jun 24, 2009 at 9:52 AM, Kenneth Spriggs<ksspriggs at gmail.com>
>>>>> wrote:
>>>>>>
>>>>>> Both VI and WMP are derived from the same xts object - they have the
>>>>>> same rows. ?When I use to.minutes() on VI it yields 618 rows but for
>>>>>> WMP it yields 614 rows.
>>>>>> (Same thing if I use to.period() but that's not surprising.) ?You can
>>>>>> see both the start times and end times are the same...
>>>>>>
>>>>>>> nrow(VI); nrow(WMP)
>>>>>>
>>>>>> [1] 401600
>>>>>> [1] 401600
>>>>>>
>>>>>>> head(VI, 1); tail(VI, 1)
>>>>>>
>>>>>> ? ? ? ? ? ? ? ? ? ? ? LastPrice
>>>>>> 2009-06-22 21:41:54.869 ? ? ? ? 0
>>>>>> ? ? ? ? ? ? ? ? ? ? ? LastPrice
>>>>>> 2009-06-23 18:59:59.735 ? ? 25689
>>>>>>
>>>>>>> head(WMP, 1); tail(WMP, 1)
>>>>>>
>>>>>> ? ? ? ? ? ? ? ? ? ? ? BidPrice
>>>>>> 2009-06-22 21:41:54.869 ? ? ? NA
>>>>>> ? ? ? ? ? ? ? ? ? ? ? BidPrice
>>>>>> 2009-06-23 18:59:59.735 115.4746
>>>>>>
>>>>>>> VI_minute2 <- to.minutes(VI,2)
>>>>>>> WMP_minute2 <- to.minutes(WMP, 2)
>>>>>>> nrow(VI_minute2); nrow(WMP_minute2)
>>>>>>
>>>>>> [1] 618
>>>>>> [1] 614
>>>>>>
>>>>>>> VI_minute2p <- to.period(VI, period = "minutes", k=2)
>>>>>>> WMP_minute2p <- to.period(WMP, period = "minutes", k=2)
>>>>>>> nrow(VI_minute2p); nrow(WMP_minute2p)
>>>>>>
>>>>>> [1] 618
>>>>>> [1] 614
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>>> -- Subscriber-posting only.
>>>>>> -- If you want to post, subscribe first.
>>>>>>
>>>>
>>>>
>>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>


From ksspriggs at gmail.com  Wed Jun 24 17:07:20 2009
From: ksspriggs at gmail.com (Kenneth Spriggs)
Date: Wed, 24 Jun 2009 10:07:20 -0500
Subject: [R-SIG-Finance] in xts behavior of to.minutes() and to.period()
In-Reply-To: <86F0B5DB-CA76-43E8-8FBF-51AD8330C48F@gmail.com>
References: <82b060880906240752t182f29f1m9004ca09c20c1ad9@mail.gmail.com>
	<86F0B5DB-CA76-43E8-8FBF-51AD8330C48F@gmail.com>
Message-ID: <82b060880906240807o37797cfar5af5fea46f1119cf@mail.gmail.com>

Hi Jeff -

> identical(index(VI),index(WMP))
[1] TRUE

>


On Wed, Jun 24, 2009 at 10:02 AM, J Ryan<jeff.a.ryan at gmail.com> wrote:
> Hi Ken,
>
> Can you check if the index is identical?
>
> identical(index(VI),index(WMP))
>
> As the process is deterministic, something beside the data must be
> different.
>
>
> Jeff
>
>
>
> Jeffrey A. Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com
>
> On Jun 24, 2009, at 9:52 AM, Kenneth Spriggs <ksspriggs at gmail.com> wrote:
>
>> Both VI and WMP are derived from the same xts object - they have the
>> same rows. ?When I use to.minutes() on VI it yields 618 rows but for
>> WMP it yields 614 rows.
>> (Same thing if I use to.period() but that's not surprising.) ?You can
>> see both the start times and end times are the same...
>>
>>> nrow(VI); nrow(WMP)
>>
>> [1] 401600
>> [1] 401600
>>
>>> head(VI, 1); tail(VI, 1)
>>
>> ? ? ? ? ? ? ? ? ? ? ? LastPrice
>> 2009-06-22 21:41:54.869 ? ? ? ? 0
>> ? ? ? ? ? ? ? ? ? ? ? LastPrice
>> 2009-06-23 18:59:59.735 ? ? 25689
>>
>>> head(WMP, 1); tail(WMP, 1)
>>
>> ? ? ? ? ? ? ? ? ? ? ? BidPrice
>> 2009-06-22 21:41:54.869 ? ? ? NA
>> ? ? ? ? ? ? ? ? ? ? ? BidPrice
>> 2009-06-23 18:59:59.735 115.4746
>>
>>> VI_minute2 <- to.minutes(VI,2)
>>> WMP_minute2 <- to.minutes(WMP, 2)
>>> nrow(VI_minute2); nrow(WMP_minute2)
>>
>> [1] 618
>> [1] 614
>>
>>> VI_minute2p <- to.period(VI, period = "minutes", k=2)
>>> WMP_minute2p <- to.period(WMP, period = "minutes", k=2)
>>> nrow(VI_minute2p); nrow(WMP_minute2p)
>>
>> [1] 618
>> [1] 614
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>


From icos.atropa at gmail.com  Thu Jun 25 12:37:53 2009
From: icos.atropa at gmail.com (Christian Gunning)
Date: Thu, 25 Jun 2009 03:37:53 -0700
Subject: [R-SIG-Finance] efficient extraction of local extrema and
	zero-crossings in large multivariate zoo?
Message-ID: <681d07c20906250337g31d04d97p1bb902f5ea3970c@mail.gmail.com>

I have an multi-year hour-sampled multivariate zoo from which i'm
trying to extract index() and coredata() of daily max and
zero-crossing for each variable and use na.approx() to fill in
intervening values, so that the resulting zoo has the same dimensions
of the input zoo. Currently i'm looping over each column and day,
extracting a day's worth of data with window(), and using which.max()
to extract the "record".  For short zoos it works fine.  It doesn't
scale well, though - for example, 2e4 points takes ~4-6 times longer
than 1e4 points with 2 variables. Am i missing major bottlenecks or
vectorization potentials here?

find.extrema = function(myzoo) {
  days = as.POSIXct(levels(as.factor(as.Date(index(myzoo)))))
  ret = myzoo
  ret[] <- NA
  for (tcol in 1:dim(myzoo)[2]) {
    for (day in days) {
      this <- window(myzoo[,tcol], start=day, end=day+24*60*60-1)
      thismax <- this[which.max(this)]
      this[this<0] <- 0 ## remove negative values first
      thiszero <- this[which.min(this)]
      ret[index(ret) == index(thismax), tcol] <- coredata(thismax)
      ret[index(ret) == index(thiszero), tcol] <- coredata(thiszero)
      ### ret[index(thismax), tcol] <- coredata(thismax)  ### gives an error
    } # end days
  } # end tcol
  ret = na.approx(ret)
  return(ret)
}

hours=1e4 # about 2 year's worth of hours
tmp=(as.POSIXct('2001-01-01')+1:hours*60*60)
tmpz=zoo(cbind(a=sin(as.integer(tmp)/1e4), b=sin(as.integer(tmp)/1.1e4)), tmp)
system.time(tmpextrema <- find.extrema(tmpz))

thanks,
christian gunning
university of new mexico


From josh.m.ulrich at gmail.com  Thu Jun 25 22:33:59 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Thu, 25 Jun 2009 15:33:59 -0500
Subject: [R-SIG-Finance] efficient extraction of local extrema and
	zero-crossings in large multivariate zoo?
In-Reply-To: <681d07c20906250337g31d04d97p1bb902f5ea3970c@mail.gmail.com>
References: <681d07c20906250337g31d04d97p1bb902f5ea3970c@mail.gmail.com>
Message-ID: <8cca69990906251333y680318c2hc71b60f01b6d63ad@mail.gmail.com>

Christian,

I hope this is one step closer to what you need.  I put it together
pretty quickly, so there may be (quite a few?) bugs.  Proceed with
caution.  apply.daily() is in the xts package.

find.extrema2 = function(myzoo) {
 days = as.POSIXct(levels(as.factor(as.Date(index(myzoo)))))
 ret = myzoo
 ret[] <- NA
 for (tcol in 1:dim(myzoo)[2]) {
   max.days <- apply.daily(myzoo[,tcol],function(x) { index(x)[which.max(x)] } )
   max.days <- myzoo[as.POSIXct(coredata(max.days),origin='1970-01-01'),tcol]
   min.days <- apply.daily(myzoo[,tcol],function(x) { x[x<0] <- 0;
index(x)[which.min(x)] } )
   min.days <- myzoo[as.POSIXct(coredata(min.days),origin='1970-01-01'),tcol]

   ret[,tcol] <- cbind(ret[,tcol],rbind(min.days,max.days))[,2]
 } # end tcol
 ret = na.approx(ret, na.rm=FALSE)
 return(ret)
}

Best,
Josh
--
http://www.fosstrading.com



On Thu, Jun 25, 2009 at 5:37 AM, Christian Gunning<icos.atropa at gmail.com> wrote:
> I have an multi-year hour-sampled multivariate zoo from which i'm
> trying to extract index() and coredata() of daily max and
> zero-crossing for each variable and use na.approx() to fill in
> intervening values, so that the resulting zoo has the same dimensions
> of the input zoo. Currently i'm looping over each column and day,
> extracting a day's worth of data with window(), and using which.max()
> to extract the "record". ?For short zoos it works fine. ?It doesn't
> scale well, though - for example, 2e4 points takes ~4-6 times longer
> than 1e4 points with 2 variables. Am i missing major bottlenecks or
> vectorization potentials here?
>
> find.extrema = function(myzoo) {
> ?days = as.POSIXct(levels(as.factor(as.Date(index(myzoo)))))
> ?ret = myzoo
> ?ret[] <- NA
> ?for (tcol in 1:dim(myzoo)[2]) {
> ? ?for (day in days) {
> ? ? ?this <- window(myzoo[,tcol], start=day, end=day+24*60*60-1)
> ? ? ?thismax <- this[which.max(this)]
> ? ? ?this[this<0] <- 0 ## remove negative values first
> ? ? ?thiszero <- this[which.min(this)]
> ? ? ?ret[index(ret) == index(thismax), tcol] <- coredata(thismax)
> ? ? ?ret[index(ret) == index(thiszero), tcol] <- coredata(thiszero)
> ? ? ?### ret[index(thismax), tcol] <- coredata(thismax) ?### gives an error
> ? ?} # end days
> ?} # end tcol
> ?ret = na.approx(ret)
> ?return(ret)
> }
>
> hours=1e4 # about 2 year's worth of hours
> tmp=(as.POSIXct('2001-01-01')+1:hours*60*60)
> tmpz=zoo(cbind(a=sin(as.integer(tmp)/1e4), b=sin(as.integer(tmp)/1.1e4)), tmp)
> system.time(tmpextrema <- find.extrema(tmpz))
>
> thanks,
> christian gunning
> university of new mexico
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ggrothendieck at gmail.com  Fri Jun 26 14:46:32 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 26 Jun 2009 08:46:32 -0400
Subject: [R-SIG-Finance] efficient extraction of local extrema and
	zero-crossings in large multivariate zoo?
In-Reply-To: <681d07c20906250337g31d04d97p1bb902f5ea3970c@mail.gmail.com>
References: <681d07c20906250337g31d04d97p1bb902f5ea3970c@mail.gmail.com>
Message-ID: <971536df0906260546r72e88250w53d46c535aa79ab3@mail.gmail.com>

Try this. It runs about 10x faster.  (Note that the na.rm = FALSE
argument has been added to the na.approx line in find.extrema
to correspond to your description.)


find.extrema = function(myzoo) {
 days = as.POSIXct(levels(as.factor(as.Date(index(myzoo)))))
 ret = myzoo
 ret[] <- NA
 for (tcol in 1:dim(myzoo)[2]) {
   for (day in days) {
     this <- window(myzoo[,tcol], start=day, end=day+24*60*60-1)
     thismax <- this[which.max(this)]
     this[this<0] <- 0 ## remove negative values first
     thiszero <- this[which.min(this)]
     ret[index(ret) == index(thismax), tcol] <- coredata(thismax)
     ret[index(ret) == index(thiszero), tcol] <- coredata(thiszero)
     ### ret[index(thismax), tcol] <- coredata(thismax)  ### gives an error
   } # end days
 } # end tcol
 ret = na.approx(ret, na.rm = FALSE) #### added na.rm=
 return(ret)
}

hours=1e4 # about 2 year's worth of hours
tmp=(as.POSIXct('2001-01-01')+1:hours*60*60)
tmpz=zoo(cbind(a=sin(as.integer(tmp)/1e4), b=sin(as.integer(tmp)/1.1e4)), tmp)
system.time(tmpextrema <- find.extrema(tmpz))
#   25.06    0.05   25.37

f <- function(x) {
	ret <- NA * x
	ix <- ave(x, as.Date(time(tmpz)), FUN = function(x) {
		logic <- rep(FALSE, length(x))
		logic[which.max(x)] <- TRUE
		logic
	})
	ix <- as.logical(ix)
	ret[ix] <- x[ix]
	a <- x
	a[a < 0] <- 0
	ix <- ave(a, as.Date(time(tmpz)), FUN = function(x) {
		logic <- rep(FALSE, length(x))
		logic[which.min(x)] <- TRUE
		logic
	})
	ix <- as.logical(ix)
	ret[ix] <- a[ix]
	na.approx(ret, na.rm = FALSE)
}
system.time(out <- do.call(cbind, lapply(tmpz, f)))
#  2.39    0.02    2.48

all.equal(tmpextrema, out, check.attributes = FALSE)
# TRUE


> find.extrema = function(myzoo) {
+  days = as.POSIXct(levels(as.factor(as.Date(index(myzoo)))))
+  ret = myzoo
+  ret[] <- NA
+  for (tcol in 1:dim(myzoo)[2]) {
+    for (day in days) {
+      this <- window(myzoo[,tcol], start=day, end=day+24*60*60-1)
+      thismax <- this[which.max(this)]
+      this[this<0] <- 0 ## remove negative values first
+      thiszero <- this[which.min(this)]
+      ret[index(ret) == index(thismax), tcol] <- coredata(thismax)
+      ret[index(ret) == index(thiszero), tcol] <- coredata(thiszero)
+      ### ret[index(thismax), tcol] <- coredata(thismax)  ### gives an error
+    } # end days
+  } # end tcol
+  ret = na.approx(ret, na.rm = FALSE)
+  return(ret)
+ }
>
> hours=1e4 # about 2 year's worth of hours
> # hours = 27
> tmp=(as.POSIXct('2001-01-01')+1:hours*60*60)
> tmpz=zoo(cbind(a=sin(as.integer(tmp)/1e4), b=sin(as.integer(tmp)/1.1e4)), tmp)
> system.time(tmpextrema <- find.extrema(tmpz))
   user  system elapsed
  25.06    0.05   25.37
>
> f <- function(x) {
+ ret <- NA * x
+ ix <- ave(x, as.Date(time(tmpz)), FUN = function(x) {
+ logic <- rep(FALSE, length(x))
+ logic[which.max(x)] <- TRUE
+ logic
+ })
+ ix <- as.logical(ix)
+ ret[ix] <- x[ix]
+ a <- x
+ a[a < 0] <- 0
+ ix <- ave(a, as.Date(time(tmpz)), FUN = function(x) {
+ logic <- rep(FALSE, length(x))
+ logic[which.min(x)] <- TRUE
+ logic
+ })
+ ix <- as.logical(ix)
+ ret[ix] <- a[ix]
+ na.approx(ret, na.rm = FALSE)
+ }
> system.time(out <- do.call(cbind, lapply(tmpz, f)))
   user  system elapsed
   2.39    0.02    2.48
>
> all.equal(tmpextrema, out, check.attributes = FALSE)
[1] TRUE


On Thu, Jun 25, 2009 at 6:37 AM, Christian Gunning<icos.atropa at gmail.com> wrote:
> I have an multi-year hour-sampled multivariate zoo from which i'm
> trying to extract index() and coredata() of daily max and
> zero-crossing for each variable and use na.approx() to fill in
> intervening values, so that the resulting zoo has the same dimensions
> of the input zoo. Currently i'm looping over each column and day,
> extracting a day's worth of data with window(), and using which.max()
> to extract the "record". ?For short zoos it works fine. ?It doesn't
> scale well, though - for example, 2e4 points takes ~4-6 times longer
> than 1e4 points with 2 variables. Am i missing major bottlenecks or
> vectorization potentials here?
>
> find.extrema = function(myzoo) {
> ?days = as.POSIXct(levels(as.factor(as.Date(index(myzoo)))))
> ?ret = myzoo
> ?ret[] <- NA
> ?for (tcol in 1:dim(myzoo)[2]) {
> ? ?for (day in days) {
> ? ? ?this <- window(myzoo[,tcol], start=day, end=day+24*60*60-1)
> ? ? ?thismax <- this[which.max(this)]
> ? ? ?this[this<0] <- 0 ## remove negative values first
> ? ? ?thiszero <- this[which.min(this)]
> ? ? ?ret[index(ret) == index(thismax), tcol] <- coredata(thismax)
> ? ? ?ret[index(ret) == index(thiszero), tcol] <- coredata(thiszero)
> ? ? ?### ret[index(thismax), tcol] <- coredata(thismax) ?### gives an error
> ? ?} # end days
> ?} # end tcol
> ?ret = na.approx(ret)
> ?return(ret)
> }
>
> hours=1e4 # about 2 year's worth of hours
> tmp=(as.POSIXct('2001-01-01')+1:hours*60*60)
> tmpz=zoo(cbind(a=sin(as.integer(tmp)/1e4), b=sin(as.integer(tmp)/1.1e4)), tmp)
> system.time(tmpextrema <- find.extrema(tmpz))
>
> thanks,
> christian gunning
> university of new mexico
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From sjansevanrensburg at gmail.com  Fri Jun 26 20:34:19 2009
From: sjansevanrensburg at gmail.com (Stefan Janse van Rensburg)
Date: Fri, 26 Jun 2009 20:34:19 +0200
Subject: [R-SIG-Finance] recommended books
Message-ID: <68cc1efe0906261134q32564712n597e28bd6befb42e@mail.gmail.com>

Dear list,

I apologise for the fact that this is somewhat off topic, but I wanted
to know if the list could recommend some good books on R. My colleague
is new to both statistics and R and I would like to get him something
to ease the "steep"-learning curve. His knowledge of statistics is
quite limited, but being a PHD in applied maths I do not want to get
anything that will insult his intelligence.

Is Modern Applied Statistics with S still a good bet? I used it a few
years back and thought it was an excellent guide. I was, however,
already proficient with R and I'm not sure how good it would be for a
novice. Also, our work is mostly concerned with econometrics / time
series.

Apart from that, I am personally in need of a book covering state
space methods in time series analysis, possibly with applications
covering dynamic latent factor models. My preference would be for an
applied type book, especially one using R.

Any and all comments are welcome.

Kind regards,

Stefan Janse van Rensburg


From ggrothendieck at gmail.com  Sat Jun 27 02:23:45 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 26 Jun 2009 20:23:45 -0400
Subject: [R-SIG-Finance] recommended books
In-Reply-To: <68cc1efe0906261134q32564712n597e28bd6befb42e@mail.gmail.com>
References: <68cc1efe0906261134q32564712n597e28bd6befb42e@mail.gmail.com>
Message-ID: <971536df0906261723v7226231dqda57c535329792bd@mail.gmail.com>

83 R books and comments are here:
http://www.r-project.org/doc/bib/R-books.html

On Fri, Jun 26, 2009 at 2:34 PM, Stefan Janse van
Rensburg<sjansevanrensburg at gmail.com> wrote:
> Dear list,
>
> I apologise for the fact that this is somewhat off topic, but I wanted
> to know if the list could recommend some good books on R. My colleague
> is new to both statistics and R and I would like to get him something
> to ease the "steep"-learning curve. His knowledge of statistics is
> quite limited, but being a PHD in applied maths I do not want to get
> anything that will insult his intelligence.
>
> Is Modern Applied Statistics with S still a good bet? I used it a few
> years back and thought it was an excellent guide. I was, however,
> already proficient with R and I'm not sure how good it would be for a
> novice. Also, our work is mostly concerned with econometrics / time
> series.
>
> Apart from that, I am personally in need of a book covering state
> space methods in time series analysis, possibly with applications
> covering dynamic latent factor models. My preference would be for an
> applied type book, especially one using R.
>
> Any and all comments are welcome.
>
> Kind regards,
>
> Stefan Janse van Rensburg
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From spencer.graves at prodsyse.com  Sun Jun 28 05:50:11 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Sat, 27 Jun 2009 20:50:11 -0700
Subject: [R-SIG-Finance] Basic Mean Variance Optimization
In-Reply-To: <196436.69222.qm@web50907.mail.re2.yahoo.com>
References: <196436.69222.qm@web50907.mail.re2.yahoo.com>
Message-ID: <4A46E873.3020707@prodsyse.com>

      I have not seen a reply to this, so I will offer a comment. 


      First, I recommend you not worry about computer time until you 
actually know you have a problem with that.  Focus first on getting 
something to do what you want.  Then if it takes too long to compute, 
think about how to get it to run faster.  Often in R, a little thought 
and experimentation can yield big improvements in performance without 
coding anything in a compiled language like C.  If that fails, before 
you run to C, it might be wise to use tools like Rprof or system.time to 
understand which parts of your code take the most time.  For example, if 
most of the time is consumed with waiting for a data base query to 
complete, writing that in C will only give you code that is harder to 
maintain with no substantive improvement in performance.  Beyond that, 
there is the "QuantLib" project (http://quantlib.org/index.shtml) and 
the RQuantLib package to interface to some of the QuantLib code. 


      Have you seen Wuertz, et al. (2009) Portfolio Optimization with 
R/Rmetrics (www.rmetrics.org)?  


      The computation of a variance optimal portfolio might involve the 
computation of something like solve(S, r), where r = a vector of 
estimated log(returns) on the different assets, and S = the 
corresponding covariance matrix.  However, with hundreds of assets, you 
need a stable way to estimate S to avoid problems with degeneracy.  You 
could do that with a singular factor analysis algorithm.  This could be 
updated regularly using the binomial inverse theorem 
(http://en.wikipedia.org/wiki/Binomial_inverse_theorem), which is a 
standard part of a traditional Kalman filter algorithm that could be 
used to update r, S and S-inverse based on the latest information 
available. 


      Hope this helps. 
      Spencer


burke nersesian wrote:
> Need single period weighting for simple portfolio (minimal assumptions and constraints).
> Each asset has a variance = 1.0 and expected return = 1.0 and the covariance with each of the other assets in the portfolio is > 0.0 and < 1.0,
> (in other words, the weighting will only be dependent on the covariance matrix to produce in essence the minimum variance portfolio). 
> If possible, I would prefer each weight to be positive. The portfolio contains hundreds of assets and must be calculated thousands of times so R seems too slow for the job.
> Does anyone know of any C function in say, GSL, BLAS or any other library that takes a Convariance Matrix as input and returns a weight vector as output?
> The solution doesn't have to be guaranteed to be robust, if it dominates the equal weighted portfolio that's sufficient.
> Any ideas will help, thank you all so much!
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From bogaso.christofer at gmail.com  Sun Jun 28 18:46:32 2009
From: bogaso.christofer at gmail.com (Bogaso)
Date: Sun, 28 Jun 2009 09:46:32 -0700 (PDT)
Subject: [R-SIG-Finance] re[R-sig-finance] commended books
In-Reply-To: <68cc1efe0906261134q32564712n597e28bd6befb42e@mail.gmail.com>
References: <68cc1efe0906261134q32564712n597e28bd6befb42e@mail.gmail.com>
Message-ID: <24243190.post@talk.nabble.com>


[Off-topic]

""steep"-learning curve" means what? which takes very little time to learn a
lot of things? I thought steep learning curve means a curve with high slope
and hence my conclusion. Can guys here please clarify that? If I am right
then Stefan perhaps rightly asked a "FLAT learning curve" i.e. it takes lot
of times to understand a small thing.

Regards,



Stefan Janse van Rensburg wrote:
> 
> Dear list,
> 
> I apologise for the fact that this is somewhat off topic, but I wanted
> to know if the list could recommend some good books on R. My colleague
> is new to both statistics and R and I would like to get him something
> to ease the "steep"-learning curve. His knowledge of statistics is
> quite limited, but being a PHD in applied maths I do not want to get
> anything that will insult his intelligence.
> 
> Is Modern Applied Statistics with S still a good bet? I used it a few
> years back and thought it was an excellent guide. I was, however,
> already proficient with R and I'm not sure how good it would be for a
> novice. Also, our work is mostly concerned with econometrics / time
> series.
> 
> Apart from that, I am personally in need of a book covering state
> space methods in time series analysis, possibly with applications
> covering dynamic latent factor models. My preference would be for an
> applied type book, especially one using R.
> 
> Any and all comments are welcome.
> 
> Kind regards,
> 
> Stefan Janse van Rensburg
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/recommended-books-tp24225333p24243190.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From ron_michael70 at yahoo.com  Sun Jun 28 18:52:10 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Sun, 28 Jun 2009 09:52:10 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Creating a VCEM data generating
	process
Message-ID: <24243230.post@talk.nabble.com>


Hi all,

Can anyone here please help me how to create a DGP which corresponds to VECM
(Vector error correction) ? Actually I want to define a arbitrary VECM as a
DGP and then study the properties of it's realizations. However I can not
construct an arbitrary VECM from my own, especially it's coefficients, which
lead to strictly I(1) process of individual variable.

Thanks and regards,
-- 
View this message in context: http://www.nabble.com/Creating-a-VCEM-data-generating-process-tp24243230p24243230.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From landronimirc at gmail.com  Sun Jun 28 19:58:35 2009
From: landronimirc at gmail.com (Liviu Andronic)
Date: Sun, 28 Jun 2009 19:58:35 +0200
Subject: [R-SIG-Finance] re[R-sig-finance] commended books
In-Reply-To: <24243190.post@talk.nabble.com>
References: <68cc1efe0906261134q32564712n597e28bd6befb42e@mail.gmail.com>
	<24243190.post@talk.nabble.com>
Message-ID: <68b1e2610906281058s22bed0cau3b6f7c0d9d3be6fc@mail.gmail.com>

On 6/28/09, Bogaso <bogaso.christofer at gmail.com> wrote:
>  ""steep"-learning curve" means what?
>
http://tolstoy.newcastle.edu.au/R/e3/help/07/10/2310.html
Liviu


From markleeds at verizon.net  Sun Jun 28 20:43:52 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Sun, 28 Jun 2009 13:43:52 -0500 (CDT)
Subject: [R-SIG-Finance] [R-sig-finance] Creating a VCEM data
	generating	process
Message-ID: <598615577.788895.1246214632281.JavaMail.root@vms183.mailsrvcs.net>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090628/3cc29689/attachment.html>

From ksspriggs at gmail.com  Sun Jun 28 21:19:06 2009
From: ksspriggs at gmail.com (Kenneth Spriggs)
Date: Sun, 28 Jun 2009 14:19:06 -0500
Subject: [R-SIG-Finance] Name of output column xts vs. dataframe using
	ifelse statement
Message-ID: <82b060880906281219o2c49ac84l5e9ed44c29fd3487@mail.gmail.com>

For xts developers.  (If this has been covered before I apologize.)
The logic ends up fine just the name of the column doesn't actually go
with the output...

TimeIndex <- Sys.time() + seq(1,5,1); x1 <- rnorm(5); x2 <- rnorm(5);
x3 <- rnorm(5)
BartSimpson <- data.frame(TimeIndex, x1, x2, x3)
xtsBartSimpson <- xts(BartSimpson[,-1], BartSimpson[,1]); xtsBartSimpson
QuestionableHeaderName1 <- ifelse(xtsBartSimpson$x1 < 100000,
xtsBartSimpson$x2, xtsBartSimpson$x3);  QuestionableHeaderName1
QuestionableHeaderName2 <- ifelse(xtsBartSimpson$x1 == 100000,
xtsBartSimpson$x2, xtsBartSimpson$x3); QuestionableHeaderName2

                            x1         x2         x3
2009-06-28 18:54:02 -0.8688258  1.3517895 -0.8920901
2009-06-28 18:54:03 -1.6324798  0.4645305  0.8773917
2009-06-28 18:54:04  0.8754520 -1.1538617  1.2920259
2009-06-28 18:54:05  0.7083217  0.2606429 -0.4256368
2009-06-28 18:54:06 -0.4125602  0.6148270 -0.7822110
                            x1
2009-06-28 18:54:02  1.3517895
2009-06-28 18:54:03  0.4645305
2009-06-28 18:54:04 -1.1538617
2009-06-28 18:54:05  0.2606429
2009-06-28 18:54:06  0.6148270
                            x1
2009-06-28 18:54:02 -0.8920901
2009-06-28 18:54:03  0.8773917
2009-06-28 18:54:04  1.2920259
2009-06-28 18:54:05 -0.4256368
2009-06-28 18:54:06 -0.7822110

# x1 output should be x3 and the second x1 output should be x2

# Let's try it as a just a data.frame as opposed to xts and see what happens:
TimeIndex <- x1 <- rnorm(5); x2 <- rnorm(5); x3 <- rnorm(5)
BartSimpson <- data.frame(x1, x2, x3); BartSimpson
#xtsBartSimpson <- xts(BartSimpson[,-1], BartSimpson[,1]); xtsBartSimpson
QuestionableHeaderName1 <- ifelse(BartSimpson$x1 < 100000,
BartSimpson$x2, BartSimpson$x3);  QuestionableHeaderName1
QuestionableHeaderName2 <- ifelse(BartSimpson$x1 == 100000,
BartSimpson$x2, BartSimpson$x3); QuestionableHeaderName2

           x1          x2          x3
1  0.09949459 -0.01799246  1.39811684
2  0.61831865  0.93604736  0.87330695
3  1.26746389  1.13709153 -0.02954153
4 -1.32262513 -0.09825617 -0.97914918
5 -1.83912138  0.44640603  0.89351793
[1] -0.01799246  0.93604736  1.13709153 -0.09825617  0.44640603
[1]  1.39811684  0.87330695 -0.02954153 -0.97914918  0.89351793

# The difference in output between xts and data.frame is that the
data.frame doesn't have an output column title at all.


From ron_michael70 at yahoo.com  Sun Jun 28 22:24:34 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Sun, 28 Jun 2009 13:24:34 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Creating a VCEM data
	generating	process
In-Reply-To: <598615577.788895.1246214632281.JavaMail.root@vms183.mailsrvcs.net>
References: <598615577.788895.1246214632281.JavaMail.root@vms183.mailsrvcs.net>
Message-ID: <24245075.post@talk.nabble.com>


Thanks Statquant for this reply, however it is still not clear. Suppose I
have following theoretical DGP :

deltaY[t] = alpha + PI * Y[t-1] + A1 * deltaY[t-1] + A2 * deltaY[t-2] + A3 *
deltaY[t-3] + epsilon[t]

Next suppose, I have chosen some particular matrices as coefficient matrices
and taken them as population value. However how can I make it sure that DGP
has some unit root, with those arbitrarily chosen coef. matrices? My finding
was that, if I chose some arbitrary matrices and then solve the ch.
equation, I do not get some solutions as 1 and rests are outside the range
[-1, 1].

The steps that I thought of are :
1. Choose some matrices for alpha, PI, A1, A2, A3 (I need to find those!!!)
such that ch. equation gives some roots as "1" & rests are outside the range
[-1, 1].
2. Generate 1,000 realizations each with size 100 (say)
3. For each realization, re-estimate the coefficients.
4. Analyze the distribution of the coef.

Someone might find it as homework, however it is not. Currently I am
studying Lutkepohl and some asymptotic dist. are discussed here. I want to
get some empirical match.

Any idea?


statquant wrote:
> 
> hi ron : the simple vecm is 1) delta y_t = delta x_t + alpha(y_t-1 -
> beta*x_t-1) + epsilon_yt? ( but check this to make sure ). so, first
> generate x_t's that are I(1) by generating x_t = x_t -1 + epsilon_xt Then.
> given the x_t's,? pick some beta and an an alpha, and generate the y_t's
> based? on 1). this will give you y_t and x_t? that are I(1) and
> cointegrated by definition. the multi vecm is more complex but the idea is
> the same. On Jun 28, 2009, RON70 &lt;ron_michael70 at yahoo.com&gt; wrote: Hi
> all, Can anyone here please help me how to create a DGP which corresponds
> to VECM (Vector error correction) ? Actually I want to define a arbitrary
> VECM as a DGP and then study the properties of it's realizations. However
> I can not construct an arbitrary VECM from my own, especially it's
> coefficients, which lead to strictly I(1) process of individual variable.
> Thanks and regards, -- View this message in context:
> http://www.nabble.com/Creating-a-VCEM-data-generating-process-tp24243230p24243230.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch  mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance -- Subscriber-posting
> only. -- If you want to post, subscribe first. 
> 
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 

-- 
View this message in context: http://www.nabble.com/Re%3A--R-sig-finance--Creating-a-VCEM-data-generating%09process-tp24244254p24245075.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From markleeds at verizon.net  Sun Jun 28 22:55:32 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Sun, 28 Jun 2009 15:55:32 -0500 (CDT)
Subject: [R-SIG-Finance] [R-sig-finance] Creating a VCEM
	data	generating	process
Message-ID: <412240773.823237.1246222532963.JavaMail.root@vms170009.mailsrvcs.net>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090628/86f43942/attachment.html>

From ron_michael70 at yahoo.com  Sun Jun 28 23:11:00 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Sun, 28 Jun 2009 14:11:00 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Creating a VCEM
	data	generating	process
In-Reply-To: <412240773.823237.1246222532963.JavaMail.root@vms170009.mailsrvcs.net>
References: <412240773.823237.1246222532963.JavaMail.root@vms170009.mailsrvcs.net>
Message-ID: <24245510.post@talk.nabble.com>


Thank you so much for this. If you dont have any problem, can you please send
them here ron_michael70 at yahoo.com ?

Thanks and regards,



statquant wrote:
> 
> HI: I have to go out so I can't say much but? I wouldn't jump right to
> lutkepohl. it's hard to visualize/understanding the matrix case.? I would
> think of the bivariate case and then extend it after undersatanding that.?
> take the simpler bivariate case ( this is taken directly from eric zivot's
> S+Finmetrics book ). generate y_2t = y_2t-1 + v_t where v_t is normal zero
> whatever. then let y_1t = b2*y_2t + u_t where u_t is normal zero whatever.
> This is a cointegrated system with cointegrating vector (1,-b2). you can
> simulate this to visualize the behavior of y_1 and y_2 over time. If you
> don't have eric's book, I can fax you the two pages tomorrow. Generally
> speaking, unless you're quite familar with this material, I would start
> out with something along the level of Eric's book or Enders and then go to
> Lutkepohl after that. I really gotta run. Hopefully someone else can help
> you more but let me know if you want me to fax you the pages. it's 421-428
> if you have the book. On Jun 28, 2009, RON70
> &lt;ron_michael70 at yahoo.com&gt; wrote: Thanks Statquant for this reply,
> however it is still not clear. Suppose I have following theoretical DGP :
> deltaY[t] = alpha + PI * Y[t-1] + A1 * deltaY[t-1] + A2 * deltaY[t-2] + A3
> * deltaY[t-3] + epsilon[t] Next suppose, I have chosen some particular
> matrices as coefficient matrices and taken them as population value.
> However how can I make it sure that DGP has some unit root, with those
> arbitrarily chosen coef. matrices? My finding was that, if I chose some
> arbitrary matrices and then solve the ch. equation, I do not get some
> solutions as 1 and rests are outside the range [-1, 1]. The steps that I
> thought of are : 1. Choose some matrices for alpha, PI, A1, A2, A3 (I need
> to find those!!!) such that ch. equation gives some roots as "1" &amp;
> rests are outside the range [-1, 1]. 2. Generate 1,000 realizations each
> with size 100 (say) 3. For each realization, re-estimate the coefficients.
> 4. Analyze the distribution of the coef. Someone might find it as
> homework, however it is not. Currently I am studying Lutkepohl and some
> asymptotic dist. are discussed here. I want to get some empirical match.
> Any idea? statquant wrote: &gt; &gt; hi ron : the simple vecm is 1) delta
> y_t = delta x_t + alpha(y_t-1 - &gt; beta*x_t-1) + epsilon_yt? ( but check
> this to make sure ). so, first &gt; generate x_t's that are I(1) by
> generating x_t = x_t -1 + epsilon_xt Then. &gt; given the x_t's,? pick
> some beta and an an alpha, and generate the y_t's &gt; based? on 1). this
> will give you y_t and x_t? that are I(1) and &gt; cointegrated by
> definition. the multi vecm is more complex but the idea is &gt; the same.
> On Jun 28, 2009, RON70 &amp;lt; ron_michael70 at yahoo.com &amp;gt; wrote: Hi
> &gt; all, Can anyone here please help me how to create a DGP which
> corresponds &gt; to VECM (Vector error correction) ? Actually I want to
> define a arbitrary &gt; VECM as a DGP and then study the properties of
> it's realizations. However &gt; I can not construct an arbitrary VECM from
> my own, especially it's &gt; coefficients, which lead to strictly I(1)
> process of individual variable. &gt; Thanks and regards, -- View this
> message in context: &gt;
> http://www.nabble.com/Creating-a-VCEM-data-generating-process-tp24243230p24243230.html
> &gt; Sent from the Rmetrics mailing list archive at Nabble.com. &gt;
> _______________________________________________ &gt;
> R-SIG-Finance at stat.math.ethz.ch   mailing list &gt;
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance  -- Subscriber-posting
> &gt; only. -- If you want to post, subscribe first. &gt; &gt; &gt;
> _______________________________________________ &gt;
> R-SIG-Finance at stat.math.ethz.ch  mailing list &gt;
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance &gt; --
> Subscriber-posting only. &gt; -- If you want to post, subscribe first.
> &gt; -- View this message in context:
> http://www.nabble.com/Re%3A--R-sig-finance--Creating-a-VCEM-data-generating%09process-tp24244254p24245075.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch  mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance -- Subscriber-posting
> only. -- If you want to post, subscribe first. 
> 
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 

-- 
View this message in context: http://www.nabble.com/Re%3A--R-sig-finance--Creating-a-VCEM-data%09generating%09process-tp24245386p24245510.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From patrick at burns-stat.com  Mon Jun 29 10:10:44 2009
From: patrick at burns-stat.com (Patrick Burns)
Date: Mon, 29 Jun 2009 09:10:44 +0100
Subject: [R-SIG-Finance] re[R-sig-finance] commended books
In-Reply-To: <24243190.post@talk.nabble.com>
References: <68cc1efe0906261134q32564712n597e28bd6befb42e@mail.gmail.com>
	<24243190.post@talk.nabble.com>
Message-ID: <4A487704.1040606@burns-stat.com>

You should get used to English terms
not saying what they mean.  However,
in this instance the term can make
sense if you change your picture to:

xlab = "Amount Learned"
ylab = "Efort and/or Frustration"



Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of "The R Inferno" and "A Guide for the Unwilling S User")


Bogaso wrote:
> [Off-topic]
> 
> ""steep"-learning curve" means what? which takes very little time to learn a
> lot of things? I thought steep learning curve means a curve with high slope
> and hence my conclusion. Can guys here please clarify that? If I am right
> then Stefan perhaps rightly asked a "FLAT learning curve" i.e. it takes lot
> of times to understand a small thing.
> 
> Regards,
> 
> 
> 
> Stefan Janse van Rensburg wrote:
>> Dear list,
>>
>> I apologise for the fact that this is somewhat off topic, but I wanted
>> to know if the list could recommend some good books on R. My colleague
>> is new to both statistics and R and I would like to get him something
>> to ease the "steep"-learning curve. His knowledge of statistics is
>> quite limited, but being a PHD in applied maths I do not want to get
>> anything that will insult his intelligence.
>>
>> Is Modern Applied Statistics with S still a good bet? I used it a few
>> years back and thought it was an excellent guide. I was, however,
>> already proficient with R and I'm not sure how good it would be for a
>> novice. Also, our work is mostly concerned with econometrics / time
>> series.
>>
>> Apart from that, I am personally in need of a book covering state
>> space methods in time series analysis, possibly with applications
>> covering dynamic latent factor models. My preference would be for an
>> applied type book, especially one using R.
>>
>> Any and all comments are welcome.
>>
>> Kind regards,
>>
>> Stefan Janse van Rensburg
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>
>


From josh.m.ulrich at gmail.com  Mon Jun 29 20:05:30 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Mon, 29 Jun 2009 13:05:30 -0500
Subject: [R-SIG-Finance] Name of output column xts vs. dataframe using
	ifelse statement
In-Reply-To: <82b060880906281219o2c49ac84l5e9ed44c29fd3487@mail.gmail.com>
References: <82b060880906281219o2c49ac84l5e9ed44c29fd3487@mail.gmail.com>
Message-ID: <8cca69990906291105s206c8afagdeaec574d25c6345@mail.gmail.com>

Hi Ken,

I'm not sure there's any way for us to *fix* this, since it is a
result of the ifelse() function.  The source below shows the function
replaces the values of "test" with "yes" or "no" depending on the
result of "test".

Further, in the case that the result contains some values from *both*
columns x2 and x3, it's not clear what the column name should be.

> ifelse
function (test, yes, no) {
    storage.mode(test) <- "logical"
    ans <- test
    nas <- is.na(test)
    if (any(test[!nas]))
        ans[test & !nas] <- rep(yes, length.out = length(ans))[test & !nas]
    if (any(!test[!nas]))
        ans[!test & !nas] <- rep(no, length.out = length(ans))[!test & !nas]
    ans[nas] <- NA
    ans
}
<environment: namespace:base>


HTH,
Josh
--
http://www.fosstrading.com



On Sun, Jun 28, 2009 at 2:19 PM, Kenneth Spriggs<ksspriggs at gmail.com> wrote:
> For xts developers. ?(If this has been covered before I apologize.)
> The logic ends up fine just the name of the column doesn't actually go
> with the output...
>
> TimeIndex <- Sys.time() + seq(1,5,1); x1 <- rnorm(5); x2 <- rnorm(5);
> x3 <- rnorm(5)
> BartSimpson <- data.frame(TimeIndex, x1, x2, x3)
> xtsBartSimpson <- xts(BartSimpson[,-1], BartSimpson[,1]); xtsBartSimpson
> QuestionableHeaderName1 <- ifelse(xtsBartSimpson$x1 < 100000,
> xtsBartSimpson$x2, xtsBartSimpson$x3); ?QuestionableHeaderName1
> QuestionableHeaderName2 <- ifelse(xtsBartSimpson$x1 == 100000,
> xtsBartSimpson$x2, xtsBartSimpson$x3); QuestionableHeaderName2
>
> ? ? ? ? ? ? ? ? ? ? ? ? ? ?x1 ? ? ? ? x2 ? ? ? ? x3
> 2009-06-28 18:54:02 -0.8688258 ?1.3517895 -0.8920901
> 2009-06-28 18:54:03 -1.6324798 ?0.4645305 ?0.8773917
> 2009-06-28 18:54:04 ?0.8754520 -1.1538617 ?1.2920259
> 2009-06-28 18:54:05 ?0.7083217 ?0.2606429 -0.4256368
> 2009-06-28 18:54:06 -0.4125602 ?0.6148270 -0.7822110
> ? ? ? ? ? ? ? ? ? ? ? ? ? ?x1
> 2009-06-28 18:54:02 ?1.3517895
> 2009-06-28 18:54:03 ?0.4645305
> 2009-06-28 18:54:04 -1.1538617
> 2009-06-28 18:54:05 ?0.2606429
> 2009-06-28 18:54:06 ?0.6148270
> ? ? ? ? ? ? ? ? ? ? ? ? ? ?x1
> 2009-06-28 18:54:02 -0.8920901
> 2009-06-28 18:54:03 ?0.8773917
> 2009-06-28 18:54:04 ?1.2920259
> 2009-06-28 18:54:05 -0.4256368
> 2009-06-28 18:54:06 -0.7822110
>
> # x1 output should be x3 and the second x1 output should be x2
>
> # Let's try it as a just a data.frame as opposed to xts and see what happens:
> TimeIndex <- x1 <- rnorm(5); x2 <- rnorm(5); x3 <- rnorm(5)
> BartSimpson <- data.frame(x1, x2, x3); BartSimpson
> #xtsBartSimpson <- xts(BartSimpson[,-1], BartSimpson[,1]); xtsBartSimpson
> QuestionableHeaderName1 <- ifelse(BartSimpson$x1 < 100000,
> BartSimpson$x2, BartSimpson$x3); ?QuestionableHeaderName1
> QuestionableHeaderName2 <- ifelse(BartSimpson$x1 == 100000,
> BartSimpson$x2, BartSimpson$x3); QuestionableHeaderName2
>
> ? ? ? ? ? x1 ? ? ? ? ?x2 ? ? ? ? ?x3
> 1 ?0.09949459 -0.01799246 ?1.39811684
> 2 ?0.61831865 ?0.93604736 ?0.87330695
> 3 ?1.26746389 ?1.13709153 -0.02954153
> 4 -1.32262513 -0.09825617 -0.97914918
> 5 -1.83912138 ?0.44640603 ?0.89351793
> [1] -0.01799246 ?0.93604736 ?1.13709153 -0.09825617 ?0.44640603
> [1] ?1.39811684 ?0.87330695 -0.02954153 -0.97914918 ?0.89351793
>
> # The difference in output between xts and data.frame is that the
> data.frame doesn't have an output column title at all.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ksspriggs at gmail.com  Mon Jun 29 21:18:46 2009
From: ksspriggs at gmail.com (ksspriggs at gmail.com)
Date: Mon, 29 Jun 2009 19:18:46 +0000
Subject: [R-SIG-Finance] Name of output column xts vs. dataframe using
	ifelse statement
In-Reply-To: <8cca69990906291105s206c8afagdeaec574d25c6345@mail.gmail.com>
Message-ID: <00032557559626da4d046d818fee@google.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090629/d24ba074/attachment.pl>

From jeff.a.ryan at gmail.com  Mon Jun 29 21:23:03 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Mon, 29 Jun 2009 14:23:03 -0500
Subject: [R-SIG-Finance] Name of output column xts vs. dataframe using
	ifelse statement
In-Reply-To: <00032557559626da4d046d818fee@google.com>
References: <8cca69990906291105s206c8afagdeaec574d25c6345@mail.gmail.com>
	<00032557559626da4d046d818fee@google.com>
Message-ID: <e8e755250906291223v4b976d7ble3502797afdbaedd@mail.gmail.com>

Hi Ken,

Try colnames.  names isn't the right function.

HTH
Jeff

On Mon, Jun 29, 2009 at 2:18 PM, <ksspriggs at gmail.com> wrote:
> Oh, I see, good point. (But should that the column output really have a
> name at all?)
>
> On a similar point I can't get names() to work with xts. Here's an example:
>
> TimeIndex <- Sys.time() + seq(1,5,1); x1 <- rnorm(5); x2 <- rnorm(5)
> MyData <- data.frame(TimeIndex, x1, x2)
> xtsMyData <- xts(MyData[,-1], MyData[,1]); xtsMyData
>
> names(xtsMyData) <- c("Xone", "Xtwo")
> #names(xtsMyData) <- c("Timestamp", "Xone", "Xtwo")
>
>> head(xtsMyData,3)
> x1 x2
> 2009-06-29 19:13:23.825 -1.05527495 -0.4014843
> 2009-06-29 19:13:24.825 -1.82892230 -1.1080668
> 2009-06-29 19:13:25.825 -0.09286556 1.6264407
>
> names(MyData) <- c("Timestamp", "Xone", "Xtwo")
>> head(MyData, 3)
> Timestamp Xone Xtwo
> 1 2009-06-29 19:13:23.825 -1.05527495 -0.4014843
> 2 2009-06-29 19:13:24.825 -1.82892230 -1.1080668
> 3 2009-06-29 19:13:25.825 -0.09286556 1.6264407
>
>
> Am I doing something wrong?
> Thanks
>
>
>
> On Jun 29, 2009 1:05pm, Joshua Ulrich <josh.m.ulrich at gmail.com> wrote:
>> Hi Ken,
>
>
>
>> I'm not sure there's any way for us to *fix* this, since it is a
>
>> result of the ifelse() function. The source below shows the function
>
>> replaces the values of "test" with "yes" or "no" depending on the
>
>> result of "test".
>
>
>
>> Further, in the case that the result contains some values from *both*
>
>> columns x2 and x3, it's not clear what the column name should be.
>
>
>
>> > ifelse
>
>> function (test, yes, no) {
>
>> storage.mode(test)
>> ans
>> nas is.na(test)
>
>> if (any(test[!nas]))
>
>> ans[test & !nas]
>> if (any(!test[!nas]))
>
>> ans[!test & !nas]
>> ans[nas]
>> ans
>
>> }
>
>
>
>
>
>
>
>> HTH,
>
>> Josh
>
>> --
>
>> http://www.fosstrading.com
>
>
>
>
>
>
>
>> On Sun, Jun 28, 2009 at 2:19 PM, Kenneth Spriggsksspriggs at gmail.com>
>> wrote:
>
>> > For xts developers. (If this has been covered before I apologize.)
>
>> > The logic ends up fine just the name of the column doesn't actually go
>
>> > with the output...
>
>> >
>
>> > TimeIndex
>> > x3
>> > BartSimpson
>> > xtsBartSimpson
>> > QuestionableHeaderName1
>> > xtsBartSimpson$x2, xtsBartSimpson$x3); QuestionableHeaderName1
>
>> > QuestionableHeaderName2
>> > xtsBartSimpson$x2, xtsBartSimpson$x3); QuestionableHeaderName2
>
>> >
>
>> > x1 x2 x3
>
>> > 2009-06-28 18:54:02 -0.8688258 1.3517895 -0.8920901
>
>> > 2009-06-28 18:54:03 -1.6324798 0.4645305 0.8773917
>
>> > 2009-06-28 18:54:04 0.8754520 -1.1538617 1.2920259
>
>> > 2009-06-28 18:54:05 0.7083217 0.2606429 -0.4256368
>
>> > 2009-06-28 18:54:06 -0.4125602 0.6148270 -0.7822110
>
>> > x1
>
>> > 2009-06-28 18:54:02 1.3517895
>
>> > 2009-06-28 18:54:03 0.4645305
>
>> > 2009-06-28 18:54:04 -1.1538617
>
>> > 2009-06-28 18:54:05 0.2606429
>
>> > 2009-06-28 18:54:06 0.6148270
>
>> > x1
>
>> > 2009-06-28 18:54:02 -0.8920901
>
>> > 2009-06-28 18:54:03 0.8773917
>
>> > 2009-06-28 18:54:04 1.2920259
>
>> > 2009-06-28 18:54:05 -0.4256368
>
>> > 2009-06-28 18:54:06 -0.7822110
>
>> >
>
>> > # x1 output should be x3 and the second x1 output should be x2
>
>> >
>
>> > # Let's try it as a just a data.frame as opposed to xts and see what
>> happens:
>
>> > TimeIndex
>> > BartSimpson
>> > #xtsBartSimpson
>> > QuestionableHeaderName1
>> > BartSimpson$x2, BartSimpson$x3); QuestionableHeaderName1
>
>> > QuestionableHeaderName2
>> > BartSimpson$x2, BartSimpson$x3); QuestionableHeaderName2
>
>> >
>
>> > x1 x2 x3
>
>> > 1 0.09949459 -0.01799246 1.39811684
>
>> > 2 0.61831865 0.93604736 0.87330695
>
>> > 3 1.26746389 1.13709153 -0.02954153
>
>> > 4 -1.32262513 -0.09825617 -0.97914918
>
>> > 5 -1.83912138 0.44640603 0.89351793
>
>> > [1] -0.01799246 0.93604736 1.13709153 -0.09825617 0.44640603
>
>> > [1] 1.39811684 0.87330695 -0.02954153 -0.97914918 0.89351793
>
>> >
>
>> > # The difference in output between xts and data.frame is that the
>
>> > data.frame doesn't have an output column title at all.
>
>> >
>
>> > _______________________________________________
>
>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>> > -- Subscriber-posting only.
>
>> > -- If you want to post, subscribe first.
>
>> >
>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From spencer.graves at prodsyse.com  Tue Jun 30 04:45:54 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Mon, 29 Jun 2009 19:45:54 -0700
Subject: [R-SIG-Finance] SPS and QLPM portfolios
In-Reply-To: <20090616025957.AIZ81634@po7.mail.umd.edu>
References: <20090616025957.AIZ81634@po7.mail.umd.edu>
Message-ID: <4A497C62.5010904@prodsyse.com>

      I haven't seen a reply to this, so I will offer a comment.  
Consider the following: 


library(RSiteSearch)
qlpm <- RSiteSearch.function('QLPM')
HTML(qlpm)


      This identified the frontierPoints function in the fPortfolio 
package.  Beyond this, Wuertz, et al. (2009) Portfolio Optimization with 
R/Rmetrics (www.rmetrics.org/ebook.htm) includes "QLMP" twice and 
"frontierPoints" 55 times. 


      Hope this helps. 
      Spencer    


ssmith88 at umd.edu wrote:
> Does anyone have any sample code on how to implement an optimization with SPS or QLPM portfolios?  Is there a way to specify all 5 parameters for the SPS risk measure?  Thanks very much.
>
> Scott Smith
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From rhelpacc at gmail.com  Tue Jun 30 05:18:32 2009
From: rhelpacc at gmail.com (R_help Help)
Date: Mon, 29 Jun 2009 23:18:32 -0400
Subject: [R-SIG-Finance] Help on constrained regression
Message-ID: <ad1ead5f0906292018w8158fc4od0e507cff9d25a3@mail.gmail.com>

Hi,

I have an AR(1) model

y[t] = ay[t-1]+b+epsilon

I'm trying to force a to be positive. So I did the constrained
regression with constraints 0 < a < 1. I used pcls in package mgcv.
However, I found that the solution is not so stable. Most of my lag 1
autocorrelation is negative. Forcing a to positive value makes the
optimizer to stick a to the boundary value. All it does is varying b.
I there anyway to solve this problem? I think the problem might be due
to my initial value is not a smart choice.

Thank you.

adschai


From spencer.graves at prodsyse.com  Tue Jun 30 09:30:53 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Tue, 30 Jun 2009 00:30:53 -0700
Subject: [R-SIG-Finance] Fundamental analysis library?
In-Reply-To: <ff45355b0906160855ld5dab8aq21dfdbde4585e93e@mail.gmail.com>
References: <ff45355b0906160855ld5dab8aq21dfdbde4585e93e@mail.gmail.com>
Message-ID: <4A49BF2D.5090005@prodsyse.com>

      I have not seen a reply to this post, so I will offer a comment.  
Consider the following: 


library(RSiteSearch)
fa <- RSiteSearch.function("fundamental analysis")
HTML(fa)


      This returned 19 matches in 9 different packages, none of which 
seemed to be related to finance. 


      Also, the character string "fundamental analysis" does not appear 
in Wuertz, et al. (2009) Portfolio Optimization with R/Rmetrics 
(www.rmetrics.org/ebook.htm). 


      I got four hits in the Quantlib documentation 
(http://quantlib.org/reference/index.html).  Quantlib is C++, but R code 
could be written to connect to portions of Quantlib not already included 
in the RQuantLib package.  If you want to write such, I suggest you 
contact the RQuantLib author and maintainer, Dirk Eddelbuettel.  The 
RQuantLib documentation says, "Further software contributions are 
welcome."  [help(package=RQuantLib) and 
http://r-forge.r-project.org/R/?group_id=117]


      Hope this helps. 
      Spencer Graves


Joshua Reich wrote:
> Hi All,
>
> Is anyone aware of any fundamental analysis oriented packages for R? In my
> previous job I had written a large set of tools in R for dealing with many
> of the data cleaning, accounting alignment and analysis issues that uniquely
> present themselves in fundamental analysis. Before I started writing that I
> never really searched around for pre-existing code, so I figure I should ask
> now before I dive into doing it all again...
>
> If no such thing exists, would anyone be interested in joining me in
> developing a package? I don't wish upon anyone the pain of dealing with
> restatements, shifting accounting periods, solving for missing quarters,
> reconciling balance sheets with income statements, etc. - so why not have a
> single package that deals with this?
>
> Josh Reich
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From icos.atropa at gmail.com  Tue Jun 30 12:41:36 2009
From: icos.atropa at gmail.com (Christian Gunning)
Date: Tue, 30 Jun 2009 03:41:36 -0700
Subject: [R-SIG-Finance] efficient extraction of local extrema and
	zero-crossings in large multivariate zoo?
In-Reply-To: <971536df0906260546r72e88250w53d46c535aa79ab3@mail.gmail.com>
References: <681d07c20906250337g31d04d97p1bb902f5ea3970c@mail.gmail.com> 
	<971536df0906260546r72e88250w53d46c535aa79ab3@mail.gmail.com>
Message-ID: <681d07c20906300341s529fa3f8vd6745aab3124542a@mail.gmail.com>

Thanks so much for all for the input - approaching this from several
directions helped me understand the problem in greater depth.

As an aside, I see now that which.min() picks the first min, so i've
used "x[x<0] <- NA" instead (see end).

Using apply.daily, the function scales linearly all the way up.  With
the improved zoo function, i see significant super-linear scaling of
time, with the apply.daily version winning on this machine above ~2e4
total samples (and losing below). I didn't look at scaling of
memory...

thanks again,
christian gunning

#### locate max and zero-crossings, interpolate between
find.extrema = function(myzoo) {
 days = as.POSIXct(levels(as.factor(as.Date(index(myzoo)))))
 ret = myzoo
 ret[] <- NA
 for (tcol in 1:dim(myzoo)[2]) {
  this = myzoo[,tcol]
  max.days <- apply.daily(this,function(x) { index(x)[which.max(x)] } )
  max.days <- this[max.days]
  this[this<0] <- NA
  min.days <- apply.daily(this,function(x) { index(x)[which.min(x)] } )
  min.days <- this[min.days]
  ret[,tcol] <- cbind(ret[,tcol],rbind(min.days,max.days))[,2]
 } # end tcol
 ret = na.approx(ret, na.rm=F)
 return(ret)
}


From sjansevanrensburg at gmail.com  Tue Jun 30 13:21:13 2009
From: sjansevanrensburg at gmail.com (Stefan Janse van Rensburg)
Date: Tue, 30 Jun 2009 13:21:13 +0200
Subject: [R-SIG-Finance] recommended books
Message-ID: <68cc1efe0906300421m28607d85lb4994ad9c6858fc4@mail.gmail.com>

Dear list,

I would like to thank those (Mark Leeds, Gabor Grothendieck and
Charles Ward) who responded to my request.

I am especially grateful to Mark who pointed out Giovanni Petris et
al's work to me.

Kind regards,

Stefan Janse van Rensburg


From weihanliu2002 at yahoo.com  Tue Jun 30 17:16:20 2009
From: weihanliu2002 at yahoo.com (Wei-han Liu)
Date: Tue, 30 Jun 2009 08:16:20 -0700 (PDT)
Subject: [R-SIG-Finance] Value-at-Risk
Message-ID: <817126.55167.qm@web53501.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090630/7c3b7b81/attachment.pl>

From Zeno.Adams at ebs.edu  Tue Jun 30 17:40:27 2009
From: Zeno.Adams at ebs.edu (Adams, Zeno)
Date: Tue, 30 Jun 2009 17:40:27 +0200
Subject: [R-SIG-Finance] Value-at-Risk
In-Reply-To: <817126.55167.qm@web53501.mail.re2.yahoo.com>
References: <817126.55167.qm@web53501.mail.re2.yahoo.com>
Message-ID: <9064522880125945B98983BBAECBA1CC9855B3@exchsrv001.ebs.local>

1. VaR can be estimated by several approaches. Whether the VaR model is applicable for forecasting strongly depends on the estimation method. For example, if you estimate the (univariate) VaR of a portfolio return series using GARCH, then you will have some autoregressive structure that allows you to make good 1-step-ahead forecasts.

2. You could compare the forecasted VaR with the estimated VaR at point t+1 and compute e.g. the RMSE. However, since you can only estimate the VaR but do not know the "true" VaR this may not be optimal. There is no VaR model that can forecast the first large negative returns in a series of large negative returns that you often see when plotting a return series. However, VaR models differ in how quickly they can adapt to this new situation once the first strongly negative return has been realized.

3. You might want to take a look at http://ssrn.com/abstract=1233442 where this issue is discussed in detail. In short: The traditional way of comparing models such as hit ratios or the Christoffersen (1998) test may not be optimal. If you are interested in the performance measure described in this paper I can send you the code. (At that time I programmed it in EViews. shame on me).



-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Wei-han Liu
Gesendet: Dienstag, 30. Juni 2009 17:16
An: R-SIG-Finance at stat.math.ethz.ch
Betreff: [R-SIG-Finance] Value-at-Risk

Dear R-users:

Several questions please on Value-at-Risk.?

Is Value-at-Risk designed for forecasting purpose??

I wonder if Value-at-Risk estimated by in-sample data can be used for out-of-sample forecasting??

If in-sample Value-at-Risk is estimated by several methods, is it appropriate to do the model comparisons based on out--of-sample performance?

Wei-han Liu


      
	[[alternative HTML version deleted]]


EBS European Business School gemeinnuetzige GmbH - Sitz der Gesellschaft: Wiesbaden, Amtsgericht Wiesbaden HRB 19951 - Umsatzsteuer-ID DE 113891213 Geschaeftsfuehrer: Prof. Dr. Christopher Jahns,  Praesident; Dr. Reimar Palte,  Kanzler/CFO;  Sabine Fuchs, CMO; Aufsichtsrat: Dr. Hellmut K. Albrecht, Vorsitzender

From matthieu.stigler at gmail.com  Tue Jun 30 19:17:30 2009
From: matthieu.stigler at gmail.com (Matthieu Stigler)
Date: Tue, 30 Jun 2009 19:17:30 +0200
Subject: [R-SIG-Finance] [R-sig-finance] Creating a VCEM data	generating
 process
In-Reply-To: <24245510.post@talk.nabble.com>
References: <412240773.823237.1246222532963.JavaMail.root@vms170009.mailsrvcs.net>
	<24245510.post@talk.nabble.com>
Message-ID: <4A4A48AA.5050909@gmail.com>

Hi

To simulate a VECM you could look in package vars in the impulse 
response functions, as this implements a bootstrap procedure.However, 
this is rather a low level function and it is based I think on the VMA 
representation.

I implemented functions to simulate VAR and VECM in package tsDyn. The 
TVECM.sim() can only simulate bivariate models, whereas the TVAR.sim() 
can handle multiple variables. Examples pages for these functions 
include code to reproduce the examples in Enders. If you want to play 
with more than 2 variables, you could convert your VECM into a VAR with 
function vec2var() from package vars (you will need to cheat) and then 
use the var coef for TVAR.sim(). or just simply write yourself a 
function to simulate a VAR, should not be too complicated.

Here is a code to simulate trivariate VAR:
B1<-matrix(c(0.7, 0.2, 0.2, 0.2, 0.1, 0.3,0.1, 0.1, 0.3), nrow=3)
var1<-TVAR.sim(B=B1,nthresh=0,n=100, lag=1,type="simul", include="none", 
show.parMat=TRUE)
ts.plot(var1, type="l", col=1:3)

The arg varcov allows to specify the complete var-cov matrix of errors 
and hence have cross-correlation. If you want more complicated 
structures, create your erros series and input is with arg innov.

Note that this is with dev version of package tsDyn 
(http://code.google.com/p/tsdyn/wiki/ThresholdCointegration) that you 
will need to compile. A new version should come soon (next week).

Let us know if you find any interesting/illustrative examples of 
VAR/VECM simulation.

I do agree with Mark concerning the books. Lutkepohl is the last book to 
read... first read after Enders, then Hamilton and then Lutkpohl!

Bests

Matthieu

RON70 a ?crit :
> Thank you so much for this. If you dont have any problem, can you please send
> them here ron_michael70 at yahoo.com ?
>
> Thanks and regards,
>
>
>
> statquant wrote:
>   
>> HI: I have to go out so I can't say much but  I wouldn't jump right to
>> lutkepohl. it's hard to visualize/understanding the matrix case.  I would
>> think of the bivariate case and then extend it after undersatanding that. 
>> take the simpler bivariate case ( this is taken directly from eric zivot's
>> S+Finmetrics book ). generate y_2t = y_2t-1 + v_t where v_t is normal zero
>> whatever. then let y_1t = b2*y_2t + u_t where u_t is normal zero whatever.
>> This is a cointegrated system with cointegrating vector (1,-b2). you can
>> simulate this to visualize the behavior of y_1 and y_2 over time. If you
>> don't have eric's book, I can fax you the two pages tomorrow. Generally
>> speaking, unless you're quite familar with this material, I would start
>> out with something along the level of Eric's book or Enders and then go to
>> Lutkepohl after that. I really gotta run. Hopefully someone else can help
>> you more but let me know if you want me to fax you the pages. it's 421-428
>> if you have the book. On Jun 28, 2009, RON70
>> &lt;ron_michael70 at yahoo.com&gt; wrote: Thanks Statquant for this reply,
>> however it is still not clear. Suppose I have following theoretical DGP :
>> deltaY[t] = alpha + PI * Y[t-1] + A1 * deltaY[t-1] + A2 * deltaY[t-2] + A3
>> * deltaY[t-3] + epsilon[t] Next suppose, I have chosen some particular
>> matrices as coefficient matrices and taken them as population value.
>> However how can I make it sure that DGP has some unit root, with those
>> arbitrarily chosen coef. matrices? My finding was that, if I chose some
>> arbitrary matrices and then solve the ch. equation, I do not get some
>> solutions as 1 and rests are outside the range [-1, 1]. The steps that I
>> thought of are : 1. Choose some matrices for alpha, PI, A1, A2, A3 (I need
>> to find those!!!) such that ch. equation gives some roots as "1" &amp;
>> rests are outside the range [-1, 1]. 2. Generate 1,000 realizations each
>> with size 100 (say) 3. For each realization, re-estimate the coefficients.
>> 4. Analyze the distribution of the coef. Someone might find it as
>> homework, however it is not. Currently I am studying Lutkepohl and some
>> asymptotic dist. are discussed here. I want to get some empirical match.
>> Any idea? statquant wrote: &gt; &gt; hi ron : the simple vecm is 1) delta
>> y_t = delta x_t + alpha(y_t-1 - &gt; beta*x_t-1) + epsilon_yt  ( but check
>> this to make sure ). so, first &gt; generate x_t's that are I(1) by
>> generating x_t = x_t -1 + epsilon_xt Then. &gt; given the x_t's,  pick
>> some beta and an an alpha, and generate the y_t's &gt; based  on 1). this
>> will give you y_t and x_t  that are I(1) and &gt; cointegrated by
>> definition. the multi vecm is more complex but the idea is &gt; the same.
>> On Jun 28, 2009, RON70 &amp;lt; ron_michael70 at yahoo.com &amp;gt; wrote: Hi
>> &gt; all, Can anyone here please help me how to create a DGP which
>> corresponds &gt; to VECM (Vector error correction) ? Actually I want to
>> define a arbitrary &gt; VECM as a DGP and then study the properties of
>> it's realizations. However &gt; I can not construct an arbitrary VECM from
>> my own, especially it's &gt; coefficients, which lead to strictly I(1)
>> process of individual variable. &gt; Thanks and regards, -- View this
>> message in context: &gt;
>> http://www.nabble.com/Creating-a-VCEM-data-generating-process-tp24243230p24243230.html
>> &gt; Sent from the Rmetrics mailing list archive at Nabble.com. &gt;
>> _______________________________________________ &gt;
>> R-SIG-Finance at stat.math.ethz.ch   mailing list &gt;
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance  -- Subscriber-posting
>> &gt; only. -- If you want to post, subscribe first. &gt; &gt; &gt;
>> _______________________________________________ &gt;
>> R-SIG-Finance at stat.math.ethz.ch  mailing list &gt;
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance &gt; --
>> Subscriber-posting only. &gt; -- If you want to post, subscribe first.
>> &gt; -- View this message in context:
>> http://www.nabble.com/Re%3A--R-sig-finance--Creating-a-VCEM-data-generating%09process-tp24244254p24245075.html
>> Sent from the Rmetrics mailing list archive at Nabble.com.
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch  mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance -- Subscriber-posting
>> only. -- If you want to post, subscribe first. 
>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>     
>
>


From davidr at rhotrading.com  Tue Jun 30 21:07:20 2009
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Tue, 30 Jun 2009 14:07:20 -0500
Subject: [R-SIG-Finance] applying na.locf to xts objects sometimes crashes R
Message-ID: <F9F2A641C593D7408925574C05A7BE7703257F13@rhopost.rhotrading.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090630/cbe588d7/attachment.pl>

From jeff.a.ryan at gmail.com  Tue Jun 30 21:26:35 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Tue, 30 Jun 2009 14:26:35 -0500
Subject: [R-SIG-Finance] applying na.locf to xts objects sometimes
	crashes R
In-Reply-To: <F9F2A641C593D7408925574C05A7BE7703257F13@rhopost.rhotrading.com>
References: <F9F2A641C593D7408925574C05A7BE7703257F13@rhopost.rhotrading.com>
Message-ID: <e8e755250906301226g71d5950bkda118ace7bcf03a2@mail.gmail.com>

Hi David,

Assuming this is an xts object, it is going to be somewhere in the xts
C code.  That said, I am trying to crash it myself, and can't seem to
do it.

Can you create a sample data set to test this against.  Ideally one
that has the erratic behavior you are seeing.

> x <- .xts(rnorm(1e5), 1:1e5)
> x[sample(1e5,10000)] <- NA
> str(which(is.na(x)))
 int [1:10000] 5 10 16 23 25 27 28 30 33 39 ...

> for(i in 1:10000) na.locf(x)
> system.time(for(i in 1:10000) na.locf(x))
   user  system elapsed
 28.335  13.410  41.805

Can you send me:

str(unclass(x))
and your sessionInfo()

Thanks
Jeff

On Tue, Jun 30, 2009 at 2:07 PM, <davidr at rhotrading.com> wrote:
> I have been having random crashes of R from applying na.locf to xts objects.
>
> Going back after a crash and trying the same line again usually works; then
>
> a later, similar line will crash R again.
>
> Has anyone seen this behavior?
>
>
>
> It is sort of hard to send a minimal example because of the flakiness.
>
>
>
> Sometimes the error Comes up in a Windows message box for MSV C++ Runtime
> Library, saying
>
> ?Runtime Error!
>
> Program: ?\Rgui.exe
>
> This application has requested the Runtime to terminate it in an unusual
> way.
>
> Please contact the application?s support team for more information.?
>
> Then R hangs and has to be killed.
>
> Other times, the error come up in window for Visual Studio Just-In-Time
> Debugger,
>
> saying that an unhandled exception occurred in Rgui.exe [6200]. ?(or [5468]
> or [7220] or ?)
>
> Going to Visual Studio yields Unhandled exception at
>
> 0x6c80b097 [or 0x6c72b532 or ?]in Rgui.exe: 0xC0000005: Access violation
> writing location 0x80000000 [or 0x00000043 or ?].
>
>
>
> I can try to supply some more info if necessary, but as I said, the same xts
> object will work the next time.
>
> Thanks,
>
> -- David
>
>
>
>> sessionInfo()
>
> R version 2.9.0 (2009-04-17)
>
> i386-pc-mingw32
>
>
>
> locale:
>
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> States.1252;LC_MONETARY=English_United
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>
>
>
> attached base packages:
>
> [1] stats???? graphics? grDevices utils???? datasets? methods?? base
>
>
>
> other attached packages:
>
> [1] xts_0.6-5 zoo_1.5-6
>
>
>
> loaded via a namespace (and not attached):
>
> [1] grid_2.9.0????? lattice_0.17-25
>
>
>
> This e-mail and any materials attached hereto, including, without
> limitation, all content hereof and thereof (collectively, "Rho Content") are
> confidential and proprietary to Rho Trading Securities, LLC ("Rho") and/or
> its affiliates, and are protected by intellectual property laws.? Without
> the prior written consent of Rho, the Rho Content may not (i) be disclosed
> to any third party or (ii) be reproduced or otherwise used by anyone other
> than current employees of Rho or its affiliates, on behalf of Rho or its
> affiliates.
>
> THE RHO CONTENT IS PROVIDED AS IS, WITHOUT REPRESENTATIONS OR WARRANTIES OF
> ANY KIND.? TO THE MAXIMUM EXTENT PERMISSIBLE UNDER APPLICABLE LAW, RHO
> HEREBY DISCLAIMS ANY AND ALL WARRANTIES, EXPRESS AND IMPLIED, RELATING TO
> THE RHO CONTENT, AND NEITHER RHO NOR ANY OF ITS AFFILIATES SHALL IN ANY
> EVENT BE LIABLE FOR ANY DAMAGES OF ANY NATURE WHATSOEVER, INCLUDING, BUT NOT
> LIMITED TO, DIRECT, INDIRECT, CONSEQUENTIAL, SPECIAL AND PUNITIVE DAMAGES,
> LOSS OF PROFITS AND TRADING LOSSES, RESULTING FROM ANY PERSON?S USE OR
> RELIANCE UPON, OR INABILITY TO USE, ANY RHO CONTENT, EVEN IF RHO IS ADVISED
> OF THE POSSIBILITY OF SUCH DAMAGES OR IF SUCH DAMAGES WERE FORESEEABLE.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


